최근 쏟아지는 정보의 양만큼 사용자들은 단순한 자료의 수준을 넘어 다양하고 매우 광범위한 정보를 요구하고 있다. 사용자마다 원하는 데이터의 유형과 처리된 결과가 상이하기 때문에 모든 사용자를 충족시킬 수 있는 질의처리 시스템이 필요한 것이다. 이에 따라 본 논문에서는 데이터베이스에 대한 전문적인 지식이 없는 일반인이 필요한 정보를 추출할 수 있도록 하기 위해 자연어를 이용한 데이터베이스를 검색하는 기술을 제안한다.
기존의 자연어 질의처리 연구는 구문분석에 따른 패턴분석이 주를 이루었다. 기존 연구는 질의문의 형식이 복잡하고 정해진 규칙에 따른 정형성이 부족하다면 사용자가 원하는 결과를 기대하기 어려웠다. 이를 극복하기 위하여 자연어 저장소라는 새로운 개념을 도입하여 자연어 검색을 위해 검색 예측, 연관 검색, 시각화 기법을 제안한다. 세 가지 방법에 대한 구체적인 연구는 다음과 같다.
첫째, 사용자가 입력한 질의문과 부가적인 정보를 분석하여 사용자가 이후 진행할 검색을 미리 발견하는 검색 예측을 제안한다. 검색 예측에 사용되는 부가적인 정보는 질의문을 입력하는 사용자, 질의문이 반복적으로 입력되는 주기, 자연어 속에 사용되는 시간과 문구 등이다. 검색 예측은 이러한 부가적인 정보를 통해 질의를 예측하는 과정을 의미한다.
둘째, 사용자가 검색을 위해 질의에 사용한 단어간 연관관계를 설정하고 사용자에게 관련된 질의문을 미리 제시하는 연관 검색 기법을 제안한다. 연관 검색은 일정한 규칙을 바탕으로 검색어 조합을 정의하는 것을 의미하는데, 사용자가 검색을 요청한 시간과 질의 이력을 활용하여 설정된 연관관계를 데이터베이스에 저장하는 것이다. 이를 위해 반복적으로 입력되는 질의어를 분석하여 일정한 검색어 조합을 생성하는 방법을 제안한다.
셋째, 데이터베이스 스키마정보를 이용하여 사용자가 단계적 선택을 통해 질의문을 생성할 수 있도록 하는 시각화를 제안한다. 시각화에서 포함되는 스키마정보는 시스템레벨을 비롯하여, 파일, 테이블, 필드, 인덱스, 기본 키, 뷰, 정의된 함수의 등이다. 시각화에서는 이들 정보를 자연어 형태로 표현하고 사용자는 단계적으로 해당 정보를 선택한 후 선택된 정보를 바탕으로 자동 질의문이 생성되는 것이다.
자연어 저장소의 세 가지 기법을 검증하기 위하여 최근 가장 많이 활용되고 있는 프로그램 형태인 웹을 기반으로 보편적이고 기본적인 환경에서 검증을 진행 하였다. 자연어 저장소를 이용하여 생성된 질의를 통해 실험 환경에서 저비용으로 처리가 되는 결과를 확인 하였다. 자연어 저장소를 이용한 검색은 의사결정을 필요로 하는 여러 기관과 단체에서 활용될 것이다. 자연어 저장소를 통해서 사용자는 다양한 자연어를 보다 유연하게 검색에 사용할 수 있다. 또한 자연어 저장소는 사용자로 하여금 제안된 자연어로 검색을 진행하게 하므로 시스템성능을 향상시킨다.제1장 서론 1 1.1 연구배경 1 1.2 자연어 질의처리의 문제점 2 1.2.1 자연어 질의응답 시스템 2 1.2.2 자연어 질의처리의 문제점 2

사람은 기억능력의 한계를 극복하고 자료를 보존하기 위해 오래전부터 다양한 기록 매체를 사용하여 기록을 하고 있다. 그 기록 대상 중에는 생활 속에서 매 순간 발생하는 메모에 대한 정보들이 존재한다. 이 메모들의 효율적인 정리와 조회를 위해서 사람들은 메모를 할 때 정보의 속성을 고려하여 카테고리를 분류하고 속성을 개별적으로 추출하여 기록한다. 이러한 분류 작업들은 기록자체를 번거롭게 만든다. 따라서 메모의 효율적인 기록과 조회를 위해서는 자연어 메모를 자동으로 분류하고 속성을 추출하는 방안에 대한 연구가 필요하다.
본 연구를 위해 자연어 메모의 자동 분류와 메모를 구성하는 항목 값을 추출하는 알고리즘의 제시, 범용 서비스를 위한 시스템의 구축, 정확한 메모 분석을 위한 메모 분석 사전의 구축과 시스템의 성능평가가 필수적이다.
본 논문에서는 자동으로 자연어 메모 문장의 주제를 분류하고 메모를 구성하는 항목의 값을 추출하는 방법을 제시하고 웹기반의 자연어 메모 분석 시스템 구현을 통해 알고리즘을 검증하였다.
다음으로 스마트폰 어플리케이션 구현을 위하여 범용 메모 서비스 아키텍처를 설계하고 구현하였다. 최근 이동통신 기술 및 모바일 단말기 내의 H/W 및 S/W기술의 성장은 더 나은 정보화 환경을 제공하고 있다. 스마트폰의 급속한 보급 증가로 사용자들은 시간과 장소의 제약 없이 정보화 시스템에 접근이 가능해졌으며 모바일 단말기의 융합기술로 인해 다양한 서비스를 이용할 수 있게 하게 되었다. 따라서 자연어 메모 분석 서비스의 모바일 서비스로의 적용은 연구 활용을 위해 필수적이다.
응용시스템 구축은 세부적으로 범용 메모 분석 서비스를 위한 아키텍처 설계, 데이터베이스 모델링, 자료교환 인터페이스 설계, 클라이언트 서비스와 서버 서비스 구현의 단계를 제시하였다. 또한 모바일 서비스로 안드로이드 기반 스마트폰용 어플리케이션을 구현하면서 다양한 외부 서비스를 연계하였다. 서버 메모 분석 서비스는 특정 클라이언트에 제한되지 않고 다양한 클라이언트가 이용 가능하도록 범용 메모 서비스 아키텍처를 구성하는 방법을 제시하였다. 또한 구축한 범용 메모 서비스를 이용하는 클라이언트의 다른 예로 음성대화 메모 입력 어플리케이션을 구현하였다. 이 음성대화 메모 입력 시스템은 음성 인식 모듈과 범용 메모 분석 시스템을 연계하여 다양한 응용 서비스로 활용할 수 있는 사례를 제시하였다.
메모 분석 사전 구축을 위해서 사용자가 입력한 메모와 스마트폰 SMS와의 연계를 통하여 입력된 2가지 유형으로의 자료들을 수집하여, 수집된 메모를 메모 주제별, 추출 형태소 품사별 등으로 분석하였다. 주제 분석 키워드의 가중치 부여방법으로 TF가중치와 IF-IDF가중치의 2가지 방법을 이용하여 분석 사전을 구축하고 각 방법별로 시스템의 주제 분류 적합성을 측정하였다. 전체 시험결과로는 TF가중치를 사용하였을 때 주제 분류 평균 재현율과 정확률은 67.96%과 68.13%, TF-IDF가중치를 적용하였을 때의 재현율과 정확률은 모두 75.53%의 값을 보이며, TF-IDF가중치를 이용한 분석이 TF가중치를 이용한 분석보다 재현율은 7.57%, 정확률은 7.4%의 높은 성능을 보인다.
본 논문을 통해 아직까지 메모 정보 분야에서 도입하지 않고 있던 자연어 메모의 자동 분류 및 항목 값의 추출에 대한 가능성을 찾을 수 있었다. 또한 제안한 알고리즘을 반영하여 실제로 구축한 범용 메모 분석 시스템을 이용하여 다양한 클라이언트가 메모 분석 서비스를 이용할 수 있는 기반을 마련하였다.I. 서론 1 1.1 연구의 필요성 및 목적 1 1.2 용어의 정의 4 1.3 연구의 방법 및 범위 6 II. 이론적 배경 7

Behavior problems of teenagers and children create side effects not only in the family but also in the community and society as a whole; and while the total number of school-age children is decreasing, the number of children with behavior problems is increasing. To support these maladjusted children, new ways of identifying behavior problems and reducing the number of their occurrences is required. In this research, natural language processing technology from the information technology (IT) field was employed to reduce instances of mediating problematic behaviors. To automatically analyze behavior problem patterns using natural language processing technology, behavior problem area analysis methods were developed, and the efficacy of the pattern analysis methods were evaluated.
For the 86,911 sentences that indicated behavior problem tendencies and 192,021 occurrences that were recorded from 2001 to 2012 in the behavior problem recording system, Homi.Info was used for the behavior problem pattern analysis by applying natural language processing technology. The following types of behavior problems were categorized: aggression, depression–anxiety, self-mutilation, antisocial behavior, stereotyped behavior, sexually abnormal behavior, physical symptoms, abnormal feeding behavior, abnormal excretion behavior, abnormal sleep behavior, and problems with concentration.
Results showed the behavior problem sentences consisted of 4.3 words and 9.3 morphemes. Regarding the top ten sentences with the highest repetition rates by type, antisocial behavior showed 14.3%, while abnormal excretion behavior showed the highest rate at 55.4%. The top ten behaviors with rates over 50% were stereotyped behavior, physical symptoms, and abnormal excretion behaviors. In order to resolve the problematic behaviors, an intervention effect can be achieved first by mediating selected problematic behaviors. Types with rates over 40% were aggression, depression–anxiety, self-mutilation, and problems with concentration and attention. On the other hand, antisocial behavior appeared at 14.3%, which indicates its need for detailed classification. Moreover, in terms of behavior support, various types of support—depending on the situation—are required instead of support based on selected methods.
Characteristics of each behavior problem type were analyzed to establish an electronic dictionary of behavior problem that was required for the natural language processing technology. As a result of developing a behavior problem type analysis method that used six natural language processing technologies by using the electronic dictionary, “analysis method 6” displayed an 84% match with experts’ classification results. Analysis method 6 decreased the time-demand by 9.9% compared with the existing analysis conducted manually. After experiencing the analysis system developed in this study, all educators who participated in the survey commented that if an online service for mediating behavior problems was developed, they would use it.아동 및 청소년의 문제행동은 본인뿐만 아니라 가족, 지역사회에 이르기까지 많은 사회적 부작용을 만들어 내고 있으며, 학령기 전체 아동수는 줄어드는 반면 부적응 문제를 보이는 아동은 지속적 증가하고 있다. 이러한 부적응 아동 지원을 위해, 새로운 문제행동 지원 방안을 수립하는 방법과 기존 문제행동 지원 방안의 업무를 대폭 줄이도록 하는 방법 등이 모색 되어야 한다. 본 연구에서는 기존 문제행동 지원방안의 업무를 줄일 수 있도록 IT 기술 분야인 자연어 처리 기술을 사용한다. 자연어 처리 기술을 활용해 문제행동 유형을 자동으로 분석하기 위해 문제행동 영역 분석 방법을 개발하고, 개발된 유형분석 방법의 효용성을 검증하였다.
자연어 처리 기술을 활용한 문제행동 유형 분석을 위해 문제행동 기록 시스템인 Homi.Info에 기록된 2001년부터 2012년까지의 86,911개 문제행동 문장과 192,021번의 발생기록을 사용하였다. 문제행동 유형은 12개 분류로 공격 행동, 우울·불안 행동, 자해 행동, 반사회적 행동, 상동 행동, 성적 이상행동, 신체증상, 섭식 이상행동, 배설 이상행동, 수면 이상행동, 주의집중문제, 기타영역을 사용하였다.
연구결과 문제행동 문장은 4.3개의 단어와 9.3개의 형태소로 구성되어 있었으며, 영역별 발생빈도가 높은 10개 문장의 비율을 보면 반사회적 행동은 14.3%이지만, 배설 행동은 55.4%로 가장 높은 비율을 보였다. 상위 10개 행동이 50%를 넘는 영역은 상동 행동, 신체증상, 배설 이상 행동으로, 문제행동을 지원하기 위해 몇 개의 특정적 문제행동을 우선 지원하여 중재 효과를 나타낼 수 있다. 40% 이상의 비율을 보인 영역은 공격 행동, 우울·불안 행동, 자해 행동, 주의집중문제 영역이다. 반면 반사회적 행동은 14.3%로 나타나 상세 분류가 필요한 영역임을 알 수 있다. 또한, 행동지원에서는 특정 방법 중심의 지원보다는 상황에 따른 다양한 지원이 필요한 영역이다.
문제행동 영역별 특성을 분석하여 자연어 처리기술에 필요한 문제행동 전자사전을 구축하였다. 구축된 전자사전을 활용하여 6개의 자연어 처리 기술을 활용한 문제행동 유형분석 방법을 개발한 결과 분석방법 6이 전문가 분류결과와 84% 일치하는 결과를 나타냈다. 분석방법6은 기존에 수기로 진행하던 행동분석에 비해 9.9%의 시간절감 효과가 있었으며, 분석시스템을 사용한 후 설문에 참여한 모든 교사가 문제행동을 지원하는 온라인 서비스가 개발되면 사용하겠다는 의견을 밝혔다.Ⅰ. 서 론 1 1. 연구의 의의 1 2. 연구의 목적 5 3. 용어의 정의 5

자연어란 프로그래밍 언어와 같이 어법이 정해져 있어 규칙만을 따르지 않고 정돈된 완벽한 문법이나 형식적인 의미가 없는 인간의 언어를 의미한다. 자연어 이해는 불규칙적인 자연어 표현에 대하여 그 의미를 기계가 이해 할 수 있도록 하는 것을 말한다. 자연어 이해의 활용 분야로 로봇 비서, 채팅 봇 등이 있으며, 가까운 예로 애플의 Siri, 삼성의 빅스비가 있다. 자연어 이해 기술은 딥 러닝(Deep Learning) 기술의 발전으로 전통적인 통계나 규칙 기반의 자연어 이해 기법의 한계를 벗어나 기존보다 대용량의 데이터에 대하여 빠르고 다양한 문맥 정보 처리가 가능하게 되었다. 본 논문은 자연어 이해를 위하여 문장 내 주요 키워드를 추출 할 수 있는 개체명 인식 기술, 문장의 의도를 분석 할 수 있는 의도 분석 기술에 대하여 기술하고 딥 러닝 기술을 기반으로 하는 합성 곱 신경망(Convolutional Neural Network)과 Bi-LSTM(Bidirectional Long Short-Term Momory) 모델을 통한 개체명 인식 및 의도 분석 기술을 제안한다.Natural language means a human language without a formal grammar or formal meaning, which is defined as a programming language and does not follow rules. Natural language understanding refers to make the machine understand the meaning of irregular natural language expressions. There are robot secretary, chat bots, and so on. There are Siri of Apple and Bixby of Samsung. Natural language understanding technology has developed beyond the limits of traditional statistics and rule-based natural language understanding techniques by the development of deep learning technology, and it has become possible to process the various context information of a large amount of data faster than before. In this paper, I describe named entity recognition technology that can extract keyword in sentences for natural language understanding. Based on the deep learning technology, I propose an named entity recognition and intention analysis models using Character Convolutional Neural Network and Bi-LSTM model제 1 장. 서 론 1 제 1 절. 연구 배경 1 제 2 절. 연구 동향 3 1. 자연어 이해(Natural Language Understanding) 3 2. 개체명 인식(Named Entity Recognition) 3

최근 신문기사 데이터베이스는 색인 및 검색에 있어서 자연어나 통제어의 한가지 유형을 고수하기 보다는 검색의 효율성과 유지의 양면을 고려하여 통제어와 자연어를 혼용하는 병용 시스템으로 발전하고 있는 추세이다.
본 연구에서는 현재 각 신문사마다 어려운 당면과제로 남아 있는 통제어의 사용 여부에 대한 검색 효율성상의 근거를 제시하기 위하여 국내 각 신문사에서 활발히 이용되는 한국언론연구원의 종합 신문기사 데이터베이스 KINDS를 대상으로 자연어와 동의어 검색의 적중률과 검색 효율성을 비교 평가하였다.
연구의 결과는 다음과 같다.
첫째, 자연어를 검색에 사용하는 경우는 동등관계의 용어들중 어떤 용어를 선택하느냐에 따라 적중률과 검색 효율성이 매우 가변적인 것으로 나타났다.
즉 자연어 검색시에는 해당 분야의 주제와 용어에 대한 배경지식을 바탕으로 적중빈도가 높은 용어를 검색어로 사용해야 하며 그렇지 않을 경우 수작업 색인이나 자동색인 모두 효율적인 검색을 기대할 수 없었다.
둘째, 동의어 검색은 자연어와 비교해 재현율이 매우 높았으나 반면 정확률은 약간 감소하여 동의어가 재현장치임을 입증하고 있다
셋째, 서울신문과 KINDS전체를 비교하였을때 적중빈도면에서 살펴본 신문용어의 사용 경향은 거의 유사했으며, 자동색인 기법을 사용한 경우 적중률은 재현율과 매우 유사한 수치를 나타냈다.
신문기사 데이터베이스는 재현율을 중시하는 사실 데이터베이스이며, 또한 전문 데이터베이스로서 보다 효율적인 검색을 수행하기 위해서는 자연어 시스템에 통제어를 병용하는 것이 바람직하다.
동의어 사전은 색인 단계에서 전거파일로 사용되거나 혹은 탐색용 시소러스로 역할할 수 있을 것이다.The recent Newspaper Articles Databases have been using a hybrid system of free keywords and controlled keywords simultaneously in consideration of the retrieval efficiency and systems maintenance, rather than using a specified types of free or controlled keywords.
The purpose of this study is to compare and analyze the hit rate and retrieval efficiency of free keywords and controlled keywords in KINDS(Korean Integrated Newspapers Database System) of the Korean Press Institute, which is commonly used by Korean Newspapers. The matter of vocabulary control for newspaper articles is a very difficult problem for most newspapers in Korea and there has been no study done on this subject.
The results of this study are as follows.
First, in case of using free keywords, the hit rate and retrieval efficiency proved to be variable, depending on selection of terms that have equal relations. That is, the searcher must have background knowledge of the keywords with highest occurrence in order to accomplish a free keyword search. Otherwise, retrieval efficiency is so poor in both the human indexing and automatic indexing method.
Second, in the instance of using controlled keywords of synonyms, the recall ratio is very high, but the precision ratio is a little lower. Therefore, controlled vocabularies prove to be important as a recall device.
Third, when Seoul Daily Newspaper articles in KINDS were compared with all articles in Kinds, the frequencies of hit occurrence of terms were discovered to be very similar. And in the case of automatic indexing, the hit rate was almost the same.
Newspaper databases are fact and full-text databases in which the recall ratio is significant. Use of free keyword and controlled keyword systems simultaneously is the most desirable method to achieve retrieval efficiency.
Synonyms can be used as an authority file or as a thesaurus for searching aids.목차 도표목차 = iii 국문초록 = v I. 서론 = 1 1. 연구의 목적 = 1

모바일 서비스는 이동통신망의 고속화와 사용자의 증가로 급속히 성장하였고, 스마트폰 사용자들 역시 스마트폰에서의 일반 웹서비스 사용을 요구하게 되었다. STOWAR 서비스는 모바일 웹으로 지역적인 상점정보와 이용자들의 리뷰, 사진 같은 정보들을 공유할 수 있는 모바일 웹 서비스이다. 또한 상점 정보는 상점주로부터 직접 상점 정보를 입력받아 그 상점의 모바일 페이지를 만들어준다.

STOWAR 서비스는 UCM (User Created Market) 이라는 개념으로 상점주로부터 상점 정보를 입력받도록 하는 것은 상점주가 입력해야 하는 정보가 많다는 것에 상점주가 직접 모든 정보를 입력하는 것에는 한계가 있다. 또한 이러한 한계로 STOWAR 서비스 내의 상점의 수가 늘어나지 못하고 있다. 그리고 컨텐츠를 이용해 서비스하는 다른 서비스에 비해 검색 서비스의 경우 기초적인 키워드 검색만을 지원하고 있다.

본 논문에서는 STOWAR 서비스의 데이터베이스를 웹 크롤링을 통해 데이터베이스를 확장하고, 그 데이터를 이용해 STOWAR 서비스에 특화된 자연어 검색 시스템에 필요한 자연어 사전을 구축하고 그것을 이용해 자연어 검색 시스템을 구축하여 위에서 언급한 문제점을 해결할 수 있는 방법을 연구하였다.Now, people in growing numbers are using mobile services, but at same time most of smart phone users are generally require use of Web services. STOWAR is a mobile Web service which can share store’s location, users’ reviews and photos. Also manager can directly builds store’s information which will appear on store’s mobile web page.

For STOWAR service based on concept of user created market, store information should be directly entered by the manager. But it difficult for the storekeeper to enter a lot of information. In addition, because of these limitations, stores has not increased and activated in STOWAR service. And comparing with other search service, STOWAR are only support basic keyword matching search.

In this paper, from what have been discussed above, we going to solve this problem by enlarging STOWAR's database through web crawler, by using that data to build specialized dictionary and by establishing natural language search system on
STOWAR service.제 1 장 서론 1 1.1 연구 배경 1 1.2 연구 목적 및 구성 2 제 2 장 STOWAR 서비스 및 문제점 3

목적지향 대화 시스템(Goal-oriented Dialogue System)은 특허 상담, 항공 예약, 쇼핑 등 특정 도메인에서 사용자의 목적 달성을 도와주는 시스템을 말하며, 최근 목적지향 대화시스템에 대한 관심이 높아짐에 따라 관련 연구가 활발히 이루어지고 있다. 이러한 목적지향 대화 시스템은 일반적으로 음성 인식(Speech to Text), 자연어 이해(Natural Language Understanding), 대화 관리(Dialogue Management), 자연어 생성(Natural Language Generation), 음성 합성(Text to Speech) 등의 다양한 모듈들로 구성되고 서로 유기적으로 작동하여 사용자의 목적 달성을 돕는다. 자연어 이해 모듈은 자연어 텍스트로 표현된 사용자의 발화를 이해하는 모듈이며, 일반적으로 화행 분석과 슬롯 필링으로 구성된다. 대화 관리 모듈은 전체 대화의 흐름을 관리하고 시스템의 다음 행동을 결정하는 모듈이며, 대화 상태 추적기와 대화 정책으로 구성된다. 마지막으로 자연어 생성 모듈은 시스템의 다음 행동을 자연어 텍스트로 생성하는 모듈이다.
본 논문은 앞에서 설명한 목적지향 대화 시스템에서 자연어 이해 모듈과 대화 정책에 대한 연구를 진행하고 간단한 템플릿(Template) 기반의 자연어 생성 모듈을 추가한 프로토타입 특허 상담 목적지향 대화시스템을 구축하고자 한다. 본 논문에서는 먼저, 최근 자연어 처리의 다양한 분야에서 좋은 성능을 보이고 있는 Joint Learning과 Attention Mechanism을 이용한 자연어 이해 모듈을 제안한다. 또한 기존에 구축되어 있는 Human to Human 대화 말뭉치를 이용하여 Machine to Human 대화 말뭉치를 생성하는 방법을 설명하고 Machine to Human 대화 말뭉치를 이용하여 자연어 이해 모듈의 결과를 자질로 사용하는 대화 정책을 제안한다. 그리고 비교적 간단한 템플릿 기반의 자연어 생성 모듈을 구축하는 방법을 설명하고 마지막으로 프로토타입 목적지향 대화 시스템에 대해 설명한다.
결과적으로 Attention Mechanism 기반의 Joint Learning 자연어 이해 모듈은 baseline 대비 화행 분류와 슬롯 필링 성능이 각각 3.35%p, 0.54%p 향상되어 85.41%, 80.94%의 성능을 보였고, 자연어 이해 모듈의 자질을 사용한 대화 정책은 baseline 대비 성능이 4.14%p 향상되어 75.21%의 성능을 보였다. 그리고 최종적으로 자연어 이해 모듈, 대화 정책과 자연어 생성 모듈을 이용하여 프로토타입 특허 상담 목적지향 대화 시스템을 구축하였다.Ⅰ. 서론 1 Ⅱ. 관련 연구 5 1. 화행 분석과 슬롯 필링에 관한 연구 5 2. 대화 정책에 관한 연구 6

With the wide spread of Web services in various fields, there is a growing interest in building a composite Web service, However, it is very difficult for ordinary users to specify how to compose services. Therefore, a convenient interface for generating and invoking composite Web services are required. This paper proposes a natural language interface to invoke services. The proposed interface provides a way to describe users’ requests for composite Web Services in a natural language. A user with no technical knowledge about Web services can describe requests for composite Web services through the proposed interface. The proposed method extracts a complex workflow and finds appropriate Web services from the requests. Experimental results show that the proposed method extracts a sophisticated workflow from complex sentences with many phrases and control constructs.복합 웹 서비스를 구성하는 방법에 대한 연구가 활발히 진행되고 있다. 그러나 유비쿼터스 환경에서 일반 사용자들이 복합 웹 서비스를 동적으로 접근하기 위한 환경은 마련되지 않고 있다. 자연어 기반의 질의로부터 복합 웹 서비스를 생성하는 방법은 복합 웹 서비스를 이용하기 위해 복잡한 워크플로우를 사용자가 직접 설계할 필요 없이 단순히 사용자가 원하는 서비스에 대한 요청을 자연어로 기술하면 이를 바탕으로 서비스를 호출할 수 있게 한다. 이러한 자연어 기반의 웹 서비스 호출을 위한 기존 연구들은 간단한 제어 구문에 대해서만 처리 하거나, 하나의 문장 블록에 대해 단일 서비스만을 대응한다. 따라서 본 논문에서는 사용자의 자연어 질의로부터 정교한 수준의 추상 워크플로우를 생성하는 방법을 제안한다. 복잡한 제어 구문들을 처리하기 위해 제안된 방법은 간단한 제어 구문의 패턴을 반복적으로 적용하여 복잡한 워크플로우를 추출한다. 그리고 각각의 문장 블록에 포함된 명사와 동사를 이용하여 행위 개념과 대상 개념을 찾는다. 행위와 대상 개념은 문장 블록에서 요구하는 서비스의 기능과 대상을 표현하며, 문장 블록에서 요구하는 서비스를 찾기 위해 사용한다. 찾은 서비스들은 추출한 워크플로우의 태스크에 대응되어 최종 추상 워크플로우를 완성한다. 제안된 방법의 성능을 평가하기 위해서 127개 자연어 입력 문장을 대상으로 실험한 결과 제안된 방법은 95.2%의 실험 데이터에 대하여 추상 워크플로우를 정확히 추출하였다.

서론
EMR의 도래로 환자 데이터가 풍부하게 축정되고 있지만, 자연어 의무기록지의 경우 다량의 데이터를 파악하는데 시간이 많이 소요되어 진료에 충분히 활용되지 못하고 있다. 본 연구에서는 자연어 의무기록지의 활용도를 높이기 위해 시간 정보를 중심으로 타임라인에 의료데이터를 시각화할 수 있는 V-Model을 제안하였다.
방법
V-Model은 ‘v’자 형태의 구조로 의료사건을 시각화하는 모델이다. 설계시 표현적, 추론적, 시각적 측면을 고려하였다. V-Model은 기존의 point, interval 중심의 표현에서 해결하지 못했던 자연어 처리 문제인 causality, non-explicit temporal information, and granularity issues 을 다룰 수 있는 틀을 제공하면서도, 시간 정보 유추가 가능하며, 직관적인 타임라인을 제공한다. V-Model은 전통적인 타임라인과의 비교를 통해 평가하였다. 총 3단계로 구성된 실험에서, V-Model이 제안한 방법에 대한 검증을 하였고, 사용성에 대한 설문 조사, 피드백을 받기 위한 인터뷰를 진행하였다. 실험에는 40명의 전공의와 40명의 의대생이 참여하였다.
결과
V-Model은 자연어 처리 문제를 표현하고 전달하는 데 기존 타임라인보다 탁월하며, 시간 추론을 위한 정보를 충분히 제공하고, 가독성이 더 뛰어남이 검증되었다. 사용성과 관련해서도 긍정적인 답변을 받았으며, 환자의 병력을 파악하는데 효과적이라는 평가를 받았다.
결론
V-Model은 실험을 통해 자연어의 문제점을 표현하고 전달할 수 있고, 정량적, 정성적 시간관계를 추론할 수 있게 해주며, 시각적으로 가독성이 좋다는 점이 검증되었다. 또한, 사용성 평가 테스트에서 긍정적인 평가를 받아 향후 진료 및 연구에서의 활용성이 기대된다.Objective
Visualizing narrative medical events into a timeline can have positive effects on clinical environments. However, Current timeline systems, which are based on a point/interval representation, have difficulties in representing narrative clinical events. The purpose of this paper is to propose an innovative time model for visualizing narrative patient history and evaluate our solutions.
Materials and Methods
The V-Model models clinical situation effectively based on v-like graphical structure. The model visualizes patient history on a timeline in a simple and intuitive way. For the design, the representation, reasoning, and visualization (readability) aspects were considered. Furthermore, the unique graphical notation helps to find hidden patterns of a specific patient group. It provides solutions for the unresolved natural language problems; causality, non-explicit temporal information, and granularity issues. For evaluation, we verified our distinctive solutions, surveyed usability, and interviewed the subjects for qualitative comments. The experiments were carried out for the V-Model group and the conventional timeline model group. Overall fourty medical students and fourty physicians participated in this evaluation.
Results
The V-Model was proven to be superior in representing narrative medical events, provide sufficient information for temporal reasoning, and outperform in readability compared to a conventional timeline model. The usability of the V-Model was assessed positively. And prominent number of the participants commented that the model is very effective for understanding a patient’s history.
Conclusion
With a novel graphical concept time frame, the V-Model innovatively resolves visualization issues of clinical documents. The V-Model successfully solves the modeling requirements and has better usability compared to conventional timeline models. The work presented here has profound implications for future clinical environment.제 1 장 서론 1 1.1 문제제기 1 1.2 의료사건의 연대기적 표현의 중요성 2 1.3 관련 연구 3 1.4 전통적 타임라인 12

대화시스템에서 자연어 생성이란 대화관리자로부터 전달받은 의미표현을 사람이 이해할 수 있는 자연어로 생성하는 것이다. 기존의 규칙기반 자연어 생성 모델은 문법적 의미적으로 정확한 문장만을 생성하기 때문에 강건한 성능을 보장한다는 장점을 가지고 있지만, 새로운 의미표현에 대해 매번 규칙을 제작하며, 하나의 의미표현에 대해 한정적인 대답만을 생성한다는 문제점을 가지고 있다. 반면, 기존의 통계 기반 자연어 생성 모델은 하나의 의미표현에 대해 말뭉치에 없는 새로운 문장을 생성할 수 있지만, 문법적이나 의미적으로 불완전한 문장이 생성된다는 문제점을 가지고 있다.
본 논문에서는 기존의 방법론의 문제점을 극복하기 위하여 최근 대두되고 있는 심층학습 모델중 하나인 LSTM(Long Short Term Memory) 언어모델을 이용한 한국어 자연어 생성 모델을 제안한다. 또한 대화시스템의 시스템발화 생성 모델을 학습하기 위해 한국어 시스템 발화 말뭉치를 소개한다. 최적의 성능을 내는 자연어 생성 모델의 문장 학습단위를 파악하고자 어절, 형태소, 음절단위로 모델을 학습하였고, 그 중에서 형태소 단위 문장생성 모델이 BLEU-4와 ERR에서 가장 높은 성능을 나타내었다. 본 논문에서는 하나의 의미표현에 대하여 다양하고 문법적으로 정확한 문장을 생성하기 위하여 빔서치 디코딩(Beam-Search Decoding)을 적용하였다. 그 결과 기존의 그리디서치 디코딩(Greedy-Search Decoding)방법론에 비해 어절, 형태소, 음절단위 문장 생성 모델에 대하여 모두 높은 성능의 문장을 생성하는 것을 확인하였다

스마트폰 및 태블릿 등 디바이스가 다양화되고 무선 인터넷 망의 확대로 악성코드가 빠르게 증가하고 있다. 악성코드의 증가량 대부분은 원본 악성코드를 기반으로 재생성된 변종 악성코드이다. 이에 변종 악성코드를 적절하게 분류하여 대처하는 방식이 요구된다. 본 논문에서는 악성코드를 분류를 위하여 악성코드가 호출하는 API를 입력값으로 사용하고 자원 소모 절약 및 특성값 계산을 목적으로 자연어 처리 기법을 적용하였다. 자연어 처리 기법은 단어 전처리 기법인 Word2Vec를 사용하고 특성값 계산을 위한 신경망은 RNN의 한 기법인 LSTM을 사용한다. 또한 중요 API에 가중치를 주기 위하여 Attention Mechanism을 적용하였다. 악성코드 API 호출과 자연어 처리 기법 이용한 악성코드 분류는 전통적인 분류 기법 또는 다른 신경망 기법보다 나은 분류 정확률을 보여주며 상대적으로 낮은 오탐률을 지닌다.Malware is rapidly increasing because devices are diversified and wireless Internet network is expanded. Most of the increase in the malware is variant malware regenerated based on original malicious code. Therefore, a method of appropriately classifying and coping with variant malwares is required. In this paper, we applied natural language processing method to reduce resource consumption and to calculate hidden value by using API called by malware. Word2Vec, a word preprocessing technique, and LSTM, a technique of RNN, were used as a neural network to calculate hidden values. We also applied Attention Mechanism to weight critical APIs. The proposed malware classification method using malware API call and natural language processing techniques showed better classification accuracy than other classical or other neural networks and has the relatively low false positive rate.제 1장 서론 1 1.1 연구 배경 1 1.2 연구 목표 2 1.3 논문 구성 3 제 2장 관련 연구 4

As the volume of data increases dramatically, it becomes very difficult to find the information you want on the Web. One important reason for information search difficulties is that keyword-based search engines, which are traditional methods of information retrieval, focus only on resources that exactly match the keywords in the user query. In general, people want to find the resource itself that best matches the query, not the document containing the resource. One of the ways of satisfying this requirement is the ontology - based approach for semantic retrieval. Semantic-based retrieval takes into account search context, word variants, synonyms, concept matching, natural language queries, etc. and returns search results that understand the intent of the searcher and the contextual meaning of the query word. Therefore, ontology - based semantic - based retrieval can resolve ambiguity of word semantics, extend lexical concept, and search intention - based retrieval resources that match user query intent.
The processing method of natural language query on the general ontology knowledge is a method of deriving the answer to the query by converting the natural language query into the query graph. However, this method has a limitation that the computation problem becomes large in the process of converting the natural language query into the query graph, and it is impossible to answer the user query in real time. Therefore, in this study, we propose an automatic SPARQL generation method based on PVS pattern for natural language query processing. To accomplish this, PVS-SPARQL Pair Pattern KB is constructed by creating a PVS (Pseudo Verbal Sentence) Pair Pattern which expresses Terminological Graph in advance as SPARQL and Terminological Graph labels that can be inquired in advance, and PVS-SPARQL Pair Pattern KB's PVS It is aimed to compare the Match Strings composed of NLQ's (Entity Recognized) NERs' labels to find the corresponding SPARQL and to derive the result for NLQ so that the user can query the query in real time.
In this study, we have searched 438 1-triple subgraphs and 100 natural 2-triple subgraphs in the system implemented by PVS-SPARQL Pair Pattern Algorithm. As a result, both cases showed 100% correct answer rate. In addition, PVS-SPARQL Pair Pattern KB can be widely applied to search using natural language query to apply to ontology search in various fields.데이터 양이 급격히 증가하면서 웹에서 원하는 정보를 찾는 것이 매우 어려워졌다. 정보 검색에 어려움을 겪고 있는 한 가지 중요한 이유는 정보 검색의 전통적인 방법인 키워드 기반 검색 엔진이 사용자 쿼리의 키워드와 정확하게 일치하는 리소스에만 집중하기 때문이다. 일반적으로 사람들은 리소스가 포함된 문서가 아닌 쿼리와 가장 일치하는 리소스 자체를 찾고 싶어 한다. 이러한 요구 사항을 충족시킬 수 있는 방법 중 하나인 의미 검색을 위한 온톨로지 기반 접근 방식으로 본 연구를 진행한다. 의미 기반 검색은 검색 문맥, 단어의 변형, 동의어, 컨셉 매칭, 자연어 쿼리 등을 고려하기 때문에 검색자의 의도와 쿼리 단어의 문맥적인 의미를 이해한 검색 결과를 반환한다. 따라서, 온톨로지 기반 의미 기반 검색은 단어 의미의 모호성 해결을 해결할 수 있으며, 어휘 개념을 확장할 수 있고, 의도 기반 검색이 가능하기에, 사용자 쿼리의 의도와 일치하는 리소스를 검색 할 수 있다.
일반적인 온톨로지 지식에 대한 자연어 질의에 대한 처리 방법은 자연어 질의를 질의 그래프로 변환하여 질의에 대한 답을 도출하는 방식이다. 그러나 이러한 방법은, 자연어 질의를 질의그래프로 변환하는 과정에서 computation 문제가 커진다는 한계점을 가지고 있어 실시간으로 사용자 질의에 답변을 얻는 것이 불가능 하다. 따라서 본 연구에서는 자연어 질의 처리를 위한 PVS패턴 기반 SPARQL 자동 생성 방법론을 제안한다. 이를 위해, 사전에 Terminological Graph를 사전에 질의 가능한 SPARQL과 Terminological Graph의 label로 표현한 PVS(Pseudo Verbal Sentence) Pair Pattern을 생성하여, PVS-SPARQL Pair Pattern KB를 구축하고 PVS-SPARQL Pair Pattern KB의 PVS와 NLQ의 NER(Named Entity Recognition)된 Entity들의 label로 구성한 Match String을 비교하여 대응되는 SPARQL을 찾아 NLQ에 대한 결과를 도출하여 사용자의 질의에 실시간 답변이 가능하도록 하는데 목적이 있다.
본 연구에서는 PVS-SPARQL Pair Pattern Algorithm을 적용하여 구현한 시스템에 438개의 1-triple subgraph, 100개의 2-triple subgraph에 해당하는 자연어 질의를 검색하였다. 그 결과, 두 가지 경우 모두 100%의 정답률을 보였다. 또한, 다양한 분야의 온톨로지 검색에 적용할 수 있도록 PVS-SPARQL Pair Pattern KB를 구축하면 자연어 질의를 이용한 검색에 폭넓게 활용할 수 있다.

본 연구는 서술식 전자간호기록 시스템 사용자의 입력과 조회의 용이성과 기록된 자료의 활용 용이성을 향상시키기 위한 목적으로 자연어 생성기술을 이용하여 임상내용모델을 서술식 간호진술문으로 변환하는 간호진술문 생성시스템을 개발하고 평가하고자 한다.
본 연구는 간호진술문 생성의 지식 도출, 간호진술문 생성, 생성된 간호진술문의 평가의 단계로 진행하였으며 단계별 연구결과는 다음과 같다.
1) 간호진술문 생성의 의미론적 지식, 구문론적 지식, 맥락적 지식을 간호기록, 간호학 문헌, 그리고 전문가로부터 도출하였다.
의미론적 지식인 임상내용모델은 분만환자의 증상 및 징후, 환자문제, 분만간호에 대한 환자의 반응을 표현한 82개가 도출되었다. 간호진술문의 역할을 의미하는 진술문 유형은 간호과정별로 기록되어야 하는 자료의 특성에 따라 관찰 진술문, 측정 진술문, 호소 진술문, 판단 진술문 유형으로 구분하였다. 문장에 조합이 가능한 속성세트인 가능한 속성의 조합은 속성들간의 위계를 결합 규칙으로 표현하였다.
맥락적 지식인 정보제공자는 유일하거나 간호기록 작성의 주체인 간호사인 경우, 간호진술문에 정보제공자를 생략하였다. 시제는 현재시제로 작성하는 것을 원칙으로 하였다.
구문론적 지식인 개체와 속성들의 결합 순서인 어순은 간호진술문의 주어와 서술어의 문장 주 성분을 먼저 배열한 후 속성이 갖는 의미적 분류에 따라 21개의 그룹으로 분류하고 의미의 중의성을 고려하여 이들의 배열 순서를 정하였다. 조사는 속성들의 문장성분에 따라 12개를 추가하였다. 서술어는 호소, 측정, 관찰, 판단 진술문 유형별로 ‘있다고 함’, ‘측정됨’, ‘관찰됨’, ‘있음’을 배정하였다.
2) 간호진술문의 생성단계에서는 도출한 간호진술문 생성 지식을 간호진술문 생성단계에 맞춰 생성의 흐름을 정의하였다. 총 82개의 임상내용모델로부터 다양한 상세수준을 가진 66,888개의 간호진술문이 생성되었다.
3) 생성된 간호진술문의 질은 간호진술문 생성시스템으로부터 생성된 498개의 간호진술문을 총 5문항으로 이루어진 5점 척도의 측정도구를 이용하여 평가한 결과 간호진술문의 문항별 평균은 전반적인 질, 내용, 문법성, 문체, 정확성의 문항별로 4.66, 4.60, 4.40, 4.13, 4.60으로 나타났다. 생성된 간호진술문의 내용 포함력은 임상에서 작성된 2,136개의 간호진술문과 간호진술문 자연어 생성시스템으로부터 생성된 간호진술문과 매핑한 결과 간호진술문의 82.7%는 매핑이 되었다.

본 연구에서는 자연어 생성시스템을 이용하여 생성된 임상내용모델 기반 서술식 간호진술문은 구조화된 자료를 간호사에게 친숙한 서술식 간호기록의 형태로 제공할 수 있을 뿐 아니라 수집된 자료의 의미론적 상호운용성 확보로 인해 자료의 질 향상에 기여할 수 있을 것이다.

주요어: 전자간호기록, 자연어 생성, 데이터모델, 간호과정
학 번: 2010-30135목차 국 문 초 록 i Ⅰ. 서론 1 1. 연구의 필요성 1 2. 연구의 목적 3

Numerous data and data piled up over time, but there was no system to integrate or refine them, making it difficult for users to search for effective information. Accordingly, various search interface functions and characteristic implementation requirements for integrated information search are increasing.
This research was implemented to build an integrated search system using data from the Korea Transport Research Institute and to obtain search results using user dictionaries.
Because the development environment is the Java framework used by each module, you can run search collection containers individually on the Admin page, and you can change the date range by setting the index period on the Admin page.
In addition, by collecting external information that collects web crawler (index work) and relevant institution bulletin information retrieved by Korean keyword, webpage is provided so that users can view external information at a glance, thus improving work efficiency and search quality.
It is possible to quickly and accurately provide search results tailored to the user's needs by grasping the interest of users having a common purpose in an organization having a specialized purpose.
Because they share internal data, they need to collect and accept information from each relevant sector, and researchers must form a culture to share and perform the information and knowledge generated by the completed reports and research.
In this experimental system, the Mariner 4 solution related to the search was applied to link the traffic research site data to the external site information.
In addition to administrative data, research knowledge, cloud data, and bulletin board information of external related organizations are dragged and displayed. Therefore, there is a great advantage that integrated management of information enables integrated search of information desired by the user.1. 서론 1 1.1 연구배경 1 1.2 연구범위 및 방법 2 2. 관련 연구 4 3. 자연어처리 통합검색시스템 설계 7

본 논문은 생의학 정보 추출을 위해 자연어 처리 기술을 효과적으로 적용하는 방법을 제안한다. 생의학 정보 추출을 개체명 인식 단계와 관계 추출 단계로 구분하고 각 단계에 자연어 처리 결과를 활용한다. 개체명 인식 단계에서는 어휘 지식 통합 방법을 통해 오류 전파를 완화할 수 있고, 관계 추출 단계에서는 심층 자연어 분석을 통해 유용한 주변 문맥을 선별할 수 있다.
생의학 개체명 인식은 용어의 경계를 확인하는 용어 인식 단계와 용어의 의미 부류를 결정하는 의미 분류 단계로 구분할 수 있다. 개체명 인식의 첫 단계인 용어 인식 단계의 오류 전파를 완화하기 위해 어휘 지식 통합 방법을 제시한다. 먼저, 상이한 말뭉치 간 비교를 통해 도메인 의존적 단어들을 추출해, 용어 인식의 전처리에서 탐색 범위를 도메인 의존적 명사구로 제한한다. 다음으로, 학습 말뭉치로부터 단어의 형태 패턴을 추출해, 학습 과정에서 단어들의 자료 부족 문제를 완화한다. 마지막으로, 생의학 분야의 대량의 텍스트로부터 바이그램 연어를 추출해, 용어 인식의 후처리에서 생의학 용어의 좌우 경계에 위치한 단어가 인접한 경계 밖의 단어와 연어 관계인지 확인한다. 용어 인식 단계에서 도입한 어휘 지식이 생의학 개체명 인식의 성능을 향상시킴을 실험을 통해 보인다.
생의학 관계 추출에서는 주어진 개체명 인식 결과로부터 구성한 후보 개체쌍 중 상호 작용이 존재하는 개체쌍을 선별하기 위해, 자연어 분석 결과를 이용하는 방법을 제시한다. 개체명이 수식하는 동사가 상호 작용을 나타내면 해당 개체명이 개체 간 관계를 구성할 가능성이 높기 때문에, 구문 분석 및 술어-논항 인식 결과를 사용해 주변 문맥 중 개체명이 수식하는 동사를 확인한다. 심층 자연어 분석을 통해 주변 문맥을 선별해 이용하는 것이 유용함을 실험을 통해 보인다. 또한 자연어 처리의 순차적인 단계인 품사 부착, 기본구 인식, 구문 분석, 술어-논항 인식 결과로부터 추출한 정보들이 성능에 미치는 영향을 분석한다. 그리고 부분 구문 분석 기반의 방법과 완전 구문 분석 기반의 방법을 비교해 어느 정도의 자연어 처리 기술을 적용하는 것이 효과적인지 제시한다.제 1 장 서론 1 1.1 정보 추출 1 1.2 생의학 정보 추출 3 1.3 연구의 범위 및 논문의 구성 7 제 2 장 관련 연구 9

Recently, the amount of information is transmitted to users through internet with rapid growth of it. It is no exaggeration to say that supplying information of a better quality for users depends on catching user's query intention correctly. Even part of search engine that uses Natural Language Query cannot reflect correctly user's query intention.
Using query that does not reflect correct relation between keywords in information retrieval causes this problem. Therefore, the method proposed in this thesis generates relation of meaning between keywords as a result of morpheme analysis and syntactic analysis of Natural Language Query.
In this thesis, Query by Natural Language Query Context Structure implies relation of meaning between keywords. This Query is different from the other queries that are generated by keywords or their simple combination. Additionally query -weight is given automatically by information of each level through Natural Language Query Context Structure.
Keywords that are generated by Context-Structure-Generation part are translated into Boolean Query that can be processed by each search engine in Meta-Search part, and it is offered to input of each search engine.
Usually, the bigger the size of searched document is, the higher the term frequency of it is. But, the size of searched document is required to be normalized because even if term frequency appears high, user cannot decide whether it is a suitable document for him.
In this thesis, we used log-weight formula to normalize documents. Normalized documents are ranked according to the weight given by Context-Structure-Generation part.
Thus, the approach in this thesis will permit efficient information retrieval, because this approach solves the problem in the existent query form that cannot describe the relation between keywords.목차 ABSTRACT 1. 서론 = 1 2. 국내 메타검색과 자연어 질의 처리 = 4 2.1. 국내 메타검색 = 4

인터넷의 보급이 급속도로 확장되면서, 전 세계의 정보 구축망은 매우 활발한 움직임을 보이고 있다. 현대인은 그야말로 정보의 홍수 시대에 살고 있는 것이다. 이렇게 많은 정보들을 일일이 검토하여 찾고자 하는 정보를 얻어내는 번거로움을 덜어주는 것이 바로 '정보 검색 (Recherche d'information) 시스템' 이다. 정보 검색 시스템은 사용자가 원하는 정보에 대한 질의문을 단어 혹은 문장 형태로 입력을 하면, 컴퓨터가 대용량의 문서를 분석하여 관련된 답변을 찾아준다.
기존의 검색 시스템은 기본적으로 단어 형태 중심의 질의를 수용할 수 있도록 만들어 졌기 때문에 사용자는 검색을 하기에 앞서 자신의 질의에 대한 검색어를 단어 형태로 만드는 불편한 작업을 거쳐야 한다. 또한 자신의 질의와 관련해 적절하지 못한 단어 형태를 입력하면, 전혀 엉뚱한 결과를 얻어내기도 한다. 또한 단어 형태 키워드(Mots Clés) 중심의 작업은 또 다른 문제를 일으키기도 한다. 예를 들어, '축구' 라는 단어를 검색어로 입력했을 경우에 결과는 '축구'라는 단어가 포함되어 있는 문서들을 단어 포함 빈도 순으로 정렬해 주는 것에 그친다. 따라서 사용자는 자신이 원하는 정보를 찾기 위해 정렬되어 보여 지는 결과 자료를 통해 일일이 문서를 검토 해야 하는 번거로움도 있다. 또한 단어 중심 키워드(Mots Clés)로 검색한 자료들은 그다지 가치 있는 정보가 될 수 없다. 왜냐하면, '축구'관련 문서에서 나타나는 '축구'라는 단어는 매우 상위의 개념이며 포괄적인 개념이다. 이런 이유로 사용자는 두개의 단어를 검색어로 지정해야 할 때도 있다.
이렇게 단어 중심의 키워드(Mots Clés)로 작동하는 검색 시스템을 사용하는 것은 자연언어를 사용하는 우리에게 매우 불편함을 제공한다. 자연언어가 지닌 생성력은 무한하기 때문에, 사용자는 자신이 하는 질문에 대한 표현 방식과 상관없이 컴퓨터가 자신의 질의를 인식하고 자신이 원하는 답을 제공해 주기를 바란다. 이것이 바로 '자연어 질의응답 시스템(Système de questions-réponses en langage naturel)'의 개념이다.
본 연구는 '자연아 질의응답 시스템'이 자연어 형태로 된 사용자의 질의의 의도를 정확히 분석하여 사용자가 원하는 답을 줄 수 있도록 코퍼스 데이터 베이스를 질의 유형에 맞게 모델링 하는 것에 그 목적이 있다. 또한 코퍼스로는 '축구 관련 인터넷 신문기사-2006 FIFA 독일 월드컵'로 한정할 것인데, 텍스트 유형에 따라 특징적인 언어 구조를 보여주기 때문이다. 코퍼스는 FIFA 월드컵 공식 사이트(http://www.FIFAworldcup.com)와 AFP(http://www.afp.com)에서 추출한 2006년 7월 1일부터 7월 18일까지의 한국어(880KB)와 불어(920KB)로 된 모든 기사를 대상으로 하였다.
'자연어 질의응답 시스템'의 성능도는 시스템이 자연언어를 얼마나 잘 인식하는지, 즉 사용자가 한 같은 의도를 지닌 다양한 질의를 하나의 유형으로 처리할 수 있는지에 달려 있다. 따라서 본고에서는 질의어 분석을 중심으로 연구를 진행할 것이며, 분석된 질의어에 기반하여 데이터 베이스를 모델링 할 것이다. 이에 '부분 문법' 이론을 도입하여 일반적인 통사 분석 단계에서 처리하지 못했던 여러 가지 중의성의 문제를 해결할 것이다. 예를 들어, 여러 가지 관용 표현에 대한 인식이나, 또는 동의 관계에 있는 여러 가지 상이한 표현들에 대한 기술을 효율적으로 할 수 있다. 이 방법을 이용하면, 한국어로 된 어휘와 동일한 의미를 가진 불어를 하나의 의미로 인식 시킬 수 있게 된다. 따라서 한국어로 한 질의에 대해 불어 텍스트에서 답을 찾는 것, 혹은 그 반대의 경우가 가능해 진다. 이것은 자연어 질의응답 시스템과 기계번역의 의미를 동시에 지니게 된다.
앞으로 전개될 본고의 내용은 다음과 같다. 제 1장에서는 인지과학으로서 언어과학이 차지하는 과학적 위상을 논해보고, 자연언어 자동처리의 단계와 각 단계별로 생기는 중의성의 문제를 살펴본다. 또한 본 연구의 주된 기술적 개념이며 자연언어 자동처리의 가장 큰 활용분야인 기계 번역과 자연어 질의응답 시스템의 개념에 대하여 설명한다. 제 2장 '언어 이론의 이해' 부분에서는 본 연구가 따른 언어 이론인 '부분 문법'의 이론적 개념과 부분 문법을 구현하기 위해 필요한 기술적 도구인 UNITEX 프로그램에 대해 언급할 것이다. 제 3장 '데이터 베이스 모델링을 위한 기초 코퍼스 연구' 단계에서는 코퍼스 언어학적 개념에서 신문기사 텍스트에서 나타나는 문체적 특징을 통계적인 실험에 입각한 방법으로 살펴봄으로써 데이터 베이스 모델링을 위한 기초 코퍼스 연구를 한다. 제 4장 'DB 모델링' 단계에서는, 2장에서 살펴본 부분 문법의 이론에 입각하여, 3장에서 다룬 문체적 특징과 통계 실험을 통하여, 최종적인 문형 구조를 찾아낼 것이다. 제 5장은 본고의 결론 부분이다.Internet offre au monde une vaste aire de transmission de données pour un accès rapide aux informations. On appelle ce système le moteur de recherche. Les moteurs de recherche traditionnels, la plupart du temps, trouvent les informations en utilisant des mots-clés C'est pourqoui les résultats de la recherche ne sont à nouveau pas satisfaisants. Cependant l'utilisateur ne peut entrer que des mots-clés dans le moteur de recherche, si bien que l'information précise qu'il recherche n'apparaît pas explicitement. En outre, les résultats de la recherche sont seulement en ligne, alors l'utilisateur doit ensuite passer les références au crible pour trouver l'information précise souhaitée.
Ceci met en évidence deux inconvénients majeurs des moteurs de recherche traditionnels. Le premier est que la réponse se perd dans un nombre astronomique de documents et qu'il peut falloir énormément de temps pour parcourir chaque document à la recherche de l'information pertinente. Le second inconvénient est que, très souvent, le contenu des documents ne ressemble que de loin à celui espéré.
Pour trouver une réponse spécifique, la fonction de recherche doit interpréter la sémantique et Ia morphologie d'une question afin d'y apporter une réponse. Mais il y a le problème de l'ambiguïté, il est difficile de réaliser un système de recherche parfait. Par exemple, si l'utilisateur veut savoir queue est la hauteur de la Tour Eiffel, il doit d'abord saisir les bons mots-clés dans le moteur de recherche de facon à éviter d'obtenir des documents concernant la Tour de Pise ou la vie de Gustave Eiffel. Non seulement le nombre de résultats est-il considérable, mais l'utilisateur est obligé de parcourir chaque document pour trouver la réponse à sa question.
L'utilisateur d'aujourd'hui veut poser une question au système de questions-réponses en langue naturelle. Par exemple, si I'utilisateur veut savoir queue est la hauteur de la Tour Eiffel, il pose une question comme 'Quelle est la hauteur de la Tour Eiffel' sans rien changer.
Le système de questions-réponses peut traiter de nombreux types de questions dans plusieurs langues. Par example, il y a quelques questions concernant des personnes, des sociétés, des acronymes, des lieux, des dates d'événements, des quantités, une cause, un effet, des raisons, des processus et etc.
Dans les domaines spécifiques comme en sports-football, il y a les structures spécifique de phrases. Le but de cette étude est de analiser de la spécifique linguistique d'un texte concernant le football pour le modèle à base de données dans le Système questions-réponses en langue naturelle.
Pour avoir le résultat plus satisfait, nous tenons compte de une methodologie nommée Grammaire Locale qui a été proposée par Maurice Gross pour construction des données linguistiques et l'analyse automatique de langue naturelle.
Ce cadre methodologique permet d'une part de décrire des phénomènes linguistiques locales de facon très souple et adéquate, et d'autre part de réaliser un analyseur morpho-syntaxique de facon simple, notamment de nature d'automates à états finis. Il s'agit d'une grammaire à états finis qui se représente sous forme de graphes correspondant à des transducteurs à états finis. Cette théorie linguistique peut résoudre le problème de l'ambiguïté qui n'a pas résolu dans les théorie linguistique traditionnelles.
La réponse est extraite sous forme de fragment de texte et est obtenue en établissant des représentations syntaxiques abstraites ("modèles") des réponses possibles.
En fait, le traitement automatique de langue naturelle contenant le système de questions-réponses et la traduction automatique qui vise à communiquer entre l'homme et l'ordinateur, il est nécessaire d'associer non seulement la liguistique et l'informatique mais aussi au niveau de l'intelligence artificielle, au viveau de la psychologie et pui encore d'autre domaine qui peut aider pour notre devoir.
Nous espérons que cette recherche sera un premier pas vers le système de questions-réponses plus achevé.【목차】 국문요약 = 1 제 1장. 언어학의 과학적 위상 = 3 1.1. 인자과학(Sciences Cognitives)과 언어과학(Sciences Linguistiques) = 3 1.2. 자연언어 자동처리(Traitement Automatique de Langue:T.A.L) = 5

임상의는 갑상선 환자의 초음파 판독문을 매일 접하게 되는데, 판독 결과 갑상선암이 의심되는 환자는 정상인 환자에 비해 그 비율이 적다. 하지만 임상의는 모든 판독문을 일일이 보고 정상인지 암환자인지 구분하고 있다. 정상인 환자의 판독문을 보는데 들어가는 임상의의 시간과 수고를 덜어주고자 본 연구를 진행하게 되었다.
본 연구는 초음파 판독문을 정상인 케이스와 한번 더 봐야 하는 판독문의 경우로 분류하는 동시에 정상인 케이스는 확실하게 정상인 케이스만을 분류하는 시스템을 개발하고자 한다. 정상인 케이스에 한번 더 봐야 하는 판독문이 끼어 있는 것을 최대한 방지하기 위함이다. 이를 위해 판독문을 분석하고 자연어처리 기법을 이용해판독문에서 의미있는 키워드를 뽑아낸 후 이를 이용해 패턴을 생성, Rule-based Classifier에 적용하였다. Rule-based Classifier는 JAVA 기반으로 개발하였으며 92.34%의 Accuracy를 보였다. 모든 클래스의 성능에 가중치를 두어 도출해낸 Weighted Avg. of Precision은 0.946, Recall은 0.923, F-measure는 0.931의 성능을 보였다.
특히, 이 연구에서 가장 중요한 목표인 정상인 환자로 분류한 데이터 중 깨끗하게 정상인 환자임을 보는 측도로 Precision을 계산할 수 있는데, 이는 1.0의 성능을 보였다.
추가로 Rule-based Classifier의 성능을 비교 검토하고자 Decision Tree와 Machine Learning Technique을 활용하였는데, Overall Accuracy에서 Rule-based Classifier가 Decision Tree보다 0.74% 낮았다. 하지만 개수로 보면 3개의 데이터를 맞지 않게 분류한 것이다. 또한 본 연구에서 중요하게 보는 정상인 환자의 Precision은 Decision Tree와 Machine Learning Technique에서는 1.0보다 낮은 0.962, 0.945를 기록하였다.Physicians encounter ultrasound reports of thyroid neoplasm everyday. Ultrasound reports are classified into three types. RECUR is a report of a patient whose cancer recurred. INTER is a report used when it is not certain whether cancer recurred or not. NED stands for No Evidence of Disease. The proportion among the three types is not uniform. It is more likely to see NED reports than RECUR or INTER reports. However, physicians have to review all the reports manually. Physicians want to see the detail of the recurrence reports, so filtering reports that do not have the evidence of disease is important and can reduce human workload. These documents are clinical texts, thus classifying RECUR documents as NED documents is unacceptable. We developed a rule-based classifier using JAVA which detects the keywords in the reports and classifies the reports into the three categories using the patterns. The evaluation showed 92.34% accuracy in classifying into the three types. Also, a very crucial result of this paper is the 1.0 of precision in NED class. The 1.0 precision in NED means that those classified as NED consist of documents that were actually only NED documents. In addition, to evaluate the rule-based classifier, we experimented the decision tree and machine learning techniques. The decision tree and machine learning technique experimented using WEKA. For this experiment, we used 80 keywords for the feature sets. The overall accuracy of the decision trees was 0.74% higher than the rule-based classifier, but it misclassified three documents. The precision in NED was 0.962 in the decision tree and 0.945 in the machine learning technique which were all lower than the precision of 1.0 in the rule-based classifier.I. 서 론 1 1.1. 연구의 배경 및 필요성 1 1.2. 연구 목적 3 1.3. 논문의 구성 5

Concerns about the profitability of the construction companies in Korea are growing due to deteriorating overseas business performance that has been launched several years ago. There are many ways to increase the profitability of overseas construction projects, filtering out malicious projects through risk management in the early stages of the project such as bidding and contracting. Identifying and reviewing the dangerous contract conditions is essential in order for risk management at the bidding and contract stages.
Extensive contract management know-how and experiences are required in order to identify and analyze risk factors included in the contract. However, Korean construction companies are not only have very few experts to perform contract management but also have no systematic tools supporting contract management of overseas projects.
The purpose of this study is to develop an automatic reviewing model for overseas construction contracts that can be used at the initial stage of the projects considering the lack of specialized contract management experts in the domestic construction environment. The automatic reviewing model for overseas construction contracts is named as an ‘Construction Risk Extraction Model’ according to the characteristics of automatically extracting risk contract clauses from the contract conditions. This model identifies the risk sentences of the contract with the help of automated text analysis techniques such as natural language processing(NLP) and information extraction(IE). This study provides supporting tools that can more effectively support contract reviewing tasks that are time consuming and error-prone depending on individual’s ability to review contracts.
In order to validate the performance of the Construction Risk Extraction Model, the results of reviewing a contract document by experts and the results of reviewing by model were compared with each other. As a result, the precision, recall, and F-measure of the Construction Risk Extraction Model were 83.3%, 79% and 81.1%, respectively. This is the result of risk extraction of the model after reflecting some types of errors found in the validation process, and some parts of extraction results showed higher performance than the results of some expert’s review.
The results of this study are expected to have an effect on the business support and educational effect in that it provides easy understanding of the major risk factors of the contract for the construction engineers who do not have experience in contract management field. In addition, on the academic aspect, this study expanded the field of construction management research by realizing interdisciplinary research using linguistic engineering and computer sciences.몇 년 전 불거진 국내 건설기업들의 해외사업 실적 악화로 인해 건설업계의 불안감이 커지면서 프로젝트의 수익성 향상에 대한 관심이 높아지고 있다. 수익성 있는 프로젝트를 수행하기 위한 방법에는 여러 가지가 있지만, 프로젝트 초기단계인 입찰 및 계약단계에서 철저한 리스크 관리를 통해 악성 프로젝트를 선별하는 것이 무엇보다 중요하다. 이를 위해서는 계약서에 포함된 위험한 계약조건들을 파악하고 검토하여 적절한 대응방안을 마련하는 것이 필요한데, 일단 계약이 성립되면 계약과 관련한 리스크 요인도 동시에 확정되는 것이기 때문에 리스크 관리는 초기단계에 수행될수록 그 효과가 크다고 할 수 있다.
계약서에 포함된 위험요인을 파악하고 분석하여 적절한 대응방안을 마련하기 위해서는 폭넓은 계약관리 지식과 클레임 및 분쟁 가능성에 대한 경험적 노하우가 필요하다. 그러나 국내 건설기업에는 계약관리를 수행할 수 있는 전문 인력이 부족할 뿐만 아니라 계약관리 지식에 대한 체계적인 시스템이 마련되어 있지 않아 소수의 전문가에 의존하는 경향이 높다. 이러한 현상은 중소기업의 경우 보다 극명하게 나타나고 있으며, 전문가 부족 및 지원체계 부재로 인해 적절한 계약 관리가 거의 이루어지지 못하고 있는 실정이다. 뿐만 아니라 최근 들어 일부 해외 건설공사의 발주자들은 시공사가 계약서의 모든 내용을 파악하지 못하도록 충분한 입찰기간을 보장해주지 않을 뿐만 아니라 독소조항, 애매모호한 표현을 의도적으로 계약서에 포함시키는 등 시공사의 계약적 지위를 불리하게 하는 요소들이 점차 증가하고 있는 추세이다. 이로 인해 계약관리 능력이 부족한 시공사일 경우 해외 사업 수행 자체에 어려움을 겪을 수밖에 없는 환경으로 변하고 있으며, 계약관리 역량이 해외 건설공사의 필수 관리요소로 대두되고 있다.
따라서 본 연구에서는 계약관리 전문 인력 및 지원 시스템이 부족한 국내 건설기업의 실정을 감안하여 프로젝트 초기단계인 입찰 및 계약단계에서 활용 가능한 해외건설 계약서 자동 검토 모델을 개발하는 것을 연구의 목표로 선정하였다. 해외건설 계약서 자동 검토 모델은 계약서 내의 위험한 계약조항을 자동으로 추출한다는 특성에 따라 계약 리스크 자동추출 모델로 명명하였다. 이 모델은 계약서를 검토하는 개개인의 역량에 따라 편차가 발생하기 쉬운 계약서 검토 업무를 보다 효과적으로 지원할 수 있는 기술적 체계를 마련하기 위해 텍스트 자동 분석 기술의 도움을 얻어 계약서의 위험 조항을 컴퓨터가 식별하여 위험요인을 추출하는 메커니즘이다. 모델의 개발을 위해 본 연구에서는 인간의 언어를 컴퓨터가 이해할 수 있도록 분석하는 자연어 처리(NLP, Natural Language Processing) 및 정보 추출(IE, Information Extraction) 기술을 적용하였으며, 텍스트 자동 분석 기술을 해외 건설공사의 계약관리 지원기술로 접목시킴으로써 기존의 취약했던 해외 건설 계약관리 업무를 효율적으로 개선하고자 하였다.
본 연구의 결과물인 건설 계약 리스크 자동추출 모델은 건설 계약관리 분야의 경험이 부족한 건설 실무자들에게는 계약서의 주요 리스크 요인을 손쉽게 파악하게 해준다는 측면에서 업무 지원 및 교육적 효과가 클 것으로 판단되며, 계약 관리 전문가들에게는 보다 정밀하고 정확한 후속 업무를 위한 기초자료를 제공한다는 측면에서 실무적 활용성을 가질 수 있을 것으로 기대한다. 또한 건설 분야 연구에 많이 적용되지 않았던 자연어 처리(NL) 및 정보 추출(IE) 방법을 통해 계약 리스크 관리를 실현함으로써 통계 분석 위주의 기존의 리스크 관리 방식을 탈피하고, 나아가 향후 관련 분야 연구의 확장성을 넓혔다는 측면에서 의미 있는 시도로 평가할 수 있다.I. 서론 1 A. 연구의 배경 및 목표 1 1. 연구의 배경 1 2. 연구의 목표 5 B. 연구의 범위 및 방법 7

목 차 표제지 = 0 목 차 = i I. 서 론 = 1 1. 연구 배경 = 1

야구는 대표적인 기록 스포츠로써 수많은 기록을 생산한다. 이렇게 생산되어 누적된 기록 정보는 선수들의 경기력 향상에 도움을 줄 뿐만 아니라 야구와 관련된 다양한 분야의 전문가들에게도 유용하다. 그래서 신속하고 편리하게 제공되어야 한다. 그러나 이러한 데이터를 수집하고 분석하는데 있어 현재 시스템은 복잡한 컨트롤 조작에 의해 비효율적으로 질의응답을 얻고 있다. 따라서 본 논문에서는 자연어처리를 사용한 질의응답 시스템을 제안한다.
자연어처리방식은 질의문장 생성과정이 간소화되므로 신속한 응답을 제공할 수 있다. 기록 정보에 대한 신속한 질의응답 시스템은 관련업무의 효율성을 높이고 사용자의 편리성을 증대시킬 수 있다. 효율성을 검증하기 위해 전문가 기록 사이트에 적용해본 결과 본 시스템이 일반 컨트롤 검색보다 질의문장 생성단계가 감소하여 신속한 응답을 얻을 수 있었다.제1장 서론 1 제2장 관련연구 3 2.1 질의응답 시스템 3 2.2 벡터 공간 모델 4

Information society can be realized based on efficient collection, management and retrieval of large - scale information. Since the database (DB) is an important and important field in the information society, a system that facilitates DB access to general users can be regarded as an important core technology in the future. However, there is a restriction that general users should have knowledge of DB and business knowledge such as SQL in order to use DB directly. In order to solve the above limitations, researches on NLIDB (Natural Language Interfaces to Database System) are under way. To construct NLIDB, natural language processing technology and processing technology for DBMS (Database Management System) query language should precede. In this paper, I propose a method to convert to SQL statement so that general users without DB and SQL knowledge can search DB through natural language processing of query for the purpose of searching and using DB with general computing performance. The proposed method generates SQL by matching the columns using N-gram and named entity dictionary through natural language query. We analyze the experiment through the proposed method and describe the experimental result.ABSTRACT I. 서론 1 A. 연구 배경 및 목적 1 B. 연구 내용 및 구성 2

무형의 기술적 창작인 발명의 보호에 대한 인식이 강화되면서, 해마다 많은 발명들이 출원되고 있다. 공개된 특허문헌 중 기술내용이 유사한 특허문헌을 찾기 위해서는 키워드 검색을 통해 특허문헌의 범위를 좁힌 후, 해당 분야에 기술적 지식을 가진 사람이 특허문헌의 내용을 확인하여 찾아내는 방법이 사용되고 있다. 그러나, 해마다 발행되는 방대한 양의 특허문헌를 검토하는 작업은 시간, 비용 및 노력이 많이 소요한다. 본 논문은 이러한 맥락에서 특허문헌에 기재된 발명의 내용을 기계적으로 분석하여 구성의 유사도가 높은 순으로 정렬해 주는 모델을 제시한다.
발명의 내용의 기계적 분석을 위해, 본 모델은 자연어 처리기법을 이용하여 특허문헌으로부터 발명의 각 구성요소들 및 기술적 특징을 추출하고, 추출된 값들을 이용하여 발명의 유사도를 산출해낸다. 발명의 각 구성요소들은 특허문헌의 기재의 형식적 특징과 형태소 분석 및 링크 파싱 등의 자연어 처리기법을 이용하여, 청구항으로부터 추출된다. 발명의 유사도는 추출된 구성요소들을 매칭시켜, 매칭된 구성요소의 유무를 기초로 계산된다. 구성요소의 매칭은 구성요소들간의 대응여부를 판단하는 논리적 추론으로, 언어적 표현이 상이한 경우에도 실질적인 대응여부를 판단할 수 있어야 하며, 이는 확률적인 계산과정을 수반한다. 실질적인 대응여부 판단을 위해, 주요구성요소 뿐만 아니라 서브구성요소 및 구성요소의 기술적 특징이 유사도 계산식에 있어서 가중치 인자로 부여한다.
사용자가 특허문헌을 선택하면, 전처리기에 의해 특허문헌 중 청구범위에 기재된 내용이 독립항과 종속항별로 구분되고, 청구항에 기재된 발명의 각 구성요소가 문장의 품사 및 문장구조분석을 통해 추출된다. 프로그램에 의해 계산된 유사도값을 WIPO의 국제조사보고서에 기재된 내용과 비교하여 유사도값의 타당성에 대해 검토하였고, 국제조사보고서에 기재된 카테고리가 X인 발명에 대해 약 67%의 정확도를 보여주었다.Many inventions have been filed yearly. Now, to find patent documents having similar technical content among many patent documents, a man having an ordinary skill in the art restricts the searching range of patent document by serching keywords and investigating the content of the patent document. However, the work to examine a great deal of patent documents annually published requires a lot of time, cost and effort. This thesis suggests a model to analyze the contents of the invention from the patent documents mechanically and sort out the patent documents in the order of high similarity.
Regarding to the mechanical analysis of the invention, the proposed model extracts each component and technical characteristic from the invention using natural language processing method and calculates the similarity amongs the inventions using the extracted values.
Each compoent of the invention is extracted from the claim by the formal characteristic of patent document description and natural language processing methods such as morpheme analysis and link parsing. The similarity of the invention is calculated by matching the extracted components and finding the existence of the matched component. The matching of the component is a logical inference process which decides the correspondency between the components. It should decide the substantial correspondency if linguistic expressions are different, and it accompanies a probabilitical calculating process. To decide the substantial correspondency, not only the main-component but also sub-component and technical features of the component are added to the weighting factor in the formula of calculating the similarity.
If a patent document is selected by a user, a preprocessor discriminates the independent claim and the dependent claim in the claim and each component of the invention is extracted by analyzing the grammar and structure of the sentence.
After comparing the similarity value which is calculated by the program with the content of the International Search Report(ISR), it is shown that about 67% of accuracy for the invention belongs to X category in the ISR.제 1 장 서론 = 1 1.1. 연구의 필요성 = 1 1.2. 연구의 범위 = 2 1.2.1 연구의 최종목적 = 3 1.2.2 논문의 구성 = 3

In the 1980's there have been active research seeking for use of new conception and principle by virtue of the 5th generation computer development project.
This research is a processing of Natural Language from the viewpoint of AI (Artificial Intelligence) using the interactive logic programming language instead of high level one which is being used now. Natural Language processing enables computer system to understand the inputs of ungrammertical language and helps a user to enter the correct sentence.
This paper describes the utilizing of Natural Language Processing in a small-scaled system(Micro-computer CP/M operating system).목차 ABSTRACT Ⅰ. 序論 = 1 Ⅱ. 本論 = 2 A. 自然語處理 (Natural Language Processing) = 3

최근에 사용되고 있는 정보검색 시스템은 원하지 않은 많은 문서를 포함하는 많은 문서가 검색된다. 이는 대부분의 사용자가 명사나 동음이의어가 포함된 짧은 질의어를 자주 사용하기 때문이다. 게다가 대부분의 정보검색 시스템은 사용자의 의도를 제약하기 위해서 불리언 연산자를 가지는 불리언 질의어를 제공한다. 불리언 질의 어는 이해하기 어려울 뿐만 아니라 자연어 에 비해 사용하기도 어렵다.
따라서 본 논문에서는 사용하기 쉽고 가능한 한 사용자가 원하는 문서만을 검색해 주는 대화 기반 지능형 정보검색 시스템을 제안한다. 초보자가 사용하기 쉽도록 자연어를 사용하는 본 시스템은 사용자의 질의를 분석하여 키워드를 추출한다. 다의어 가 키워드로 추출되면 사용자의 의도를 파악하기 위해 대화를 시도한다. 다의어를 적당한 키워드로 대체한 다음 정보를 검색한다. 이렇게 함으로써 사용자가 원하는 문서만을 검색해 준다.제목 차례 = 4 서론 = 9 2. 관련 연구 = 12 2.1 연구 배경 = 12 2.2 관련 연구 = 17



The image generation from given text is one of problems in the spotlight in these days. Many studies that deal with this problem employed GAN, which has proven its superiority in image generation.
However, basic GANs that have one discriminator have critical limits for the generation of images from given texts. To generate a plausible image for the given text, GANs should consider two factors. One is how the generated image is good in terms of the quality of image, the other is how well the generated image reflects the content of the given text. In this paper, we propose a novel GAN which has two discriminators that deal with the two factors respectively. To prove the effectiveness of our model, we evaluated generated images by the proposed model qualitatively on the two benchmark datasets. As the results, the proposed GAN showed better performance than a baseline model in terms of both the image quality and the text reflection degree.I. 서 론 1 II. 관련 연구 5 2.1 Generative Adversarial Network 10 2.2 텍스트로부터 이미지 생성하는 GAN 10 2.3 BEGAN:Boundary Equilibrium Generative Adversarial Networks 12

병리진단은 검체에 대한 바이오마커 시험 결과를 바탕으로 이루어지므로, 질환별 바이오마커의 양성률 양상 데이터 확보는 근거중심의학의 질 향상에 기여할 수 있다. 현재 이러한 정보를 얻기 위해서는 출판된 문헌 자료를 직접 분석하여야 신뢰도 있는 자료를 얻을 수 있기 때문에, 대량의 데이터를 기반으로 한 양성률을 파악하기가 어려운 실정이다. 따라서 본 논문은 대량의 병리보고서로부터 질환별 바이오마커 양성률 정보를 분석하기 위한 기반 기술로써, 면역화학검사보고서로부터 바이오마커 정보를 추출하는 방법에 관해 기술한다.
본 연구는 면역화학검사보고서를 입력으로 받아 바이오마커의 대표표현형과 그에 해당하는 결과 정보를 구조화하여 출력하는 시스템 개발을 목표로 한다. 이를 위해, 본 시스템은 사전에 정의한 병리보고서 작성 문법을 바탕으로 구문 분석을 수행하여 중간 구조화 데이터를 얻는다. 그다음, 바이오마커 정규화기를 통해 다양한 표현으로 기술된 바이오마커를 시스템 대표표현형으로 변환하여 최종 바이오마커 정보를 생성한다.
특히, 단순 사전 검색을 이용한 대표표현형으로의 변환은 오타가 포함된 바이오마커명에 대한 인식 및 수정을 할 수 없다는 문제점을 해결하기 위해, 후보생성 과정과 대표표현형 선정 과정을 거쳐 오타 검정(檢正)이 가능한 정규화를 시도한다.
후보생성 모듈은 병리 보고서의 특성을 고려한 규칙기반 후보생성, UMLS(Unified Medical Language System) 데이터베이스 검색을 통한 후보생성, 유사 문자열 검색을 통한 후보생성 과정을 통해 후보 대표표현형을 생성한다.
대표표현형 선정 모듈은 동일 조직에 같이 검사된 바이오마커 집합을 문맥정보로 주었을 때 해당 대표표현형 후보가 함께 사용될 조건부 확률과 문자열 유사도를 후보 적합성 측정 기준으로 이용하여 최종 시스템 대표표현형을 결정한다.
시스템 성능평가를 위해 서울대학교병원에서 2012년도에 생성한 면역화학검사보고서 8,566건에 대해 시스템을 실행해 본 결과, 바이오마커 정규화기의 바이오마커 표현단위 정확도가 0.9039, 바이오마커 빈도수단위 정확도는 0.9825로 높은 성능을 기록하였다.I. 서 론 1 1.1. 연구의 배경 및 필요성 1 1.2. 연구 목적 5 1.3. 연구 범위 6





This paper is based upon semantic network as the knowledge representation model of HANGUL sentences that need to construct system understanding HANGUL natural language.
A matching algorithm is presented that can represent semantic network through the syntactic analysis of input sentences.
Also, the system is designed which represents knowledge as storing rules and facts in knowledge base.목차 = 0 제1장 서론 = 1 제2장 한국어 문장의 구조 = 4 제1절 한국어 문장의 성분구조 = 4 제2절 체언과 용언의 구성 = 5

본 논문은 국소 문맥을 사용하여 단어의 형태적 중의성을 제거하는 방법을 기술한다.
형태소 분석 결과에는 이형 동품사, 동형 이품사, 그리고 비동기 문제 등의 중의성이 발생하는데 이 중에서 이형 동품사 문제(형태적 중의성 문제)를 해결하기 위해서는 어휘의 확장 문맥이 중요하다. 하지만 품사 부착 말뭉치를 사용해서는 충분한 어휘 문맥을 고려할 수 없으므로 교사 학습 방식을 사용하는 기존의 태깅 방법으로는 이러한 형태적 중의성 문제를 해결할 수 없었다. 그러므로 본 논문에서는 충분한 어휘 문맥을 고려할 수 있도록 주변 정보로부터 쉽게 획득하여 사용할 수 있는 대량의 원시 말뭉치를 이용하는 비교사 학습 방식을 사용하여 형태적 중의성을 해결하고자 한다.
중의성 제거 과정 중 가장 중요한 점은 중의성을 가진 문장들에 Decision List를 적용하고 중의성이 제거된 결과들을 다시 자가 학습하는 반복과정에 의해 Decision List의 수행능력을 향상시킨다는 점이다. 이 방법은 단어의 형태적 중의성 제거에 일정 거리 이내의 공기 정보가 가장 큰 영향을 끼친다는 직관에 바탕을 두고 추가적인 사람의 교정을 필요로 하지 않는 비교사 방식(대량의 원시 말뭉치에 기반한)에 의해 수행한다. 학습을 통해 얻어진 최종 Decision List는 연세대 형태소 분석기인 MORANY의 형태소 분석 결과에 적용되어 형태적 중의성이 제거된 결과를 태깅 시스템에 넘겨주어 태깅 시 성능을 향상시킨다.
본 모델의 대상이 될 수 있는 형태적 중의성을 가진 단어 형태들은 23가지가 있으며 그 중 이형태가 3개 이상이거나 거의 한쪽의 형태로만 사용되는 것들을 제외한 12가지 형태들에 대하여 실험하였다. 500만 어절의 실험 말뭉치에 본 알고리즘을 적용한 결과 긍정적인 결과(90.61%)를 얻었다.
본 모델의 성능을 평가하기 위해 들었다 단어를 실험하여 기존의 태깅 시스템과 비교하였는데 은닉 마코프 모델의 바이그램(bigram) 기본 모델에 의한 태깅 결과(72.61%)보다 뛰어난 결과(94.25%)를 얻어서 형태적 중의성 해소에 확장 문맥을 적용시키는 것이 유용하다는 점과 필요한 정보를 원시 말뭉치로부터 자동으로 학습할 수 있다는 것을 확인할 수 있었다.This paper presents a model using unsupervised learning to resolve lexical ambiguities in Korean morphological analysis. Much lexical information is required for resolving a particular type of ambiguity such as the verb-verb (VV) morphological ambiguity that an eojeol is analyzed as haying the same part of speech (POS) verb in spite of different stems. We first construct a very small size of seed collocation from. Then, using the small prior knowledge, rich but accurate lexical information for VV disambiguation is acquired by repetitively applying to a raw corpus and learning. For experiments, we compare disambiguation results by our model with those by other models using restricted information. The experiments show that our method gives much better result than the other models. In particular, this model shows the possibility that this unsupervised learning can be effectively applied for specific problems that require rich lexical information which cannot be obtained by supervised learning.차례 = i 그림차례 = ii 표차례 = ii 국문요약 = iii 제1장 서론 = 1

관계형 테이타베이스 설계에 있어서는 개념설계 단계시에 E-R다이어그램에서 변환된 최종적인 릴레이션들에서 발생하는 여러가지 이상현상(anomaly)들을 제거하기 위한 정규화(Normalization)가 가장 중요한 문제라고 할 수 있다.
본 논문에서는 자연어(Natural Language)문답에 의해 형성된 E-R다이어그램을 가지고 최종 제3정규형(The Third-Normal Form)에 있는 관계형 데이타베이스 설계하는 시스템을 구현하였다.In relational database design, normalization is the most important problem to eliminate many anomalies which occur in the final relations translated from E-R diagram in the conceptual design.
In this paper, system was implemented to design relational database in the Third-normal form with E-R diagram was delivered by natural language dialog.목차 초록 = II I. 서론 = 1 II. 이론적 배경 = 3 2.1 데이타베이스 설계 = 3



대다수의 개방영역 자연어 질의응답 시스템은 답을 선택할 수 있는 개념영역을 미리 정의 하고 있기 때문에 시스템이 준비하지 못한 범주의 개념을 묻는 질의문에 대해서는 올바른 응답을 생성하지 못하거나 예외 처리 방식으로 응답을 생성해 낸다. 본 논문에서는 전형적인 범주에 속하지 않는 명사 개념에 관한 질의문에 대해 범용적으로 대응할 수 있는 개방영역 자연어 질의응답 시스템을 제안한다.
제안하는 시스템은 상위 개념 명사구(Hypernym)에 포함되는 하위 개념의 명사구(Hyponym)들을 추출할 수 있는 일반적인 패턴들을 그 신뢰도와 함께 가지고 있다. 따라서 질의문이 임의의 명사구 개념을 요청할 때 정답의 후보들을 동적으로 생성되는 가상의 is-a 의미관계 사전으로부터 신뢰도 순위로 정렬하여 추출해 낼 수가 있다.
제안하는 시스템은 “What 명사구 동사구” 형태의 질의문들 중에서 개체명 인식기나 시소러스를 이용하여 정답 후보를 손쉽게 생성할 수 있는 질의문을 배제한 실험용 질의문 집합을 이용한 실험에서 58%의 재현율을 보였다.Most of existing open domain question answering systems predefine the conceptual category to which answers can belong. So, they can not generate appropriate answers or must use a strategy that handles exceptions when the requested concept in the question is not prepared in the system.
In this thesis, we suggest a flexible strategy which can generate the candidate answers that correspond to any target concept.
The suggested question answering system is equipped with general patterns which can extract hyponyms of the nominal target concept with their confidence scores. Therefore, the alternative system can create a set of candidate answers from the dynamically generated lexicon when the user requests any nominal concept.
For experiment, we selected the questions with a type of "What Noun-Phrase Verb-Phrase", which causes difficulty in aggregating candidate answers with Wordnet or named entity recognizer. In our experiment, our system performance produced a reasonable result of 58% recall by standard measurement.



본 논문은 자연어 처리 기술을 사용하여 변호사와 의뢰인의 만남을 돕는 인공지능 챗봇 서비스를 Lawper라 이름 짓고 사업 계획서의 형태로 기획하였다. 본 사업 계획서에서 구상한 사업은 크게 두 가지로 구성된다. 첫째, 변호사를 위한 챗봇 컨설팅 및 제작 서비스는 변호사가 개별적으로 사용하는 카카오톡 계정에 챗봇을 탑재해주는 서비스이다. 둘째, 변호사와 의뢰인의 만남을 위한 챗봇 플랫폼은 아직 변호사를 선택하지 않은 의뢰인을 위하여 제공하는 Lawper의 챗봇 서비스이다. 본 사업이 개발하고자 하는 카카오톡 챗봇은 변호사와 의뢰인이 진행하는 법률 상담을 카카오톡 채팅으로 진행할 수 있도록 법률 상담을 자동화하는 기능을 제공한다. 불특정 다수를 통해 들어오는 카카오톡 상담 신청에 변호사가 직접 응답하는 것은 많은 시간을 요구하는 일이다. Lawper 서비스는 인공지능 챗봇이 변호사를 대신하여 의뢰인과 기초 법률 상담을 진행하게 함으로써 변호사가 더 많은 의뢰인을 만날 수 있게 한다. 또한, 의뢰인은 1:1 채팅을 통해 사생활을 보호받고, 카카오톡 채팅이라는 접근성 높고 편리한 방법을 통해 법률 상담을 진행한다는 장점이 있다. 본 사업이 마주하게 될 가장 큰 위험 요인은 변호사법의 규제이다. 변호사법은 변호사에게 의뢰인을 소개해주는 행위를 영리적으로 진행하는 것을 금지하고 있다. 본 사업 계획서는 이에 따라 변호사법에 위배되지 않는 수익 모델을 구상하였다. 또한, 이러한 사업 모델이 실제로 작동할 수 있을지를 점검하기 위하여 인공지능 챗봇의 프로토타입을 개발하여 정확도를 테스트하였다. 마지막으로 본 사업 계획서는 개발 계획을 토대로 한 추정 손익 계산서와 손익 분기점을 도출하였고, 이에 따른 마케팅 계획을 설명하였다. 4차 산업혁명과 인공지능의 발전과 더불어 리걸 테크가 향후 성장할 것이라 예측된다. 국내 리걸 테크 시장은 이제 시작하는 단계에 있으므로 개척되지 않은 블루 오션에 새로운 서비스를 시도할 수 있다는 점에서 본 사업 계획서는 의의를 가진다.Lawper is an artificial intelligence chatbot service for helping legal counseling between lawyers and clients. Lawper consists of two services. First, Lawper helps lawyers to implement chatbot in their own KakaoTalk account. Second, Lawper provides Lawper's chatbot separately for the clients who do not decide the lawyer they will contact. Lawper’s chatbot provides legal counseling service to the clients on behalf of laywers. Lawyers can enhance their business productivity by reducing costs to respond lots of contacts from clients via KakaTalk. In addition, the clients can have legal counseling in mobile chatting environment conveniently, and they can relieve their privacy concern. The critical risk of this service is Attorneys-at-Law Act of South Korea, because it regulates mediation between clients and lawyers with economic benefit. Therefore, this paper designs revenue stream and cost structure that are not against the Attorneys-at-Law Act. Furthermore, income statement and break-even point are estimated with system development plan. This paper also includes marketing plan to promote Lawper when entering the legal market. Legal technology has potential to be advanced consistently with the growth of artificial intelligence technology. Lawper is expected to find a new blue ocean since South Korean legal technology market is on initial stage.



목차 = ⅰ 第1章 序論 = 1 第2章 한글의 Redundancy = 3 2.1 情報理論 = 3 2.2 Shannon의 公式 = 4





By treating a recent embedded system, you can see that it is performed complicate and intellectual functions. Corresponding, It has increased functional requirement and strengthen. As a result it make much account of system’s requirement management. through requirement management owing to the requirement management make over, system’s functions are modified or addition/delete during the development. finally, in case of every requirement satisfied then be existed system development. Consequently, It is effected in all area of interior system development
Carrying on these Processes, the thing which fill out exactly requirement can do smoothly communication each of participant of project and then conclude development of project. This thesis is introduced about requirement how to description and suggested technical problems. And it suggested how to description of solvable new requirement to problems , these methods design and confirm result.최근 임베디드 시스템(Embedded System)들을 다루어 보면, 복잡하면서도 지능적인 기능들을 수행하는 것을 볼 수 있다. 따라서 기능적인 요구사항이 많아지고, 강화되고 있는 것이 현실이다. 그 결과, 시스템의 요구사항 관리(Requirement Manangement)가 중요시되고 있다. 이 요구사항 관리를 통하여 시스템의 개발을 시작하고, 개발 도중, 요구사항의 변경으로 인한 시스템의 기능이 수정되거나 추가/삭제가 된다. 끝으로 모든 요구사항을 만족시켰을 경우, 시스템의 개발을 종료하게 된다. 결론적으로 내장 시스템의 개발의 모든 분야에 영향을 준다.
이러한 과정들을 진행시켜 시스템을 개발함으로, 요구사항을 정확하게 작성하여 프로젝트 참여자간의 원활한 의사소통을 할 수 있게 하여야 하고, 프로젝트를 개발완료 할 수 있도록 하여야 한다. 본 논문에서는 현재 요구사항 기술 방법에 대하여 소개하고, 그 기술 방법의 문제점을 제시한다. 그리고 문제점을 해결할 수 있는 새로운 요구사항 작성 방법을 제안하여, 이를 설계하고 구현하여 결과를 확인한다.본문차례 (LIST OF TEXT) = 2 그림차례 (LIST OF FIGURE) = 4 표차례 (LIST OF TABLE) = 5 제1장 서론 = 6 제2장 요구사항 관리의 개요 = 9





자연어 처리과제의 대부분은 서열자료를 사용하므로 기본 신경망인 순방향신경망(feed forward neural networks, FFNN)을 사용하였을 때 우수한 성능을 기대하기 어렵지만 서열자료에 특화된 순환신경망(recurrent neural network, RNN)을 사용하면 우수한 성능을 얻을 수 있다.
본 논문에서는 FFNN의 한계점을 극복하고 더 정확한 분석을 위하여 문자형 데이터를 숫자형 데이터로 표현하는 방법 중 하나인 단어 임베딩을 이용한 RNN 분석방법을 제안하였다. 제안한 방법을 네이버영화 리뷰 자료를 이용하여 감성분석에 적용한 결과 기존의 방법인 TF-IDF(term frequency - inverse document frequency)방법보다 정분류율이 약 6% 높은 86.31%로 나타났다. 기계번역에서 문장이 길어질 때 번역 품질이 떨어지는 단점을 보완하기 위해 Attention을 추가한 기계번역을 제안하였다. 분석용 자료인 OpenSubtitles2018을 RNN을 이용한 방법과 제안한 방법으로 기계번역을 실행하였다. 기계번역 평가는 사람과 기계가 번역한 문장을 n-gram으로 나누어 함께 등장한 단어의 비율을 계산하는 BLEU(bilingual evaluation understudy)를 사용하였다. Attention을 추가한 기계번역인 경우 기존방법보다 BLEU 적용결과 약 7정도 높은 96.54로 나타났다.국문초록 ⅰ Abstract ⅲ Ⅰ. 서론 1



This study is for experiment of Sentiment Analysis. We use lot of Sentiment words in conversation and sentence. The Sentiment word appears Happiness or Sadness of people. The word has power of life in our real life. The study focuses on the sentiment word for finding people who feel between Happiness or Sadness.(but this study is not focused on happiness score.)
In this study, Suggestion that study of unsupervised leaning using Morphology Analyzer(twitter), sample dataset(suicide note) and emotion dictionary.
The dictionary and sample dataset are created by precedent thesis.
This study is following steps. First, Dividing sentence for searching word using Morphology Analyzer, Second, Comparing each word on emotion dictionary, Third, Getting sum score of compare words.
In the result, People get dangerous when the score is low or similar than index score of –16.85.4차 산업혁명이 전 세계에 적용되었고, 최근 5G 서비스가 상용화됨에 따라 초연결 시대가 다가오고 있다. 인터넷은 사람과 사람을 이어주는 소통과 삶의 터전이 되고 있으며, 금융, 보험, 정부 민원 서비스 등 모든 업무 영역이 인터넷으로 처리될 만큼 전송 처리 속도가 빨라짐에 따라 데이터의 축적과 처리량도 늘어나게 되었다. 첨단 정보통신과 멀티미디어의 발전은 사람과 사람을 이어주는 사회 인적 네트워크(Social Network)로 발달됨에 따라 소셜 네트워크는 사용자에게 대화(Conversation) 및 정보(Information)등을 전달하는 효율적인 매개체가 되었다. 초창기 소셜 네트워크는 정보를 전달하는 기능을 장점으로 들 수 있었지만, 오늘날의 소셜 네트워크는 테러계획, 자살계획 등 사회적 부작용이 심각해지고 있어 단점으로 거론되고 있다.
이에 본 연구에서는 오피니언 마이닝 분야 중 감정 분석(Emotion analysis)을 연구 주제로 선정하고 소셜 네트워크를 통해 공유되는 글의 내용을 긍정적 단어(Positive word), 부정적 단어(Negative word)로 구분하고 메시지의 단어들을 점수화한 감정표현단어 사전(Emotion dictionary)을 통해 메시지 속 감정을 연구해보고자 하였다. 본 논문에서는 자연어처리 기술을 통해 분류된 단어들을 데이터베이스 기반 감정표현단어 사전과의 대조작업을 통해 텍스트 감정 점수를 부여하는 방법을 제안하고, 그 결과를 분석하였다.





기술 기획의 어려움과 급변하는 기술 동향에 대한 우려로 인해 기술 기획, 지적재산관리 (IP, Intellectual Property) 및 연구개발 전략 수립과 같은 많은 산업 분야에서 특허 마이닝 및 분석이 점차 주목 받고 있습니다. 특허는 중요한 기술 정보가 포함된 정형화된 기술 문서이며 기술 기획의 필수 요소입니다. 특허 분석은 필요한 기술 내용과 특허 집합 간의 관계를 확인하여 향후 전략 수립을 지원하기 위해 사용되곤 합니다.

현재까지 대부분의 연구자들은 자연어 처리 (NLP, Natural Language Processing)를 기반으로 하는 의미론적 특허 분석에 관심을 보여 왔습니다. 기존의 서지 정보 기반 특허 분석은 기술의 진정한 의미를 식별할 수 없었기 때문에 단어의 문맥을 분석하는 자연어 처리에 대한 연구가 활발하게 이루어졌습니다. 그러나 특허 문서의 구조적 어려움과 데이터 전처리의 비효율성으로 인해 혁신적인 의미론적 특허 분석에 대한 요구 또한 증가하고 있습니다.

또 다른 문제점은 분석 과정에서 이해 관계자의 요구사항을 적절히 반영하지 않으면서 다양한 방법을 사용하는 맹목적인 특허 분석만이 수행된다는 것입니다. 체계적인 요구사항 분석이 이루어지지 않으면 비즈니스 성공을 보장할 수 있는 기술 기획을 수립하는 것이 불가능합니다.

위에서 설명한 한계점들을 극복하기 위해서 본 연구에서는 NLP 기술을 활용하여 이해관계자 요구사항 기반 기술정보를 도출하는 반자동 방법론을 제안했습니다. 요구사항은 문법 규칙을 기반으로 구조화된 의미 정보로 분해된 다음, 요구사항 및 특허에서 추출한 기술 기능, 컴포넌트, 컨텍스트 및 목적을 포함하는 4가지 기술정보와 연결하여 요구사항 기반 기술정보를 도출 할 수 있습니다. TF-IDF (Term Frequency – Inverse Document Frequency) 및 전문가 리뷰는 각 정보의 중요성을 평가하는 기준으로 사용됩니다. 추출된 결과를 이용하여 기술정보의 정성 분석을 지원하기 위해 네트워크 및 매트릭스 형식으로 정보를 시각화 했습니다. 네트워크는 ‘node’와 ‘edge’ 간의 기술적 관계를 파악할 수 있으며 매트릭스는 공백 영역 및 고밀도 산업영역에서 비즈니스 기회를 제안할 수 있습니다. 이 방법론은 나노 센서와 관련된 특허 데이터 분석을 통해 입증되었습니다.Due to concern about difficulties of technology planning and rapidly changing technology trend, patent mining and analysis have received increasing attention in many industrial sectors including technology planning, management of Intellectual property (IP) and establishment of R&D strategy. Patent is a formal technical document containing important technical information and is an essential part of technical planning. Patent analysis is used to support future strategy establishment by identifying required technical contents and relationship among patent set.

Up to present time, most researchers have been concerned with semantic patent analysis based on natural language processing (NLP). Since the existing bibliographic information based patent analysis has not been able to identify the real meaning of technology, research on natural language processing that analyzes context of a document in words has become active. However, due to the structural difficulties of patent documents and the inefficiency of data preprocessing, the demand for innovative semantic patent analysis is also increasing.

The other problem is that only blind patent analysis using various method is performed without properly reflecting the requirements of stakeholders in the analysis process. Without systematic requirements analysis, it is difficult to plan the technology to ensure business success.

To overcome the limitations described above, in this paper, we proposed a semi-automated methodology for deriving technical information based on stakeholder requirements using NLP technology. The requirements are decomposed into structured semantic information based on grammar rules, and then the requirements based technology information can be derived by linking with the four technical information including function, component, context and purpose extracted from the requirement and patent. TF-IDF (Term Frequency – Inverse Document Frequency) and expert reviews are used as criteria for evaluating the importance of each information. Using extracted result, we visualized this information in network and matrix form for supporting qualitative analysis of technical information. The network can identify technical relation from node and edge, and matrix can suggest business opportunities from vacuum and intense area. The methodology has been verified through analysis of patent data related to Nano sensors.CHAPTER 1. Introduction 1 1.1. Problems of existing patent analysis 1 1.2. Necessity for Stakeholder Requirements Analysis 2 1.3. Needs for technical information mining based on NLP 4 1.4. Patent analysis application: IRM / Network 5

본 논문에서는 설계자가 자연어(natural language)문답으로 E-R 모델(Entity-Relationship Model)을 형성하는데 도움을 주는 시스템에 관해 연구하였다. E-R 모델 조건은 기초로 시스템과 설계자와의 대화로써 E-R 모델을 형성하도록 하며, 이러한 E-R 모델 조건은 경험적(heuristic)인 지식에 의해 제공된다. 경험적인 지식은 E-R 모델링할 때 위반 사항에 대한 의미적 해석에 사용되며, 생성된 E-R 모델은 내부 표현인 Prolog 언어의 사실(fact)로 변경된다. 본 시스템은 IBM PC/AT의 MS/DOS상에서 수행되는 Turbo Prolog로 구현되었다.This paper describes an experimental system that assists designers to develop an E-R(Entity-Relationship) model in natural language dialog.
The system uses a set of well-formed E-R model requirements to guide designers in E-K modelling. The guidance is based on a set of heuristics that are defined for violations of the well-formed E-R requirements. These heuristics are semantic interpretations of the violations and E-R model is translated into Prolog facts as internal representation. This System is implemented in Turbo-Prolog on MS/DOS of IBM PC/AT.목차 초록 = iii 第1章 序論 = 1 第2章 E-R 모델 = 3 2.1 E-R 접근 방법의 역사적인 관점 = 3



문제기술서를 클래스 다이어그램으로 변환하는 UML 도구가 현재로는 존재하지 않는다. 기존의 UML 도구에 새로운 기능을 추가하는 것은 시스템 무결성을 유지하기 위한 문서가 필요하기 때문에 새로운 UML 도구를 개발하는 것보다 더 복잡하다. 본 논문에서는 문제기술서로부터 클래스 다이어그램의 요소들을 식별하기 위해 자연어 처리(NLP)기법을 이용하고 식별된 요소들을 화면에 나타내기 위해 WPF(Windows Presentation Foundation)플랫폼을 사용하여 새로운 UML 도구를 개발하고자 한다.
클래스 다이어그램의 요소들에 대한 식별은 클래스 이름 후보에 대한 식별, 속성 및 연산에 대한 식별, 그리고 클래스의 관계에 대한 식별 등 여러 단계로 구성된다. 식별 알고리즘은 문맥 자유 문법(CFG)을 사용하여 문자열의 패턴을 생성하며, 패턴을 사전과 대조한다. 또한 종속성 분석 알고리즘은 종속성 구문 분석과 유사성 검사를 통하여 속성 정보를 추출하고 클래스 간의 관계를 분석한다. 알고리즘의 가장 중요한 특징은 종속성 구문 분석을 이용하여 클래스 다이어그램의 관계를 식별한다는 것이며, 여러 단계의 정제 과정을 거쳐 추출 결과를 향상시킨다는 것이다.
성능평가를 위하여 도서관 관리 시스템, 온라인 가게, 기상 예보 시스템 그리고 현금 자동지급기(ATM)에 대한 문제 기술서를 분석하고 클래스 다이어그램의 요소들과 클래스간의 관계를 추출하여 클래스 다이어그램으로 나타내었다. 정제 과정을 거치기 전의 정확율 및 재현율의 평균은 각각 80%, 90%이다. 정제 과정을 거치면 정확율 및 재현율은 각각 100%와 100%로 높아진다. 성능평가의 결과는 정제 과정이 재현율과 정확율의 향상에 매우 중요함을 보여준다.Currently available UML tools for converting a problem statement into diagrammatic notation are limited. Adding new features in the software development phase is a major problem. Further, modifying an existing UML tool is more complicated than developing a new application because it requires documentation to maintain system integrity. Therefore, This study proposes the development of a UML tool with Windows Presentation Foundation (WPF) that is integrated with natural language processing (NLP) to identify class diagrams from a problem statement.
The identification of class diagrams comprises several phases; preprocessing, identification of class name candidates, attributes, operations, and relationships of class. The identification algorithm implements grammar rules (design rules) from previous studies that were constructed using context-free grammar (CFG) technique to generate patterns of strings. This study proposes refinement methods to enhance the extraction results. These methods comprise analyzing the relationship among classes with dependency parsing, similarity check, and extracting attribute information based on a dictionary. The most important feature in the refinement methods is the dependency parsing algorithm to identify the relationships of class diagrams.
The performance evaluation evaluates problem statements of library management system, online shop, forecasting management system, and automated teller machine. Before the application used refinement methods, the average of precision and recall are 80% and 90%. The usage of refinement methods increased the average of precision and recall to 100% and 100%. The performance evaluation shows the refinement methods can improve the percentage of recall and precision.1. Introduction 1 2. Literature Review 4 2.1 Natural Language Processing 4 2.2 WPF Framework 5 2.3 spaCy 6













우리가 데이터베이스의 데이터에 접근하기 위해서는 기본적으로 SQL과 같은 데이터베이스 쿼리 언어를 사용한다. 하지만, 데이터베이스 쿼리 언어에 익숙하지 않은 사용자에게는 이런 언어를 익히기 위한 문법이 쉽지가 않다. Natural Language Interfaces for Database system (NLIDBs)이란 데이터베이스 쿼리 언어를 사용하지 않고 자연어를 이용하여 데이터베이스의 데이터를 이용하고자 하는 시스템을 말한다.
NLIDBs는 1960년 중반부터 연구가 시작 되었으며, 그 동안 많은 기법들이 소개되어져 왔다. 본 논문은 여러 기법들 중에서 중간 매개 표현 언어 (intermediate representation)를 사용하여 도메인이 바뀌더라도 사람의 개입이 거의 없이도 적응할 수 있는 방법을 제안한다. 같은 의도의 질문이라도 사람은 다양한 표현을 사용하여 질문을 할 수 있으며, 데이터베이스의 디자인 역시 다양하게 설계되어질 수 있다.
중간 매게 언어는 사람의 언어 표현과 데이터베이스의 스키마 사이에서의 차이(gap)를 줄이기 위한 역할을 한다. 언어 분석 단계에서의 의존 파서의 결과는 의미사전을 기반으로 중간 매개 표현 언어로 변환되고, 이 중간 매개 표현 언어는 변환 규칙을 이용하여 SQL과 같은 데이터베이스 질의 언어를 생성되어 진다.
제안한 모델은 버스 예약과 영화 소개의 도메인에서 실험을 하였다. 먼저 동서울 터미널 게시판의 사이트에서 질의를 수집하여 버스 예약 도메인에서 시스템을 테스트 및 실험하였고, 시스템에 대한 수정 없이 새로운 영화 소개 도메인의 데이터베이스 스키마 정보만을 제공하여 새로운 도메인에서의 적응력을 테스트 해보았다.
버스 예약 도메인에서는 215개의 질의로 실험을 하였고, 이 중에서 179개의 질의를 정확하게 SQL로 변환하여 83.26%의 정확도를 보였다. 그리고 새로운 영화 소개 도메인에서는 86개의 질의 중 72개를 정확하게 변환하여 83.72%의 정확도를 보였다. 버스 예약 도메인에서 새로운 영화 소개 도메인의 적응에 사람의 개입이 거의 없이 데이터베이스 스키마만 제공함으로써 비슷한 정확도를 보임을 알 수 있었다.In order to access data in database, we utilize database query language such as structure query language (SQL). However, these languages are complex and difficult for users not to be familiar with database. Natural language interface to a database system (NLIDBs) provides that users can access data in natural languages instead of database query language.
NLIDBs had already appeared in the late sixties. This thesis is focused on minimizing human intervention for natural language interface and proposes intermediate representation language which can adapt to a new domain with a little human intervention. Although a question with same intention, users can use various linguistic expressions. Database can be also designed variously in a domain. The purpose of intermediate representation is to narrow the gap between the linguistic expressions and database query languages in database.
In this thesis, an approach to realize intermediate representation language is to use semantic dictionary. The dictionary can be automatically constructed from database schema. The result of dependency parser with semantic dictionary is translated into intermediate representation. Then, This intermediate representation is automatically translated into database query language with some translation rules.
The domain for our experiment is bus-guidance and movie-introduction. The implemented system first has constructed NLDBIs for bus guidance domain. Without altering any system modules, the system has been provided only database schema and lexical dictionary intervened a little by domain expert for movie instruction domain. Then, the system has experimented for a new domain.
In bus guidance domain, The implemented system has generated correct SQL expressions 179 among 215 queries (83.26%). In a new domain, movie introduction, this system has generated correct SQL expressions 72 among 86 queries (83.72%). The rate of correct queries in movie introduction domain is similar to bus guidance domain and shows high correctness.

A Study on Deep-Learning for Speaking Assessment of Korean Language : Focused on Research Trend in Natural Language Processing

The purpose of this research study is to provide the theoretical base for developing deep learning-based programs for the test of speaking proficiency in Korean. By beginning to conduct a research on it, this study is going to develop programs to evaluate spoken language skills of Korean learners, based on the results of machine learning that Artificial Intelligence (AI) undertakes ultimately, using a spoken corpus in Korean. Thus, this research study meant to understand technological standards that can be met now and present the future research direction. To achieve the purpose, this study examined about the cases of English where National Language Processing (NLP) technologies are researched and utilized more and more than in the cases of the Korean language, to discuss methods for incorporating deep learning-based NLP technologies into area of evaluation, especially, in Korean language education.
In this regard, this study began to understand and consider the theoretical background related to evaluation of grammatical ability in spoken Korean, based on the characteristics of spoken language, and then analyzed the research direction of NLP at home and abroad. Also this study investigated the current status of NLP at home before understanding the functions and principles of spelling checker for the Korean language and automated scoring system for supply-type item. Subsequently, this research study investigated the development stage of NLP technologies and the cases where NLP technologies were utilized for the evaluation of foreign language proficiency of L2 (second language) learners. Among any other things, this study made an exhaustive analysis and review of difficulty in test that could be directly related to speaking test, and contents of automated scoring programs.
This study went one step further to propose deep learning as a part of a bid to complement the limits of NLP technologies that are being used now, and analyzed the contents investigated regarding the concepts of deep learning, and the availability of NLP, explaining why deep learning technologies are suitable for programs for the test of speaking proficiency in Korean, with a remark that data about a spoken corpus of learners for machine learning is necessary. Also, this research study considered that it is necessary to present the level of difficulty in applying deep learning technologies and develop such technologies step by step, based on factors of speaking test. In addition to these, this study considered about principles by which evaluation is being made when applying NLP technologies into the specific examples of questions of evaluation of ability in spoken Korean.
Lastly, this study discussed the research direction that Korean language education ought to head in order to implement deep learning-based programs for the test of speaking proficiency in Korean, and proposed about how such programs will be developed and applied in the future.1. 서론 1 1.1. 연구 목적 1 1.2. 선행 연구 4 1.2.1. 한국어 말하기 평가 관련 연구 5 1.2.2. 한국어 자연어 처리 관련 연구 6



Wie bekannt la‥ßt Fillmoredie Dichotomie der 'internen' und der 'externen' Sematik ungelo‥st. In der intensionalen Logik wird oft eine 'disambiguierte' Ebene vorausgesetzt, die mehr oder weniger genau der 'internen' Semantik entsprechen ko‥nnte und die durch eine formal-semantische Interpretation mit der Menge der Propositionen verbunden wird. Auch dann bleiben manche Fragen unklar, die vielleicht - wie die Variabilitat der an einzelne Theorien gebundenen wissenschagtlichen Termini - als beschra‥nkte Einzelfragen betrachtet werden ko‥nnen.
In manchen Richtungen der heutigen theoretischen Linguistik wird vorausgesetzt, daß die 'interne' Semantik nicht einzelsprachlich gebunden ist. Fu‥r diese Annahme gibt es aber keine U‥bersetzungen Argumente: ebenso wie im Lexikon z.B. den englischen Verben go-walk-ride usw. oder speak-talk-tell usw. keine Menge von Oppositionen im Deutschen genau entspricht, gibt es auch im Bereich der Grammatik Diskrepanzen in verschiedenen Sprachen, wie die zwischen den Systemen der Verbalaspekte und Tempora, der Artikel usw.
Deshalb scheint es angemessener vorauszusetzen, daß es eine Ebene der sprachlichen (wo‥rtlichen) Bedeutung gibt, die zwar einzelsprachlich bedingt, aber doch frei von Ambiguitat ist und die als Grundlage sowohl einer semantischen Interpretation im Sinne der formalen Logik dienen kann, als auch eines U‥bergangs zur psychologisch bedingten Wissenstrukturierung, die im experimentellen Bereich der ku‥nstlichen Intelligenz mit Hilfe von Szenarien, Rahmen u.a. beschrieben wird; auch die metaporischen Bedeutungen konnen vielleicht vor Hintergrund der wo‥rtlichen Bedeutung ero‥rtert werden.
Eine solche Konzeption der sprachlichen Bedeutung und ihrer Beziehung zur Semantik der Wahrheitswerte ergibt eine realistische Arbeitsteilung zwischen linguistischer und logischer Semantik: Eine Aufgabe der Linguistik ist es dann, von der a‥ußeren Form der Sa‥tze einer naturlichen Sprache zur Ebene der sprachlichen Bedeutung zu kommen. Zu den Aufgaben der Logik geho‥rt es, ein universales formales system zu konstruieren, das alle fur die Wahrheitswerte relevanten Oppositionen formal erfassen kann. Die Entsprechungen zwischen den Repra‥sentationen der sprachlichen Bedeutung der Sa‥tze und den Formalen des logischen Systems solltendann in Zusammenarbeit von Logikern und Linguisten gelo‥st werden.
Wenn die Ebene der sprachlichen Bedeutung als die Menge der Repra‥sentationed der Satzbedeutungen aufgefaßt wird (ohne Ambiguita‥t), dann muß zuerst gefragt werden, wie die Struktuierung dieser Repra‥sentation aussieht, welche Hierarchien, Einheiten und Kategorien bestehen und welche Kriterien zu ihrer Unterscheidung gebraucht werden ko‥nnen.
Die Grundzuge der Bedeutungsrepra‥sentationen ko‥nnen der traditionellen Linguistik entnommen werden. Denn keiner der modernen Ansa‥tze hat entscheidende Argumente gegen das angefu‥hrt, was in dieser Hinsicht schon la‥nger bekannt war. Nur mu‥ssen auch solche Aspekte beru‥cksichtigt werden, die in dieser oder jener pra‥systematischen oder formalen Konzeption am Rande der Aufmerksamkeit geblieben sind.목차 1. 서론 = 1 2. 의미해석상의 제반문제 = 3 2.1. 의미해석 = 3 2.2. 낱말의미와 격삽입소의 탈애매성 = 7

사람과 사물이 인터넷에 쉽게 연결될 수 있는 사물인터넷 분야가 연구 되고 발전하고 있다. 그 중에서도 사물간의 상호성에 대한 연결, 관계에 대한 연구가 관심을 얻기 시작하였다. 추가적으로 사람이 사물과 자연어 를 통해 대화를 할 것으로도 내다보게 되었다. 따라서 사물인터넷 환경에 서 사람 및 사물, 사물 및 사물의 상호 연결 관계를 만들 수 있는 환경으 로 소셜 네트워크를 이용하는 방법을 생각하였다. 본 논문에서는 소규모 의 소셜 네트워크 환경을 구축하고 사람과 사물이 기계적인 대화를 할 수 있도록 실험하였다. 소셜 네트워크상에서 사물 또한 사람처럼 대화할 수 있을 수준의 개체로 인식할 수 있게 고유의 계정과 관리 시스템을 구축하 였고, 장치의 세부 정보를 프로파일로써 저장하여 관리할 수 있게 하였 다. 대화를 위해 소셜 네트워크에 글을 작성하면 해당 글을 읽어들이여 어떠한 사물과 대화를 진행할지 결정하고, 어떠한 정보를 받아들여서 실 행해야 하는지를 판단한다. 그에 따라 추론된 결과와 제어할 정치 정보를 이용하여 장치에게 명령을 전달하고, 장치의 실행 결과를 받아와서 다시 답변하는 형태로 구현을 진행하였다. 대화 수준은 간단한 기계적 수준의 대화를 진행하였다. 이를 통해 소셜 네트워크상에서 사물이 사람과 같이 대화를 알아듣고, 그에 따라 제어되고, 그 결과를 알려줌으로써 대화를 통 해 사물을 제어할 수 있다.The Internet of things that can easily connect people and things to the Internet is being researched and developed. Among them, research on connection and relationship to the mutuality of objects has begun to get attention. In addition, I was also looking for people to talk through things and natural language. Therefore, I thought about using social network as an environment to create interconnection relation of people and things, things and things in social environment of things. In this paper, we construct a small– scale social network and experiment to make a human conversation like people. I have built a unique account and management that objects can be recognized as objects that can communicate with each other on a social network, and the device can be stored and managed as a details profile. When you write on a social network for a conversation, read the article, decide what things want to talk to, and decide what information need to take. The result is transmitted to the device using the inferred result and the political information to be controlled, and the result of the execution of the device is received and then answered again. Dialogue level was a simple mechanical level. This allows objects on the social network to take control of the conversation as well as to get control of the conversation and to inform the results of the conversation.1. 서론 1 2. 연구 배경 3 가. machine to machine 및 관련연구 정리 3 나. machine to machine 연구 정리 3 다. machine to machine 관련 연구 추적 7

목 차 표제지 = 0 요 약 = i 목 차 = iii 그림 목차 = v

목차 표제지 = 0 목차 = i 요약 = 0 Abstract = 0

Learning is carried out by rote, speech, examples or analogy.
Problem representations are classified to descriptors space representation require representation, tree representation being used in the game etc.
In this paper, the Tower of Hanoi is taken as an example of data moduler. Problem is subdevided into easier problems and state-space is described.
Prolog is used to process the program.목차 = 1 도목차 = 2 ABSTRACT = 3 Ⅰ. 서론 = 4 Ⅱ. 학습의 고찰 = 6

시맨틱 웹의 발전과 더불어 기존의 정보자원을 RDF, OWL과 같은 웹 온톨로지 언어를 활용하여 온톨로지로 구축하는 사례들이 증가하고 있고 온톨로지에 기반한 검색 기술도 날로 발전되고 있다. 하지만 기존의 검색 방법은 키워드 기반의 검색으로 문서 내에 해당 키워드의 포함 여부를 확인하여 검색 결과를 도출하였기 때문에 사용자의 의도에 맞는 결과를 제공하지 못하였다.
본 연구에서는 사용자의 자연어 기반의 질의에 대한 의미 해석을 통해 온톨로지에 질의 가능한 그래프 패턴의 구조적 질의 형태로 변환시키는 방법을 제안하고자 한다. 논문에서 제안하고자 하는 바는 단순히 질의어가 포함된 결과가 아닌 질의어들간의 관계를 파악하고 그 관계를 기반으로 구조적인 온톨로지 쿼리를 생성함으로써 사용자의 질의 의도에 보다 적합한 검색 결과를 제공할 수 있을 것으로 기대한다.

Speech synthesis is the process of extracting and pronouncing of speech by using a computers or electronic.
Especially in speech synthesis area, the Text-to-Speech system have been investigated far many years aimed principally at realizing typewriters and robots capable of speech synthesis.
In this thesis, the Text-to-Speech system is implemented using a phoneme code that is supported by general sound card in personal computer, And this system is aimed at for the development of the intelligibility and natuality.목차 (ABSTRACT) = ⅰ Ⅰ. 서론 = 1 Ⅱ. 관련 연구 = 3 2.1 음성 합성의 분류 = 3







The most representative phenomena of rapidly changing information society today is explosively increasing digital information.
People usually collect lots of digital information through internet. However, it could be diffcult for them to build inquiry, reflecting precise query intention according to the character of search engines.
Especially, elementary school students or people having a little learning experiences don't have enough vocabulary to reflect their thoughts.
Such people frequently use pronouns for expressing their thoughts.
Even, some people who have affluent vocabularies to express their thoughts sometimes use pronouns when they can't think of certain words.
Such phenomenon is appeared as it is when people use pronouns in natural-language query for web page search.
Question Answering System itself can build database in order to form correct answer sentence for the query. Thus it shows correct answer sentence for query having interrogative pronouns. Besides, various portal sites let users select correct answer for their query so that it plays the role of QA system.
However, in usual web page search, it is necessary to change the query with interrogative pronouns to specific query that the user's intention for query could be more clarified.
It is shown that verbs´ acting information can be used to change interrogative pronoun to specific query language in this thesis that we could search web pages with abstracted key word to have more precise result like QA system. which has better accuraly like QA system, This people can search information from the changed query language with abstract key word.
For the purpose of it, the study selected the words of high, middle, low frequency among frequently used words and certain number of verbs having various acting information in order to see respective subjects and objects of acts and to show it by the table.
This study will show that if such acting information of verbs would be build for every verb, it could be an important knowledge base.제 1장 서론 = 1 1.1. 연구배경 및 목적 = 1 1.2. 연구 내용 = 2 1.3. 논문 구성 = 3 제 2장 자연어 질의 처리 기법 = 5



In this thesis, we introduce an adaptive convolution for natural language understanding to give stronger flexibility to existing convolutional neural networks (CNNs). Traditional convolution uses the same set of filters regardless of different inputs. Unlike the traditional convolution, adaptive convolution utilizes adaptively generated convolution filters which are conditioned on inputs. To this end, we attach filter-generating networks, carefully designed networks to generate input-specific filters, to each convolution block in existing CNNs. We show the efficacy of our approach in existing CNNs based on our performance evaluation. Our evaluation indicates that adaptive convolutions improve all the baselines, without any exception, as much as up to 2.6 percentage point (%p) on sentiment analysis, 1.6 %p on text classification and 3.6 %p on textual entailment.1 Introduction 1 2 Related Works 3 2.1 Natural Language Understanding 3 2.1.1 Text Classification 3 2.1.2 Sentiment Analysis 4





















서론: 본 연구의 목적은 통제자연어 형태의 진료서술문을 표현하기 위한 온톨로지를 시멘틱웹 온톨로지 언어 OWL을 통해 모델링하고 진료서술문 작성 시 발생할 수 있는 규칙을 시멘틱웹 규칙 표현 언어인 SWRL을 통해 표현하고 이를 기반으로 인터페이스들을 구현 하는 것이다. 더 나아가 구현 된 인터페이스에서 올바른 통제자연어 입력을 위해 제공하는 용어 추천 목록의 순서를 다양한 관점을 산출한 용어의 사용 빈도를 고려하여 정하였다.

방법: 의료 서식 온톨로지는 서식 자체에 대한 정보인 Template attribute와 서식에서 사용될 수 있는 개념의 집합 인 Clinical entity로 분류하여 모델링 하였다. Template attribute는 서식의 제목과 설명을 포함하여 규칙에서 사용할 수 있는 선택 사항 Template option을 표현한다. 예를 들어 대장암 병리보고서 작성 시 기술 하고자 하는 검체의 종류에 따라 진료서술문 기재사항의 내용이 변경되므로 검체의 종류는 Template option으로 표현 될 수 있다. Clinical entity는 의무 기록이 표현하고자 하는 대상을 묘사하기 위한 개념과 기재 사항을 표현한다. 의무 기록 작성 규칙은 서식의 선택 사항이나 특정 기재 사항의 값에 따라 다른 기재 사항 값의 범위가 변경 되는 것을 표현하였으며, 기재 사항의 값 범위 변경 시 새로운 값의 범위로 변경하는 ‘Replace’, 혹은 기존의 값의 범위에 새로운 값들을 추가하는 ‘Merge’의 두 가지 방법으로 변경한다. 또한, 용어추천 시 용어자체, 의무기록자, 환자의 3가지 관점에서 사용 빈도를 산출하여 최근 많이 사용한 용어 일수록 높은 점수를 부여한다.
결과: OWL과 SWRL로 의무기록 서식 온톨로지와 규칙을 저작할 수 있는 저작 인터페이스와 저작 된 결과물을 활용하여 구조적인 진료서술문을 작성할 수 있는 통제자연어 진료서술문 작성 인터페이스로 구성 된 Owlpath를 구현하였다. 이 중, 작성 인터페이스는 규정된 문법과 의무기록 서식 온톨로지 및 규칙을 통해 사용자가 입력할 수 있는 용어를 추천하고 상이한 점이 있을시 이를 알려주어 올바른 통제자연어 진료서술문 작성을 유도한다. 작성 인터페이스로 작성 된 진료서술문은 RDF로 출력 하였다.

결론: 구현 된 Owlpath 인터페이스들을 활용하여 국내외에서 제안 된 병리보고서 기재사항 표준안 및 웹 폼 형태의 의료 서식들의 온톨로지 및 작성 규칙을 입력하고, 이를 통해 통제자연어 진료서술문을 작성한 뒤, RDF로 추출함으로써 제안 된 시멘틱 웹 기술을 이용한 의료 서식 및 규칙 표현의 의료현장 사용 가능성을 보여 주었다.초록 i 목차 iii 표 및 그림 목록 iv 서론 1 1. 연구의 필요성 1





공학 분야에서 감정 인식은 기계가 인간의 감정을 인식하기 위한 방법을 연구하는 학문으로써 최근 로봇, 스마트폰과 같은 인간과 상호작용하는 기기들이 실생활 속에 가까이 다가옴에 따라 더욱더 관심을 받고 있다. 감정인식은 어떤 모달리티(정보의 소통 경로)를 대상으로 하는가에 따라 세분할 수 있는데 대표적인 모달리티로 음성, 얼굴 표정, 생체 신호, 텍스트 등이 있다. 본 논문에서는 이 중 인간의 주된 의사소통 방식인 자연 언어 정보를 전달할 수 있는 자연어 텍스트 모달리티로부터의 감정 인식을 다룬다.
기존의 자연어 텍스트 모달리티기반 감정 인식연구들은 자연어 처리나 인공지능 분야의 공학자들이 주도적으로 연구해왔다. 이들은 감정 인식이라는 문제를 자연어 텍스트의 처리 단위(단어, 문장, 문단, 문서 등)내에서 인식된 감정 자질 정보에 따라 분류하는 문제로 접근하였다. 대표적인 접근법이 핵심어 검출 알고리즘을 사용한 연구들이다. 핵심어 수준을 넘어서 자연어 문장의 통사 정보, 의미 정보나 기타 언어 정보들을 사용하고자 한 연구들도 있었다. 하지만 이러한 기존연구들이 공통적으로 갖는 특성은 언어 자질로 구성한 문제 공간에서 기존의 분류 모델들을 사용한 분류를 한다는 것이다.
감정 인식은 감정이라는 주제가 갖는 특성을 충분히 잘 활용해야 한다. 본 논문에서는 감정에 대하여 가장 심도 있게 연구해온 심리학 분야의 감정에 대한 분석 이론을 이해하고, 이를 바탕으로 도출된 심리학적 감정 인식 기법을 공학적으로 모델링하여 새로운 형태의 감정 인식 방법을 제안 한다. 연구의 동기가 된 심리학 이론은 Lazarus의 감정 이론으로, 감정이 생성되기 위해서는 감정 주체가 처한 주변 상황, 주어진 상황에 대한 개인적인 이해, 감정 생성 규칙이 필요하다는 내용이다. 본 논문에서는 감정 생성을 위한 필수요소로서 상황과 성격을 정의내리고, 이를 감정 인식에 사용할 수 있는 형태로 모델링한다. 또한 자연어 문장으로부터 감정 주체, 감정 정보, 감정 대상을 추출하기 위하여 의존 문법 구문 분석을 통한 의존 관계 패턴을 사용한다.
최종적으로 상황 모델과 성격 모델, 그리고 종합적인 감정 인식 시스템을 실험을 통해 평가하여 제안한 감정 인식 방법의 우수한 감정 인식 성능을 증명한다.제1장 서 론 1 제2장 연구 배경 4 2.1 감정 인식 4 2.2 모달리티와 멀티 모달 감정 인식 4 2.3 감정에 대한 연구 5

4차 산업혁명은 디지털과 물리적, 생물학적 기술 영역의 경계가 사라지고, 기술적으로 융햡되는 것으로 인능지능과 같은 지능정보기술이 이를 이끄는 핵심 동인으로 평가되고 있다. 이로 인해 인공지능, 로봇 등 첨단 기술들을 기존의 산업계에 적용하려는 시도가 늘고 있다.
하지만, 인공지능을 산업계로 들여오기 위해서는 개별 산업의 서비스나 업무별로 발생하는 비효율성 문제의 특징을 파악하고, 전체 시스템의 최적화 관점에서 인공지능의 활용 방안이나 적용 기법을 발굴해야 한다. 하지만 지금까지 컨택센터와 관련된 인공지능의 연구는 컨택센터의 일부 기능들을 인공지능 기술을 통해 구현하는 연구가 대부분이었다.
기존의 연구 내용을 살펴보면 컨택센터가 가지고 있는 기능들의 일부를 보조하기 위한 수단으로 인공지능을 활용하는 방안에 대한 연구가 많았다.
하지만, 컨택센터를 구성하는 전체 계층에 대한 인공지능 기술의 적용에 관한 통합된 관점에서의 연구는 없었다. 이에 본 연구에서는 인공지능을 이용하여 컨택센터의 기능을 강화하고, 컨택센터의 지표를 향상시킬 수 있는 다양한 적용 기술들을 알아보고, 이를 활용하여 인공지능기반의 컨택센터 시스템에 대한 프레임워크의 기반을 개발하였다. 그리고, 고객 관점뿐 아니라 인공지능과 같은 정보시스템의 역량과 비즈니스적인 측면을 고려한 컨택센터의 품질평가 모델을 개발하였다.
먼저 컨택센터의 당면한 문제점들을 해결하기 위해 컨택센터의 핵심 요구사항을 인공지능 기술을 사용해 해결하기 위한 방안을 연구 개발하였다.
첫째, 콜센터 IVR의 초기 메뉴 서비스 추천 시스템을 개발하였다. 콜센터를 포함하는 컨택센터에서 고객이 전화를 하면 가장 먼저 응대하는 시스템이 IVR이다. 금융권의 경우 전체 콜의 90%를 IVR이 처리하고 있어서 IVR의 중요성은 더욱 커져가고 있다. 하지만 IVR의 특성상 서비스 선택에 시간이 많이 걸리고 이용이 불편하다. 이에, IVR의 사용성 및 만족도를 향상하고 IVR의 생산성 향상을 위해, 고객이 IVR에 접속하였을 때, 고객에게 가장 적합한 IVR 서비스를 추천하는 초기 메뉴 추천 시스템을 개발하였다. 추천 알고리즘은 고객 사용 패턴의 유사도를 분석하여 추천하는 방식과 인공신경망을 이용한 추천 기법을 개발하였다. 기존 업계에서 많이 사용되던 빈도 기반의 서비스 추천 기법과 비교하였을 때, 인공신경망 기법을 활용한 경우가 평균 적중률 62.18%로 기존의 빈도 방식에 비해 18% 이상의 적중률 향상을 보여주었다.
둘째, IVR의 추가 메뉴 추천 시스템을 개발하였다. IVR의 특성상 모든 메뉴를 음성 안내를 듣고 선택하기 때문에 서비스 메뉴의 단계가 많고 복잡할 경우 다른 서비스 메뉴로 이동하기 위해서는 상위 메뉴로 가서 안내 멘트를 듣고 번호를 선택하고 다시 하위 메뉴로 이동해야 하기 때문에 사용자들의 사용성 저해가 심하여 이용자들의 불만이 많고 사용시간이 길어져 시스템 자원의 효율성도 떨어지는 문제점이 있다. 이에 고객이 자주 사용하는 메뉴의 패턴을 분석하여, 서비스 메뉴 이용 후에 다른 메뉴로 가는 규칙을 개발하였다. 메뉴 이동의 패턴 규칙은 연관 규칙 분석을 사용하였으나, 빈발도가 떨어지는 메뉴는 연관 규칙 분석에서는 분석이 어렵기 때문에 유사도 분석 기법을 결합하여 추가 메뉴를 추천하였다. 추천 결과는 연관 규칙 분석 기반의 추천에서 58.11%의 적중률을 보였으나, 유사도 분석을 더한 경우 70.14%의 추천 적중률을 보여 TTI(Touch Tone Interface) 방식의 IVR에서 충분히 추천의 효과가 있음을 확인하였다.
셋째, 딥러닝을 포함한 시계열 분석을 통합 인입 콜량 예측 모델을 개발하였다. 컨택센터 산업은 인건비의 비중의 큰 산업이다. 컨택센터의 응대는 실시간으로 이루어지기 때문에 고객의 콜량이나 접촉량에 맞는 적정 상담사를 배치하지 않으면 영업 손실이 발생하거나 운영 비용의 과다 지출이 발생할 수 있다. 그래서 컨택센터 운영자의 중요한 의사결정 작업은 비용을 최소화하면서 고객 응대 비율을 유지할 수 있는 적정 수준의 상담 인력 수준을 결정하는 것이다. 이를 위해서는 정확한 콜량 예측이 선행되어야 하는데, 국내 외에 이에 대한 연구는 많지 않고, 실제 콜센터에서는 담당자의 경험과 직관에 의지한 단순한 계산 방법이 여전히 주로 사용되고 있다. 이에 분해기법(STL), 평활화(TBATS), ARIMA, 회귀분석(ARIAMX), 인공신경망 기반의 TLFN, 딥러닝 기반의 RNN-LSTM기법을 활용하여 콜센터의 실제 데이터를 기반으로 콜량을 예측하는 모델을 만들고 각각 최적화한 후 성능을 비교하였다. 개발된 모델의 예측 결과는 딥러닝 기법인 RNN-LSTM이 MAPE가 3.9%, RMSE가 511.5로 가장 우수한 예측 결과를 도출하였다.
넷째, 시뮬레이션을 통한 옴니채널 컨택센터의 상담사 배치 최적화 방안을 개발하였다. 기존의 콜센터는 전화만 응대하는 단일 채널이었는데, 옴니채널 컨택센터의 경우 기존 전화와 다르게 이메일과 같은 비동기 처리 채널, 채팅과 같은 동시 다수 처리 채널 등이 혼재되어있어 기존의 전화 상담과 같은 상담사 배치 전략을 쓰기 어렵다. 이에 시뮬레이션 기법을 통해 옴니채널 컨택센터의 상담사 배치 방식에 따른 채널별 인입과 대기량을 실제 컨택센터의 인입량과 환경을 기반으로 모델링하고 시뮬레이션을 통해 분석하여 상담사 배치를 위한 최적 방안을 연구하였다. 상담사가 모두 멀티채널을 처리하는 블렌딩 전략을 취했을 경우 채널별로 상담사를 분리 배치를 한 경우의 최적 배치 전략 보다 더 많은 인입량을 처리하면서 대기건수는 5.7%, 대기시간은 4.4% 감소시킬 수 있는 것으로 확인되었다.
다섯째, 인공지능 기반 콜센터 실시간 상담 도우미 개발에 대한 사례 연구이다. 금융권 콜센터의 경우 800여 개 이상의 서비스 유형을 삼담사가 처리해야 하기 때문에 상담사의 업무 지식이나 경험이 무척 중요하다. 이에 통화 중에 실시간으로 상담사에게 대응 답변을 제공하는 인공지능 기반의 상담 도우미 시스템의 구축에 대해서 N은행의 사례를 기반으로 시스템 아키텍처와 지식구축 방안에 대해 연구하였다. 이를 통해서 실시간으로 고객과 상담사의 통화내용을 텍스트로 바꾸는 STT시스템의 구성 방안과 질의응답 처리를 위한 AI 엔진의 구축 방법을 확인하였다. 또한, 실제 상담사의 평가가 답변 내용이 93.1%의 긍정 반응을 얻으며 실제 업무에 유용함을 증명하였다. 본 연구는 실시간 STT, 질의응답 기술, 자연어처리 기술 등이 모두 결합된 인공지능 기술 기반의 전형적인 콜센터 지원 시스템이라고 할 수 있으며, 실제 금융권에서 구체적인 결과를 보여주는 첫 번째 사례 연구로써 중요한 의미가 있다.
여섯째, 자연어 의도 분석 관점의 기업 챗봇 서비스 품질평가 모델을 연구하였다. 인공지능 기술의 본격적 성장과 함께 고객 응대나 민원 서비스를 인공지능을 통해 수행하는 챗봇 서비스가 민간과 공공기관을 중심으로 확산되고 있다. 하지만 이런 챗봇에 대한 사용자 평가에 대한 연구는 많지 않다. 특히, 챗봇의 핵심은 사람이 표현하는 문장을 잘 이해하는 자연어 의도 분석이 중요한데, 이에 대한 국내 연구는 많지 않다. 이에 본 연구는 자연어 처리의 요소 기술을 선행 연구를 통해서 조사하여 구문 분석, 의미 분석, 유의어, 띄어쓰기, 어순 분석의 5가지 항목을 도출하였고, 이 부분을 평가할 평가 지표를 만들어 실제 은행, 카드사의 서비스 중인 챗봇에 적용하여 평가 모델을 실증하였다. 은행의 경우 9개 업무의 30개 서비스에 대해서 438건의 질의를 하고 결과를 평가하였으며, 카드사의 경우 6가지 업무에 35개의 세부 서비스에 대해 의도를 도출하고 의도별 14개 질의를 변형하여 502건의 질의를 하고 결과를 평가하였다. 본 연구는 챗봇의 자연어 의도 분석 관점의 평가 항목을 학문적으로 도출하고, 이를 최초로 실증하여 챗봇의 문제를 파악하고 서비스를 개선할 수 있는 평가 모델을 만들었다는 것이 중요한 의의를 가진다고 할 것이다.
그리고, 지금까지 연구된 내용과 기존의 선행 연구들을 모두 포함하여 실제 컨택센터 시스템에 적용 가능한 인공지능 기반 컨택센터 시스템 프레임워크의 기반을 개발하였다. 컨택센터는 전화를 응대하기 위한 전화 인프라와 채팅, 이메일 등 텍스트 기반의 상담을 하는 컨택시스템, 그리고 이들을 연동하기 위한 CTI 미들웨어, 셀프서비스를 위한 IVR 등 다양한 시스템들로 구성이 되어 있다. 인공지능을 통해 단순 상담을 일부 대체한다고 하더라고 앞서 연구에서 보았듯이 자연어 인식률이 높지 않은 상황이라 전문적인 상담이나 복잡한 상담은 상담사가 지속적으로 처리를 해야 하기 때문에 기존 컨택센터 인프라와 인공지능 시스템이 앞선 연구들을 지원하면서 유기적으로 운영할 수 있는 시스템 프레임워크가 필요하다. 이에 기존 컨택센터의 시스템 구성과 업무 프로세스를 분석하고 본 연구의 연구 내용과 선행 연구를 조사하여 컨택센터에 적용 가능한 인공지능 기술 요소를 도출하였다. 이를 기반으로 컨택센터에 인공지능을 적용할 경우의 업무 프로세스와 시스템 간의 연동 방안을 설계하고, 전체적인 시스템 프레임워크의 기반을 개발하였다.
마지막으로, AHP와 내용 분석을 이용한 컨택센터의 평가 모델을 연구하였다. 인공지능과 같은 고도화된 정보시스템 기반의 컨택센터의 품질을 평가하기 위한 평가 모델을 제시하여, 컨택센터의 평가 기준을 고객 서비스 관점뿐 아니라 정보시스템 관점에서도 평가할 수 있는 기준이 필요하다. 기존의 컨택센터 평가 관련 연구는 고객 관점의 평가 중심이다. 이에 기존 컨택센터, 콜센터의 평가 모델 연구와 정보시스템, 비즈니스 관련 평가에 대한 선행 연구를 통해 평가 후보 항목을 도출하고, 이를 내용 분석을 통해 정리하여 평가 항목과 계층 구조를 만들고, AHP를 통해 가중치를 계산하여 평가모델을 개발하였다. 그리고 이를 실제 5개의 컨택센터를 대상으로 적용하여 평가 모델을 실증하였다. 이를 통해 컨택센터에 대한 종합적인 관점의 평가 모델을 개발하였고, 기존의 고객 만족도 중심의 평가 결과와 다르다는 것을 확인하여 현업에서 차별화된 사용성을 검증한 것이 연구의 중요한 의의이다.
본 연구를 통해 컨택센터의 문제점과 이를 인공지능으로 해결할 수 있는 부분에 대한 구체적인 기법과 사례를 확인하였다. 그리고 이들을 실제 컨택센터 시스템에 적용하기 위한 프레임워크의 기반을 개발하여 제시하였다. 이를 통해 컨택센터 연구자의 입장에서는 적용 가능한 인공지능 기술에 대한 이해를 할 수 있고, 실제 적용하기 위한 구체적인 방안에 대해서도 프레임워크를 통해 설계할 수 있을 것이다. 그리고 인공지능을 연구하는 연구자 입장에서는 컨택센터라는 산업군의 당면한 문제를 이해하고 더 좋은 인공지능 기술을 가지고 있다면 적용할 수 있는 시작점이 될 것으로 기대한다.
또한, 이런 인공지능 등의 최신 정보기술 시스템을 포함하는 컨택센터의 종합적인 평가 모델을 통해서 보다 기술과 환경의 변화에 맞는 객관적이고 효율적인 컨택센터 평가 지표를 마련한 점이 본 연구의 중요한 의의라고 결론지을 수 있을 것이다.The fourth industrial revolution is the disappearance of boundaries between digital, physical and biological technologies, and technological convergence, which is considered a key driver of intelligence information technology such as an artificial intelligence. As a result, more and more high-tech technologies such as artificial intelligence and robots are being applied to existing industries.
However, in order to bring artificial intelligence into the industry, it is necessary to identify the characteristics of inefficiency problems arising from each service or task of the individual industry and to discover ways to utilize or apply artificial intelligence from the perspective of optimization of the entire system. However, most of the research related to the contact center has been done using artificial intelligence to implement some functions of the contact center.
There is a lot of research on how to use artificial intelligence as a means to assist some of the functions of contact center. However, there have been no integrated studies on the application of artificial intelligence technology to the entire hierarchy of contact centers. In this study, we investigate various application technologies that can enhance the contact center function by using artificial intelligence and improve the indicator of contact centers, and developed the framework of artificial intelligence based contact center systems. We also developed a model for evaluating the quality of the contact centers, which considers the competence and business aspects of the information system such as artificial intelligence as well as the customer 's viewpoint.
To solve the immediate problems of the contact center, we first studied and developed a plan to address the key requirements of the contact center using artificial intelligence technology.
First, the initial menu service recommendation system for call center IVR was developed. When a customer calls from a contact center that includes a call center, IVR is the first system to respond.
In the case of financial institutions, IVR is handles 90% of the total calls, so the importance of IVR is growing. However, due to the nature of IVR, it takes a long time to select a service and it is inconvenient to use. In order to improve the usability, satisfaction and productivity of IVR, we developed an initial menu recommendation system that recommends the best IVR service to the customer when the customer connects to it. The recommendation algorithm is based on analyzing the similarity of customer usage pattern and recommend a method using artificial neural network. Compared with the frequency-based service recommendation method, which was widely used in the existing industry, the average hit rate using the artificial neural network technique for recommendations is 62.18%, which is higher than the existing frequency-based recommendation by 18%.
Second, we developed an IVR supplementary menu recommendation system. Because of the nature of IVR, all menus listen to voice guidance, so if you have many steps in the service menu, you have to go to the top menu to select the number, There is a problem that users are dissatisfied, the use time is prolonged, and the efficiency of system resources is deteriorated. We analyzed the patterns of menu that customers frequently use, and developed rules go to another menu after using the service menu. We used the association rule analysis for the pattern rule of the menu movement, but the additional menu was recommended by combining the similarity analysis technique because it is difficult to analyze the menu which has low frequency in the association rule analysis. The recommendation result showed a hit rate of 58.11% in the recommendation based on the association rule analysis, but a recommendation hit rate of 70.14% when the similarity analysis was added. It is confirmed that the proposed recommendation system has enough recommendation effect in IVR of TTI (Touch Tone Interface) method.
Third, we developed a model to forecasting the total incoming call volume by time series analysis including deep learning. The contact center industry is a large industry of labor costs. Because contact center response is done in real-time, if you do not arrange appropriate agent according to the customer's call volume or contact volume, you may incur business loss or overspending of operating cost. Thus, the critical decision-making task of the contact center operator is to determine the appropriate level of consultant workforce to maintain the customer response rate while minimizing costs. In order to do this, accurate call prediction should be preceded. There is not much research on this in Korea. In actual call centers, a simple calculation method based on experience and intuition of the person in charge is still mainly used. A model for predicting the call volume based on actual data of the call center using the decomposition technique (STL), smoothing (TBATS), ARIMA, regression analysis (ARIAMX), artificial neural network based TLFN and deep learning based RNN-LSTM and optimized for each. The forecasted result of the developed model is that the deep learning technique RNN-LSTM has the best forecasting result with MAPE 3.9% and RMSE 511.5.
Fourth, a strategy was developed to optimize the staffing of Omni Channel Contact Centers through simulation. While the existing call center was a single channel for answering calls only, it is difficult to use a strategy for setting up an agent such as a traditional phone call because the Omni Channel Contact Center combines multiple channels such as e-mail, asynchronous channels such as chatting, etc. Through simulation techniques, the inflow and the amount of air per channel according to the method of placing consultants at Omni Channel Contact Center were modeled based on actual contact center penetration and environment, and analyzed through simulation to study the optimal method for placing consultants. If all consultants have taken a blending strategy that deals with multi-channel channels, it has been confirmed that they can reduce latency by 5.7% and 4.4% while handling more incoming and outgoing inputs than the optimal deployment strategy when placing agents separately on each channel.
Fifth, it is a case study on the development of an artificial intelligence-based call center real-time advisor. As more than 800 service types are handled by agents in the case of call centers in the financial sector, it is very important to have an agent's work knowledge or experience. In response, the Commission studied the system architecture and knowledge-building measures based on N Bank's example on the establishment of an artificial intelligence-based consulting helper system that provides responses to agents in real-time during the call. Through this process, we identified how to construct an STT system that converts the call contents of customers and consultants into text in real-time and how to establish an AI engine for Q&A processing. In addition, the assessment by the actual consultant received a 93.1% positive response and proved useful for the actual work. This study is a typical call center support system based on artificial intelligence technology combined with real-time STT, Q&A technology, and natural language processing technology, and it is important as it is the first case study that shows specific results in the real world of finance.
In the case of a call center in the financial sector, more than 800 types of service must be handled by an agent, so the consultant's work knowledge and experience are very important. In response, the Commission studied the system architecture and knowledge-building measures based on N Bank's example on the establishment of an artificial intelligence-based consulting helper system that provides responses to agents in real-time during the call. Through this process, we identified how to construct an STT system that converts the call contents of customers and consultants into text in real time and how to establish an AI engine for Q&A processing. In addition, the assessment by the actual consultant received a 93.1% positive response and proved useful for the actual work. This study is a typical call center support system based on artificial intelligence technology combined with real-time STT, Q&A technology, and natural language processing technology, and it is important as the first case study that shows specific results in the real world of finance.
Sixth, we studied the company chat-bot service quality assessment model from a natural language purpose analysis perspective. Along with the full-scale growth of AI technology, chat-bot services that perform customer response and civil service through artificial intelligence are spreading to private and public organizations. However, there is not much research on user evaluations of chat-bots. In particular, the core of chat-bots is the analysis of the intention of natural language, which is well understood by humans, but there are not many domestic studies. In response, this study investigated elemental technologies of natural language processing and derived five elements of parsing, semantic analysis, notes, spacing, and order analysis. The evaluation index was developed to evaluate these parts and applied to chat-bots in service by actual banks and credit card companies to demonstrate the evaluation model. In the case of banks, 438 inquiries and results were evaluated for 30 services of nine tasks. In case of credit card companies, 35 detailed services were derived for 6 tasks, 14 inquiries by intent were modified, and the results were evaluated before 502. In this study, it is important that the evaluation items of chat-bot's analysis of natural language intent are derived academically and, for the first time, it is demonstrated that chat-bot's problems are identified and a model has been developed to improve its service.
In addition, we have developed the foundation for an AI-based contact center system framework that can be applied to real-world contact center systems, including both the content studied so far and existing prior studies. The contact center consists of a telephone infrastructure for answering calls, a contact system for text-based counseling such as chatting, e-mail, and a variety of systems such as CTI Middleware to link them, and an IVR for self-service. As we saw in the previous study, professional counseling or complicated counseling needs to be dealt with continuously by agents because the recognition rate of natural language is not high, so the existing contact center infrastructure and artificial intelligence system needs a system framework that can operate organically while supporting advanced research. Thus, the system configuration and task processes of the existing contact center were analyzed, and the research contents and prior research were investigated to derive artificial intelligence technology elements applicable to the contact center. Based on this, we designed work procedures and interworking methods for applying artificial intelligence to the contact center, and developed the basis for the overall system.
Finally, the assessment model of the contact center using AHP and content analysis was studied. By presenting an assessment model to assess the quality of advanced information system-based contact centers such as artificial intelligence, the assessment criteria of the contact center needs to be evaluated not only from the customer service perspective but also from the information system perspective. Research on existing contact center assessments is central to assessment from a customer perspective. Therefore, assessment candidate items were derived through prior study of assessment model, information system, and business-related assessment of existing contact center and call center, and assessment items and hierarchy were organized through content analysis, and weights were calculated through AHP to develop assessment models. We applied these to five actual contact centers to demonstrate the assessment model. It is important to research that a comprehensive model of assessment was developed for the contact center, and that the results of assessment focusing on customer satisfaction were different from those of the existing customer service, thus verifying differentiated usability in the field.
Through this study, we identified specific techniques and examples of the contact center's problems and areas where they can be solved through artificial intelligence. The framework was developed and presented for applying them to the actual contact center system. This will give the contact center researchers an understanding of the applicable artificial intelligence technologies and allow them to design specific methods for actual application through the framework. And for researchers who study artificial intelligence, it is expected to be a starting point for them to understand the immediate problem of the industry group called the contact center and to apply it if they have better artificial intelligence technology.
In addition, it can be concluded that the study was important to have objective and efficient assessment indicators for the center's change in technology and environment through a comprehensive assessment model of the contact center that includes the latest information technology systems such as artificial intelligence.요약 i 표 목 차 x 그림 목차 xii I. 서 론 1

질의응답시스템(Question Answering System)은 자연어로 구성된 사용자의 질문을 분석하여 질문의도를 파악한 후, 사용자가 원하는 정확한 정보를 다양한 문서로부터 추출하여 구 또는 절단위로 정답을 제시해주는 시스템이다. 이를 위해 질의응답시스템은 질의처리 기술, 문서검색 기술, 정확한 응답을 추출하는 기술 및 추론기술 등이 사용된다. 이와 같이 질의응답시스템에 관련하여 위에서 언급한 여러 가지 기술들이 많이 연구되고 있다. 질의응답시스템은 질의를 처리하는 부분과 검색을 하는 부분으로 나눌 수 있으며 서로 밀접한 관계를 가지고 있다.
본 논문은 질의응답시스템에서 질의처리에 속하는 질의변환 방법에 초점을 둔다. 질의변환은 질의를 처리하는 기술 중 하나로 기존의 일반검색엔진을 사용하면서 질의변환방법을 통해 자연어로 구성된 사용자의 질문을 검색엔진에 적합한 질의로 변환시켜 검색함으로 전체적인 질의응답 시스템의 성능을 높이는 방법이다.
본 논문에서는 임의의 질문유형에 대한 응답을 대표하는 패턴을 추출하는 방법과 추출된 패턴을 이용하여 질의변환을 하는 방법을 제안한다. 먼저 임의의 질문유형에 대한 응답을 대표하는 패턴을 추출하기위해 N-gram을 이용하여 패턴을 생성하고 생성된 패턴들은 논문에서 제안한 패턴중요도측정척도에 의해 순위화 되어 최종적인 패턴을 추출한다. 그리고 사용자의 자연어 질문을 논문에서 제안한 질문유형 정규표현식에 의해 분류하고 해당되는 질문유형을 대표하는 패턴을 사용하여 질의변환 하는 방법을 제안한다. 본 논문에서 제안한 질의변환방법을 실험하기 위해 TREC-11과 유즈넷 뉴스그룹의 FAQ파일 940개를 데이터 집합 구축하고 사용자의 자연어 질의와 본 논문에서 제안한 질의변환 방법에 의해 새롭게 생성된 질의를 사용하여 일반 검색엔진에서 각각 비교 평가 하여 사용자의 자연어 질의보다 본 논문에서 제안한 질의변환 방법에 의해 생성 된 질의로 검색하였을 때 질의응답 시스템의 성능이 향상되었다. 그리고 의문사별로 구글 검색엔진에서 실험을 한 결과 모두 정답정확률이 향상되었고 그중 what질문에서 가장 많은 15%가 향상되었다.그림 및 표 목차 <그림 1> Wu와 Pottenger이 제안한 패턴 예 5 <그림 2> Wu와 Pottenger의 패턴 학습 흐름도 6 <그림 3> 질의응답 시스템 구성도 7 <그림 4> Harabagiu 시스템의 패턴 적용 예 8

Objectives: This study proposes a credit risk evaluation model using company's news information. By adding natural language understanding technology this model could be useful when natural language query is given. Furthermore, this model improves timeliness and classification performance by employing news information compared to the financial statement-based model.
Methods: The methodology consists of two phases. First, representing the news that consisted of variable length natural language data as a fixed length numerical vector. Second, a statistical model is developed to classify the news into bankrupt firm's and normal firm's. In the first phase, the keyword scoring method used in previous studies and the paragraph vector(Le and Mikolov, 2014) proposed in this study are compared. Paragraph vector is a state of art method related to natural language processing in the machine learning field, which is based on distributional hypothesis in linguistics. The representation based on distributional hypothesis aim to make similar words, sentences, and documents closer in the vector space, whereas words, sentences, and documents containing different meanings are positioned farther from each other in the vector space. Paragraph vector employs a neural network that predicts context words or target words from represented words, sentences, or documents. Through this, a general representation vector is created. In contrast, the keyword scoring method extract relative frequencies of words after checking all news label about bankrupt firm's and normal firm's. This will make a difference in how the model is utilized in the future. In the second phase, statistical classification model employs a logistic regression for comparison. Here, a traditionally used financial statement-based model is also compared.
Results: The model using news information performed better than the model using financial statements in terms of AUROC values. No significant differences in classification performance were revealed between the keyword scoring method and the paragraph vector. However, in terms of reasonable level of predicted default probability the paragraph vector performed better when natural language query is given. Keyword scoring method showed unreasonable results corresponding to some queries with words that did not appear in bankrupt firm's news. It’s expected probability of default was similar level with meaningless characters such as blank("") or just repeated characters("xxxxxxx"). It is because that when using keyword scoring method, words that did not appear in bankrupt firm’s news have zero value as independent variable, regardless of whether they are good news such as "New drug development, FDA approval" or not. On the other hand, when using the paragraph vector method, a reasonable probability was given.
Conclusions: The method using news information showed better results in terns of classification performance than the financial statement-based model. It also had an advantage in terms of utilizing timely information. A similar level of performance was revealed between the keyword scoring method and the paragraph vector as a method that represent news as a vector. However the paragraph vector showed more advantages in terms of its applicability to other dependent variables and utilization in broad fields, the flexibility of arbitrary natural language queries, and being less affected from analyzer’s experiences and subjectivity.목적: 본 연구에서는 기업의 뉴스정보를 이용하여 신용위험을 측정하는 모델을 구축하는데 뉴스정보라는 자연어를 이해하는 기능(natural language understanding)을 추가하여 뉴스뿐만 아니라 사용자의 자연어 질의에 대해서도 기업의 신용위험 수준을 나타낼 수 있도록 하는 방법론을 제시한다. 또한 재무제표 기반의 신용평가모형에 더하여 뉴스정보를 이용하는 것이 신용평가에 대한 적시성을 높이고 보다 높은 수준의 변별력을 얻을 수 있음을 보인다.
방법론: 방법론은 두 단계로 이루어진다. 첫 번째는 뉴스라는 자연어로 이루어져있고 단어의 수가 가변적인 데이터를 통계적 모형에서 사용할 수 있도록 수치화하고 고정된 길이의 벡터로 나타내는 것이다. 두 번째는 뉴스를 부실기업에 대한 뉴스와 정상기업에 대한 뉴스를 분류하는 통계 모형을 구축하는 것이다. 첫 번째 단계에서 기존 연구에서 활용된 키워드 가중치 부여 방식과 본 연구에서 제안하는 방법인 paragraph vector(Le and Mikolov, 2014)를 비교하였다. paragraph vector는 머신러닝 분야에서 자연어처리 관련 최신의 방법론 중 하나로 언어학에서의 분포적 가정(distributional hypothesis)에 기반하고 있다. 분포적 가정에 기반을 둔 자연어의 표현 방법론들은 유사한 단어, 문장, 문서들은 벡터공간상에서 서로 가깝게 분포하도록 표현하고, 서로 상이한 의미를 가지는 단어, 문장, 문서들은 서로 멀리 떨어져 있도록 표현하는 것을 목표로 한다. paragraph vector는 단어, 문장, 문서를 벡터공간상에 나타내기 위해 주변 맥락 단어(context words) 혹은 목표 단어(target words)를 예측하도록 하는 인공신경망(neural network)을 이용한다. 이를 통해 뉴스에 대한 분류 라벨을 사전적으로 보지 않고 뉴스를 일반적으로 표현할 수 있는 벡터를 생성해낸다. 이에 반해 키워드 가중치 방법은 먼저 부실기업을 나타내는 뉴스와 정상기업을 나타내는 뉴스를 보고 단어의 빈도를 추출해야한다. 이는 향후 모델의 활용에 있어 차이를 가져온다. 두 번째 단계에서 통계적 분류모형은 로지스틱 회귀모형을 이용하여 비교하였다. 이때 전통적으로 사용되는 재무제표 기반의 모형과도 비교하였다.
결과: 뉴스 정보를 이용한 모형이 재무제표를 이용한 모형에 비해 AUROC값 기준으로 더 높은 성과를 보였다. 뉴스를 통계 모형에 활용하기 위해 수치화 하는 방법론을 비교해 보았을 때 키워드 가중치 부여 방법과 paragraph vector가 분류 성능에서는 큰 차이를 보이지는 않았다. 하지만 임의의 자연어 질의에 대한 부실 확률 예측값의 합리적인 정도를 비교해보았을 때는 paragraph vector가 우수하였다. 단어 빈도를 활용한 키워드 가중치 부여 방법은 부실로 분류된 뉴스에 등장하지 않는 단어가 자연어 질의에 나타났을 때 공백(“”)이나 의미 없는 문자(“xxxxxxx”)와 같은 수준의 부실 확률 예측값을 제시하여 합리적이지 않은 결과를 보였다. 부실 기업의 뉴스에 등장하지 않는 단어는 그것이 “신약개발, FDA승인” 같이 좋은 뉴스이거나 의미 없는 문자이건 간에 모두 통계 모형의 독립변수 값으로 “0”이 들어가기 때문이다. 반면 paragraph vector를 이용하였을 때는 합리적인 부실 확률 예측값을 제시하였다.
또한 키워드 가중치 부여 방법은 키워드를 선정하고 가중치를 부여하는 방법을 선택하는데 있어 분석가의 경험과 주관에 따라 성과의 차이가 크게 나타날 수 있다는 단점이 있다.
결론: 재무제표 기반모형보다 뉴스를 이용했을 때 분류 성능면에서 더 나은 결과를 얻을 수 있었고 적시성 있는 정보로 활용할 수 있다는데 장점이 있다. 뉴스를 벡터로 나타내는 방법 간에는 키워드 가중치 부여 방법과 paragraph vector가 유사한 수준의 성과를 보였다. 하지만 다른 종속변수에 적용 가능성, 임의의 자연어 질의에 대한 유연성, 넓은 분야에 대한 활용성, 분석가의 경험과 주관이 미치는 영향이 적다는 측면에서 paragraph vector에 장점이 있다.초록 i 표 목록 iv 그림 목록 v Ⅰ. 서론 1 1. 연구의 배경과 목적 1

최근 빅데이터가 이슈로 떠오르면서 수많은 정보들이 홍수와 같이 쏟아져 왔고, 이 홍수 속에서 원하는 정보를 기계가 아닌 인간의 언어로 효율적으로 찾고자 하는 요구가 높아지고 있다. 특히 컴퓨팅 파워가 커지고 인터넷과 휴대 매체가 활성화 되면서 영상물이 급속히 유통 되면서 이 영상물에서 원하는 정보를 쉽게 찾기 위한 검색 시스템이 요구되고 있다.
영상물을 검색하기 위해서 MPEG7 표준화와 같이 영상물에 메타를 태깅하는 연구와 영상물 특성을 반영한 검색 방법론에 대해 연구가 진행되고 있으며, 메타에서 색인 데이터를 생성하여 색인 단어 매칭을 통한 검색과 온톨로지를 기반으로 하는 시멘틱 검색이 대표적이다.
본 논문에서는 인간이 이해하는 영상물 특화 검색 방법을 목적으로 자연어 처리를 위한 메타데이터 생성과 검색 방법론을 제안하고자 한다. 기존 단순 텍스트 매칭 통한 검색시스템과 시멘틱 검색이 갖는 자연어 처리의 한계점을 LSP(Lexico-Semantic-Pattern, 어휘 의미 패턴)을 접목함으로서, 인간의 질의 구문을 이해하여 정확도 높은 검색 시스템을 연구하고자 한다.Recently, a lot of information has been poured into floods in big data and AI (artificial intelligence) issues, and there is a growing demand to search for desired information efficiently in human language rather than machine. Especially, as the computing power is getting bigger and the internet and the portable media are activated, video images are rapidly being produced and actively circulated, and a search method specialized for video images is required.
Traditional search methodologies include index search based on index data and semantic search based on ontology, and attempts have been made to increase the retrieval rate and accuracy of search results.
Recent researches for searching video contents are proceeding in two forms. One is the tagging of metadata in the video such as MPEG7 standardization, and the other is the search methodology reflecting the video characteristics.
In this paper, we propose metadata generation and retrieval methodology for natural language processing aimed at human - specific video - based retrieval methods. By combining the limitations of natural language processing of search systems and semantic search through existing simple text matching with Lexico-Semantic-Pattern (LSP), we aim to study a highly accurate retrieval system by understanding human query syntax.Ⅰ. 서 론 1 Ⅱ. 관련 분야 동향 및 연구 2 2.1 자연어 처리를 위한 글로벌 경쟁 2 2.2 영상 의미기반 검색을 위한 온톨로지 구조 연구 2 2.3 의미 분석을 위한 자연어 처리 연구 4

이 논문의 목적은 백석 시에 나타난 근대적 특성에 주목하여 그의 시선과 시간의식을 살피고 그의 시의 특성인 토속성 속에서 근대적 의미를 재구해 보면서 자연어로서의 백석의 시어가 30년대 후반 우리 시어의 발달 과정에서 발휘한 효과와 의미를 규명하는데 있다. 백석은 동시대 이미지스트들이 서구 이론을 성급하게 그리고 생경한 지식의 언어들로 실천하고자 했던 데서 오는 문제점을 선명한 언어 운용으로 극복한다. 그는 “언어의 고의성에로의 지향은 언어를 인공적인 것으로 만들며 생명이 없는 것으로 만든다”는 시어관을 피력하는데 이는 30년대 이미지스트 이론가들이 타개하지 못했던 혁신적 견해라고 판단된다. 또한 백석은 “자신의 과거를 기억하는 모더니즘”을 구현한 시인으로 그 동안 반근대적 특성으로 논의되어 온 '토속성', '과거의 기억',‘근원 지향성’등의 특징은 실은 근대 의식과 내밀하게 관련되어 표리 관계를 이루고 있는 것이다.
백석 시의 역동성은 거친 어투와 자연어를 그대로 취택하는 데서 온다. 백석이 거친 어투의 자연어 속에서 미감을 발견하고 그것을 과감하게 시어로 채택한다는 것은 그가 사물의 구체성에서 비롯한 안목, 즉 사물을 개념화하고 이상화하면서 사물과 거리를 만드는 것이 아니라 사물 그 자체의 즉자적 아름다움을 발견하는 근대적 인식체계를 가졌다는 것을 의미한다. 백석의 시어를 동시대 시인들 김기림, 이상, 정지용, 김영랑 들의 시어들과 비교의 관점에서 보면 백석은 김기림과 이상이 근대적 현실이라고 표현하였던 인공적인 언어에 저항하는 입장이다. 백석은 근대적 개념어에 저항하면서 토속적 자연어 그 자체에서 언어의 미감을 발견한다. 정지용이 전통적 언어를 그의 감각으로 조립하는 과정 속에서 시를 찾는다면 백석은 전통적 언어를 일상 의식 안에 있는 자연어 속에서 발견해 낸다. 이때 백석은 대상의 미추와 귀천을 구별하지 않고 모든 사물과 인간정서가 동등한 가치를 누리는 정신적 높이를 지향하고자 한다. 또한 김영랑이 음성구조를 지나치게 강조하여 의미 내용을 모호하게 처리하면서 대상을 신비화하고 음악적 장치를 통해 미화시킨다면 백석은 거친 그대로의 자연어로 현실의 역동성을 재현해 낸다.
30년대 후반 우리말 시어에 혁신적인 신장을 가져온 이들 시인들의 업적 위에 백석의 일상적 자연어가 발휘한 효과 그리고 당대의 현실을 주체적 시선으로 바라보고 표현한 백석의 감성을 덧붙여 논의해야만 1930년대 한국시의 근대성에 대한 논의는 심화 확장될 것이라 판단된다.I. 서 론 1 1. 연구 목적 및 방법 1 2. 연구사 검토와 문제 제기 12 Ⅱ. 근대 형식으로서의 시선 24 1. 근대와 표현 방식의 변화 24

최근의 시스템, 제품, 서비스들에 대해서 사용자 인터페이스(UI)와 더불어 사용자 경험 구축(UX)의 필요성이 증가함에 따라, 사용자 경험 조사의 중요도 또한 함께 상승하고 있다. 사용자 경험 조사는 사용자의 핵심적인 관심 및 포커스들을 추출하는 과정으로서, 사용자의 의도를 더욱 정확히 구하기 위해 정성적 방법인 맥락적 질의를 주로 사용하지만, 본 방법의 경우 해석에 있어서 많은 시간 및 자원의 소비 및 해석자의 역량에 따라 추출 데이터의 방향이 잘못 틀어지게 되는 단점 또한 가지고 있다.
본 연구에서는 사용자 데이터의 해석으로 인한 많은 자원의 소모를 효과적으로 줄이고, 해석자에게 있어 사용자 포커스를 더욱 원활하게 파악할 수 있는 지원 자료들을 생성코자, 자연어 처리를 통한 요약 및 추출 알고리즘인 텍스트 랭크 알고리즘을 인터뷰 데이터에 적합하도록 수정, 본 모델에 가장 효과적인 형태소 분석기 및 유사도 계산 함수 모델을 제안하여 인터뷰 데이터에서 사용자의 핵심 포커스 및 컨텍스트를 추출하는데 도움이 되도록 프로세스를 구축하고자 하였다.
따라서 본 연구과정에서는 관련 연구에서 사용된 유사도 함수 모델들을 각각 적용시킨 시스템이 생성한 결과 값과, 실험 참여자가 태깅한 평가세트를 비교함으로서 인터뷰 형식의 자연어 데이터를 처리할 때의 최적 모델을 제시하였다.제1장 서론 1 제1절 연구배경 1 제2절 연구목적 3 제3절 논문구성 5

이 논문에서는 자연어처리를 위해 'LR' 파싱 알고리즘에 기반한 차트 파싱의 하나를 제안한다. 이 논문에서는 기존의 알고리즘에서 실행시간과 메모리 사용량에 있어 개선된 알고리즘을 제안한다. 'LR' 파싱에 기반한 알고리즘의 가장 큰 장점은 미리 파싱을 위한 테이블을 만들어 놓는다는 것이다. 차트 파싱에서는 테이블에서 다음 상태로 갈 수 있는지 없는 지 미리 알 수 있다.

이러한 자연어처리를 위한 챠트파싱 형태의 알고리즘에 대한 연구는 Tomita에 의해 제안되었다. Tomita가 제안한 알고리즘은 전형적인 LR 파싱의 형태를 띄고 있다. 여기서 제안하는 알고리즘은 LALR 파싱의 방법을 사용하여 파싱의 상태를 미리 예견해 볼 수 있는 새로운 테이블을 구성함으로써 보다 파싱의 실행에 있어 시간이나 공간 사용의 측면에서 보다 효과적인 알고리즘을 제안하고 있다.

이러한 알고리즘은 또한 자연어 상의 오류에 대해 적용하였다. 오류 처리를 위해서는 챠트파싱에 기반한 여러 가지 정책들이 고려되었는데 그 중에서도 자연어에 나타나는 오류처리를 위해 Metamorphosis Grammar를 위한 recursive가 고려되었다.

또한 이 알고리즘은 Discontinuous Grammar에도 적용이 되었는데 SKip(X)라는 특별한 심볼이 병렬처리 정책과 같이 사용되었다. Discontinuous Grammar의 구현에 가장 큰 문제는 SKip(X)의 값을 결정하는 문제인데 이는 LR에 기반한 파싱 테이블을 미리 봄으로써 결정할 수 있는 근거를 마련하였다.

여기서 제안한 알고리즘은 사용된 문법에 상관없이 사용될 수 있으며 LR 파싱에 기반한 LALR 파싱 방법을 사용하여 구현되었다.1. Introduction = 6 1.1 Motivation = 6 1.2 Cadre de la these = 8 2. Preliminaires sur les grammaires et les strategies d'analyse = 10 2.1 Grammaires independantes du contexte = 10

본 논문은 최근의 인공지능 자연어 처리 스토리 생성 과정에서 메타러닝 효용성을 분석함으로써 인간과 기계의 지적 소통에 주목하고자 한다. 영어의 스토리 제너레이션 성과는 텍스트 요약, 질문 응답 시스템, 의미 추론의 영역에서 꾸준하게 자연어 기술의 최첨단을 걷고 있다. 반면, 한국어 자연어 처리의 경우에는 데이터 세트 부족으로 자연어 처리 기술 성과물이 크지 않 다. 언어학의 의미론과 화용론의 이론을 자연어 처리 기술에 포괄적으로 접 목하여 언어인지 이론과 인공지능 자연어처리 발달이 유사 병렬구조로 발전 되어 왔음을 점검한다. 본 논문은 인간이 봇과 이야기하기 행위는 인지 발달 의 핵심 요인이라는 믿음을 바탕으로, 딥러닝과 강화학습의 융합적 발전은 메타러닝을 타깃하는 인공지능의 스토리 제너레이션 기술 방법론에서 발견되 기에, 외국어를 배우는 학습자에게도 봇과의 놀이는 스스로 언어학습을 터득 하게 하는 최적의 상호작용 도구임을 천착한다.I. Introduction: 1 A. Machine Intelligence and Alphago: 2 B. From Alphago into NLP: 4 II. Meta Learning both in case of NLU and Deep Reinforcement Learning: 6 A. Natural Language Understanding . 6

(필요성) 최근 인공지능, 빅데이터, IoT 등 ICT가 지속적으로 발전하고 있으며, 이를 이용한 생산기술 및 안전보건기술이 현장에 적용되고 있다. ICT를 활용하여 제품, 부산물 및 폐기물을 지속적으로 모니터링하고 있고, 인간이 접근하기 어려운 파이프라인이나, 연소탑과 같은 곳에 드론을 이용하고 있다. 또한 드론의 RL 방법을 적용하여 공장단지 내의 화학물질 누출원 탐색, 레이저 초음파를 이용한 배관 및 설비 비파괴 검사기법 개발 및 실증 등에 활용되고 있다. 향후 장비 수리, 차단, 예지분석, 장비 고장예측, 이상징후 포착 및 경고와 장애대응 방안에 점차 활용성이 높아질 것으로 예상된다.
빅데이터 분석은 시스템 모델링을 통한 예측은 물론 강화학습을 통한 시스템 자동제어 분야까지 그 영역이 확대되고 있다. 과거 정형 데이터 분석 중심에서 텍스트 데이터와 이미지 데이터를 포함하는 멀티미디어 데이터 분석을 요구하는 연구가 증가하고 있다. 관련 연구소, 엔지니어링, 언론, 안전 관련 국가기관 등에서는 업무수행 중에 발생한 많은 텍스트와 이미지 데이터를 분석하여 업무 효율화 및 예측업무 추진을 계획하고 있다. 신경망 기술을 적용한 멀티미디어 빅데이터 기술은 십 수 연간의 답보상태에서 벗어나 실제 산업계에서 직접 응용이 가능할 정도로 매우 급속히 발전하고 있다.
안전의 문제는 과거 하인리히와 버드의 이론처럼 여러 복합적 원인에 의해 발생된다. 이러한 원인을 반영한 안전성 평가는 이미지·영상, 정형 데이터, 비정형 데이터 등의 다양한 매체에서 생성된 데이터를 기반으로 평가되어야 한다. 현재 안전·보건에서 활용되고 있는 인공지능 기술은 화재 감지, 사물 인식, 단일 장치의 고장 등 대부분 하나의 문제를 예측하기 위해 사용되고 있다. 안전·보건에 있어 의사결정은 한방의 의사결정(one-shot decision) 문제가 아닌 순차적 다요소 의사결정의 문제로 귀결되므로, 이에 대한 해결방안을 수립할 필요가 있다.
(목적) 따라서 본 연구는 인공신경망 기술을 활용하여 멀티미디어 빅데이터를 통한 안전성 평가방법을 제시하였다. 과거 각 센서의 모니터링만 하던 방법을 벗어나 정형 데이터, 비정형 데이터, 이미지, 영상 데이터 등을 활용하여 종합적인 안전성 평가방법을 연구하였다.
(방법) 멀티미디어 빅데이터 처리기술 중에서 통계적 기술, 이미지 인공신경망 기술 및 자연어 처리 인공신경망 기술을 연구하였고, 각각의 기술에 대해 조사·응용하였다. 멀티미디어 빅데이터는 방대한 자료로부터 이상상태를 찾고, 이상징후 모델에 대한 과거 정보를 분석한 후 과거 패턴과 유사한 위험 특성을 검출할 수 있는 기법을 검토하였다. 이미지·영상 분석에 탁월한 CNN, U-Net, Deep U-Net, NASNet 및 Mask R-CNN을 연구하였고, 자연어 처리성능에 탁월한 RNN, LSTM, BERT, DNC 등의 다양한 인공신경망을 연구하고, 이를 안전성 평가방법에 응용하였다. 또한 사고의 복합적인 원인 모델링을 위해 SEM 분석방법을 활용하였다. 이때, 멀티미디어 빅데이터 분석에 필요한 데이터는 가상의 200,000개 데이터를 생성하여 사용하였다.
(사례연구) 본 연구의 응용은 LNG 가스 파이프라인에 적용하였다. 현장의 설비는 온도, 압력, 유량, 진동 등의 많은 센서 데이터로부터 측정되고 있으며, ERP, 이메일 등 다양한 형태의 데이터로 저장되고 있다. 빅데이터를 활용하여 의미 있는 결론을 도출하기 위해서는 이 데이터의 표준오차와 변수들의 상호 관련성을 분석하는 방법과 모델링 기술이 핵심이다. 이러한 통계적 분석을 위해 설비의 고장이나 사고를 발생시킬 수 있는 리스크 소스 함수가 있어야 한다. 따라서 본 연구에서는 이 함수의 모델링을 인공신경망을 사용하여 생성하였다. 즉, SEM의 관측변수를 인공신경망으로 적용하였다.
(결론) 본 연구에서는 멀티미디어 빅데이터를 사용하여 관측변수 측정은 인공신경망을 활용하도록 하였고, 잠재변수는 구조방정식 모형을 사용하여 안전성 평가방안을 제시하였다. 그리고 Guideline 중심의 전통적인 안전진단에서 벗어나서 첨단기술을 활용하여 장비수리, 차단, 예지분석, 장비고장 예측, 이상징후 포착 및 경고, 장애대응 방안에 대해 지능기반의 안전성 평가방법론을 제시하였다. 사례 적용결과로는 제3자, 부식, 설계, 부적절한 운전 및 누출 대응에 의한 영향 5가지에 대해 SEM으로 모델링하였고, 잠재적 위험성 및 사고 발생 가능성 예측과 예비정비 우선순위를 결정하였다.
(활용방안) 향후 멀티미디어 빅데이터를 통해 다양한 위험설비의 운영 안전성 및 안전진단 핵심기술 개발에 활용될 것으로 예상될 뿐만 아니라, 빅 데이터 기반 계통·기기 감시 및 진단기술에 활용될 수 있다. 이를 통해 위험설비·시스템 안전운전 및 안전 여유도 향상, 유용한 데이터를 활용한 계통기기 감시진단 및 예측진단, 운영 유지보수 기간 및 비용 절감, 예방정비 등 효율적 설비 운영, 위험설비 운영 및 경영 리스크 저감에 활용될 것으로 기대한다.(Necessity) Recently, ICT such as artificial intelligence, big data, IoT has been developed continuously, and production technology and health and safety technology using it has been applied to the field. We use ICT to continuously monitor products, by-products and waste, and use unmanned reconnaissance aircraft in places such as pipelines and combustion towers where human access is difficult. In addition, applying the method of Drone's RL, it is utilized in the development and demonstration of piping and equipment nondestructive inspection technology using piping and equipment using laser ultrasonic waves, etc., to search for leakage sources of chemical substances in factory complexes. It is expected that it will be increasingly useful for future equipment repair, shut down, predictive analysis, equipment failure prediction, anomaly capture and warning, and failure countermeasure.
Big data analysis has expanded its scope to prediction through modeling of systems as well as the field of automatic control of systems through reinforcement learning. There is an increasing number of studies that require analysis of text data and multimedia data including image data, focusing on analysis of past structured data. Related research institutes, engineering, media, and national agencies related to safety analyze many texts and image data generated during business operations, and plan business efficiency and forecast business promotion. Multimedia big data technology that applies neural network technology has been out of stride for over a dozen years and is developing so rapidly that it can be directly applied from the real industry.
Safety problems are caused by multiple complex causes, as in the past Heinrich and Byrd's theory. Safety assessments reflecting these causes should be evaluated based on data generated by various media such as images / videos, fixed data, unstructured data. The artificial intelligence technology currently used in safety and health is used to predict almost one problem such as fire detection, things recognition and failure of a single device. In health and safety, decision making is not a Kampo decision (One-shot decision), but it will result in multi-factor decision making, so it is necessary to formulate a solution.
(Purpose) Therefore, in this research, we use artificial neural network technology to present a safety evaluation method using multimedia big data. We will study a comprehensive safety evaluation method using structured data, unstructured data, images, images, data, etc., excluding the method used to monitor each sensor in the past.
(Method) Among multimedia big data processing technology, we researched statistical technology, image artificial neural network technology, natural language processing, artificial neural network technology, and researched and applied about each technology. Multimedia Big Data detected abnormal condition from huge data, analyzed past information of abnormal indication model, and examined a method that could detect risk characteristics similar to the past pattern. We study CNN, U-Net, Deep U-Net, NASNet, Mask R-CNN excellent in image/video analysis, and various artificial neural networks such as RNN, LSTM, BERT, DNC, etc. that excel in natural language processing performance. Was applied to the safety evaluation method. The SEM analysis method was used to model the complex causes of the accident. The data required for multimedia big data analysis generated and used 200,000 hypothetical data.
(Case study) The application of this study was applied to the LNG gas pipeline. Facilities in the field are measured from many sensor data such as temperature, pressure, flow rate, and vibration, and stored as data of various formats such as ERP and e-mail. In order to make use of big data and draw meaningful conclusions, methods and modeling techniques that analyze the correlation between standard error and variables of this data are important. For these statistical analysis, a risk source function that can cause equipment failure or accident is required. In this study, modeling of this function was created using an artificial neural network. That is, the observation variables of SEM were applied to the artificial neural network.
(Conclusion) In this study, measurement of observational variables using multimedia, big data is to make use of artificial neural networks, and potential variables are evaluated on safety using structural formula model I presented the method. Get out of traditional Guideline-centric safety diagnostics and use advanced technology to present intelligence-based safety assessment methods for equipment repair, shut down, predictive analysis, equipment failure prediction, anomaly capture and warning, and failure response measures did. As a result of case application, the effects of a third party, corrosion, design, improper operation, and leak response are modeled with five SEMs to predict potential hazards and the possibility of accidents, and prioritize maintenance Were determined.
(Utility plan) Using multimedia big data in the future, it is expected not only to be utilized for the development of safety and safety diagnostic core technology of various risk facility operation, but also system / equipment based on big data Can be utilized for monitoring and diagnostic techniques. As a result, risk facility/system safe operation and safety margin improvement, monitoring, diagnosis and prediction diagnosis of grid equipment utilizing useful data, operation maintenance period and cost reduction, operation of efficiency facilities such as preventive maintenance, risk facility It is hoped that it will be used to reduce management and management risks.요 약 ⅰ 표 목 차 ⅵ 그림목차 ⅷ 용어설명 ⅺ Ⅰ. 서 론 1

인터넷을 사용하는 인구가 점점 증가함에 따라 오늘날에 이르러서는 수많은 지식 정보들이 웹상에 존재하게 되었다. 이 웹상의 지식 정보들은 다양한 방면의 정보를 포함하고 있고 그 수 또한 방대하므로 중요한 자원으로 활용 될 수 있다. 이 지식 정보들은 대다수가 자연어 문장으로 이루어져 있는데 자연어 문장은 구조의 일정함이 없고 어떠한 지식 정보를 나타내는지가 명확하지 않아 컴퓨터가 활용하기에 적합하지 않다. 그러므로 자연어 문장으로 이루어진 지식 정보들을 재가공하여 일정한 구조를 갖춘 지식 정보로 정형화 할 필요가 있다.
지식 베이스(knowledge base)는 정형화된 지식 정보들이 축적된 데이터베이스를 이른다. 지식 베이스의 대표적 예로는 DBpedia와 Freebase가 있다. DBpedia와 Freebase는 Wikipedia의 자연어 문장이 정형화 된 구조인 지식 트리플(knowledge triple)로 변환되어 축적되어 있다. 지식 트리플은 정형화된 지식 정보로 주어 개체와 목적어 개체 그리고 두 개체 간의 의미적 관계를 나타내는 관계로 구성된다.
지식 베이스를 구성하는 지식 트리플의 개체는 단 하나의 관계와 연관된 것이 아니라 여러 가지 관계들과 연결될 수 있다. 2개의 지식 트리플 <서울, country, 한국>, <서울, mayor, 박원순>을 예로 들면 ‘서울’이라는 개체는 관계 ‘country’에 의해 개체 ‘한국’과 연결되고 관계 ‘mayor’에 의해 개체 ‘박원순’과 연결된다. 이렇게 지식 베이스 개체들은 다양한 관계에 의해 서로 연결되어 있다. 지식 베이스를 구성하는 지식 트리플의 수 또한 많은데 Freebase는 1억 개가량의 지식 트리플을 DBpedia는 30억 개가량의 지식 트리플을 가지고 있다. 두 지식 베이스 모두 많은 수의 지식 트리플을 가지고 있지만 이는 웹상의 지식 정보를 모두 표현하기엔 턱없이 부족하다. 따라서 더 많은 양의 지식 트리플을 추가적으로 생성할 필요가 있다.
그러나 사람이 직접 자연어 문장을 지식 트리플로 바꾸는 작업은 시간과 비용이 많이 든다. 그렇기에 자동으로 자연어 문장에서 지식 트리플을 생성하는 시스템이 필요하다[9]. 자연어 문장에서 자동으로 지식 트리플을 생성하는 기존의 시스템으로는 Yoon et al.[1]이 제안한 자가 지식 학습 프레임워크가 있다. 자가 지식 학습 프레임워크에서는 기존의 지식 베이스를 활용하여 패턴을 만들어 자연어 문장에서 지식 트리플을 생성한다.
하지만 자가 지식 학습 결과로 생성된 지식 트리플 정확률은 그리 높지 않다. 이는 잘못된 지식 트리플이 반복해서 학습 과정에 사용되는 자가 지식 학습의 특성 때문이다. 이로 인해 발생하는 오류가 누적되어 자가 지식 학습으로 생성된 지식 트리플의 정확률이 떨어진다. 그러므로 정확률을 개선하기 위해 지식 트리플들 중에서 오류가 있는 지식 트리플들을 걸러낼 필요가 있다.
본 논문에서는 Translate-based 모델인 TransR 모델[4]의 스코어 함수 값을 기준으로 지식 트리플을 필터링하는 방법을 제안한다. TransR 모델의 임베딩 학습을 통해 벡터 공간(vector space)상에서 지식 트리플의 개체 벡터와 관계 벡터를 잘 표현 할 수 있다. 이렇게 학습된 지식 트리플 임베딩을 바탕으로 스코어 함수 값을 구할 수 있으며 이를 기준으로 자가 지식 학습으로 생성된 지식 트리플을 필터링한다.As increasing of population using the internet, people upload vast amount of knowledge on the internet. The knowledge covers various fields of knowledge so it can be important resource. Most of the knowledge is composed of natural language. But natural language don’t have consistent structure and sometimes it is unclear what information that indicates. The issue makes it hard to use information in knowledge by computer. Thus, the knowledge has to transform to consistent and fixed form. Knowledge base is database that includes large number of fixed form knowledge. The uniform knowledge is called knowledge triple and it has <Subject Entity, Relation, Object Entity> form. DBpedia and Freebase are well-known knowledge base. Those have numerous number of knowledge triples but it is inadequate to represent all knowledge on the internet. Therefore we need to generate more knowledge triples. It takes much time and money to generate knowledge triples by human. To solve the problem it is necessary to develop a system that generate knowledge triple automatically. There is existing system, self-knowledge learning system, generates knowledge triples from natural language. But accuracy of knowledge triples created by self-knowledge learning is not enough high. To improve the accuracy of knowledge triples, this paper propose filtering knowledge triples by TransR model score function value. TransR model is translate-based model that learn entity vectors and relation vectors on vector space. By learning entity vectors and relation vectors, knowledge triples can get score function value. We assume a knowledge triple that has low score function value is likely to correct knowledge triple. As all knowledge triples have score function values, the knowledge triples can be reorder by the value. We get upper rank knowledge triples about 5 relations and evaluate accuracy of the knowledge triples. Average accuracy of 5 relations is 180% higher than accuracy of knowledge triples generated by self-knowledge learning system.Ⅰ. 서 론 1 Ⅱ. 관련 연구 4 Ⅲ. 지식 트리플 생성 및 필터링 7 3.1. 지식 트리플 생성 단계 9 3.2. 지식 트리플 필터링 단계 10

자연어로 작성된 요구사항을 실현하기 위해 구현된 소프트웨어가 요구사항을 만족하는 지를 확인하는 것은 반드시 필요하다. 대부분의 소프트웨어 테스트에서 소프트웨어가 자연어 작성된 요구사항을 만족하는 지를 확인하는 것은 테스터의 경험이나 노하우를 기반으로 구축된 테스트 케이스를 사용한다. 하지만 이 테스트 케이스와 자연어 요구사항이 일치하는 지를 검증할 수 있는 방법이 없기 때문에 소프트웨어가 요구사항을 만족하는 지를 검증하기 위한 테스트 케이스를 생성하는 것은 매우 어렵다. 본 논문에서는 자연어로 작성되는 요구사항의 정형화를 위한 규칙을 사용하고, 제안된 규칙에 작성된 요구사항으로부터 테스트 케이스 및 테스트 스크립트를 생성하는 체제를 제안한다. 제안된 체제에서는, 무기 체계 분야에서 사용하는 자연어 요구사항을 분석하여 해당 분야에 맞는 요구사항 작성 규칙을 사용한다. 요구사항 규칙을 준수하여 작성된 요구사항을 파싱하여 필요한 요구사항 정보를 추출하고 이를 테스트 케이스 생성에 필요한 자료로 변환한다. 변환된 테스트 케이스 생성 정보를 이용하여 필요한 테스트 케이스 생성 전략에 따라 테스트 케이스를 생성하고, 실제 소프트웨어 입력 정보를 이용하여 테스트 스크립트를 생성한다. 제안된 방법은 실제 무기 시스템 소프트웨어 요구사항에 적용 시도 중에 있으며, 본 논문에서는 실험적인 요구사항에 적용하여 그 가능성을 보인다.제 1 장 서 론 1 제 2 장 관련 연구 4 제 3 장 요구사항 정형화 및 테스트 스크립트 자동 생성 방안 7 제 1 절 요구사항 정형화 10 제 2 절 요구사항 기반 테스트 케이스 생성 13

목적 지향 대화 시스템은 사용자와 에이전트간의 대화를 통해 사용자의 목적을 이룰 수 있도록 에이전트가 적절한 응답을 하는 지능형 소프트웨어를 말한다. 대화 시스템은 사용자의 의도를 파악하는 자연어 이해 모델, 사용자의 의도에 알맞은 에이전트의 의도를 도출하는 대화 관리자 모델, 에이전트의 의도를 바탕으로 자연스러운 문장을 생성하는 자연어 생성 모델로 구성되어있다. 전통적인 목적 지향 대화 시스템은 자연어 이해 모델, 대화 관리자 모델, 자연어 생성 모델과 같은 세분화된 모델들이 파이프라인 구조로 이루어져 있기 때문에 오류 전파의 문제가 있다. 본 논문에서는 오류 전파 문제를 해결하기 위해 심층 순환 Q 네트워크에 기반 한 자연어 이해 모델과 대화 관리자 모델을 제안한다. 또한 대화 전략의 유연성을 높이기 위해 심층 순환 Q 네트워크 강화 학습을 적용한다.A goal oriented dialogue system is intelligent software in which an agent responds appropriately in order to achieve user's task through dialogues between a user and an agent. The dialogue system consists of a natural language understanding model that catches user’s intention, a dialogue management model that derives the agent' s intention according to user’s intention, and a natural language generating model that generates a surface sentence based on the agent’s intention. Traditional goal oriented dialogue system has a error propagation problem. Because the natural language understanding model, dialogue manager model and natural language generating model are combined in a pipeline structure. To resolve the error propagation problem, we propose a natural language understanding model and a dialogue management model based on deep recurrent Q network. Then we apply reinforcement learning to the deep recurrent Q network in order to increase the flexibility of dialogue strategy.Ⅰ. 서 론 1 Ⅱ. 제안 모델 4 1. 의도의 일반화 4 2. LSTM 기반 목적 지향 대화 시스템 6

In recent times, with the increasing interest in conversational agents, task-oriented dialog systems have studied actively. However, most of studies are focused on individual modules of a system; there is an evident lack of research on a dialog framework that can integrate and manage the entire dialog system. In this thesis, a framework that enables the user to effectively develop an intelligent dialog system is proposed. The proposed framework ontologically expresses the knowledge required for the task-oriented dialog system's process and can build a dialog system by editing the dialog knowledge. In addition, the framework provides a module router that can indirectly run externally developed modules. Further, it enables a more intelligent conversation by providing a hierarchical argument structure (HAS) to manage the various argument representations included in natural language sentences. In addition, this thesis propose dependency tree decoding method as a novel decoding method for natural language generation. Studies into natural language generation often employ sequence decoding, which generates words in sequential order in a sentence and uses the input generated by each word in the previous question step. On the other hand, the proposed decoding method employs a sequence generated by traversing to a dependency tree. As a result, the most important words can are generated preferentially and relevance between the input and predict words can be more improved. To verify the practicality of the framework, an experiment was conducted in which developers without any previous experience in developing a dialog system developed task-oriented dialog systems using the proposed framework. The experimental results show that even beginner dialog system developers can develop a high-level task-oriented dialog system. Meanwhile, in order to evaluate dependency tree decoding in natural language generation, SC-LSTM for natural language generation is implemented, and the input and
predict word sequence are converted to allow dependency tree decoding. Experimental results show that dependency tree decoding method provides significantly improved performance and that the generated sentences exhibit a promising level of variation in sentence pattern.최근 지능형 대화 에이전트에 대한 관심이 증가함에 따라 목적지향 대화 시스템의 연구가 활발하게 진행되고 있다. 대부분의 목적지향 대화 시스템의 연구는 시스템을 구성하는 개별 요소(자연어이해, 대화관리, 자연어생성)의 성능을 높이는데 초점이 맞추어져 있는 반면, 전체 대화 시스템을 관리하고 통합하는 프레임워크의 연구는 매우 부족하다. 본 논문에서는 이러한 문제에 대응하여 쉽고 빠르게 지능형 대화 시스템의 개발할 수 있는 대화 프레임워크를 제안한다. 제안하는 프레임워크는 대화에 필요한 지식을 온톨로지(Ontology) 형식으로 정의하여 쉽고 간편하게 지식을 추가, 편집할 수 있다. 또한 대화에서 나타나는 다양한 인자 표현을 관리할 수 있는 계층적 인자 구조(Hierarchical Argument Structure)를 제공한다. 뿐만 아니라, 대화 시스템의 소스코드를 직접 편집하지 않고 간접적으로 대화 시스템을 수정할 수 있는 모듈 라우터(Module Router)를 제공하여 편리하게 추가 기능을 관리할 수 있다. 또한 본 논문에서는 의존 트리 디코딩(Dependency Tree Decoding)을 이용한 자연어 생성 방법을 제안한다. 기존의 자연어 생성은 문장을 구성하는 단어를 순서대로 생성하는 순서열 디코딩(Sequence Decoding)을 사용하였다. 제안하는 의존 트리 디코딩은 문장의 의존 트리를 순회 알고리즘으로 방문하는 순서대로 출력을 수행하고, 이 때 입력을 부모 노드, 형제 노드로 설정하고, 출력을 방문하는 자식 노드로 설정한다. 이러한 방법은 출력 순서에서 중요한 단어를 먼저 출력하고, 입력과 출력 단어의 연관성을 높일 수 있다. 본 논문에서 제안하는 대화 프레임워크의 유용함을 증명하기 위해 자연어처리 비전문가가 프레임워크를 이용하여 대화 시스템을 개발
하도록 하였고, 개발된 대화 시스템은 만족스러운 성능을 제공함을 보였다. 또한 의존 트리 디코딩의 성능을 평가하기 위해 SC-LSTM(Semantically Controlled Long Short-term Memory) 기반의 자연어 생성을 실험 환경을 구성하였고, 의존 트리 디코딩과 기존의 순서 열 디코딩을 비교하였다. 실험 결과, 의존 트리 디코딩 방법이 비교적 자연스러운 문장을 생성하고, 좀 더 다양한 문장 패턴을 제공하는 장점이 있음을 증명하였다.

자연어 파싱은 자연어처리기술을 요구하는 많은 작업 분야에서 해결해야 할 중요한 문제이다. 언어를 처리하는 많은 작업에서는 술어-논항 관계나 수식어-피수식어 관계에 대한 정보를 이용하는데, 파싱은 문장의 단어나 구 사이의 관계를 파악해 줌으로써 이 정보의 추출을 가능하게 한다. 그러나 자연어 문장이 내포하고 있는 중의성 때문에 문장을 정확하게 파싱하는 것은 어려운 일이다. 최근 십년 동안 통계적인 방법이 자연어 파싱, 혹은 통사적 중의성 해소에 널리 사용돼왔다. 통계적 자연어 파싱에서 가장 중요한 두 가지 일은 통사적 중의성 해소에 유용한 자질을 선택하는 것과 이들을 이용한 통계적인 모형을 설계하는 것이다.
본 논문에서는 수식 거리와 지역 문맥과 같은 표층 문맥 정보가 한국어의 통사적 중의성 해소에 미치는 영향을 논하며, 어휘 바이그램 의존 선호도 및 특정 지역 문맥에서의 수식 거리 선호도를 고려한 파싱 모형을 제안한다. 이들 선호도는 지역 문맥에 대한 조건부확률로 표현된다. 이 파싱 모형은 의존 이론에 기반한 모형인데, 이는 한국어와 같은 자유어순언어의 통사적 특징을 반영하는데 적합한 이론이다. 제안하는 통계적 의존 파싱 모형은 어휘 의존 확률과 수식 거리 확률의 두 확률로 이루어지는데, 어휘 의존 확률은 의존 규칙 선호도 및 선택 선호도를 반영하고, 수식 거리 확률은 수식어의 주위 문맥이 주어졌을 때, 수식어로부터 시작되는 의존 관계의 길이에 대한 선호도를 반영한다.
어떤 언어에 대한 파싱 모형의 패러미터 형태를 정의할 때에는 고려하는 언어의 특징을 잘 반영하여야만 한다. 수식 거리에 대한 확률은 한국어와 같은 자유 어순 언어의 특징을 고려하여 정의되었는데, 이는 의존하는 두 단어 사이의 거리를 반영하는 새로운 방법이다. 제안하는 모형은 KAIST 트리뱅크를 이용한 평가에서 의존 관계 단위 F_(1) 기준으로 86.75%의 성능을 보였다. 자유 어순 언어에서도 수식 거리와 지역 문맥을 고려하는 것이 수식어의 올바른 지배소를 선택하는 데 도움이 됐고, 제안한 방법으로 수식 거리를 고려함으로써 수식거리를 고려하는 다른 통계적 모형들보다 더 나은 성능을 얻을 수 있었다.Natural language parsing is a key problem to many tasks that require natural language processing. Many language processing tasks use the information on predicate-argument relation or modifier-modifyee relation and the parsing makes the extraction of the information possible by identifying relations between words, or phrases in sentences. However, it is dificult to parse a sentence correctly, because of the ambiguity inherent in the natural language. During the last decade, the statistical approach becomes the major trends in natural language parsing, or syntactic disambiguation. The two most important things in the statistical natural language parsing is selecting appropriate features that help syntactic disambiguation and designing a statistical model using them.
This dissertation argues that the influence of surface contextual information, such as modification distance and local context, in solving syntactic ambiguity of the Korean language, and proposes the parsing model that considers a modification distance in a certain local context in addition to the preference for lexical bigram dependency. All of these preferences are expressed by probabilities conditioned on local context. The parsing model is based on the dependency theory, which is widely known as an adequate formalism to reflect the syntactic characteristic of the Korean language, or other variable word-order languages. The statistical dependency parsing model consists of two probabilities, which are the lexical dependency probability and the modification distance probability; the lexical dependency probability reflects selectional preference and the preference on each dependency rules. The modification distance probability reflects the preferred length of a dependency relation from a certain modifier based on the context of the modifier.
We believe the parameterization of the parsing model for a language should be done with the deliberation of the characteristics of the language. The probability on modification distance is designed to consider the property of variable-word-order language, which includes Korean, and this is a new way to reflect the distance between two depending words. Evaluation on the KAIST Treebank text shows that the proposed model recovered dependency relations with 86.75% F_(1)-score. The consideration of the modification distance and local context helps selecting correct modifyee of modifier even in variable-word-order language, and the proposed way to deal with the modification distance in the parsing model outperforms other methods dealing with the distance in the statistical model.Abstract = i Contents = iii Chapter 1 Introduction = 1 1.1 Practical Motivation for Parsing = 3 1.2 Statement of Thesis = 5

웹에 존재하는 대부분의 데이터는 자연어로 작성된 것으로써 사람이 직접 해당 자료에 접근하여 유용한 정보를 도출해내고 재구성해야 비로소 재사용 가능한 정보로서의 가치를 지니게 된다. 정보 재구성의 필요성은 두 개 이상의 문서 혹은 정보 소스로부터 얻어진 결과를 종합해야 답변이 가능한 복합 질의(Complex Queries)의 경우에 더욱 극명하게 나타난다.
이러한 복합 질의를 처리하기 위해 질의와 관련된 여러 문서의 내용을 종합하여 하나의 요약된 문서를 생성하는 방법이 있다. 그러나 이는 단순히 주어진 다양한 질의 키워드를 포함하는 문서를 검색하고 해당 문서 집합들의 요약을 생성하는 데에 그 목적을 둔 것으로 실제 복합 질의에 대한 응답의 정확율이나 요약된 정보의 재사용에 대해서는 고려하지 않았다. 반면, 본 논문에서 제안하는 방법은 자연어로 작성된 비구조적인 자료가 관계 모델과 같은 형태로 구조화되어 관계형 데이터베이스(RDBMS)와 같은 구조화된 저장소에 저장되면 이를 SQL(Structured Query language)을 통해 필요한 정보를 쉽게 얻을 수 있고, 관리가 용이하며 재사용이 가능하다.
본 논문에서는 도시에 관한 정보의 구조화에 초점을 맞추어 각 도시에 대해 기술하는 웹 문서들을 수집하였다. 문장의 패턴 분석을 통해 도시의 인구, 면적, 기온, 강우량에 대한 정보를 수집하고, 이를 구조화하여 관계형 데이터베이스에 저장 후, SQL을 이용해 질의를 수행하였다. 또한 도시 정보를 포함하는 각 웹문서내의 문장들은 자연어로 표현되었고, 자연어는 그 특성상 문맥에 따라 다르게 해석될 수 있는 다의성을 지니기 때문에 문장 간 의미 불일치 문제를 야기할 수 있다. 이러한 문제를 해결하기 위해 본 논문은 Apriori 알고리즘을 이용하여 서로 다른 문장의 엔티티들이 같은 의미를 갖는 엔티티인지를 판단하고 필요하지 않은 데이터를 제거하는 기법을 제안한다.
시스템의 성능을 평가한 결과 다의어로 구성된 노이즈 데이터가 약 22% 포함되어 있는 데이터에 대한 복합 질의응답의 정확율은 평균 40% 이하의 낮은 성능을 나타내었으나 제안한 알고리즘을 통해 불필요한 데이터를 제거한 후 성능을 평가한 결과 평균 80% 이상의 정확율을 나타내었다.국문요지 iv 제1장 서 론 1 제2장 관련 연구 3

인터넷의 발달로 인해 자신의 생각을 표현하고 상대방과의 교류가 활발해지면서 한글로 감정을 표현할 때 한글의 자·모음을 독립적으로 이용하는 특징이 있었으며 이러한 특징을 가진 같은 의미의 여러 다른 형태의 단어들을 하나의 대표 단어로 전처리하였을 때 감성 분석에 어떠한 영향을 미치는지 연구해보고자 하였다.
본 연구에서는 한글의 자·모음을 독립적으로 이용하는 것을 초성어라 정의하였고 SNS 상에서 감성을 전달하기 위해 초성어를 활용하는 방식이 얼마나 사용되고 있는지 그리고 감성을 표현하는 초성어의 전처리 과정을 거쳤을 때 감성에 대한 검증 정확도가 어떻게 달라지는 지를 연구하였다.
유투브는 1인 미디어 역할과 SNS의 확장 형태인 모습으로 발전하고 있어 유투브 댓글을 본 논문의 실험데이터로 선정하였고 인기있는 트렌드 분야의 댓글과 정치분야 그리고 청소년 컨텐츠 분야에서 수집한 데이터를 이용하여 대표 감성 초성어들을 추출하였다. 각각의 분야에서 수집된 댓글의 감성 초성어를 분석하였으며 그 결과 초성어 6개가 전체 실험 데이터에서 사용되는 감성 초성어의 98%를 차지하는 것을 확인하였고 각각 사용 비율이 가장 높은 감성 초성어를 대표 감성 초성어로 정의하였으며 대표 감성 초성어로 정규화하는 전처리유형, 원시 데이터를 사용한 유형 그리고 대표 감성 초성어를 삭제하는 3가지 유형으로 데이터를 구별하여 생성하였고 3가지 유형을 Doc2Vec을 이용하여 유형마다 각각 12가지 설정의 학습 모델을 만들어 이를 이용하여 긍,부정 감성에 대한 판별 정확도를 비교하는 실험을 하여 대표 감성 초성어를 정규화하는 전처리 유형이 판별 정확도를 높인다는 것을 확인하였다.As the Internet developed, it used consonants or vowels of Han-gul independently to express one's thoughts and to express one's feelings in Korean. A study was conducted on how different types of words of the same meaning with these characteristics could affect sentimental analysis when pre-processing in a single representative word.
In this study, using the consonants and vowels of Han-gul independently was defined as the First-Phoneme Korean Words.
The method of using First-Phoneme Korean Words to convey emotion on SNS is used and how the accuracy of verification of sensibility varies after the pre-process of First-Phoneme Korean Words expressing emotion
YouTube selected YouTube comments as the experimental data for the paper as it developed into an extension of its single-person media role and SNS, and extracted representative sentimental First-Phoneme Korean Words using the data collected in the field of comments, politics and youth content in the popular trend field.
Sentimental First-Phoneme Korean Words in comments collected in each field was analyzed and the results showed that six First-Phoneme Korean Words accounted for 98% of the sentimental First-Phoneme Korean Words used in the entire experimental data, each with the highest percentage of use defined as the representative sentimental First-Phoneme Korean Words. In addition, data were generated by distinguishing between pre-processing types that are normalized as representative First-Phoneme Korean Words, types that use raw data, and three types that delete First-Phoneme Korean Words. Three types of learning models were created using Doc2Vec in 12 settings, and the pre-processing type that normalizes First-Phoneme Korean Words was found to increase the determination accuracy.제1장 서론 1 1.1 연구 배경 및 목적 1 1.2 논문 구성 3 제2장 관련연구 4

최근 딥 뉴럴 네트워크 기술의 급속한 발전이 이루어지면서, 인공지능에 대한 다양한 분야들에서 연구 및 활용이 이루어지고 있다. 특히 대화형 시스템은 자연어를 기반으로 하여, 사용자가 필요로 하는 정보를 나타내는 질의에 대해 적절한 정보를 제공하거나, 사용자와 지속적으로 대화를 진행해 가는 시스템으로, 인공 지능 연구에 있어 중요한 연구 분야들 중 하나이다.
대화형 시스템에서는 자연어를 통해 커뮤니케이션이 이루어지므로, 사용자의 상황과 목적에 따라 시스템의 응답이 적절한 형태의 자연어로 표현되어야 할 필요성이 있다. 대화형 시스템의 성능이 아무리 좋더라도 부적절한 형태로 표현이 이루어진다면, 사용자의 만족도는 낮아질 수 밖에 없을 것이다. 자연어의 적절한 표현을 위해서는 다양한 요소들이 고려되어야 하며, 특히 사용자가 나타내는 감정은 자연어의 표현을 결정하는데 있어 필수적으로 고려되어야 하는 부분이라고 할 수 있다.
본 논문은 대화 상황에서 사용자들이 나타내는 발화가 어떤 감정을 나타내는 지를 특정한 감정 분류 체계에 따라 분류하고자 한다. 감정은 매우 주관적인 영역이기 때문에, 동일한 텍스트에 대해서도 해당 텍스트에서 나타나는 감정에 대해 사람마다 해석이 엇갈릴 수 있다. 또한 일반적인 텍스트가 아닌 대화라는 상황적 특성을 고려해야 정확한 감정 분류를 할 수 있다.
본 논문에서는 대화에서 이루어지는 발화의 감정 분류를 위해 전통적인 자질(feature) 기반의 분류 모델을 사용하는 대신, 최근 자연어 처리 관련 연구에서 활발하게 사용되고 있는 딥 뉴럴 네트워크 모델을 사용하는 방법을 제안한다. 이를 통해, 기존의 사람이 직접 만들어야 했던 분류를 위한 자질들을 딥 뉴럴 네트워크 모델을 통해 자동으로 추출하게 하고, 대화라는 상황적 특성을 딥 뉴럴 네트워크 모델의 구조적 특성을 통해 반영할 수 있다. 실험 결과, 딥 뉴럴 네트워크 모델의 사용이 대화에서의 감정 분류에 있어 효과적임을 보여준다.요약 .......................................................................................................................... i Contents .................................................................................................................... iii List of Figures ............................................................................................................ v List of Tables ............................................................................................................ vi Chapter 1 서 론 ..................................................................................................... １

IT 기술이 발전함에 따라 학습의 채널은 더욱 발전하고 다양해지고 있다. e-러닝은 이제 우리 주변에서 쉽게 접할 수 있는 학습 형태가 되었다.
그러나 이런 발전에도 e-러닝 등의 가상교육은 학습자와 교수자의 상호작용이 원활하게 이루어 지지 않는 경우가 많다. 이런 상호작용의 부재는 자기 주도적으로 이루어지는 가상교육의 흥미를 저하시키고 동기부여에 어려움을 겪는 등 가상교육의 학습을 저해하는 커다란 요인 중에 하나이다.
따라서 본 연구에서는 실제 교수자에게 질의하는 느낌을 갖게 하고 실시간으로 응답을 받게 함으로써 학습에 흥미를 갖게하고 가상학습의 고립감을 해소하며 친밀감을 높일 수 있도록 대화형 질의응답 시스템을 설계ㆍ구현하였다.
시스템은 형태소 분석을 통해서 색인어를 검출하여 자연어에 가까운 질의를 할 수 있도록 하였으며, 교수자는 학습자의 질의 통계를 통해서 학습자와 상요작용하며 학습 내용을 구성ㆍ수정할 수 있도록 하였다.
본 연구에서의 향후 연구과제는 대화형 질의의 의미를 정확히 파악하는 것이다. 문장의 형태소를 분석한다고 해도 그 형태소 만으로 문형을 만들고 그것에서 색인어가 아닌 의도를 명확하게 파악하기가 쉽지 않기 때문이다. 하지만 자연어 처리에 관한 연구는 계속 이루어지고 있고 앞으로 많은 발전이 있을 것으로 생각된다.The development of IT technology is improving and diversifying learning channels. E-learning has become a type of learning to which we can easily get access.
Despite such development, virtual education, including e-learning, often fails to have learners and teachers interact with each other smoothly. The absence of interaction, which reduces interest in self-directed virtual education and makes it difficult to be motivated, is one of the significant factors interfering with learning in virtual education.
This study designed and implemented a conversational question-answer system which actually gave a feeling of asking a teacher and which gave real-time answers so that learners could take interest in learning, resolve the sense of being isolated for virtual learning, and get more familiar.
The system made it possible to make a question close to natural language by detecting an index work through morphemic analysis and for teachers to compose and revise learning contents through interaction with learners by taking the statistics of learners' questions.
This study has a follow-up task of understanding the correct meaning of conversational questions. That is because even with analysis of morphemes in a sentence, it may not be easy to make a sentence pattern with the morphemes alone and clearly grasp the intention, not an index word, from it. However, researches on natural language processing continue to be conducted and are expected to make great progress in the future.Ⅰ. 서론 1 Ⅱ. 이론적 배경 4 1. 자기 주도적 학습의 정의 4 2. 상호작용(Interaction) 5 1) 상호작용의 개념 5

국내의 사망자 중에서 80% 이상이 만성질환에 기인하는 것으로 보고되고 있다. 최근 자료에 의하면 대표적인 만성질환인 당뇨병의 유병률이 지속적으로 증가하는 추세를 보인다. 반면에 당뇨병의 치료율과 인지율은 50∼70% 수준이며, 혈당 조절율은 30% 미만으로 전반적인 당뇨병 관리가 제대로 되고 있지 않다.
당뇨병 발병은 환경적 요인이 크게 작용한다. 특히 현대 사회는 신체활동의 감소, 서구화되고 불균형한 식이습관, 과체중, 수명연장, 일회 식이섭취분량의 증가 등은 당뇨병의 발병 원인에서 가장 큰 비중을 차지한다. 당뇨병은 발병이 되면 치료가 어렵고 당뇨로 인해 발생되는 합병증은 당뇨병 자체보다 사망 위험성이 높다. 이러한 위험성을 낮추기 위해서는 당뇨병 정보 수집과 생활습관 개선을 위한 관리는 통해서 당뇨병 자체를 예방하는 것이 가장 좋은 방법이다.
이 논문에서는 당뇨병을 예방하는 방법으로 당뇨병 정보제공 및 예방을 위한 인공지능기반의 챗봇 시스템을 설계하고 구현했다. PC를 기준으로 웹기반의 대화형 인터페이스를 설계하여 사용자와 챗봇의 대화 환경을 구현했다. 챗봇이 사용자에게 제공하는 정보는 웹 크롤링(Web Crawling)을 사용하여 대한당뇨협회 홈페이지에서 일반인을 대상으로 게시되어 있는 당뇨병 정보를 수집하고 Data Base를 통하여 관리한다. 사용자의 질의 메시지를 인식하고 인공지능의 학습데이터 가공을 위해서 KoNLPy의 Mecab과 Wore Embedding 기술의 Word2Vec 기법을 사용하여 자연어처리를 했다. 사용자의 당뇨병 관련 질의나 예방기능 실행을 위한 챗봇의 인공지능은 딥러닝 기술 중 RNN(Recurrent Nenural Network) 기반의 Seq2Seq 훈련모델과 Seq2Seq 추론모델을 TensorFlow을 사용하여 구현했다. Seq2Seq 훈련모델을 사용하여 당뇨병 정보와 예방기능 실행을 위한 메세지를 학습한다. 학습 결과와 Seq2Seq추론모델을 사용하여 사용자의 질의문에 적절한 답변을 제공한다. 당뇨병 예방을 위한 기능 수행 시 사용자의 신체정보(성별, 나이, 신장, 체중, 허리둘레, 엉덩이둘레)를 이용하여 체질량지수, 기초대사량, 복부비만도 측정하고, 섭취음식 입력을 통하여 영양섭취량을 파악하고, 이 결과를 챗봇 메시지를 통하여 제공한다.More than 80% of domestic deaths are reported to be caused by chronic diseases. According to recent data, the prevalence of diabetes, a typical chronic disease, is continuously increasing. On the other hand, the treatment rate and cognitive rate of diabetes are 50 ~ 70%, and the blood sugar control rate is less than 30%, so overall diabetes management is not done properly.
Environmental factors play a major role in the onset of diabetes. In particular, the decrease in physical activity in modern society, westernized and unbalanced diet, overweight, life expectancy, and increased dietary intake account for the largest portion of the causes of diabetes. Diabetes is difficult to treat at the time of onset, and complications caused by diabetes are more likely to die than diabetes itself. To reduce this risk, it is best to prevent diabetes itself through the collection of diabetes information and management to improve lifestyle.
In this paper, we designed and implemented an artificial intelligence-based chatbot system to provide diabetes information and prevent diabetes as a way to prevent diabetes. Designed a web-based interactive interface to create a chat environment for users and chatbot. The information provided by Chatbot to users is collected using web crawling and managed through Data Base, which is available to the general public on the website of the Korean Diabetes Association. To recognize the user's question and to process the learning data of artificial intelligence, we used Mecab of KoNLPy and Word2Vec of Word Embedding technology to process natural language. Chatbot’s artificial intelligence for user diabetes queries and preventive functions has implemented a training model for learning and a inference model for response generation through the Seq2Seq model based on RNN (Recurrent Neral Network). At this time, the implementation used TensorFlow, Google's artificial intelligence development package.
When performing the function to prevent diabetes, the BMI, WHR, and consuming energy are calculated using the user's physical information (sex, age, height, weight, waist circumference, and hip circumference), and the status is checked through the status table for each item. And I entered the food that the user ingested and checked the nutritional status. These results were displayed through Chatbot's message.1. 서론···························································································································1 1.1 연구 배경 및 목적···························································································1 1.2 연구 방법···········································································································2 2. 관련 연구 ·················································································································4 2.1 만성질환: 당뇨병······························································································4

2012년 구미 불산누출 사고 이후 화학물질 관련 법률이 신설·강화 되었다. 화학물질 관련 소관 부처는 환경부를 비롯하여 8개 부처이고, 화학물질 관련 법률은 화학물질관리법 등을 비롯하여 27개 법률에 의해 규제되고 있다. 소방청 조사 결과에 따르면 기업의 화학물질안전관리 위반 건수가 화관법 제정 전인 ‘15년 대비 ’17년에는 무려 1,200%가 증가하였다.
최근 생활법률의 질의응답 서비스인 버비, 한국전자통신연구원의 엑소브레인(Exobrain), 의료기관의 챗봇(Chatbot), 비정형 빅데이터 재난안전 정보패턴 분석 등 다양한 지능형 서비스들이 연구되고 있다. 또한 딥러닝(Deep Learning) 등 인공지능기술의 발전으로 비전문가도 전문가와 같이 정보 서비스를 이용하는 추세이다. 과거 대부분의 정보는 키워드를 이용해 데이터베이스로부터 검색하는 방식이나, 법률 전문용어를 잘 모르는 일반인에게 원하는 정보를 제공하는 데는 한계가 있다.
따라서 본 연구는 자연어처리와 온톨로지, 시맨틱 웹 기술을 활용하여 지식기반의 데이터 모델을 제시하고, 향후 인공지능기술에 활용하여 추론이 가능하도록 하는 학습데이터 구축 방법론을 제시하는데 연구 목적이 있다.
본 논문은 자연어처리 분야에서 화학물질관련 법령정보 질의응답의 지능형 서비스를 제공하기 위해 지식베이스 구조와 학습데이터 구조에 대해서 연구하였다. 질의응답에 필요한 인공지능 알고리즘 연구는 본 논문에서 제외하였다. 인공지능 질의응답의 지능형 서비스 제공을 위하여 학습에 필요한 ①학습데이터 셋을 어떻게 구축해야 하고, ②화학물질의 물질안전보건자료(MSDS)와 ③화학물질관리법, 화평법, 산업안전보건법 등 화학물질관련 법령정보에 대해 온톨로지 구축 방법론을 제시하였다.
특히 지식기반 데이터 모델은 화학물질관련 법령의 함축적 표현방법의 구조화, 법률의 개체명/형태소 분석, 학습데이터 구조의 Intent와 Slot구성, 온톨로지의 클래스/속성/개체명의 분류방법, 스파클(SPARQL)언어 구성방법을 제시하였다. 또한 학습데이터 구축은 순환신경망(RNN)의 장기단기메모리모델(LSTM), Key-Value Memory Network모델 등 다양한 인공지능 알고리즘을 이용하여 질의응답 서비스를 구축하고자 할 때 가장 기초가 되는 학습데이터 구축과 전문지식구조 구축 방법을 제시하였다.
본 연구는 온톨로지(Ontology) 구축을 통해 추론이 필요한 100개의 질문에 대해 검증을 실시하였다. 연구결과 SPARQL언어를 통해 100개의 질문에 대해 답변이 가능함을 확인하였다.
향후 LSTM모델을 통해 질의 응답하는 서비스의 경우 필요한 데이터는 적어도 수십만 개 이상의 데이터 셋이 필요하다. 빅데이터가 구축되고 LSTM 등 인공지능기술을 활용한 질의응답 서비스가 제공된다면 전문지식이 없는 일반인에게도 화학물질관련 지식을 제공할 수 있을 것으로 기대된다. 또한 학습데이터 구조도 함께 제시되어 향후 지능형 추론 기술개발에 활용할 수 있을 것으로 기대되며, 다른 법률에도 확대 적용이 가능할 것으로 예상된다.After the Gumi hydrogen fluoride leak accident in 2012, laws related to chemical substances were newly established and strengthened.
The Ministry of Environment and other ministries are responsible for chemical substances, and the chemical laws are regulated by 27 laws including the Chemical Substances Control Act. According to the results of the investigation by the Fire Department, the number of violations of chemical safety management by enterprises increased by 1,200% compared to the 15th year before the law was enacted.
A variety of intelligent services such as Burby, a question and answer service of the recent living law, Exobrain of Korea Electronics and Telecommunications Research Institute, Chatbot of a medical institution, and an analysis of unstructured big data disaster safety information pattern are being studied. In addition, development of artificial intelligence technology such as Deep Learning is a tendency for non-experts to use information services like experts. Most of the information in the past is retrieved from the database using keywords, but there is a limit to providing information to the general public who is not familiar with the legal terminology.
Therefore, this study aims to present a knowledge - based data model using natural language processing, ontology, and semantic web technology, and to suggest a learning data construction methodology that makes it possible to use in artificial intelligence technology for future inference.
In this paper, we have studied knowledge base structure and learning data structure to provide intelligent service of Q & A information query in natural language processing. The artificial intelligence algorithms required for query response are excluded from this paper.
In order to provide intelligent service of intelligent question and answer, we need to construct ① learning data set which is necessary for learning, ② material safety data sheet (MSDS) of chemical substance and ③ chemical substance management such as chemical management law, peace process law, Ontology construction methodology is presented for legal information.
In particular, the knowledge-based data model is composed of the structure of the implication of the chemical-related laws, the object name / morpheme analysis of the law, the intent and slot composition of the learning data structure, the classification method of the class / attribute / object name of the ontology, And suggested a method of construction. In addition, learning data construction is the most basic learning data construction and professional development when we want to construct question-answering service using various artificial intelligence algorithms such as long-term short-term memory model (LSTM) and key-value memory network model of circular neural network And how to construct knowledge structures.
In this study, 100 questions that require reasoning through ontology construction were tested. The study confirmed that 100 questions can be answered through the SPARQL language.
In the case of a service that queries through the LSTM model in the future, the required data requires at least several hundred thousand data sets. If big data is established and a question and answer service using artificial intelligence technology such as LSTM is provided, it is expected that it will be possible to provide knowledge about chemical substances to the general public without expert knowledge. In addition, the learning data structure is presented, and it is expected that it can be utilized in the future development of intelligent reasoning technology, and it is expected to be extended to other laws.Ⅰ. 서 론 1 1. 연구의 필요성 1 2. 연구의 목적 4 3. 연구범위 및 방법 5 4. 국내 지능형 질의응답 관련 연구동향 7

온톨로지는 데이터에 의미적인 정보나 관계 정보와 같은 메타데이터를 추가함으로써 구조적 정보 관리의 제한점을 극복하기 위한 방법으로 온톨로지 기반 검색은 각 분야에서 다양한 형태로 연구되어 지고 있다.그러나 모든 연구와 정책의 기초가 되는 통계와 관련된 온톨로지에 관해서는 여타 분야에 비해 활발하게 연구되고 있지 않다.
이는 연구 및 기타 필요에 의해서 필요한 통계 자료를 얻기 위해서는 통계 자료를 보유하고 있는 통계청을 비롯하여 각 정부 산하 기관 통계 홈페이지에 수록되어져 있는 통계를 활용하여야 하는데 여러 가지 단점들로 인하여 연구 및 기타 필요에 의해서 통계를 얻기까지 많은 수고로움이 수반되고 있는 것이 현실이기 때문이다.
즉, 현재의 통계 정보는 양적으로 광범위해서 정작 사용자가 원하는 형태의 통계를 추출하기는 매우 힘들다. 이러한 방대한 통계 속에서 온톨로지 기반 검색을 이용한 지능형 통계 검색 모델은 사용자에게 유용한 통계를 보다 정확하고 쉽게 찾게 해 줄 것이다.
본 연구는 자연어 온톨로지 기반 검색 기술과 온톨로지를 활용하여 지능형 검색 시스템을 만들고자 하였다. 본 논문에서 제안하는 방법은 온톨로지 기반 검색시스템으로 복잡한 질의문을 수행하기 위하여 온톨로지를 구축하고 메타데이터에 대한 표준화와 함께 명명화하여 사용자가 입력하는 자연어에 맞는 온톨로지 기반 질의문으로 자동생성하고 추론에이전트로 하여금 OWL에서 온톨로지와 메타데이타 지식베이스의 추론을 통해 추출된 통계데이터를 재가공하여 사용자가 원하는 형태로 재생성하는 방식이다.The existing statistical information is very broad in terms of quantity so that it is hard for users to extract the desired form of statistical result.
In this circumstance, the intelligent statistical search model via natural language ontology-based retrieval enables users to have useful statistical results more accurately and easily.
This study intends to create an intelligent retrieval system using natural language ontology based search technology and ontology.
The method suggested in this paper is a natural language ontology-driven retrieval system which constructs the ontology for implementing complex query, and also naming as well as standardizing the metadata to automatically produce ontology-based queries that correspond to the natural language entered by users. Then a reference agent reprocesses at OWL the statistical data collected through the reference process of ontology and metadata knowledge base to finally create it in the form desired by users.I. 서론 = 1 1. 연구의 배경 = 1 2. 연구의 목적 = 3 3. 연구의 범위 및 방법 = 4 II. 관련연구 = 6

현재 중·고등학교 교재의 내용은 주로 컴퓨터 활용 즉, 특정 소프트웨어의 기능에 대한 내용으로 구성되어 있으며, 일부분에서 컴퓨터의 기초개념에 대하여 언급하고 있지만, 컴퓨터의 기초개념에 대한 내용은 컴퓨터 용어 및 간단한 기초설명에 그치고 있어 컴퓨터 과학에 대한 지식을 얻기 위해서는 많은 어려움이 있다.
제7차 교육과정에서 ‘정보통신 기술 교육을 더욱 강조하면서 컴퓨터 활용에 중점을 두고 컴퓨터의 기초 개념과 다양한 응용 소프트웨어와 컴퓨터 통신 기능을 복합적으로 이용하여 학습에 활용했던 것’을 문제로 하여 개정안에서는 ‘정보통신기술의 원리, 개념, 알고리즘 등 컴퓨터 과학에 대한 내용으로 교육과정을 개정’하기로 되어있다.
최근 컴퓨터 교육에서 알고리즘에 대한 관심이 높아지고 있다. 또한, 알고리즘 교육에 관한 연구보고가 계속 나오고 있다. 알고리즘은 문제 해결의 과정에서 분석력, 논리력, 창의력을 기르는데 우수한 효과가 있다는 연구보고가 많다. 본 연구는 컴퓨터 과학의 기초 개념 중 알고리즘을 선택하여 알고리즘 표현방법에 따른 교육효과에 대해 연구 하였다. 컴퓨터의 기초개념인 알고리즘을 동일한 문제에 대해 알고리즘 표현방법의 4가지를 기술하여 교육함으로써, 문제해결에 있어서 학생들의 분석력, 논리력, 창의력을 기르고, 각 알고리즘 표현방법 중에서 어떤 표현방법이 중학교 학생들의 알고리즘 교육에 효과가 있는지 연구하였다.
본 연구를 하기위해 정보처리기능사 자격증 취득을 원하는 중학교 학원학생을 대상으로 교육을 통하여 다음과 같은 결론을 얻었다.
첫째, 자연어 표현방법은 문제를 해결하는 방법을 표현한 것으로 알고리즘에 대해 이해하기는 쉬우나, 프로그래밍으로 바꾸기에는 조금 어려운 부분도 있었다.
둘째, 순서도 표현방법은 순서도를 보면서 프로그램의 순서를 알 수 있어서, 알고리즘도 이해가 빠르며, 프로그래밍으로 바꾸기도 쉬웠다.
셋째, 프로그래밍 표현방법은 알고리즘을 이해하려면 어느 정도 프로그래밍을 알아야하고 이해하기 힘든 부분도 있다.
넷째, 의사코드 표현방법 역시 알고리즘을 이해하려면 어느 정도 프로그래밍을 알아야하고, 프로그래밍으로 바꿀 때 사용하는 언어의 형식만 바꿔서 코딩하면 되므로 쉬웠다. 라는 결론을 얻었다.
본 논문의 결과로 자연어 표현방법과 순서도 표현방법을 적절하게 이용하는 것이 학생들이 알고리즘 이해에 적당하다는 것을 알았다. 앞으로 컴퓨터 교육에 있어서 알고리즘 교육방법에 대한 연구를 더욱 함으로써, 초&#8228;중학교 학생들에게 응용 소프트웨어의 기능 교육과 함께 알고리즘 교육이 활성화되어, 컴퓨터를 올바르게 활용할 수 있는 교육이 되어야 할 것이다.Currently, the contents of middle school and high school textbook of computer science are focused on applying computer skills. It deals with computer software application mainly but the basic concept of computer is hard to find. Therefore it is quite difficult to get information on basic concept of computer science.
The 7th national education guide renewed the existing instruction of education purpose for computer science. The existing instruction was focusing on Information and Communication skills and computer appliances including software skills and computer communication functionalities. But the 7th national education guide has changed the instruction to ‘overall computer science materials including the principal, concept and algorithm of Information and Communication.
In these days, algorithm has been one of the issues on computer education. Studies on Algorithm education is being updated quite frequently. And many of the study agree that the algorithm has the effectiveness of enhancing developing analytic, reasonable, creative capability while finding solution for problems. This study is aimed for describing the educational effectiveness of algorithm, the basic concept of computer science.
The four types of algorithm identification were trained to middle school students to study which of them is suitable for educating algorithm for middle school students.
The study concluded that the natural type and flow chart type of algorithm identification are proper for middle school students to understand the concept of algorithm. From now on, algorithm education should be expanded so that elementary school students and middle school students can get the chance to apply computer better along with the software education.목 차 Ⅰ. 서 론 1 1.1. 연구의 필요성 및 목적 2 1.1.1. 알고리즘 교육의 필요성 2

소프트웨어 개발 프로세스는 요구사항 기술, 소프트웨어 설계, 구현, 테스트, 유지보수의 단계로 진행 된다. 그 중 첫 단계인 요구사항 기술은 다양한 이해관계자로부터 소프트웨어에 대한 요구사항과 조건을 도출하고 결정하는 단계로써 소프트웨어 프로젝트, 특히 대규모 소프트웨어 프로젝트에서 매우 중요한 단계이다. 이러한 요구사항 기술의 단계는 요구사항의 도출, 분석, 명세의 차례로 진행되며 대다수의 소프트웨어 프로젝트에서 자연어를 사용하여 작성되는 요구사항 명세서를 사용한다. 하지만 자연어가 가진 모호성은 소프트웨어 개발 시 해석하는 시각에 따라 원래 의도한 바와 다른 요구사항이 반영될 가능성이 있다. 또한, 일반적으로 문서 형식으로 작성되는 요구사항 명세서는 그 양이 방대하며 검토 및 분석이 용이하지 못하여 이해관계자의 원활한프로젝트 참여를 저해하는 주요 원인으로 작용한다. 이와 같은 요구사항의 모호한 명세 및 이해관계자의 낮은 프로젝트 참여도는 요구사항 기술 이후의 개발 단계에서 소프트웨어의 재설계 및 재개발 등의 작업을 필요로 하게 한다. 이 결과로 프로젝트의 전체비용 및 시간은증가되며 심지어는 프로젝트의 실패를 야기시킬 수 있다.
요구사항 명세의 공통적인 이해와 요구사항 기술 단계에서 여러 이해관계자의 참여도를 높이기 위하여 현재 사용되는 가장 기본적인 방법은 요구사항 명세에 USE CASE와 같은 시각적 모델을 추가하여 보충 설명하는 것이다. 하지만 이러한 방법은 개발자 이외의 이해관계자에게 소프트웨어 공학 분야의 전문지식을 추가적으로 요구하기 때문에 효과가 크지 않다. 이러한 문제점을 해결하기 위하여 자연어로 작성된 요구사항 명세를 일반인들이 더욱 이해하기 쉬운 시각화된 모델로 표현하는 연구들이 많이 진행되었다. 하지만 대부분의 관련 연구들은 자연어 처리도구를 이용하여 모델을 자동적으로 생성하는 기법을 사용하므로 정확하고 직관적인 요구사항의 명세를 제공할 수 없으며 이는 여전히 요구사항들의 의미 소실로 이어질 수 있다. 또한, 현재까지 제안된 다수의 요구사항 명세 모델을 정확히 해석하고 이해하기 위해서는 제안된 모델에 대한 전문지식이 필요하기 때문에 이해관계자의 프로젝트 참여에 방해요소가 된다.
이에 본 논문에서는 생각, 개념, 지식 등을 마음속에 지도를 그리듯이 표현함으로써 효과적인 기억, 학습, 사고를 가능하게 하는 마인드맵을 확장하여 요구사항을 명세하는 기법 및 도구를 제안한다. 마인드맵은 이미 일반 대중에게 잘 알려져있으며 널리 사용되고 있기 때문에 이를 사용하여 명세된 요구사항은 특별한 전문지식 없이 쉽게 이해가 가능하다. 이와 더불어, 마인드맵의 간단 명료한 표기법은 자연어에 비하여 모호성이 크게 낮아 요구사항 명세 시 의미 손실을 최소화 할 수 있다. 그러므로, 본 논문에서 제안하는 기법 및 도구는 소프트웨어 개발 프로젝트의 다양한 이해관계자 모두가 쉽게 이해할 수 있으며 공통적인 의미를 가진 요구사항의 명세를 가능하게 하여 명세된 요구사항의 정확성 및 일관성을 보장하고 이해관계자의 프로젝트 참여도 역시 높일 수 있도록 하는 데에 목표를 설정하였다. 더불어, 본 연구는 향후에 대규모 소프트웨어 개발 프로젝트의 효과적인 진행 방법 개발 및 관련 도구 구축을 위한 기틀을 마련하기 위해 진행되었다.

본 논문에서 강화 학습(reinforcement learning)이란 기계 학습 방법의 한 종류로 환경으로부터 주어지는 보상을 최대화하는 방식의 학습 모형을 가리킨다. 이 방법론이 기계 학습 방법의 주류인 지도 학습(supervised learning)과 원리적으로 다른 점은 이 방법론이 에이전트가 무엇을 할 지에 대한 구체적인 지도(supervision)를 하지 않고, 에이전트가 처한 환경 안에서 여러 시행을 통해 그것을 발견하도록 한다는 점에 있다. 이 과정에서 에이전트가 취하는 행위, 즉 행동은 바로 주어지는 보상 외에도 계속적으로 이어지는 상황들에 따른 보상들을 최대화하는 것을 목적으로 하게 된다.

본 연구의 주요 목적은 이 강화 학습 방법론을 통해 자연어 텍스트를 점진적(incremental), 연속적(continuous)으로 처리하는 방법을 연구하는 데 있다. 자연어 처리, 혹은 전산 언어학은 다양한 과제를 가지고 있는 방대한 분야이므로 본 논문에서는 크게 두 가지 종류의 과제, 상태 값 측정 과제와 상태 유형 예측 과제를 중점적으로 다룬다. 전자의 경우에는 어휘의 감정 극성 값을 강화 학습의 핵심적 알고리즘인 시간차(temporal difference) 알고리즘을 통해 측정하는 방법을 연구하고, 후자의 경우에는 의료 텍스트 문장의 연속적 상태 측정을 강화 학습 방법론을 통해 수행하는 방법을 살펴볼 것이다.

자연어 텍스트의 점진적, 혹은 연속적 처리 과정은 인간의 언어 심리학적 처리양상을 볼 때 인지 친화적인 접근법으로 보이는데, 왜냐하면 여러 심리학적, 신경학적 연구결과들을 통해 언어 처리의 과정이 기본적으로 자동적, 점진적 처리임이 알려져 있기 때문이다. 본 논문에는 그런 측면에서 강화 학습을 보다 인지 모형에 입각하여 활용하고자 하는 노력이 들어 있다. 마지막 논의 부분에서는 인지모형 기반 강화 학습에 대해 논의하고 이 방법을 자연어 처리에 어떻게 활용할 것인지에 대한 전망을 다루었다.1 서론 1 1.1 강화 학습의 짧은 역사 . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 연구 질문들 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 논문의 전체적 구성 . . . . . . . . . . . . . . . . . . . . . . . . . 5

As a result, public interest in the implementation of the pledge of local autonomous governors elected by the support of voters is on the rise. In addition, monitoring and monitoring of citizen organizations (Korea Manifesto headquarters, Legal Consumer Federation) and periodical evaluation of quarterly and semi-periodic evaluations based on the contents of manifestos Communication with local residents is becoming increasingly important through the systematic management of details, public information disclosure and publicity.
In order to get closer to the residents through web communication, the systematic management and management of the road map are required and the road map should be presented.
However, in each municipal government, the employees of the pledge department manage the pledge project execution management status from each department by hand and manage it in a non-efficient manner. The data related to the pledge business were generated and stored while carrying out work in the On-Nara document system, but it was difficult to utilize it efficiently because it was managed and archived as unstructured data.
In addition, it has been difficult to obtain the latest information by publishing contents related to the pledge (quarterly, semiannual, annually) and on the homepage. In this paper, we study the construction and application of information technology applied to identify the areas where efficiency can be maximized through application of information system and to support it.
In other words, considering the application of Machine Learning technology in the process of extracting the pledge business related data from the unstructured document of the On-Nara document, efficiency can be secured even in semi-automation process such as user intervention(Win-Win of human and machine with creative cooperation ability suitable for the 4th industrial revolution era), linkage of “On-Nara document system” and “Election pledge management system”(tentative name) that systematically manage the work performance of local government To design real-time collection
methods of the pledge business performance, Extraction of data necessary for management of pledge status from unstructured data managed as PDF file on On-Nara document system storage by applying natural language processing-based machine learning technology, design and implementation of pledge database management plan, evaluation criteria of citizen's pledge implementation status, It is possible to automatically check the execution status of the promise of the director by using the preliminary simulation function of the promise rate and to visualize the information from the promise management system in real time and in the chart And to create digital visualization contents in various forms such that the local governor is able to promote his achievement to the voters.
In this way, through this study, we have implemented the model of construction of the pledge management system that systematically and efficiently manages the pledge project of the local autonomous governor and can produce the content automatically for visualization promotion of the head of the province. However, 86.9% and 89.9%, respectively. Therefore, it is necessary to continue the research using structuring and Text Mining techniques to improve the extraction rate.선거 시 정책을 공약으로 제시하고, 이를 기반으로 하여 유권자의 지지로 당선된 지방자치단체장의 공약 이행에 대한 국민의 관심이 점차 높아지고 있다.
또한 시민단체(한국매니페스토실천본부, 법률소비자연맹)에서의 모니터링 및 홈페이지 내에 매니페스토(공약집) 관련 공개되는 내용을 근거로 분기별, 반기별 주기적인 평가 및 최우수 지방자치단체장의 시상으로 자치단체장에게는 공약 이행 내역의 체계적인 관리, 적극적인 정보공개 및 홍보를 통해 지역 주민과의 소통이 점점 중요시 되고 있다.
그리고 지방자치단체장이 웹 소통으로 주민에게 좀 더 가까이 다가기 위해 공약의 체계적인 목표와 로드맵(Road Map)의 제시가 필요하고, 이를 이행하는 단계에서의 시스템적인 관리가 필요하게 된다.
그러나 각 지방자치단체에서는 공약사업 담당부서의 직원들이 수작업으로 공약사업 이행관리 현황을 각 부서로부터 취합하는 형태의 비(非) 효율적으로 관리하고, 홈페이지를 통해 이를 공개하고 있는 실정이다.
공약사업 관련 데이터는 온-나라 문서시스템에서 업무를 수행하면서 생성되고 보관되어지고 있으나, 비정형 데이터 형태로 관리, 보관되어 효율적 활용이 어려웠었다(유소영 외. 2010).
또한 주기(분기, 반기, 매년)별 공약 관련 콘텐츠의 제작 및 홈페이지에 공개함으로써 공개되는 자료의 최신성 확보가 어려웠다.
고로 이런 사항을 개선하고자 본 연구에서는 공약사업관리 절차 중 정보시스템 적용을 통한 효율화가 극대화 될 수 있는 영역을 식별하고, 이를 지원할 수 있는 정보기술을 적용한 시스템의 구축과 활용에 대하여 연구를 하였다.
즉, 온-나라 문서의 비정형문서에서 공약사업 관련 데이터를 추출하는 과정에 Machine Learning 기술 적용을 고려하되, 업무처리 결과의 정확성 확보를 위한 사용자 개입 등 반(半) 자동화 프로세스에서도 효율성을 확보할 수 있도록 하였고(4차 산업혁명 시대에 적합한 창조적 협동역량으로 인간과 기계의 Win-Win), 자치단체의 업무 수행 내역을 실시간 체계적으로 관리하는 온-나라 문서시스템과 공약관리시스템(가칭)의 연계를 통해 공약사업관련 실적의 실시간 수집 방안을 설계하고, 자연어처리 기반 기계학습 기술을 적용하여 온-나라 문서시스템 스토리지에 PDF 파일 형태로 관리 되는 비정형 데이터로부터 공약이행 현황관리에 필요한 데이터 추출 및 공약 데이터베이스 관리방안 설계 및 구현, 시민단체의 공약이행현황 평가기준을 기반으로 공약 이행지표를 자동 생성하여 공개페이지에 공개하고, 공약이행율 사전 시뮬레이션 기능 등으로 기관장의 공약이행 현황의 사전 점검을 가능하게 하고, 공약관리시스템으로부터 정보를 실시간 및 도표로 시각화하여 제공하는 등 다양한 형태로 디지털 시각화 콘텐츠를 생성하여 자치단체장이 유권자에게 본인의 업적을 홍보할 수 있도록 관리하는 정보시스템을 구축하였다.
이처럼 본 연구를 통해 지방자치단체장의 공약사업을 체계적이고 효율적으로 관리하고 기관장의 시각화 홍보를 위한 콘텐츠 자동 제작 등을 할 수 있는 공약관리시스템 구축 모델을 구현하였으나, 비정형 데이터 1차, 2차 추출률이 약 86.9%, 89.9%로 향후 추출률 향상을 위해 구조화 및 Text Mining 기법을 활용한 지속적인 연구가 필요하다고 본다.국문초록 ⅷ 영문초록 ⅺ 제 1 장 서론 1 1.1 연구의 배경 및 목적 1

The Part-of-Speech(POS) tagging is a task assigning proper POS tag to the given word. As an agglutinative language, Korean's spacing unit - Eojeol - consists of a serise of pairs of morpheme and POS tag. Therefore, the Korean POS Tagging task can be divided into two sub parts. The one is the morphological analysis and the other is the POS tagging. The morphological analysis is a task which analyzes all possible series of pairs of morpheme and POS tag. The POS tagging is a task which assigns proper serise of pairs of morpheme and POS tag from the morphological analysis result. Thus, Korean POS tagging is more difficult and ambiguous than English POS tagging. Moreover, because POS tagging is essential to understand natural language, it is applied to most natural language processing applications such as parsing, machine translation, information retrieval, and question answering. Therefore, ths POS tagging accuracy significantly affects other applications' accuracy.
State-of-the-art of Korean POS tagger shows 95\% accuracy. It is high accuracy. However, because POS tagger's accuracy significantly affects other applications' performances, 5\% error rate should be improved.
During error analyzing, I found that certain POS tags are highly ambiguous and they make almost error. Moreover, they have typical error patterns. In addition to typical error patterns, large POS tagged corpus is recently constructed. It allows classifiers to be trained with large training data. It also increases classifier accuracy significantly.
In order to improve POS tagging accuracy, this thesis proposes two post-processing methods using characteristics which I mentioned above. First proposed method trains classifiers using wider contexts than state-of-the-art POS tagger. these classifiers use fetures which are extracted from large POS tagged corpus. Second proposed method trains highly ambiguous POS tag pair classifiers. Each classifiers classifies POS tag between certain highly ambiguous POS tag pair. Thus, classifiers assigns proper POS tag to a given word with higher accuracy than state-of-the-art POS tagger and improve POS tagging accuracy.
Each proposed method reduces 6.86\%(95.251\% → 95.577\%) and 13.11\%(95.251\% → 95.874\%) error rate measured by eojeol accuracy. Combining two proposed method, result shows 16.91\%(95.251\% → 96.054\%) error rate reduction measured by eojeol accuracy.한국어 형태소 분석은 주어진 문장에 대하여 문장을 구성하는 각 어절에 대응하는 가능한 모든 형태소, 품사열을 밝히는 작업이며, 품사 부착은 형태소 분석에서 나온 가능한 모든 형태소 분석 후보들 중 주어진 문장에 대하여 가장 적절한 형태소, 품사 열을 결정하는 것이다. 또한 한국어는 교착어(agglutinative language)로서 하나 이상의 형태소가 결합하여 어절을 이룰 수 있기 때문에 중의성이 매우 높다. 이러한 한국어 형태소 분석 및 품사 부착은 자연어를 이해하여 처리하는데 있어서 가장 기본적인 기술이어서 구문 분석, 기계번역, 정보 검색, 질의 응답 등 다른 모든 자연어처리 시스템에 사용된다. 따라서 품사 부착의 오류는 다른 상위 시스템에 전파되기 때문에 자연어처리 시스템의 성능에 영향을 미치는 부분이다.
그러나 현재 한국어 품사 부착의 성능은 어절 단위로 95\% 수준이며, 높은 수준이기는 하나 품사 부착이 다른 자연어처리 시스템에 미치는 영향을 고려한다면 성능을 더욱 향상할 필요가 있다.
품사 부착의 오류를 분석하면 일부 중의성이 높은 품사에서 많은 오류가 발생하며 이들 품사가 전체 오류 중 대부분을 차지하고, 유형은 거의 정해져 있다. 또한 최근 대량의 품사 부착 말뭉치의 구축으로 대량의 학습 집합을 얻기 쉬워졌다.
본 논문에서는 한국어 품사 부착의 성능을 향상하기 위하여 위에서 언급한 두 가지 특징을 이용한 두 가지 후처리 방식을 제안한다. 첫째는 기존 자동태거보다 넓은 문맥 자질을 이용한 분류기를 대량의 학습 집합을 이용하여 학습시킨 후 다른 자동태거의 결과에 후처리 하여 성능을 향상한다.둘째는 높은 중의성으로 인하여 낮은 정확률을 보이는 품사쌍을 대상으로 각 품사쌍에 적합한 자질을 이용하여 높은 정확률의 분류기를 학습시킨 후 다른 자동태거의 결과에 적용하여 성능을 향상한다.
제안하는 두 가지 후처리 방식은 어절 단위 평가로 각각 6.86\%(95.251\% → 95.577\%)와 13.11\%(95.251\% → 95.874\%)의 오류 감소율을 보였으며, 두 방식을 모두 적용하였을 경우 16.91\%(95.251\% → 96.054\%)의 오류를 감소하였다.ABSTRACT Contents Chapter 1 Introduction 1.1 Korean Morphological Analysis and Part-of-Speech Tagging 1.2 Error types of Korean Part-of-Speech Tagging

자연어 DB 질의 시스템은 사용자가 자연언어로 표현된 요구 사항을 입력하여 데이터베이스에 저장된 정보를 쉽고 편리하게 검색할 수 있도록 하는 시스템이다. 본 논문에서는 이러한 시스템 설계를 위해 자연어 질의를 분석하여 질의에 대응되는 SQL문 생성에 관한 확률적 접근 방법을 제안한다.

자연어 질의는 애트리뷰트(attribute)나 값(value)등을 표현 하는 단어가 개념 중의성을 발생 시킨다. 따라서, 자연어 DB 질의 시스템을 위해 이러한 단어의 개념 중의성 해소가 필수적이다.

제안된 방법에서는, 사용자 질의에서 나타나는 단어의 개념 중의성을 의미 태깅 문제로 변환하고, 그러한 단어로부터 개념을 결정하기 위하여 은닉 마르코프 모델을 사용한다. 그리고, 은닉 마르코프 모델의 최적 순서열을 얻기 위해서 비터비(Viterbi) 알고리즘을 이용하여, 질의로부터 DB 검색을 위한 정보를 추출한다. 마지막으로 시스템은 이러한 추출된 정보로부터 질의에 대응되는 SQL문을 생성한다.Natural Language Interface for Database(NLIDB) is a system that a user can easily and conveniently search information from a database by using a natural language query. We propose a statistical method which generates SQL from a natural language query.

Because words in a natural language query have sense ambiguity, it is very important to disambiguate the sense of a word.

To disambiguate sense of a word, we regard this problem as semantic tagging by using Hidden Markov Model, we use Viterbi algorithm to acquire the best sequence of concepts in Hidden Markov Model. The proposed system generate SQL for a given natural language query by extracting information from results of Viterbi algorithm.

딥 러닝은 기존의 기계학습 방법들과 달리 여러 층의 비선형 변환 hidden layer들을 통해 높은 수준의 추상화(abstraction)를 시도하여 새로운 자질의 조합과 표현을 학습할 수 있다. 이러한 딥 러닝은 음성인식, 이미지처리 등 다양한 분야에서 우수한 성능을 보이고 있으며, 최근 자연어처리 분야에 적용되는 연구가 많이 진행되고 있다.
딥 러닝의 가장 기본적인 모델인 Feed Forward Neural Network(FFNN)은 지정된 길이의 정보만을 보는 모델로 순차적인 열을 처리하는 대부분의 자연어처리에 적용하기에는 한계가 있다. 이에 따라 순차적인 열을 처리할 수 있는 Recurrent Neural Network(RNN)이 연구 되었으며, 이를 이용하여 sequence labeling, sentence classification, neural machine translation등에 적용하는 연구가 진행되었다.
본 논문에서는 한국어의 오류 교정 문제 중에서 언어 분석기 오류인 형태소의 원형 복원 오류와 문법 오류인 문맥의존 철자오류, 띄어쓰기 오류에 딥 러닝을 적용하고, 추가적으로 최근에 거의 연구가 되지 않은 한국어 구구조 구문분석에 딥 러닝의 모델인 Sequence-to-sequence 모델을 적용하였다. 실험 결과, 한국어 오류 교정 문제에 대해서 규칙과 통계를 사용하는 시스템이나 기존의 기계학습 방법인 Support Vector Machine(SVM) 보다 좋은 성능을 보였으며, 한국어 구구조 구문분석에 기계번역에 쓰이던 Sequence-to-sequence 모델을 적용하여 좋은 성능을 보였다.Deep learning, unlike existing machine learning methods, can learn the combination and expression of new qualities by attempting a high level of abstraction through several layered nonlinear transformation hidden layers. Such deep learning has shown excellent performance in various fields such as speech recognition, image processing.
Feed Forward Neural Network (FFNN), which is the most basic model of deep learning, is a model that only displays information of a specified length and has a limit to apply to most natural language processing that processes sequence. Recurrent Neural Network (RNN), which can process sequence, has been studied and applied to sequence labeling, sentence classification, and neural machine translation.
In this paper, we apply the deep learning to the error of the morpheme recovery and the grammatical errors which are the context-sensitive spelling error, and the word spacing error. In addition, Sequence-to-sequence model, a model of deep learning, is applied Korean phrase structure parsing. Experimental results show that the proposed system has better performance than SVM which is the existing machine learning method, rules and statistics based system for the Korean error correction problem and phrase structure parsing.1. 서론..........................................................1 2. 관련 연구.....................................................3 2.1 한국어 형태소의 원형 복원 관련 연구.........................3 2.2 문맥의존 철자오류 교정 관련 연구......................4 2.3 한국어 자동 띄어쓰기 관련 연구.....................5

자연어처리에서 필연적으로 부딪치는 데이터 부족 문제를 해결하기 위해서 클러스터링 기술을 이용하고자 하는 것은 일반적인 접근 방법이다. 즉 단어 클러스터링은 자연어처리 분야의 중요한 요소 기술 중의 하나이다.
단어 클러스터링을 수행하기 위해서는 어떠한 단어들을 하나의 클래스로 포함시킬 것인가를 결정하여야 하는데, 이를 사람이 단어마다 그 의미를 고려하여 수작업을 위주로 진행하는 방법이 있다. 이 경우 막대한 시간과 노력을 필요로 하며, 작업에 참여하는 사람 간에 그리고 같은 사람의 작업이더라도 작업 시점 간에 클러스터링 관련 결정을 내리는 데 있어서 일관성을 유지하기 어렵다. 결국 수작업에 의한 단어 클러스터링은 한계가 존재하며 자동화가 요구된다. 여기서 단어 클러스터링의 자동화 기술 중 대표적인 것으로 Brown 단어 클러스터링 기술을 들 수 있다.
하지만 Brown 단어 클러스터링 기법은 실제로는 계산양이 매우 커서 이용하기 쉽지 않은 것으로 관찰되었다. 본 논문은 Brown 자동 단어 클러스터링 기법을 구현함에 있어 속도의 향상을 목표로 하였다. 이를 위해 다음과 같은 두 가지 방법을 사용하였다. 첫째로 작업 중에 단어 쌍 공기정보를 하드디스크 대신에 메모리에 저장하여 사용하며, 둘째로 클래스 사이의 합병기준으로 사용하는 상호정보 손실량(loss)의 수식을 유도하여 시스템을 구현하였다. 이러한 기법을 적용한 결과 클러스터링 시스템의 속도가 그렇지 않은 경우 보다 약 7.9배 이상 향상됨을 실험으로 관찰하였다.

엔티티 링킹은 주어진 텍스트 입력에서 텍스트를 구성하는 단어열과 관련성이 가장 깊 은 하나의 엔티티를 부착하는 자연어처리의 한 분야이다. 그리고 본 논문에서 다루고 자 하는 쿼리 엔티티 링킹은 웹 쿼리를 텍스트 입력으로 한다. 쿼리 엔티티 링킹 시스 템들은 웹 쿼리에서 쿼리 단어열과 그에 관련성 있는 하나의 엔티티를 부착한다.
쿼리 엔티티 링킹은 엔티티 정보를 통해 사용자의 검색 의도 파악에 도움을 줄 수 있다는 측면 때문에 구글, 야후, 마이크로소프트 등 세계적인 검색 엔진 회사를 중심으 로 발전하여 왔다. 또한, 쿼리 엔티티 링킹을 응용한 시스템인 엔티티 검색 및 물품 검 색은 현재 검색 연구의 중요한 트렌드 중 하나이다.
쿼리 엔티티 링킹의 기존 엔티티 링킹과의 가장 큰 차이는 짧은 쿼리 입력이다. 기존의 엔티티 링킹 연구들은 문서 또는 문장 입력 내에 존재하는 충분한 텍스트 정보 로부터 여러 자질(Feature)을 추출함으로써 정확성을 향상시켜 왔다. 그러나, 짧은 길
이의 쿼리가 입력되는 쿼리 엔티티 링킹은 부족한 텍스트 정보 대신 위키피디아, Freebase 등의 공개된 지식 베이스에 기반하여 이들의 여러 자질을 발굴하는 방향으 로 발전하였다. 하지만 자질 엔지니어링 기반의 문서 및 쿼리 엔티티 링킹 연구들은 자질 추출의 복잡성 및 추출되지 않은 데이터의 특성을 학습시키기 어렵다는 한계점이
있다.
최근 자연어처리 분야 전반에 딥러닝을 활용하고자 하는 경향에 따라 엔티티 링킹 에서도 딥러닝을 사용한 여러 시도가 있었다. 이들 연구는 문서 내에 존재하는 문장 단위의 풍부한 문맥을 바탕으로 멘션과 엔티티의 문맥과의 관련성을 학습하거나 문맥 을 공유하는 엔티티 간 관련성을 측정하는 등의 방법을 통해 엔티티 중의성을 해결하 고자 하였다. 하지만, 문맥 정보가 거의 없는 쿼리에서는 엔티티 중의성을 해결하기 어 려워져, 이 문제를 쿼리 내의 정보 내에서 해결하는 방법론이 필요하다.
본 논문에서는 딥러닝 아키텍처를 이용하여 쿼리 엔티티 링킹을 수행하는 모델을 제안한다. 제안한 모델은 딥러닝을 적용함으로써 기존의 자질 엔지니어링 기법을 이용 한 쿼리 엔티티 링킹 모델들과 달리 자동으로 멘션과 엔티티 간 표현을 추출할 수 있 다. 본 논문에서는 여러 자질 엔지니어링 기반의 엔티티 링킹에서 수행해 온 여러 요 소의 특징을 자동적으로 학습하기 위해 멘션과 엔티티 간 표현 및 관련성을 학습하는 멘션-엔티티 쌍 인코더, 여러 멘션-엔티티 쌍 간 관련성을 학습하는 멘션-엔티티 열 인코더, 두 엔티티 간 관련성을 측정하는 엔티티 관련성 인코더를 제안한다. 본 모델은 기존 엔티티 링킹에서 활용되어 온 문맥 정보가 쿼리 입력에서 활용이 불가능함에 따 라 이에 맞게 멘션-엔티티 쌍 및 임베딩된 엔티티 정보를 활용함으로서 엔티티 중의 성을 해소하고자 하였다.
그리고 본 논문에서는 제안한 모델을 검증하기 위해 쿼리 엔티티 링킹의 기존 연 구에서 활용해 온 데이터셋에 대해 여러 가지 평가 태스크에 대한 실험을 수행하였다. 본 논문은 여러 모델 실험 및 타 연구와의 비교 실험을 수행하고 결과를 여러 측면에 서 분석함으로써 본 모델의 우수성을 입증하였다.Entity linking is a subfield of natural language processing that involves identifying and mapping entities, which are most relevant to a word sequence in a given text input to a target knowledge base. In particular, entity linking in queries refers to associating entities, which are related to a certain word sequence with the input web query.
Entity linking in queries has been employed by several global search engines such as Google, Yahoo!, and Microsoft, since it facilitates the analysis of the search intent of users through the study of linked entities’ information. In addition, products and systems incorporating entity linking techniques (such as entity retrieval) are important subjects of research for search engines.
The most significant difference between entity linking in queries and entity linking in documents is that the former involves short input queries. Conventional entity linking systems have superior execution performance since they extract features from a text input based on the adequate information present in the text itself. However, entity linking in queries employs short query texts with inadequate text features as input, and it thus identifies various features based on the information present in open-source knowledge bases such as Wikipedia and Freebase. Furthermore, entity linking in queries incorporates feature engineering and has drawbacks in terms of the complexity of feature extraction and difficulty in learning the characteristics of latent data.
Numerous recent studies in the field of entity linking have attempted to determine the degree of relatedness between mentions and entities by employing deep learning techniques. However, it is difficult to identify such relatedness within a short input text such as a query text, since deep learning techniques are based on the assumption that the input text is contextually adequate. Additionally, in the case of conventional deep learning methods, the training cost can become extremely high while mapping the entity representations to the pre-trained word embedding space.
In this paper, a model is proposed for entity linking in queries that can automatically extracts representations from mention-mention pairs by employing deep learning architecture. Additionally, the followings are proposed in this paper: a mention-entity pair encoder for determining the relatedness between each mention and its corresponding entity, a mention-entity sequence encoder for identifying the relationships among various mention-entity pairs, and an entity relatedness encoder for identifying the semantic relevance between two entities in query texts so that learning is performed more effectively than the existing deep learning methods. The relatedness between mentions and associated entities was directly determined by incorporating the ConVec, which simultaneously trains both entities and words so that they can be represented in the same semantic vector space. Furthermore, several entity linking tasks were performed by the proposed model, and the experimental results with the corresponding state-of-the-arts are discussed in this paper.1장 서론 1 1.1. 엔티티 링킹 및 쿼리 엔티티 링킹 1 1.1.1. 엔티티 링킹 2 1.1.2. 쿼리 엔티티 링킹 4 1.2. 문서 및 문장 엔티티 단위의 엔티티 링킹의 요소 6

의미역 결정은 문장의 각 서술어의 의미와 그 논항들의 의미역을 결정하여 “누가, 무엇을, 어떻게, 왜” 등의 의미 관계를 찾아내는 자연어처리의 한 단계이며 정보 추출, 문서 분류, 질의응답 시스템의 중간 과정으로 사용될 수 있다. 의미역 결정 연구는 크게 격틀사전에 기반을 둔 방법과 말뭉치에 기반을 둔 방법으로 나눌 수 있다. 최근 의미역 결정 연구에는 의미역 말뭉치와 기계학습 알고리즘을 이용한 연구가 주를 이루었다. Structural SVM과 같은 기계학습 알고리즘을 이용한 기존의 의미역 결정 연구는 사람이 고안한 자질을 입력으로 받고 반복적인 실험을 통해 입력 자질들의 최적의 가중치를 구한다. 그러나 각 자연어처리 모듈마다 적합한 자질을 설계하고 최적의 자질 조합을 구하는 것은 많은 시간과 노력을 필요로 하는데, 최근 이러한 문제점을 해결하기 위해 자질들을 높은 수준의 표현으로 추상화 시켜줄 수 있는 딥 러닝 기술이 기존 기계학습 알고리즘의 대안으로 떠오르고 있다. 따라서 본 논문에서는 여러 딥 러닝 모델을 한국어 의미역 결정에 적용하여 그 결과를 분석한다.
기존 의미역 결정 연구에서는 의미역 결정의 성능 향상을 위해 구문 분석 정보를 의미역 결정의 자질로 사용하였다. 하지만 구문 분석 정보의 사용은 의미역 결정 이전에 구문 분석을 수행해야 하는 비용이 발생하게 되고, 구문 분석 단계에서 발생하는 오류를 그대로 답습하게 되는 단점을 가지고 있다. 이러한 문제점을 해결하기 위해 본 논문에서는 구문 분석 정보를 제외하여 형태소 분석 정보만을 사용하는 end-to-end 방식의 한국어 의미역 결정 시스템을 제안하고 이를 Bidirectional LSTM-CRF 모델에 적용하여 기존 연구보다 더 높은 성능을 얻을 수 있음을 보인다.Semantic Role Labeling (SRL) is a basic step of natural language processing for semantic analysis of natural language text. The task of SRL system is to find “who, what, how, why”in natural language sentences by determining the semantic role labels of the arguments of the predicates in natural language sentences. Also SRL systems can be used in other natural language processing such as information extraction and question answering systems. SRL studies can be seperable two sides: one is a frame based method, the other is a corpus based method. Recently, the interest of SRL research is focused in corpus based method using machine learning algorithms. However, finding well designed features are expensive and time-consuming. Owing to this problem, Deep learning that models high-level abstractions is getting rised for altenative the existing machine learning algorithm. So, in this paper, we apply several deep learning models to korean semantic role labeling and analyze their results.
Previous researches use syntactic information for improving the performance of semantic role labeling system. However, syntax analysis cause computational overhead and incorrect syntactic information. To solve this problem, we do not use the syntactic information and we only use the morpheme information. In this paper, we propose an end-to-end system using Bidirectional LSTM-CRF model. Our experimental results show that our proposed model has a better performance than traditional models.1. 서론..........................................................1 2. 관련 연구.....................................................4 2.1 한국어 의미역 결정 기존 연구.............................4 2.2 구문 분석 정보...........................................5 2.3 딥 러닝..................................................6

본 논문에서는 자연어, 특히 한국어 이해에 적합한 의미표현방법을 제안하였다. 한국어의 개념구조 관계를 predicate 와 argument 를 중심으로 이원적으로 분석 하고, predicate 의 고유기능을 6개 격 범주로 형식화 할 수 있음을 보였다.
predicate 와 argument 의 결합관계를 분석하여 합리적으로 문법, 의미를 표시할 수 있는 semantic schema 구조를 유도하여 이들의 실제 구성방법을 여러가지 예를 통하여 실증하였다.
제안된 schema 구조는 문법과 의미의 관계를 동시에 표현할 수 있기 때문에 추론 등의 개념연산 뿐 만 아니라 자연어 이해를 위한 시스템에 적합함을 보였고 ATN 으로 schema를 implementation 한 시스템도 제시하였다.The paper presents a semantic representation more adequate for Korean Language as a natural language.
By analyzing the conceptual structure of Korean as a relationship between the predicates and the arguments, six primitive case frames are suggested.
A semantic schema structure which represents the syntacto-semantic relation is derived from the cohesive relation between the predicates and the arguments. Several examples are given to clarify its representational ability.
Proposed schema structure is suitable for natural language understanding and inference logic, as it can represent the syntacto-semantic relations.
A system for semantic schema configurated with the ATN defined in this paper is also presented.목차 = i 요약 = ii Abstract = iii 제1장 서론 = 1 제2장 한국어의 형태구조 분석 = 3

정보검색은 사용자의 정보 요구에 적합한 정보를 신속하게 찾아내어 사용자에게 제공하는 것을 말한다. 정보검색 시스템을 사용하는 사용자는 정보의 요구에 대한 답을 기대하지만 현재 우리가 사용하고 있는 정보검색 시스템은 정보 요구에 연관된 문헌들의 집합을 제공한다.
오늘날 정보검색 시스템이 사용자의 정보 요구에 검색 결과로 제공하는 연관 문헌 집합에는 사용자의 의도와 다른 비 연관 문헌이 포함된다. 비연관 문헌이 검색 결과에 포함되는 원인은 색인어가 문헌의 내용을 대표할 수 없고 질의가 사용자의 정보 요구를 제대로 표현하지 못하기 때문이다.
정보의 요구가 있는 사용자들이 양질의 검색 결과에 대한 기대가 커지면서 정보검색 시스템은 텍스트로 이루어진 연관된 문헌 집합을 검색하는 것에서 사용자의 정보 요구에 정답을 제공하는 자동 질의응답 시스템(automatic question & answering system)에 대한 연구가 수행 중이다. 자동 질의응답 시스템은 사용자의 정보 요구에 유사한 문헌 집합이 아닌 정답이나 연관된 단락을 제공하지만 자연어 처리 기술의 한계로 성능이 떨어져 사용자로부터 큰 호응을 얻지 못하고 있다.
본 논문은 기존의 정보검색 시스템의 검색 결과에 비연관 문헌이 포함되는 단점과 질의응답 시스템이 가지는 자연어처리 기술의 한계를 극복하고, 사용자의 정보 요구에 실시간으로 정보를 제공하며 사용자의 참여를 적극 활용하여 웹 2.0 환경으로의 변화에 적응할 수 있는 집단지능을 이용한 실시간 질의응답 시스템을 제안한다.Information retrieval means providing users with proper information to the users' request after promptly finding it. Users employing information retrieval system expect answers to their requests, but our information retrieval system we use provides collection of documents related to requested information.
In today's information retrieval system unrelated documents different from users' intention are included in collection of documents provided with results of searching for users' request. The reason that unrelated documents are included in search result is that index languages can not represent the contents of documents and the inquiry does not rightly express users' request of information.
Research is underway for automatic question & answering system which provides answers to users' request of information in what information retrieval system is searching collection of documents related composed with texts as users' expectation for good quality search results gets increased. Automatic question & answering system doesn't get great response from users with limitation of technology handling natural language though it provides answers or related phrases, not similar documents collection for information request.
Overcoming limitation of natural language handling technology that Q&A system has and demerits that unrelated documents are included in the results of searching in existing information retrieval system, This research suggests manpower based Q&A system using collective intelligence that can adapt to change to Web2.0 environment by actively applying users' participation and providing real-time information to users' request of information.제1장 서론 = 1 제1절 연구배경 = 1 제2절 연구내용 = 3 제3절 논문구성 = 5 제2장 관련연구 = 6



일리노이 공대(IIT)에 있는 어휘사전 연구실에서 수년동안 자연어처리용 전자사전(lexicon)의 구축을 위해 노력해 왔다. 전자사전 구축을 위해서 사용되는 입력 정보원은 일반 사전을 컴퓨터가 처리할 수 있는 파일 형태이다. 그런데 일반 사전을 이용해서 전자사전 구축할 때 가장 큰 문제점은 일반 사전에는 고유명사에 관한 내용이 극히 제한적이다. 새로운 신문기사나 잡지들을 처리하는 시스템에서는 문제가 심각하다. 이와 같은 문제점을 해결하기 위하여 월스트리트 저널 (Wall Street Journal)에서 전자사전에 수록될 고유명사에 관한 정보를 추출할 수 있는 자연어처리 시스템(EXPLEX)을 개발하였다.
시스템은 신문기사의 문장에 포함되어 있는 고유명사와 관련된 부분들을 찾아내고, 그 부분에서 고유명사에 관한 정보를 추출한다. 이러한 정보는 어휘사전의 구축에 사용된다. 시스템은 영어 고유명사의 구조와 문장의 지역적인 문맥(context)을 이용하여 전자사전에 필요한 고유명사에 관한 어휘들을 만들어간다.
이 시스템 월스트리트 저널에서 고유명사와 그와 관련된 내용을 추출한다. 추출된 정보는 어휘사전이나 지식베이스의 요소(entry)로 사용된다. 따라서 EXPLEX는 입력된 문장에서 고유명사와 관련된 문장의 일부 (fragments)를 찾아내고, 여러 곳에 분리된 조각들을 분석하여 원하는 정보를 추출하기 위해 특별한 목적으로 구현되었다. EXPLEX는 크게 네 부분의 모듈로 구성된다: lexical analyzerm semantic taggerm semantic parserm coreference resolution.
어휘분석기(lexica analyzer)에서는 주어진 문장의 입력 문자열을 분석하여 토큰 (troken)을 만들고, 그 토큰에 해당하는 문자 패턴(character pattern)과 의미범주 (semantic category)를 활당한다. 어휘분석기의 출력은 다음 단계의 semantic tagger에서 입력으로 사용된다. 이 단계에서는 고유명사를 인식하고, 고유명사를 의미범주로 분류한다(예:회사이름, 상품이름, 사람이름등). 뿐만 아니라 고유명사와 관련된 의미정보를 포함하는 fragment를 찾아낸다. 찾아낸 고유명사에 관한 정보는 프레임(frame)으로 출력된다. 다음 단계인 semantec parser에서는 context-free와 context-sensitive 규칙(rule)을 이용하여, 전 단계에서 고유명사에 관한 부분적인 정보를 모아서 프레임(frame) 구조의 결과를 생성한다. EXPLEX는 160만 단어를 포함하고 있는 코퍼스(corpus)를 처리하여 13만개의 고유명사를 위한 어휘사전 요소를 만들었다. 그 결과에는 사람이름, 단체명, 문서이름, 사건명, 사물, 작품명, 국가, 지리명 등과 같은 종류의 고유명사들이 있다. 서로 다른 종류의 명사들은 슬롯 (slot) 의 개수가 다른 프레임 구조를 갖는다.

의미역 결정은 자연어 이해 중 한 분야로써, 문장의 술어를 찾고 그 술어와 연관된 논항들의 의미 관계를 파악하는 작업이다. 의미역 결정 결과는 자연어처리 전 분야의 자질로 사용될 수 있다. 기존 통계 기반 연구에서는 hand-craft된 언어적 자질이 주로 연구되었지만, 심층 학습 기반 모델은 이를 데이터로부터 스스로 학습한다는 장점이 있다. 기존 심층 학습 기반의 한국어 의미역 결정 연구에서는 Bi-LSTM-CRFs(Bidirectional Long Short Term Memory Conditional Random Fields)가 일반적으로 사용되었다. 현재까지의 입력 정보들을 고려하여 현재 입력에 대한 의미역 태그를 출력해야 하는 구조이기 때문에 시계열 데이터 처리에서 우수한 성능을 보이는 RNNs(Recurrent Neural Networks) 구조를 적용한 모델이다. 또한 인접 의미역 태그 간 관계를 모델링하기 위해 마지막 출력 계층에 CRFs를 적용했다는 특징이 있다. 하지만 이와 같은 Bi-LSTM-CRFs 구조에서는 술어 정보를 단어 임베딩(Embedding)으로 밖에 표현할 수 없기 때문에 술어와 의미역 태그 사이의 복잡한 관계를 모델링하기 힘들다는 한계점이 존재한다. 따라서 본 논문에서는 술어와 의미역 태그 사이의 관계를 모델링하기 위해 입력 술어에 따라 동적인 값을 갖는 학습 가중치 방법을 적용했다. 또한 문맥 표현을 담고 있는 한국어 ELMo(Embedding From Language Model) 임베딩을 대용량 데이터로부터 학습하여 입력에 추가하였다. 결과적으로 본 논문의 제안 모델은 기존의 Bi-LSTM-CRFs 모델에 비해 1.42 향상된 F1 성능을 실험을 통해 얻을 수 있었다.SRL(Semantic Role Labeling) is the task of NLU(Natural Language Understanding) that extracts predicates in the sentence and then figure out semantic relations of the arguments. Result of SRL can be used as the features in the various task of NLP. In the previous statistical-based studies, hand-crafted linguistic features were mainly considered, and in the deep learning-based models have the advantage of self-learning the linguistic features from the training data. Bi-LSTM-CRFs(Bidirectional Long Short Term Memory Conditional Random Fields) are often used in deep learning-based Korean SRL studies. Since the SRL needs to output a semantic role tag for the current input word considering all the input information so far, Bi-LSTM-CRFs based on RNNs(Recurrent Neural Networks) Also, CRFs are applied to the last output layer to model the relationship between adjacent semantic role tags. However, since the Bi-LSTM-CRFs structure can represent predicate information only by word embedding, modeling of the complex relationship between the predicate word and the semantic role tag is limited. Therefore, in this paper, to model the relationship between the predicate word and the semantic role tags, dynamic weight parameter method according to input predicate was applied. We also apply the Korean ELMo(Embedding From Language Model) embedding, which represents contextual information. The Korean ELMo embedding trained from large news corpus. As a result, the proposed model in this paper got F1 accuracy improved by 1.42 compared with the Bi-LSTM-CRFs baseline model.차 례 I 그림 차례 II 표 차례 III ABSTRACT IV 요 약 V

최근, 머신 러닝에 대한 관심이 증가하고 있다 . 빅데이터 시대의 도래와 더불어, 컴퓨팅 성능은 좋아졌으며 컴퓨팅 자원을 합리적인 가격에 갖출 수 있게 되면서, 머신 러닝은 각종 연구, 사업 영역에서 폭 넓게 주목을 받고 있다. AI(Artificial Intelligence)의 한 분야인 자연어 처리(NLP: Natural Language Processing)또한 그런 흐름의 영향을 받은 분야 중의 하나다.
자연어 처리는 컴퓨터가 사람의 언어를 이해할 수 있도록 하는 방법을 연구하는 하나의 분야로써 사람이 직접 작성한 규칙을 기반으로 사람의 언어를 해석하려는 고전적인 방식부터, 통계적 언어 모델(Statistical Language Model)이 도입되고, 머신 러닝이 적용되면서 점차 발전해 왔다. 이러한 연구들은 인공신경망을 적용한 NNLM(Feedforward Neural Net Language Model), RNNLM(Recurrent Neural Net Language Model) 방식이 등장하면서, 큰 성능 향상을 가져왔다. 최근에는 word2vec 방식이 등장하면서, 기존 인공신경망 방식들에 비해 효율성을 높였다.
위키피디아는 전 세계인이 협력하여 작성하고, 유지하는 방대한 지식 스키마이다. 따라서 이 텍스트 데이터 셋을 분석하여, 각 단어들의 상관관계를 분석하는 것은 유의미한 작업이 될 것이다. 그러나 이러한 작업들이 실제로 이뤄지고 있으나, 비교적 최신 알고리즘인 word2vec의 위키피디아 데이터 셋에 대한 적용되는 사례는 많지 않다.
따라서, 본 논문에서는 데이터를 분석하기 위한 word2vec 라이브러리, 모듈 사이의 통신을 위한 Django 프레임워크, 단어 사이의 유사성을 시각화 하여 보여주기 위한 D3.js를 이용하여 효율적으로 위키피디아 데이터 셋을 분석하고, 해당 데이터 셋의 단어들이 서로 얼마나 상관성을 갖고 있는지 보여줄 수 있는 애플리케이션을 구현한다.Recently, interest about machine learning is increasing. With advent of big data era, computing performance is better, computing resource is affordable. That’s why machine learning is attracting wide attention by all sorts of businesses and researches. NLP(Natural Language Processing), field of AI(Artificial Intelligence), is also one that is affected by that flow.
As NLP is field of studying method that computer understands human language, from conventional way that translates human language by rule that people write directly, to introduction of statistical language model and machine learning, NLP have improved until now. As introduction of neural net such as, NNLM(Feedforward Neural Net Language Model), RNNLM(Recurrent Neural Net Language Model), NLP rapidly improved. Recently, as introduction of word2vec method, efficiency is improved compared with conventional neural net way.
Wikipedia is written globally and collaboratively, and also maintains huge amount of knowledge schema. Therefore, analyzing Wikipedia data set, identifying correlation among words is meaningful works. Those works are being done but case of analyzing Wikipedia data set applied word2vec algorithm, which is relatively state-of-art, is not many.
So, in this thesis, as using word2vec library for data analysis, Django framework for communication with each module, D3.js for visualizing similarity among words, implements application analyzing Wikipedia data set efficiently and visualizing how much words are similar.국문초록 ⅳ 영문초록 ⅵ 제 1 장 서론 1 1.1 연구 배경 및 목적 1 1.2 연구 방법 및 범위 2

이중언어 말뭉치는 자연어처리분야에서 아주 중요한 자원으로 간주되어 왔으며, 통계적 기계번역, 교차언어 정보검색, 이중언어 사전의 편찬, 그리고 패러프레이징 등 다양한 분야에 활용이 가능하다.
따라서, 이중언어 말뭉치의 획득 및 활용은 전통적으로 아주 중요한 연구 분야로 인식되어 왔다.
병렬 말뭉치는 이중언어 말뭉치의 특수한 형태이며, 이는 각각의 원시언어 문장이 목적언어 문장으로 번역 되어진 형태를 의미한다.
자연어처리 분야에서 병렬말뭉치는 귀중하게 여겨지고 있으나, 연구 목적이나 상업적 용도로 사용하기에는 그 양이 충분치 않고, 또한 좀처럼 구하기 힘들다는 어려움이 있어왔다.
만일 수작업으로 새로운 병렬말뭉치를 구축하려 한다면 상당히 많은 시간과 인력을 필요로하게 되며, 많은 비용이 소모될 수 있다는 어려움이 있다.
왜냐하면 두가지 이상의 언어에 능통하면서, 번역하고자 하는 문서의 분야를 잘 이해하고 있는 전문가가 흔하지 않기 때문이다.
따라서, 자연어처리 분야의 많은 연구자들은 이러한 병렬 말뭉치를 자동으로 수집하기 위해 상당한 노력을 기울여왔다.
하지만, 대부분의 연구 결과들은 자동으로 추출한 병렬 말뭉치의 양과 품질에서 의미있는 성능향상을 보이지 못하였거나, 혹은 자동으로 추출하는 방법들이 다른 언어쌍이나 도메인에는 적용이 힘들다는 단점을 가지고 있었다.

본 논문에서는 통계적 기계번역에 활용이 가능한 대용량 병렬말뭉치를 웹으로 부터 자동으로 구축하는 새로운 방법을 제안한다.
기존 연구에서는 병렬 문장쌍을 추출하기 위해 곧바로 활용이 가능한 비교 말뭉치(comparable corpora)를 제공받아 사용하였다.
하지만, 영어-한국어로 구성된 비교 말뭉치는 쉽게 구할 수 없다는 단점이 있다.
본 논문은 이러한 단점을 극복하면서, 기존연구에 비해 훨씬 도전적인 웹 기반 병렬 문장쌍 추출이라는 방법을 시도한다.
또한, 뉴스와 같은 문어체 병렬 말뭉치에만 촛점을 맞춘 기존연구와는 달리, 제안하는 방법은 문어체 병렬 말뭉치 뿐만 아니라 구어체 병렬 말뭉치도 함께 자동으로 구축할 수 있도록 확장한다.

본 논문의 궁극적인 목표는 자동으로 구축된 병렬 말뭉치가 과연 문어체 번역 혹은 구어체 번역과 같은 서로 다른 목적의 기계번역 연구에 잘 활용될 수 있는지를 검증하는 것이라 할 수 있다.
이러한 목표를 달성하기 위해, 우리는 제안하는 방법을 1) 웹으로 부터 이중언어 말뭉치의 검색, 2) 검색된 이중언어 말뭉치로 부터 병렬 문장쌍 추출, 3) 추출된 병렬 문장쌍을 기계번역 시스템의 학습 데이터로 활용하는 세 가지의 하위 문제로 나누어 각각의 문제에 적합한 방법을 제안하고 그 성능을 평가하도록 한다.
최종적으로, 통계적 기계번역 실험에 통하여, 본 논문은 제안한 방법론이 기계번역시스템의 품질을 효과적으로 향상시킬 수 있음을 증명한다.Bilingual corpora are very important resources in Natural Language Processing (NLP).
They can be used in statistical machine translation (SMT), cross language information retrieval (CLIR), the construction of bilingual lexicon, and paraphrasing.
Thus the acquisition of bilingual corpora has received much attention.
Parallel corpora are a special kind of bilingual corpora in which each sentence in one language has a genuine translation in another language.
Although parallel corpora are very valuable for many NLP fields, they are very rare either for research purpose or commercial use.
The manual construction of new parallel corpora requires considerable amount of time and cost as human annotators who are proficient in more than two languages are very few.
Consequently, during the last decade or two, many researchers have tried to automatically locate or construct parallel corpora.
However, most of the studies have not shown a significant increase of the quantity or quality of extracted parallel corpora, or the methods are simply not applicable to other language pairs or other domains.

This dissertation presents a method of utilizing the Web for the automatic construction of parallel corpora which can be used as training data for statistical machine translation.
In contrast to the conventional approaches which use a readily available collection of comparable corpora to extract parallel sentences,
this paper attempts the much more challenging task of directly searching for high-quality sentence pairs from an open-ended document collection, i.e., the Web.
Also, unlike the conventional approaches which has focussed on a news corpora, we exploit parallel corpora from two different styles: literary-style corpora and colloquial-style corpora.
The ultimate goal of this dissertation is to verify the acquired parallel corpora as machine translation training data for different purposes, i.e., for literary-style translation or for the colloquial-style translation.
In order to achieve this goal, we decompose the problem 1) by retrieving the bilingual document pairs on the Web, 2) by mining parallel sentences from the retrieved bilingual documents of different nature, 3) by using the mined sentence pairs as training data for statistical machine translation.
Based on end-to-end machine translation experiments, this dissertation shows that the proposed approach can significantly improves the performance of statistical machine translation.1 Introduction 1 1.1 Statement of Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Problem De?nition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Outline of Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2 Preliminary 9

This thesis is for structural analysis for natural language interface using H-Frame between smart TV and user. We can easily observe the omissions of core information and pronouns while conversations. Therefore it is not an easy task to process and understand the utterance with only information contained in it. In order to complete the sentence that has insufficient information, our analysis should find the missing information through previous conversations and we need a way to manage entire conversations to build up sentence structures. To do this, we introduce a frame management structure called H-Frame, which contains the core information. With H-Frame, user’s utterance can be analyzed. And information retrieval and TV control can be processed. In particular, we show in this thesis that our approach can produce efficient and accurate results by using 2-gram methodology to recognize program titles in user’s discourses. In addition to this, we can also improve our results and control the smart TV by utilizing time information of user’s utterance.
The contents and results of this thesis can be summarized as follows: It is required to process what a user requests through analyzing utterance between the smart TV and the user. In order to do this, we need to analyze structures and semantics of the utterance. However, the omissions of words and pronouns in conversations happen. Therefore, it is required to find the missing information to complete the syntax. To address this, we proposed H-Frame methodology which manages core information, where H-Frame is used to fill up missing information for complete sentence structure. Our proposed approach can help to get accurate results because we can extend sentence structures by filling up the omitted information from H-Frame.
In this area, we can take advantage of the technique of H-Frame showing that 97% of incomplete sentences can be recovered to complete ones. In addition, two structures in H-Frame were used for usage extensions.: One to manage program being broadcast and the other to manage conversation. Such internal structures can help to use less memory and better accessibility of information than existing stack-based systems.
It is common to abbreviate the title of a program or to use only part of the title in user’s utterances. In order to recognize such utterances, two approaches were used. In the first approach, we use word-based 2-gram search methodology if an utterance is a part of a program title. In this approach, there is one exception if more than three syllables should be processed. In this case, we use 1-gram search methodology. In the second approach, we developed four combination rules for searches if abbreviation forms are used. We can detect abbreviated titles up to 97% in title database containing titles from 2006 to 2011.
We defined 23 semantic features extracted by analyzing the meanings of words used in sentences, which are necessary for this thesis. We also found 63 sentence structures in our analysis. In order to provide more accurate results, we use context awareness. Such context awareness utilizes time information when a user started talking, leading to meaningful results. In addition, it prevents from doing meaningless actions when a user controls a TV.
In this thesis, we showed that our approach, structure analysis natural language interface using H-Frame between users and smart TV, is more efficient than a stack-based system. If we are able to use EPG information which is not dealt in this thesis, we can build more effective system and expect more accurate results.I. 서 론 1 1. 연구 배경 및 목적 1 2. 연구 내용 및 구성 7 II. 관련 연구 13

안전 필수 시스템에서 소프트웨어의 결함은 심각한 결과를 초래하므로, 개발의 첫 단계인 요구사항 명세부터 안전성을 고려해야 한다. 요구사항의 결함은 테스트 단계에서 발견하기 어려우며, 결함을 발견하더라도 수정하는데 큰 비용이 발생하기 때문에 시스템을 제어하는 소프트웨어의 안전성 확보를 위한 요구사항 명세단계는 매우 중요하다. 안전 필수 시스템에서 요구사항은 일반 요구사항과 안전 요구사항으로 구분되고 시스템 요구사항으로 통합된다. 일반적으로 요구사항은 자연어로 작성된다. 자연어로 작성된 요구사항은 여러 이해관계자들에게서 도출되어 모호함과 부정확성에 의한 결함을 갖고 있어도 검출하기 어렵다. 이러한 문제를 해결하기 위해 표준문안과 GSN모델을 이용한 요구사항 명세방법을 제안한다. 표준문안은 선 정의된 서식에 맞춰 요구사항을 작성하는 준-정형 언어이다. 정의된 서식에 사용자가 기입하는 값은 속성으로 간주한다. 표준문안에 맞춰 요구사항을 작성하면 표현의 일관성을 갖기 때문에 이해관계자들이 요구사항이 의미하는 바에 대한 모호함을 방지하여 요구사항의 정확한 의미를 정의하는데 도움이 된다. GSN은 시스템이 안전하다는 것을 관련기관에 증명하기 위한Safety Case 작성에서 적합성을 인정받고 있는 표기법으로 기능목표, Safety Evidence를 표현한다. 본 연구에서는 안전 필수 시스템의 요구사항을 명세하기에 적합하도록 설계된 표준문안과 GSN 모델을 이용하여 요구사항 명세단계에서부터 결함을 쉽게 식별하고, Safety Evidence와의 연결을 통해 안전 적합성을 증명할 수 있다. 이 과정을 통해 발견된 요구사항의 결함을 수정하여 안전성 있는 소프트웨어를 개발할 수 있다.제 1 장 서 론 - 제 1 절 연구배경 및 문제점 - 제 2 절 연구목적 - 제 3 절 논문구성 제 2 장 관련연구

본 연구는 상호운용성 8대 항목을 작성하는 요구사항을 소요요청 단계에서부터 분석하고 구체화할 수 있는 상호운용성 요구사항 구체화 분석기법에 대해 제안하는 것을 목적으로 하였다.
먼저 기존 상호운용성 분석 프로세스와 도구들에 대하여 소개하고, 상호운용성 요구사항 상세항목을 요청하고, 다운로드 및 작성, 등록하는 상호운용성 8대 항목 요구사항 구체화 분석 프로세스를 새롭게 제안하였다. 또한 온톨로지 개념을 적용한 자연어 요구사항 구체화 기법을 통하여 소요요청자마다 각기 다른 서술방식으로 작성된 요구사항을 하나의 개념으로 통합하는 통합 상호운용성 요구사항 언어 시스템(UIRLS)을 제시하였고, 구조적 요구사항 구체화 기법을 사용하여 요구사항을 작성할 때, 기존의 컴포넌트 산출물 메타데이터로부터 데이터를 분석하여 소요요청자에게 적절한 요구사항 구체화 옵션을 제공하여 쉽게 요구사항을 작성할 수 있는 프로세스를 제안하였다. 이렇게 제안된 프로세스를 상호운용성 업무지원체계(DISS)에 적용하여 단순히 기존의 소요요청 단계에서 상호운용성 8대 항목에 대한 요구사항을 등록하는 기능에서 등록된 요구사항을 분석하고 구체화할 수 있는 기능으로 확장할 수 있었다.
이렇게 본 연구에서 제안한 상호운용성 8대 항목 요구사항 구체화 분석 프로세스를 통해 소요요청 단계에서 초기에 구체화된 요구사항을 분석하였고 이를 통해 선행연구 이후의 상호운용성 요구사항에서 발생될 수 있는 문제점들을 미리 발견할 수 있게 되었다.Ⅰ. 서 론 1 1. 연구의 배경 및 목적 1 2. 연구의 범위 4 Ⅱ. 한미 국방 상호운용성 분석기법 5 1. 미국 상호운용성 요구사항 분석 방법 현황 5

초등학교에서 6차 교육과정 이후 프로그래밍 교육은 교육과정 총론에서 완전히 사라지고 말았다. 이는 필요성이 없어졌다기 보다는 교수-학습이 어렵고 가르칠 교사가 거의 없기 때문이었다. 프로그래밍은 컴퓨터에게 무엇인가를 시키기 위한 명령의 집합이 아니라, 창의적 사고력과 논리적 문제 해결 능력을 가진 인재양성에 필수적인 요소이다. 그러나 초등학생뿐만 아니라 대학생들도 프로그래밍에 입문하는 데 많은 어려움을 겪는다. 그 이유는 문법 위주의 프로그래밍과 실제 문제와 프로그래밍을 통한 문제 해결에 커다란 차이가 있기 때문이다.
따라서 초등학생들에게는 학생들에게 익숙한 자연어로 프로그래밍에 입문할 수 있는 코스웨어가 초등학교에서 프로그래밍 교육을 위해 필요한 교수-학습 도구라 할 수 있다.
이 논문에서는 Bottom Up 방식의 Syntax-Free, 문제 해결 기반의 프로그래밍 교육 코스웨어의 개발을 통해 초등학교에서 프로그래밍 교육이 활성화될 수 있는 환경을 조성하기 위해 자연어를 사용한 문법과 디버깅의 부담이 적은 프로그래밍 교육 코스웨어를 설계 제작하고자 한다.목 차 1. 서 론 1.1 연구의 배경 및 목적 1.2 연구 내용 2. 관련 연구

대표적인 마이크로 블로그인 트위터(Twitter)에서 사용자들은 상호교류를 통해 사회적 관계 확장하고, 정보를 공유한다. 특히,정보 공유 과정에서 실시간으로 필요한 정보를 얻기 위해 질문과 답변을 한다. 이 같은 질의응답 활동으로 생성된 문서인 트윗(Tweet)은 사용자에 의해 실시간으로 직접 작성된다는 측면에서 동일한 정보를 찾는 다른 사용자에게 도움이 된다. 그래서 정보성있는 질의응답 트윗을 찾아내는 것은 중요한 문제이다. 기존 연구들은 방대한 양의 트윗들로부터 정보성있는 질의응답 트윗들을 찾기 위해 다양한 방법을 제시하였으나, 이를 활용할 방법에 대하여 구체적으로 논의되지 않았다.
본 논문은 정보성있는 질의응답 트윗들을 활용한 검색 시스템을 제안한다. 이 시스템은 대량의 트윗으로부터 질문을 식별하고, 정보성 필터링을 하여 검색 데이터로 구축하는 부분과 구축된 검색 데이터를 활용하여 자연어 질의에 적합한 데이터를 검색하는 부분으로 구성되어 있다. 검색 데이터를 구축하는 부분에서는 질문 식별을 통해 일반적인 모든 내용의 트윗들로부터 질의응답 여부를 식별한 후, 정보성 필터링을 통해 검색 데이터로 활용할 트윗을 선별한다. 다음으로 구축된 데이터로부터 사용자의 정보 요구에 적합한 문서를 검색하는 부분에서는 입력된 자연어 질의를 정련하여 검색모델에 입력하고, 이에 대한 검색 결과를 적합성 피드백에 활용하여 보다 정확한 결과를 출력하게 해준다. 제안된 시스템은 자연어 질의를 통해 필요한 정보를 검색한다는 측면에서 Yahoo! Answer와 같은 커뮤니티형 질의응답 서비스와 유사하다. 하지만 커뮤니티형 질의응답 서비스와는 달리 실시간으로 작성된 질의응답 정보를 검색할 수 있고, 트위터 데이터를 활용하므로써 대량의 정보를 자동으로 얻을 수 있다는 점에서 차이점이 존재한다.
본 논문은 질의응답 쓰레드 검색을 위해 다음과 같이 3가지 측면에서 성능을 평가한다. 1) 쓰레드 내 질문 식별, 2) 식별된 질문 쓰레드의 정보성 필터링, 3) 자연어 질의에 대한 검색. 다양한 실험 결과는 제안하는 검색 시스템이 한국어 질문 분석과 정보성 필터링 작업에 효과적임을 보여준다. 정보성있는 질의응답 데이터를 검색하는 제안된 방법을 통해 특정 질의 주제에 대한 사용자들의 의견을 분석하는데 활용할 수 있다.1 서론 1 2 관련 연구 4 2.1 질문 식별 연구 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 정보성 기반 필터링 연구 . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 질의응답 쓰레드 검색 연구 . . . . . . . . . . . . . . . . . . . . . . . . . 5

본고의 연구 목적은 崔世珍이 편찬한 訓蒙字會의 字訓을 중심으로 하여 語彙的인 측면에서 고찰하는 일이다.
첫째로 訓蒙字會의 국어교육관과 字訓의 語彙體系는 다음과 같다.
1. 訓蒙字會의 국어교육관
1) 崔世珍은 抽象的이고 觀念的인 것보다는 具體的이고 實際的인 言語敎育을 강조하였던바 이러한 言語敎育觀은 현대의 言語 습득 이론과 부합된다.
2) 그리고 現 國民學校 교육 목표에서 제시되는 구체적 내용이 訓蒙字會의 部類들과 깊이 관련되어 있어 崔世珍이 단순히 漢字 敎習에만 그치지 않고 아동의 全人的인 교육을 하고자 한 편찬 의도를 알 수 있다.
3) 국민학교 교육용 어휘 중 1학년 교과서에 나타나는 어휘의 頻度順 203위까지와 韓國方言調査項目表를 對象으로 하여 訓蒙字會, 千字文, 新增類合의 字訓과 나란히 비교 검토하였다. 그 결과 前者의 비교에서는 몇몇 高頻度語가 나타나지 않아 어휘 選別上 약간의 無理를 안고 있었으며, 前者나 後者의 비교에서 全體 字訓에 대한 該當語彙 出現 比率이 그리 높게는 나타나지 않고 있다.
2. 訓蒙字會 字訓의 語彙 構造
1) 訓蒙字會의 각 목록은 全體的으로 체계성을 유지하면서 2차적으로 派生되는 小部類意味(minor class meaning)를 부여할 수 있다.
2) 그러나 下卷에서는 唯一한 部類인 雜語에 具象名詞類와 動詞類를 포괄하고 있어 上卷이나 中卷에 나타났던 字訓들이 다시 보이는 경우가 상당히 있으므로 分類上의 無理도 드러난다.
3) 그리고 訓蒙字會의 部類 體系를 P. M. Roget의 分類語彙表(Thesaurus)에 適用, 比較한 결과 空間 部類와 관련된 字訓이 가장 많고, 이어 物質, 意志, 抽象關係, 物理學, 感性, 知性, 感覺의 順으로 관련 어휘가 나타난다. 관련 어휘가 가장 많이 나타나는 空間 部類의 경우, 단순하고 구체적인 어휘가 많이 나타난다.
둘째로 訓蒙字會 字訓의 語彙 意味 構造와 語場 構造는 다음과 같다.
1. 訓蒙字會 字訓의 語彙 意味 關係 構造
類意 關係는 名詞類·動詞類·形容詞類로 구분하고 이를 다시 相異型과 派生型 및 合成型으로, 多意關係는 自然語·植物語·身體語·人類語·宮宅語·彩色語·感覺語·形容詞類·動詞類로, 同音異意 關係는 表記 形態나 傍點이 同一한 것·表記 形態나 發音이 다른 것·品詞를 달리하는 것으로, 反意 關係는 二分對立語·多分對立語·兩極對立語·關係對立語·階層對立語·逆對立語로 각각 분류하여 풍부한 용례를 찾을 수 있었다.
2. 訓蒙字會 字訓의 語場 構造
訓蒙字會의 각 부류 중 日常生活과 관련이 깊고 基本 語彙와 상관이 높은 天文, 地理, 禾穀, 蔬菜, 獸畜, 身體, 天倫, 人類, 宮宅, 器皿, 食饌, 服飾, 彩色, 布帛, 疾病, 喪葬등 16개 部類를 對象으로 語場을 設定하여 이들이 갖는 體系的 특징을 확인하였다.
셋째로 訓蒙字會 字訓의 國語史的 變遷은 다음과 같이 나타난다.
1. 意味 領域面의 變遷
訓蒙字會의 意味 領域面의 變遷은 一般的 擴大 現象과 漸層的 擴大 現象과 縮小 現象과 具象化 現象으로 나누어 볼 수 있다.
漸層的 擴大 現象은 程度 强化 類型과 段階 發展 類型과 前段階 類型으로, 具象化 現象은 性別 提示 類型과 數値 提示 類型과 具體物 提示 類型으로 다시 분류될 수 있는바 一般的 擴大 現象에는 植物語가, 漸層的 擴大 現象에는 動詞類가, 縮小 現象에는 身體語와 道具語가, 具象化 現象 中 性別 提示 類型에는 動物語가, 數値 提示 類型에는 自然語와 道具語가, 具體物 提示 類型에는 自然語가 가장 많이 나타난다.
2. 意味의 轉移 現象
訓蒙字會 字訓의 意味의 轉移 現象은 有緣的 轉移 現象과 無緣的 轉移 現象으로 나누어질 수 있는바 이 兩 現象의 예로는 動詞類가 가장 많이 나타난다.
3. 感覺語의 變化 現象
訓蒙字會에서 나타나는 感覺語가 後代의 字典類에 이르면서 큰 의미 변화 없이 이어져 오고 있다.
4. 意味 價値面의 變化 現象
訓蒙字會 字訓의 의미는 價値面에서 向上과 下落 둘 다 보이는데 向上에는 服飾語가, 下落에는 動詞類가 가장 많고 向上보다는 下落의 예가 더 많이 나타난다.
5. 對立的 意味의 變遷 現象
訓蒙字會 字訓의 의미가 때로는 대립적으로 變遷하는 경우가 있는바 예로는 道具語 등이 나타난다.
6. 同字訓 異漢字의 意味 分化
訓蒙字會에서 同字訓으로 나타나는 漢字들이 後代의 字典類에 이르면서 구체화·특수화하면서 의미가 分化하고 있다.The Hunmongjahoi, a workbook of Chinese characters being compiled by Choi Sei Jin in sixteenth century Chosun dynasty, was studied particularly on synchronic description and historical change of the meaning of the characters. The results obtained can be summarized in several view points as followings.
1. In the Hunmongjahoi, rather the concrete and practical language education which coincides well with the modern language learning theories was emphasized than the abstractive and ideal language education.
2. Considering the fact that classes in the Hunmongjahoi are deeply related to the detailed principles of the present primary school education, it is considered that the Hunmongjahoi might be compiled at that time not only for teaching Chinese characters but also for educating children to make perfect persons.
3. Lexical structure of paraphrases of characters listed in the Hunmongjahoi is on the whole systematic. Moreover, as the result of both application of class system to Roget's thesaurus and comparison of each other, the paraphrases related to space are found the most of all and then to matter, volition, abstract relations, physics, affection, intellect and sensation in order.
4. The paraphrases of characters listed in the Hunmongjahoi show abundent illustrations including various meaning relationships such as synonyms, polysemys, antonyms and so on.
5. The linguistic fields of sixteen classes, not only related closely to everyday life but also interrelated deeply with basic vocabulary, were drew up and it was found that the linguistic fields structured in the Hunmongjahoi are comparatively systematic.
6. Change in domain of meaning of characters listed in the Hunmongjahoi can be classified into four phenomena such as general widening, gradual widening, narrowing and embodying. The gradual widening phenomenon can be further subclassified into strengthening type, stepwise forwarding type and stepwise backwarding type, and in same way the embodying phenomenon also into sex distinction type, numerically figuring type and type of embodying concrete object.
7. Transfer phenomenon of paraphrastic meaning of characters listed in the Hunmongjahoi can be classified into related transfer and unrelated transfer.
8. Among the paraphrases of characters listed in the Hunmongjahoi, paraphrases related to sensation were classified into sensation in general, touch, taste, smell, sight, hearing and organ, and discussed the differenciation as well as change of their meanings.
9. The paraphrastic meaning of characters listed in the Hunmonjahoi was elevated or degenerated in value and it was found that the more paraphrases were degenerated than elevated.
10. The paraphrastic meaning of characters listed in the Hunmongjahoi partly changed to opposite meaning particularly in words related to tools.목차 第一章 序論 = 1 第一節 硏究의 意義 = 1 第二節 硏究 目的·方法 및 範圍 = 2 第三節 先行 硏究 槪觀 = 3

자연어처리가 다양한 응용분야에 사용되면서 새로운 기법의 자연어처리 연구가 활발하게 이루어지고 있다.자연어처리를 올바르게 분석하고 이해하기 위해서는 자연어에서 발생하는 모호성을 해결하는 과정이 반드시 요구된다.이 과정을 태깅이라하고 태깅은 주로 형태소단위의 품사 태깅을 한다.품사 태깅 방법으로는 규칙을 이용하는 방법,확률을 이용하는 방법 그리고 규칙과 확률을 이용한 혼합형 방법이 있다.
특히 양질의 말뭉치가 제공된 현재에는 확률을 이용하는 방법을 많이 사용하며,본 논문에서 다루게 될 최대엔트로피 또한 확률을 이용한 방법 중 하나이다.
주어진 문장의 각 단어에 올바른 품사를 매기는 품사태깅문제는 문맥의 이전 정보를 토대로 즉,통계를 기반으로 한 접근방법으로 기존에는 말뭉치로부터 하나의 통계정보를 추출하여 품사를 부착하는 시스템으로 이루어졌다.하지만 이러한 단일 통계정보로는 예외가 많은 품사 결정문제에 에러가 많이 발생한다.따라서 추가적인 통계정보를 같이 사용한다면 좀 더 정확률을 높일 수 있을까하는 아이디어에서 출발한다.
본 논문에서는 이러한 상이한 통계정보를 통합하는 방법으로 최대엔트로피 모델을 제시한다.최대 엔트로피 모델은 자연언어를 모델링하기위한 좋은 방법 중의 하나이다.이종의 통계정보로는 확률을 이용한 방법 중 가장 널리 쓰이는 n-gram과,문맥의 새로운 정보로써 trigger pair를 사용하게 된다.간단히 말해서 triggerpair란 상호 밀접한 관계를 가지는 쌍을 토대로 통계를 추출하는 방법이다.물론 모든 가능한 pair중에서 유용한 정보로 사용가능한 triggerpair를 선택하는 것 또한 상당히 어려운 문제이다.이러한 triggerpair통계정보를 n-gram과 함께 최대엔트로피 모델링을 했을 경우 퍼플렉시티가 어떻게 변화하는지에 대한 실험 결과를 관찰하게 될 것이다.triggerpair는 또한 다양하게 문맥사이즈를 변화할 수 있으며,n-gram의 확률 모델도 다양하기 때문에 여러 종류의 실험결과를 예상 할 수 있다.
따라서,퍼플렉시티가 가장 낮은 최대엔트로피 모델을 찾음으로써,한국어 품사태깅 정확도를 최대한 올릴 수 있는 성과를 가져올 것이다.마지막으로 정확한 품사태깅은 음성인식(speech recognition),정보 검색(information Retrieval),기계번역(machine translation)등 여러 분야에 좀 더 정확한 확률모델을 제공하게 될 것이다.Natural Language Processing is been widely used in different fields, and the Natural Language Processing with new technique is being researched at the same time.
To analysis and understand the Natural Language Processing correctly, the solution of ambiguous in nature language is required.
This is called tagging which uses the POS tagging of the morpheme unit.
There are 3 types of POS tagging, which are the method using rules, the method using probability and the hybrid method using both rule and probability.
Especially the method using probability is widely use at present.
A method which uses the maximum entropy and probability is presented in this paper.
The POS tagging problem that classify the POS correctly from the given article is a system which ticks the POS by the statistic information of the corpus - an approach based on the statistics.
But there will be many error if we use such single statistic information for the POS determine problem where there are many exceptions.
So, we have the idea that use more statistic information to increase the accuracy.
The maximum entropy model which employes those different kind of statistic information is presented in this paper. The maximum entropy model is a good method for modeling nature language.
By using this statistic information, the n-gram which is most widely used method using the probability, and the trigger pair which employ the new information of context are used.
In one word, it's a method using the pair called which has close relation called trigger pair to each other, of curse, the selecting of trigger pair- the useful information int all the pairs - is also a different problem.
The result of transforming the perplexity, while modeling the trigger pair statistic information with the n-gram is presented.
Because the context size of the trigger pair can be changed also , the probability model of n-gram is diverse different kinds of results can be predicted.
So, we get the conclusion that the accuracy of korean POS tagging can be increased by means of looking for the minimum maximum entropy of the perplexity.
At last, accurate POS tagging provides a more accurate probability model for those fields such as speech recognition, information retrieval, and machine translation.목차 제1장 서론 1 제1절 연구의 목적 1 제2절 연구방법 및 범위 3 제2장 본론 4

자연어 처리는 컴퓨터의 큰 처리 능력과 저장 능력을 요구한다. 이러한 컴퓨팅 능력은 단일 노드로는 만족시키기 어렵기 때문에 자연어 처리는 분산 환경 프레임워크를 이용한다. UIMA는 이와 같은 분산 환경 프레임워크에 대한 요구에 맞춰 IBM에서 일반적인 텍스트 처리와 자연어 처리를 위해 개발한 프레임워크다. 자연어 처리에 대한 요구는 지금도 계속 커져가고 있다. 이런 요구 증가에 따라 현재 Apache의 ‘Spark’, ‘Storm’ 등의 프레임워크가 개발되고 있다. 이러한 최신의 프레임워크들과 비교한 UIMA의 장점은 사용법의 간단함과 뒤처지지 않는 성능이다. 무엇보다도 브로커 서비스를 사용하는 메시징 시스템을 이용한 텍스트 분산은 개발자에게 간단한 사용법을 제공한다. 하지만, UIMA 의 단점은 스케일 아웃이 쉽지 않다는 점이다. 분산 처리 노드들의 성능을 항상 최대로 활용하기 위해서는 컴퓨팅 노드에서 데이터를 처리하는 Annotator 의 스케일 아웃이 필요하다. 본 논문은 이를 위해 규칙 기반 관리 시스템을 사용하여 UIMA 의 처리 능력을 더욱 끌어올리는 것을 가능하도록 했다.Natural Language Processing(NLP) require huge computer processing capabilities and storage capabailities. Because these compute capacities are difficult to satisfy as a single node, natural language processing uses a distributed environment framework. UIMA(Unstructured Information Management Architecture) is a framework for common text processing and natural language processing is response to the need for this distributed environment framework. Demand for natural language manipulation continues to grow. These demands are currently being developed under the development of Apache ‘Spark’ ,‘Storm’. The advantage of UIMA compared to these latest frameworks are the simplicity and performance. First of all, text distribution using a messaging system that uses a broker service provides developers with simple ways to use simple recipe. However, the disadvantage of UIMA is that it is not easy to scale out. In order to utilize the performance of the distributed processing nodes, a scale out is needed to handle the data from the compute nodes. This paper enabled the use of a Rule based system to further enhance the processing capacity of UIMA.1. 서론 1 2. 관련 연구 5 2.1 Yarn 환경 연구 6 2.2 AWS 의 Auto scaling 연구 8 2.3 ActiveMQ 9

빅데이터 환경의 급속한 발전으로, 거대한 양의 문서로부터 의미 있는 지식을 얻기 위한 정보 추출에 대한 필요성이 증가하고 있다. 개체명 인식은 자연어에서 사용되는 인명, 지명, 조직명 등과 같은 카테고리로 단어 열을 분류하는 자연어처리 기술이다. 개체명 인식기는 정보 추출 시스템이나 질의응답 시스템에 필수적인 모듈이다. 이러한 개체명 인식기를 구현하기 위해서는 개체명 사전이 필요하다. 그러나 개체명 사전을 구축하기 위해서는 많은 인력과 시간이 소비 된다. 또한 어렵게 개체명 사전을 구축하더라도 지속적으로 생성되는 개체들을 추가하고 관리하는 것은 현실적으로 많은 어려움이 있다. 이러한 문제를 해결하기 위하여 본 논문에서는 능동학습 방법을 이용한 개체명 사전 반자동 구축 방법을 제안한다. 첫 번째 단계로 개체명 카테고리를 정의하고, 위키피디아의 데이터베이스 버전인 디비피디아로부터 카테고리 당 30개의 엔트리를 선택하여 시드 엔트리를 구축한다. 두 번째 단계에서는 구축된 시드 엔트리의 분류 정보와 개체명 카테고리 사이의 상호정보량을 계산한다. 세 번째 단계에서 디비피디아 엔트리와 개체명 카테고리 사이의 상호정보량 점수로부터 표준편차 값이 낮은 하위 0.1%를 능동학습 대상으로 선택한다. 네 번째 단계에서는 능동학습 대상으로 선택된 엔트리를 수동 태깅하여 올바른 개체명 카테고리로 분류한다. 마지막으로 개체명 카테고리의 성능이 수렴할 때 까지 2~4단계를 반복한다. 본 논문에서 제안한 방법으로 실험한 결과 18개의 개체명 카테고리에서 75.32%(정확률:74.31%, 재현율:75.32%)의 F1-measure을 보였다.Abstract

With the rapid revolution of the big data environment, demand for information extraction for obtaining meaningful knowledge from a huge amount of documents is increasing. Named entity recognition is a part of natural language processing that classifies specific word sequences with some semantic categories such as person, location, organization, and so on. The named entity recognizer is an essential module of an information extraction system or a question answering system. To implement this named entity recognizer, a named entity dictionary is needed. However, it is a time-consuming and labor intensive job to manually construct a named entity dictionary. In addition, it has many difficulties to consistently manage and add new entries to a named entity dictionary. To resolve these problems, we propose a semi-automatic model to construct a named entity dictionary by using active lerning. In the first step, we define named entity categories and construct an initial named entity dictionary including a small number of seed entries that are manually selected from DBpedia (http://dbpedia.org), a database version of Wikipedia(http://www.wikipedia.org; an on-line free encyclopedia). In the second step, the proposed model calculates mutual information scores between each DBpedia entry and the named entity categories, by comparing category names of a DBpedia entry to those of seed entries in a named entity category. In the third step, we select the bottom 0.1 percent of DBpedia entries that have low standard deviations of mutual information scores between a DBpedia entry and named entity categories. In the fourth step, we classify the selected DBpedia entries into correct named entity categories by hand. Finally, we repeat the second step to the fourth step until the performance of named entity classification is converged. In the experiments, the proposed model showed a macro F1-measure of 75.32% (a macro precision of 74.31% and a macro recall rate of 75.32%) at 18 named entity categories.목 차 I. 서 론 ··························································



목차 = 0 요약 = 0 제1장 서론 = 1 제2장 자연어 해석 과정중에 생기는 모호성과 그 해결책들 = 3 2.1 어휘 분석 과정중에 생기는 모호성 = 3

본 논문은 대규모 의학논문 관련 데이터베이스인 MEDLINE에 대한 향상된 자연어 검색기법을 제안한다. 제안하는 기법은 자연어 문서에 대한 클러스터링 기법 두 가지와 MEDLINE 데이터에 대한 새로운 가중치 재부여 기법에 기반한다.

먼저 향상된 점층적 클러스터링 알고리즘 기법인 Dynamic ART를 제안한다. 제안한 알고리즘은 대규모 고차원의 문헌들을 클러스터링할 수 있는 공간 k 분할(Spherical k-means) 알고리즘과 점층적인 방식으로 클러스터링을 수행하는 퍼지 ART(Adaptive Resonance Theory)의 장점을 결합한 것으로, 공간 k 분할 알고리즘에서 사용한 벡터 공간모델의 개념 벡터와 퍼지 ART의 경계 변수 개념을 사용한다. 결과적으로 제안한 알고리즘은 점층적인 클러스터링을 수행할 뿐 아니라, 자동으로 적절한 수의 클러스터의 개수를 결정할 수 있는 장점을 가지며 CLASSIC3 데이터 집합을 대상으로 실험한 결과 제안하는 알고리즘은 평균 응집도에서 공간 k 분할 알고리즘에 비해 8%의 향상이 있었고, 퍼지 ART에 비해서는 14%의 향상을 보여준다.

이와 함께 정보 조직화를 위한 빠른 클러스터링 알고리즘을 제안한다. 정보 조직화는 효과적인 인터넷 문헌 검색과 검색된 결과의 브라우징(browsing)을 위한 것으로, 본 논문에서 제안한 휴리스틱(heuristic) 정보 조직화 알고리즘은 의 시간 복잡도를 가지면서 의 시간 복잡도를 가지는 기존의 스타 클러스터링 알고리즘과 비슷한 결과를 도출한다. 제안하는 알고리즘은 클러스터의 개념 벡터를 이용하였고, 가장 많은 0이 아닌 값을 갖는 벡터를 초기 클러스터 중심 값으로 설정하였다. TIME, MEDLINE, CLASSIC3 데이터 집합에 대해 실험을 수행한 결과 스타 클러스터링 알고리즘보다 훨씬 빠른 시간 안에 결과를 얻을 수 있었다.

그리고 MEDLINE에 대한 클러스터링과 정보 검색을 위한 용어 가중치 재부여 기법에 대해 제안한다. MEDLINE은 널리 사용되고 있는 자연어로 이루어진 대규모 의료 데이터베이스이며, 의학 관련 연구 논문들의 요약을 주로 저장하며, 각 문헌에 MeSH 용어 사전을 이용하여 수작업으로 키워드를 포함시키고 있다. 본 논문에서는 MeSH와 요약을 벡터 공간 모델을 이용하여 표현하고 MeSH 용어에 추가적인 가중치를 부여함을 통해 현재 MEDLINE에서 사용하고 있는 불리언 모델 보다 약 2.4배의 향상된 결과를 입증한다. 또한 MeSH용어와 요약을 결합하는 방식이 MeSH 혹은 요약만을 이용하는 것보다 더 나은 성능을 나타냄을 보여준다. 연관성 피드백을 통한 자동 질의 확장에서도 이와 같은 MeSH 용어 가중치 조절은 현재 가장 좋은 성능을 나타내는 벡터 모델 기반의 시스템에 비해 17%의 성능향상을 보여준다.

마지막으로 위의 방법들을 결합하여 가중치를 조절한 MEDLINE 데이터에 대해 클러스터링 하는 기법에 대해 제안한다. 이는 가중치가 조절된 MEDLINE 문헌을 Dynamic ART 클러스터링 알고리즘의 입력 데이터로 이용한 것으로, 평균 응집도 측면에서 공간 k 분할 클러스터링 알고리즘에 비해 3.8%, MeSH 용어 가중치 조절을 하지 않은 Dynamic ART에 비해 3.2%의 향상을 보여 준다.제 1장 서 론 8 1.1. 연구 배경 8 1.2 연구 범위 9 1.3 논문의 기여 11 1.3.1 클러스터링에 관한 연구 11

인터넷상에는 수많은 정보가 존재한다. 인터넷에서 얻을 수 있는 정보는 뉴스와 같이 객관적 정보뿐만 아니라, 주관적인 의견이나 감성이 들어 있는 경우가 많다. 감성이 들어 있는 데이터에서 정보를 얻는 방법을 감성 분석이라 하고 데이터를 감성에 따라 나누는 것을 감성 분류라 한다. 지금까지 감성 분류는 크게 자연어 처리와 감성어 사전 구축을 기반으로 이루어져 왔다.
이러한 방법으로 감성 분류를 하는 것은 자연어 처리 과정에서 형태소 분석이 제대로 이루어지지 않는 문제와 감성어 사전 구축 시 등록할 단어를 선별하고 단어의 감성 정도를 정하는 데에 명확한 기준을 정하기 힘든 문제가 있다. 이러한 이유로 만족할만한 수준의 성능을 얻지 못하였기에, 언급한 어려움을 해결하고 성능 개선을 위하여 감성 분류에 대용량 데이터와 통계적 접근을 제안한다.
제안하는 방법은 형태소 분석으로 글의 의미 단위를 찾아가는 방법 대신 수많은 데이터를 읽고, 데이터에 등장한 표현들의 통계치를 이용하여 감성 판단을 하는 것이다. 대용량 데이터 처리를 위해서는 하둡과 맵리듀스를 이용한다. 본 논문에서는 이 방법으로 구현한 감성 분류 시스템을 소개하고 실험을 통해 방법의 타당성을 보이고자 한다.1. 서론 1 2. 관련 연구 4 2.1 감성 분류 4 2.1.1 자연어 처리 4 2.1.2 감성어 사전 8

본 논문에서는 환경에 관한 일련의 실시간 입력 부분 영상들을 토대로, 3차원 환경 전체를 대상으로 하는 자연어 질문들에 답하는 새로운 시각 질문 응답 문제인 VEQA(Visual Experience-based Question Answering)를 제안한다. VEQA 문제들에서는 실세계와 유사하게, 부분적으로만 관측 가능한 환경, 그리고 에이전트의 행동에 의해 동적으로 변화하는 환경을 가정한다. 이러한 VEQA를 해결하기 위해, 본 논문에서는 실시간 입력 영상들로부터 생성하는 3차원 장면 그래프와 대규모 배경 지식 베이스, 그리고 이들을 이용한 지식 추론을 통해 자연어 질문에 대한 답변을 생성하는 새로운 시각 질문 응답 시스템을 제안한다. 제안 시스템은 크게 3차원 장면 그래프 기반의 상태 지식 생성 단계와 지식 추론을 통한 답변 생성 단계로 구성된다. 제안 시스템에서는 입력 영상을 이용한 상태 인식과 에이전트의 행동 모델을 이용한 상태 예측 결과를 상호 보완적으로 결합하여, 불확실성이 존재하는 동적 환경에 관한 보다 정확한 장면 그래프를 생성한다. 또한 제안 시스템에서는 입력 영상에서 얻어지는 장면 그래프 형태의 상태 지식뿐만 아니라 대규모 배경 지식 베이스를 함께 지식 추론에 활용하여, 입력 영상과 자연어 질문에 대한 피상적인 이해로는 해결하기 어려운 질문들에 대해서도 답변 생성이 가능하다. 본 논문에서는 3차원 실내 가상 환경인 AI2-THOR를 이용한 다양한 실험들을 통해, 제안 시스템의 높은 성능을 입증하였다.In this paper, we propose a new visual question answering problem named VEQA(Visual Question-based Question Answering), that requires the correct answer to natural language question about entire 3D environment based on a series of real-time input partial images. The VEQA assumes a partially observable and dynamically changing environment alike a real-world. In order to solve this VEQA, we propose a visual question answering system that can answer through knowledge reasoning based on 3D scene graph generated from real-time input images, and large-scale background knowledge base. The proposed system consists of a state knowledge generation step through 3D scene graph generation, and an answer generation step through knowledge reasoning. The proposed system generates accurate scene graph in a dynamic environment in which uncertainty exists, through fusion of result of state recognition using input images, and result of state prediction using action model. The proposed system can answer to questions that are difficult to be solved by a superficial understanding of input images and natural language, through utilizing not only state knowledge of the scene graph form obtained from input images but also a large background knowledge. In this paper, we demonstrate the high performance of the proposed system through various experiments using a 3D virtual environment AI2-THOR.제 1 장 서 론 1 제 1 절 연구배경 1 제 2 절 연구목표 3 제 2 장 이론적 배경 7

1960년 중반에 프로그래밍 언어를 해석하기 위한 구구조 (phrase-structure) 구문분석 기법으로 처음 개발된 LR (Left-to-right scan, Rightmost derivation) 파싱 기법은, 그 자체가 지닌 훌륭한 처리 속도와 기본적인 문맥의존성 덕택에 자연어 파싱 연구자들도 관심을 보여왔다. 1980년 후반에 자연어 문법의 중의적 성질을 반영할 수 있는 일반화 LR 기법이 개발되면서, PCFG보다 나은 확률적 구구조 구문분석을 LR 기법을 통해 수행해보려는 노력이 다수 시도되었다. 1990년 후반으로 들어오면서 대용량의 구문분석말뭉치를 쓸 수 있게 되자, 말뭉치 기반의 문법 구축과 생성 모델에 의한 통계적 파싱 방법이 각광 받으면서 확률적 LR 모델의 연구는 활발히 이루어지지 않았다. 그러나, 말뭉치에서 학습한 규칙의 개수가 수천 개를 넘게 되면서 효율 문제는 여전히 남아있게 되었기 때문에, 실시간 처리가 가능한 효율을 필요로 하는 분야에서는 높은 문맥 의존성을 가질 수 있는 통계적 LR 파싱 모델의 연구가 필요하게 되었다.
기존에 발표된 확률적 LR 파싱 모델은 LR 파서 자체에 묶여 있어 자연어 파싱에 필요한 많은 문맥 정보를 고려하는 데에 많은 무리가 있을 뿐만 아니라, 부분 파스 정보를 지닌 LR 스택을 상태정보 혹은 상태 시퀀스로 한정하고 있다. 이러한 모델은 모두 영어권을 대상으로 하고 있는데, 더 많은 정보를 고려해야 하는 한국어 파싱에서는 높은 정확도를 기대하기 힘들다.
본 논문에서는 풍부한 문맥 정보를 융통성 있게 사용할 수 있는 단순하고 직관적인 통계적 LR 파싱 모델을 제안한다. 제안한 모델은 문맥정보에 따라 Shift 혹의 Reduce 연산만을 수행하는 순위화 모델(ranking model)로서, LR 파싱 테이블 구축 시중의적인 연산(ambiguous action)을 가진 엔트리에만 연산 스코어를 설정한다. 또한, 본 모델은 효과적인 한국어 파싱을 위해 한국어 구문구조에 기반하여 구성한 문맥정보를 사용한다. LR 파서에서 기본적으로 주어지는 정보 이외에, 본 모델은 LR 스택 첨단에 위치한 서브트리 정보와 주변 단어의 정보를 사용한다. 이 정보는 기능어를 기반으로 만들어지는데, 서브트리 정보는 구문 구조를 반영하는 단말 기능어 열과 내용 중심어로 구성되며, 주변 단어의 정보는 서브트리에 주변의 구조 정보를 부분적으로 제공하는 기능어 열이다.
제안한 모델의 테스트를 위하여, 12000 여 문장으로 구성된 구구조 구문분석 말뭉치에 대해 본 모델을 적용하였다. 본 모델에서 사용한 기능어 기반의 문맥 정보는 파싱 정확도를 높이는 데에 효과적으로 기여했을 뿐만 아니라, 적은 학습 자료에도 파싱 성능을 크게 떨어뜨리지 않았다. 본 모델은 LR 파서에서 주어진 기본 문맥 정보만으로도 기존의 확률적 LR 파싱 모델보다 나은 성능을 보였다. 실험 집합에 대한 본 모델의 정확도는 86.71%의 F-score이다. 기존에 발표된 문맥 의존형 한국어 구구조 구문분석 모델과 비교했을 때, 약 2.5%의 F-score 향상을 보였다.LR (Left-to-right scan, Rightmost derivation) parsing techniques were originally developed for parsing programming language in the late 1960s, In the late 1980s, the generalized version of LR algorithm that can handle natural language grammars was published, and this algorithm were actively considered by natural language parsing researchers, because of its excellent efficiency and Inherent mild context-sensitivity. From the middle of 1990s, large amount of tree-annotated corpus were available. CFG rules extracted from the treebanks have been widely used for statistical parsing and so many researchers have focused on corpus-based generative parsing models. Meanwhile, the studies of the LR approach for natural language parsing was less popular than before. Still, LR approach would be needed in some application areas that require efficient parsers. However, incidentally or not, the previous LR parsing models has a deficiency in making available sufficient contextual information in the LR stack, being constrained by the LR parsing framework for CFGs itself.
The previous probabilistic LR parsing models are strictly restricted by the LR parser itself (a LR parsing table, a graph-structured stack, and a lookahead), thus they are not allowed to use other rich contextual information beyond it. They simplified the LR stack as a state alone or state sequence. The previous models are mainly for English. For more effective parsing for Korean, we need a more flexible model that can use rich contextual information.
In this thesis, we propose a statistical parsing model that can use contextual information more flexibly than the previous probabilistic LR parsing models. It is the ranking model that just performs Shift or Reduce action using the given contextual information of the stack and the input. For effective parsing of Korean, Our model uses the contextual information based on syntactic information. Let alone the basic information given by the LR parser (state and lookahead), our model utilizes the information of the sub-trees on the top of the LR stack and the surrounding words. This information is based on functional-morphemes; the sub-tree information consists of the functional morpheme sequence representing the syntactic structure and the content head morpheme, and the surrounding words information is the limited length of the functional morpheme sequence.
We tested the proposed model using the Korean treebank with 12,000 trees. Experimental results have shown that the functional morpheme based contextual information is effective in improving parsing accuracy, and does not decrease the parsing accuracy to much with a small training set. Moreover, our statistical ranking model outperforms the previous standard probabilistic LR parsing models even when using a state and a lookahead alone. The parsing accuracy of the proposed model is 86.71% F-score on the test corpus. When we compared our model with the previous context-sensitive PCFG parsing model for Korean, our model outperformed it by about 2.5% F-score.Abstract = i Contents = iii Chapter 1 Introduction = 1 1.1 Practical Motivation for Parsing = 3 1.2 Background of the Researches on LR Method for Natural Language Parsing = 5

SNS 데이터는 사용자들의 일상적인 대화내용 뿐만 아니라 특정 지역에 관한 이벤트, 경제, 사회적, 산업적, 학문적 관점과 같은 복잡한 지식정보를 제공한다는 점에서 분석 활용가치가 뛰어나, 지리정보시스템(GIS) 분야에서도 꾸준히 연구되어 왔다. 그러나 극히 일부에 해당하는 지오태깅된 SNS 데이터의 수로 인해 분석 대상이 될 수 있는 데이터의 수가 부족해지는 현상이 발생해 왔다. 이러한 데이터의 비효율성으로 인해서, 지오태깅이 설정되어 있지 않은 데이터들은 분석대상에서 제외되어 연구 결과 도출을 위한 분석 범위에 포함시킬 수 없었다.
이에 본 연구에서는 지리정보탐색 기법(Geographic Information Retrieval)을 활용하여 사용자 개인이 생성한 단일 데이터가 의미하는 공간적 정보를 추출하는 기법을 제안하고, 이를 활용하여 데이터의 비효율성을 최소화하고, 분석에 활용될 수 있는 데이터의 수를 확장해 보고자 한다. 자연어처리(Natural Language Processing)와 형태소 분석, 그리고 자체 구축한 공간정보 사전(lexical resource)을 활용하여 트윗 데이터에서 공간적 단서가 될 만한 개체를 추출하고, 해당 개체들이 의미하는 후보 관심지점들을 도출하였다. 이를 기반으로 분석 대상이 되는 트윗 데이터가 의미하는 지리적 정보를 관심지역(Area of Interest, AOI)으로 표출하였다.
본 연구의 분석 수행 결과 분석 대상이 되는 트윗의 구문에 ‘지명정보’와 ‘공간적 단서’가 포함되었는지의 여부에 따라 5가지 유형의 트윗에서 위치정보를 추출할 수 있었다. 총 60개의 트윗 중 45개(75%)의 트윗에서 위치정보를 추출하여 데이터의 비효율성을 해소할 수 있었다.1. 서론 1 1.1. 연구 배경 및 목적 1 1.2. 연구 범위 및 방법 4 2. 이론적 논의 및 관련 연구 7

텍스트 자동 요약은 문서를 적은 노력으로 이해할 수 있도록 입력 텍스트로부터 핵심 내용을 추출하는 연구 분야이다. 텍스트 자동 요약은 크게 생성 요약과 추출 요약으로 구분된다. 생성 요약은 문서의 요약을 제공하기 위해 새로운 자연어를 생성해내는 요약 방법으로 아직 자연어 처리 연구 분야에서 사람과 같은 수준의 자연어 생성은 어렵기 때문에 기존 토픽 추출 방법을 적용하는 추출 요약이 주로 연구된다. 추출 요약 중 대표적인 TextRank는 추출 요약을 위해 문서를 그래프로 변환하고 두 문장에 동시에 출현하는 단어의 빈도를 기반으로 문장 간 유사도를 계산한다. 이 문장 간 유사도 계산 방식은 문장 내 단어 간의 의미적 유사성을 충분히 고려하지 못하는 문제점이 존재한다. 이러한 문제점을 해결하기 위하여 본 연구에서는 문장 내의 모든 단어 쌍에 대한 동시 출현 관계를 그래프로 정의하여 단어 간 유사도를 계산하는 방법을 제안한다. 또한 문장 간 유사도 계산 시 단어 간 동시 출현 관계를 적용하기 위한 새로운 문장 벡터 함수를 제안하며, 제안 방법을 통하여 문서에 대한 추출 요약을 수행한다. 제안 방법의 성능과 TextRank와의 성능을 실험적으로 평가하여 비교한 결과, 제안한 방법을 이용한 추출 요약 모델이 보다 정확한 모델이라는 것을 확인할 수 있었다.Automatic summarization is a research area to extract important information from an input text in order to help users easily understand the document. In general, studies on automatic summarization can be divided into two parts : abstractive summarization and extractive summarization. Extractive summarization is more popular than abstractive summarization because recent natural language processing techniques are hard to rephrase the important information from an input text. Therefore topic extraction methods are usually applied to extractive summarization. TextRank, one of the most popular extractive summarization models, calculates sentence similarity using the frequency of co-occurrence words in two sentences when converting a document into a graph. The sentence similarity metric in TextRank has a limitation in that it is not enough to concern semantic similarity of words in a sentence. To overcome this limitation, this paper proposes a novel extractive summarization model using a graph-based word similarity and sentence similarity. The word similarity is measured by relationships of co-occurrence words in a sentence. A sentence vector function is also proposed to apply the word similarity when calculating the sentence similarity. The performance of the proposed method is evaluated and compared to TextRank. Based on the experimental results, the proposed extractive summarization model was more accurate than TextRank.제 1 장 서론 1 1.1 연구 배경 및 개요 1 1.2 논문의 구성 2 제 2 장 관련 연구 3

제 1 장 서론 1 1.1 연구배경 1 1.2 연구의 필요성 및 목적 2 1.3 논문의 구성 4 제 2 장 관련연구 5

월드 와이드 웹(World Wide Web)이 널리 보급되기 이전부터 대부분의 사람들은 상품을 구매할 때 다른 사람의 의견을 많이 물어보고 어떤 상품이 좋은지 추천을 받아서 구매를 결정하였다. 이처럼 사람들은 그들이 상품을 구입하기 전에 상품에 대해서 많은 정보를 얻으려고 한다. 이 때 다른 사람의 상품에 대한 의견은 아주 중요하게 작용한다. 오늘날 웹2.0의 발전과 함께 웹 상에 많은 사용자 데이터가 축적되었다. 이러한 사용자 데이터들 중에는 사람들이 상품이나 서비스에 대한 의견을 작성해놓은 블로그나 리뷰 같은 데이터가 있다. 그 결과로 블로그나 리뷰 데이터로부터 유용한 정보를 추출해내는 오피니언 마이닝에 대한 연구가 활발히 진행되고 있다.
본 논문에서는 연관 규칙을 이용하는 오피니언 마이닝 방법을 제안한다. 상품평 데이터를 분석하기 위하여 자연어처리기법을 통해 상품의 특징과 오피니언을 본 논문에서 정의하는 데이터 형태로 저장한다. 정의된 데이터 형태는 연관 규칙 마이닝 알고리즘을 쉽게 적용하기 위해 필요한 형태이다. 연관 규칙 마이닝 알고리즘을 활용하여 저장된 데이터로부터 네가지 형태의 연관 규칙을 찾는다. 그리고 PMI 기법을 사용하여 네가지 형태의 연관 규칙으로부터 오피니언을 요약하여 상품의 장점과 단점에 대한 정보를 제공한다. 본 논문에서 제안하는 기법은 자연어로 구성된 상품평들을 정의한 데이터 구조로 변환하기 때문에 데이터를 저장하기 용이하다. 그리고 새로운 상품평이 추가되어도 데이터 셋에 쉽게 추가할 수 있다. 향후 연구로 오피니언 마이닝 결과의 정확도를 향상시키기 위해서 상품평을 더 정확하게 분석할 수 있는 자연어처리 기법에 대한 연구가 필요하다.Most people inquire about other people’s opinion and refer to recommended product, when they buy the products. Today, the result of explosive development of the Web makes it easy to consult other people’s opinion information. These variety of opinion data are not only useful to customers, but also manufacturers. As a result, opinion mining research to analyze opinion data on the web has become a popular topic recently.
In this paper, we propose opinion mining method for product reviews. In our approach, we first do POS tagging on each review sentence, and we extract feature and opinion words in form of transaction data. We define transaction data schema for association rule mining, in order to analyze the data. Then we discover four types of association rule from the transaction data, and provide information that is summarized in advantages and disadvantages using PMI-IR algorithm. Our proposed method is easy to store data, because we transform product review written in natural language into defined data schema. And if it adds new product review, it is easy to add in data set. In our future work, we need research on improving natural language process technique for precise analysis of product review.Abstract 1 1. Introduction 3 2. Related Works 6 3. Background Concept 8 3.1 Part-of-Speech(POS) Tagging 8

시각적 질의응답이란 기계학습 분야에 최근 등장한 과제로서, 이미지와
자연언어 질문이 주어졌을 때, 이미지에 있는 정보를 바탕으로 질문에 대한
정확한 답변을 하는 과제를 말한다.
이는 컴퓨터 비전과 자연언어처리(Natural Language Processing)이라는 큰
갈래로 나누어져서 진행이 되던 연구들의 교차로 기로에 있는 중요한 연구
주제이다. 위 과제를 해결하기 위해 최근 다양한 딥러닝 기반의 모델들이
쏟아져 나오고 있다. 이러한 다양한 모델들이 좋은 성능을 보이지만, 장소를
묻는 질문에 ‘예(yes)’ 라고 대답하고, 비슷하지만 다른 질문에 똑같은 대답
을 하는 등의 자연어 질문에 대한 이해 부족으로 짐작되는 현상들이 두루
나타난다.
따라서 본 연구에서는 위에서 지적한 문제점을 해결하기 위해 문장 분류
와 품사 태깅을 이용하여, 좋은 성능을 유지하면서 기계의 자연어 질문에
대한 이해도를 높이는 새로운 시각적 질의응답 모델을 제안한다. 정확도를
포함한 다양한 실험 결과, 본 연구가 제시하는 자연어 질문에 대한 이해도
를 평가하는 척도에서 다른 모델보다 좋게 나왔으며, 성능 또한 기존의 최
고 성능을 보이는 모델[4]과 비슷한 것을 확인했다.제 1장 서론 . １ 제 2장 관련 연구 ４ 2.1 기본 모델(Baseline Model) . ４ 2.2 어텐션 기반 모델(Attention Based Model) ７ 2.3 Bilinear Pooling 기반 모델 . １０

본 연구는 한국 시단에서 시대적 상황의 변화에 따라 보여주는 인식의 변화 속에서 보여주는 한국 근대시의 서정성을 연구하여 그 특성을 파악하는 데 중점을 두었다. 그 가운데 서정시의 대표성을 가질만한 석정시의 서정성을 비중 있게 살펴보는 데 주안점을 두었다. 이러한 방법 중의 하나로 한 작가를 선택하게 되었는데 신석정이다. 한국 시문학사에서 신석정을 논하게 될 때마다, 조심스럽게 사용되고 있는 용어가 있다. 서정시인, 이미지스트, 낭만주의, 모더니스트, 순수시파, 사상파, 참여시인, 사상시(思想詩)라는 말이 그것이다. 그러나 이 중 어느 용어도 확고한 논증을 가지고 시인 신석정의 존재의미를 규정해 주지는 않는다. 그 용어들은 서로 넘나들면서 혼류현상으로 보여 지고 있다.
본 논문은 주저 없이 사용되고 있는 목가시인, 전원시인에 대한 의미와 조심스럽게 사용되고 있는 모더니스트, 이미지스트, 서정시파들의 용어의 타당여부에 대한 결과를 얻게 될 것이다. 진정한 연구자란 시인 자신도 깨닫지 못하고 있던 시인의 의미를 발견해 내야하기 때문이다.
신석정의 시어는 자연어가 주를 이룬다. 자연어는 산, 꽃, 나무 등 움직이지 않는 것과 물, 구름 ,바람, 새 등 동적인 자연어로 나뉘는데 그중에서 움직이지 않는 시어가 신석정시의 주 언어가 된다. 신석정은 시에서 감각어 보다는 감정어를 사용했으며 향토어와 방언을 사용하는 한편 후기에는 영어도 사용한다. 감정어의 과다사용 현장은 그의 언어관이 지니는 감정의 토로라는 동양적 문학관에서 나타난다.
한문투의 표출에서 당시와 시조가 그의 시의 기반이었음을 알 수 있으며 지방어와 고어, 영어의 과다사용 현상에서는 그의 언어관이 아직 개성을 초월한 시인의 언어, 다시 말해 공인된 감각의 단순하고 보편적인 언어의 시도로 여겨진다.
신석정의 시의 이미지는 빛 이미지이며 이는 어머니로 대표되는 구원의 이미지, 산으로 상징되는 침묵의 이미지, 꽃으로 상징되는 순환의 이미지와 보조를 갖는다. 석정시에 있어서 빛은 햇빛, 달빛, 별빛 등의 자연 빛이며 석정의 초기 시에서 후기시에 까지 모든 가치의 최상의 자리에 놓인다.목 차 국문 초록 ······························································· ⅲ Ⅰ. 서론 ································································· 1 1. 연구의 목적과 의의 ····················································· 1

Call Center is an emotional labor job with a lot of customer enquiries, and while the number of users increases over time, it is not able to deal with the stress of employees' work, resulting in a vicious cycle of hiring new employees. In addition, it will require a lot of training time to actually do the job. However, if training is insufficient, the quality of customer handling due to new employees' input is poor, It takes a lot of time to find the correct answer and it also causes the company's image or products to depreciate as the customer's latency increases.
Counselors use natural language analysis to find the correct answer to a question. The natural language analysis method is to use formal analysis and Word2Vec, Learning data is learned using historical call data, Insert new questions into the learning result and extract similar items, We want to reduce the time and stress required to find the correct answer to the questionnaires ranking from past consultation data.
By presenting candidate data in combination with web applications using natural language analysis methods and text data analysis, we reduced staff stress and call time, Considering the wide range of possibilities that Word2Vec has, it seems that it will not only be worthwhile to analyze text but also to analyze voice recognition data.콜 센터는 고객 문의가 많은 감정 노동 직업으로서 시간이 지날수록 이용자가 늘어나는 데 비해 직원들의 업무 스트레스를 해결해 주지 못하고, 이에 따른 직원들의 퇴직자가 발생함에 따라 신입 직원을 새로 뽑아야 하는 악순환을 반복하게 된다. 또한, 실제 업무 투입까지의 많은 교육 시간이 필요하게 된다. 하지만 교육이 미흡할 경우 신입사원의 업무 투입으로 인한 고객 응대의 품질이 떨어지고, 정확한 답변을 찾기 위한 많은 시간이 소요되고 고객의 대기 시간이 늘어남에 따라 기업 이미지나 상품의 가치가 떨어지는 요인이 되기도 한다.
상담 직원들이 문의에 대한 정확한 답변을 찾기 위해 자연어 분석을 사용하며 자연어 분석 방법은 형태소 분석 및 Word2Vec 사용하고, 학습 데이터는 과거의 상담 데이터를 기반으로 사용하여 학습시킨 후, 새로운 문의 사항을 학습결과에 대입하여 유사도가 높은 항목을 추출 후, 과거의 상담 데이터에서 문의에 맞는 데이터 후보 순위를 표현 정답을 찾는 데까지 필요한 시간 및 스트레스를 감소시키고자 한다.
자연어 분석 방법 및 텍스트 데이터 분석을 통한 웹 애플리케이션과 결합하여 후보 데이터를 제시함으로써 상담 직원들의 스트레스 및 상담 시간을 감소시킬 수 있었고, Word2Vec 가 가지는 폭넓은 활용 가능성을 고려할 때, 텍스트 분석뿐만 아니라 녹취데이터를 분석한 음성인식 부분에까지 활용 가치가 있을 것으로 보인다.국문초록 ⅴ 영문초록 ⅶ 제 1 장 서론 1 1.1 연구배경 1

The ability to understand the story is essential to make humans unique from other primates as well as animals. The capability of story understanding is crucial for AI agents to live with people in everyday life and understand their context. However, most research on story AI focuses on automated story generation based on closed
worlds designed manually, which are widely used for computation authoring. Machine learning techniques on story corpora face similar problems of natural language processing such as omitting details and commonsense knowledge. Since the remarkable success of deep learning on computer vision field, increasing our interest in research on bridging between vision and language, vision-grounded story data will potentially improve the performance of story understanding and narrative text generation.
Let us assume that AI agents lie in the environment in which the sensing information is input by the camera. Those agents observe the surroundings, translate them into the story in natural language, and predict the following event or multiple ones sequentially. This dissertation study on the related problems: learning stories or generating the narrative text from image streams or videos.
The first problem is to generate a narrative text from a sequence of ordered images. As a solution, we introduce a GLAC Net (Global-local Attention Cascading Network). It translates from image sequences to narrative paragraphs in text as a encoder-decoder framework with sequence-to-sequence setting. It has
convolutional neural networks for extracting information from images, and recurrent neural networks for text generation. We introduce visual cue encoders with stacked bidirectional LSTMs, and all of the outputs of each layer are aggregated as contextualized image vectors to extract visual clues. The coherency of the generated text is further improved by conveying (cascading) the information of the previous sentence to the next sentence serially in the decoders. We evaluate the performance of it on the Visual storytelling (VIST) dataset. It outperforms other state-of-the-art results and shows the best scores in total score and all of 6 aspects in the visual storytelling challenge with evaluation of human judges.
The second is to predict the following events or narrative texts with the former parts of stories. It should be possible to predict at any step with an arbitrary length. We propose recurrent event retrieval models as a solution. They train a context accumulation function and two embedding functions, where make close the distance between the cumulative context at current time and the next probable events on a latent space. They update the cumulative context with a new event as a input using bilinear operations, and we can find the next event candidates with the updated cumulative context. We evaluate them for Story Cloze Test, they show competitive performance and the best in open-ended generation setting. Also, it demonstrates the working examples in an interactive setting.
The third deals with the study on composite representation learning for semantics and order for video stories. We embed each episode as a trajectory-like sequence of events on the latent space, and propose a ViStoryNet to regenerate video stories with them (tasks of story completion). We convert event sentences to thought vectors, and train functions to make successive event embed close each other to form episodes as trajectories. Bi-directional LSTMs are trained as sequence models, and decoders to generate event sentences with GRUs. We test them experimentally with PororoQA dataset, and observe that most of episodes show the form of trajectories. We use them to complete the blocked part of stories, and they show not perfect but overall similar result.
Those results above can be applied to AI agents in the living area sensing with their cameras, explain the situation as stories, infer some unobserved parts, and predict the future story.스토리를 이해하는 능력은 동물들 뿐만 아니라 다른 유인원과 인류를 구별짓는 중요한 능력이다. 인공지능이 일상생활 속에서 사람들과 함께 지내면서 그들의 생활 속 맥락을 이해하기 위해서는 스토리를 이해하는 능력이 매우 중요하다. 하지만,
기존의 스토리에 관한 연구는 언어처리의 어려움으로 인해 사전에 정의된 세계 모델 하에서 좋은 품질의 저작물을 생성하려는 기술이 주로 연구되어 왔다. 기계학습 기법을 통해 스토리를 다루려는 시도들은 대체로 자연어로 표현된 데이터에 기반할 수 밖에 없어 자연어 처리에서 겪는 문제들을 동일하게 겪는다. 이를 극복하기 위해서는 시각적 정보가 함께 연동된 데이터가 도움이 될 수 있다. 최근 딥러닝의 눈부신 발전에 힘입어 시각과 언어 사이의 관계를 다루는 연구들이 늘어나고
있다. 연구의 비전으로서, 인공지능 에이전트가 주변 정보를 카메라로 입력받는 환경 속에 놓여있는 상황을 생각해 볼 수 있다. 이 안에서 인공지능 에이전트는 주변을 관찰하면서 그에 대한 스토리를 자연어 형태로 생성하고, 생성된 스토리를
바탕으로 다음에 일어날 스토리를 한 단계에서 여러 단계까지 예측할 수 있다. 본 학위 논문에서는 사진 및 비디오 속에 나타나는 스토리(visual story)를 학습하는 방법, 내러티브 텍스트로의 변환, 가려진 사건 및 다음 사건을 추론하는 연구들을
다룬다.
첫 번째로, 여러 장의 사진이 주어졌을 때 이를 바탕으로 스토리 텍스트를 생성하는 문제(비주얼 스토리텔링)를 다룬다. 이 문제 해결을 위해 글랙넷(GLAC Net)을 제안하였다. 먼저, 사진들로부터 정보를 추출하기 위한 컨볼루션 신경망, 문장을
생성하기 위해 순환신경망을 이용한다. 시퀀스-시퀀스 구조의 인코더로서, 전체적인 이야기 구조의 표현을 위해 다계층 양방향 순환신경망을 배치하되 각 사진 별 정보를 함께 이용하기 위해 전역적-국부적 주의집중 모델을 제안하였다. 또한,
여러 문장을 생성하는 동안 맥락정보와 국부정보를 잃지 않게 하기 위해 앞선 문장 정보를 전달하는 메커니즘을 제안하였다. 위 제안 방법으로 비스트(VIST) 데이터 집합을 학습하였고, 제 1 회 시각적 스토리텔링 대회(visual storytelling challenge)에서 사람 평가를 기준으로 전체 점수 및 6 항목 별로 모두 최고점을 받았다.
두 번째로, 스토리의 일부가 문장들로 주어졌을 때 이를 바탕으로 다음 문장을 예측하는 문제를 다룬다. 임의의 길이의 스토리에 대해 임의의 위치에서 예측이 가능해야 하고, 예측하려는 단계 수에 무관하게 작동해야 한다. 이를 위한 방법으로
순환 사건 인출 모델(Recurrent Event Retrieval Models)을 제안하였다. 이 방법은 은닉 공간 상에서 현재까지 누적된 맥락과 다음에 발생할 유력 사건 사이의 거리를 가깝게 하도록 맥락누적함수와 두 개의 임베딩 함수를 학습한다. 이를 통해 이미 입력되어 있던 스토리에 새로운 사건이 입력되면 쌍선형적 연산을 통해 기존의 맥락을 개선하여 다음에 발생할 유력한 사건들을 찾는다. 이 방법으로 락스토리(ROCStories) 데이터집합을 학습하였고, 스토리 클로즈 테스트(Story Cloze Test)를 통해 평가한 결과 경쟁력 있는 성능을 보였으며, 특히 임의의 길이로 추론할 수 있는 기법 중에 최고성능을 보였다.
세 번째로, 비디오 스토리에서 사건 시퀀스 중 일부가 가려졌을 때 이를 복구하는 문제를 다룬다. 특히, 각 사건의 의미 정보와 순서를 모델의 표현 학습에 반영하고자 하였다. 이를 위해 은닉 공간 상에 각 에피소드들을 궤적 형태로 임베딩하고,
이를 바탕으로 스토리를 재생성을 하여 스토리 완성을 할 수 있는 모델인 비스토리넷(ViStoryNet)을 제안하였다. 각 에피소드를 궤적 형태를 가지게 하기 위해 사건 문장을 사고벡터(thought vector)로 변환하고, 연속 이벤트 순서 임베딩을
통해 전후 사건들이 서로 가깝게 임베딩되도록 하여 하나의 에피소드가 궤적의 모양을 가지도록 학습하였다. 뽀로로QA 데이터집합을 통해 실험적으로 결과를 확인하였다. 임베딩 된 에피소드들은 궤적 형태로 잘 나타났으며, 에피소드들을 재생성 해본 결과 전체적인 측면에서 유사한 결과를 보였다.
위 결과물들은 카메라로 입력되는 주변 정보를 바탕으로 스토리를 이해하고 일부 관측되지 않은 부분을 추론하며, 향후 스토리를 예측하는 방법들에 대응된다.Abstract i Chapter 1 Introduction 1 1.1 Story of Everyday lives in Videos and Story Understanding . . . 1 1.2 Problems to be addressed . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Approach and Contribution . . . . . . . . . . . . . . . . . . . . . 6

자연어처리에서 중의성의 해결은 시스템의 정확도 향상을 위해 필수적인 기술이다. 그 중에서도 단어의 의미중의성은 하나의 단어에 대해서 여러 가지의 의미로 사용되는 것을 말한다.
본 논문은 문장 안에서 의미중의성을 갖는 단어가 출현했을 때 그 단어가 어떤 의미로 사용되고 있는지 판별해 주는 방법을 제시하고자 한다. 이를 위해서 먼저 중의적 의미를 가지는 단어의 각 의미 (sense) 마다에 대하여 이 의미를 나타내는 주요단어 즉 종자단어와 연관성이 있는 단어들로 벡터를 구성하여 이 의미를 나타내고자 한다. 본 논문에서 중의적 의미를 가지는 단어의 의미를 나타낼 종자단어는 사전에 기반하여 선택할 수도 있으며 임의로 적당한 단어를 제시할 수 있으므로 이를 ‘종자단어’ 또는 ‘seedword’ 라고 부른다. 종자단어 선정에 따라 유사도 값이 영향을 받기 때문에 종자단어 선정에 있어 후보를 제시하여 후보간에 먼저 말뭉치에서 후보단어가 들어있는 문장의 빈도를 가지고 비교한 다음에 가장 큰빈도를 가지는 후보단어를 최종적인 종자단어로 사용하였다.
본 논문에서는 종자단어와 말뭉치의 문장을 통하여 연결된 경로를 가진 단어는 이 종자단어에 해당하는 의미를 나타내는 데 기여하는 정보로 본 것이다. 경로는 동일 문장에서 나타나는 두 단어 사이는 링크가 있다고 보고 이러한 링크를 통하여 이루어 질 수 있는 연결 관계를 나타낸다. 이 기법의 장점은 대량의 말뭉치에서 실제 사용되고 있는 문장들속에서 데이터를 확보하고 있으며 링크를 통해 데이터를 확장하기 때문에 자연어처리에서의 통계적 방법의 주요문제인 데이터 부족으로 야기되는 문제를 경감시킬 수 있다는 점이다. 또한 링크를 통해서 연관성이 있는 데이터를 연결하기 때문에 기존의 수작업에 의한 의미중의성 해소의 의미벡터 구축작업을 자동화시킬 수 있다는 장점이 있다.
본 논문에서는 실험을 위해 Hantec 품사 부착된 말뭉치를 이용하여 의미정보벡터를 구축하였으며 외부실험을 위해서 ETRI 품사 부착된 말뭉치에서 중의적 단어가 포함된 문장을 추출하여 실시하였다. 외부 실험 결과 기존의 수작업에 의한 방법보다 나은 성능을 보임이 밝혀졌다.This paper presents a model using word link and co-occurrence words information for word sense disambiguation in Korean. In the sentence, Much co-occurrence words are very useful. We first construct a very small size of seedword form. Then, the small prior seedword is applied to the large tagged corpus. and then we search sentences that seedword is included in the large tagged corpus and then we extract nouns and verbs in the searched sentences.
This nouns and verbs are co-occurrence words of seedwords. This co-occurrence words contribute to word sense disambiguation.
For experiments, we construct informations of word sense using HANTEC tagged copus and test sentences of ETRI tagged copus. The results show that our model better than baseline result. This result indicates that word link and co-occurrence words can provide a useful way of word sense disambiguation.차례 = i 그림 차례 = ii 표 차례 = iii 국문 요약 = iv 제1장 서론 = 1

자연어와 자연어로 쓰여진 형식(문헌)은 비정형 데이터의 가장 일반적인 정보 추출용 자원이다. 방대한 양의 인간 지식이 비정형 텍스트로 저장되며, 이들은 전자적으로도 이용이 가능하다. 생명 과학 문헌의 개방 데이터베이스는 2천 1백만건 이상의 레퍼런스들을 보유하고 있으며, 현재에도 MEDLINE fact sheet에 따르면 매 분당 2개 정도의 레퍼런스들이 추가되어 지고 있다. 이러한 많은 정보의 과부하로 인하여 특별한 단일 분야일지라도 생명 과학자들은 관련된 모든 논문들을 탐색하는 것이 불가능한 실정이다.
이에 따라 생명 과학자들은 관련된 연구를 지속적으로 파악하고 발전시키기 위하여 의생물학 정보 추출 도구들을 활용하고 있다. 의생물학 정보 추출(BioIE)에 대한 연구는 개체명 인식에서부터 시작하여 복잡한 이벤트 추출까지 광범위한 연구에 초점을 두고 있다. 그러나, BioIE는 레이블이 있는 훈련집합으로. 사용가능한 적절한 자원들의 부족으로 인하여 아직 해결해야할 문제들이 많은 실정이다. 따라서, 이 논문에서는 레이블 되지않은 데이터와 결합된 oracle supervision의 잇점을 얻기 위하여 준지도 학습 기법들을 중심으로 연구의 범위를 설정하였다.
이 논문은 의생물학 정보들의 효과적 추출을 위한 주요 기능들에 대한 연구이며, 우선적으로 생명과학 분야의 의생물학 개체명 인식(BioNER)과 다중 영역 개체명 인식(NER)에 초점을 두어 연구한 후 그를 기반으로 의생명학 이벤트 추출(BioEE)을 위한 연구를 진행 하였다. 이러한 각각의 연구 단계를 위하여 우리는 준지도 학습 방식을 제안한다. 이러한 접근 방식은 레이블 되지 않은 데이터의 활용뿐 아니라 기존 시스템의 문제점들을 해결하는데 도움을 준다. 우리가 제안하는 ACT(Active Co-Training) 알고리즘과 후보 구문 분석 모델은 개체의 이름 경계 문제를 해결할 수 있다. ACT 알고리즘은 레이블 되지 않은 데이터로부터 질의된 유익한(유용한 정보를 주는) 예제들로부터 반복적으로 학습된 두 개의 서로 특징 집합들을 기반으로 한 두 개의 분류기를 적용하는 준지도 학습 방법이다. 이를 이용하여 이 논문에서는 레이블 되지 않은 데이터에 포함된 임의의 예제의 유용성을 측정하기 위한 새로운 분류 방법을 설계한다. 이 분류 문제에서 각각의 분류기에 따라 유용하거나 유용하지 않다고 분류된 특징 집합의 공통의 뷰에 따라서 예시들은 분류 된다. 분류를 위한 학습 데이터 집합을 만들기 위해서 우리는 query-by-committee 방법을 사용하였다. ACT알고리즘에서는 두 분류기들을 각각의 예제에 유용한 레이블을 주기 위하여 레이블 된 데이터에 사용 되는 하나의 committee가 되는 것으로 간주한다. ACT 기법은 기존의 f-measure척도에서 학습 반복 횟수에 따라 좋은 분류 모델을 만드는 co-training 알고리즘보다 좋은 성능을 보여준다. 제시 된 기법은 작은 숫자의 예시들로 선택된 유용한 정보를 가졌을 뿐 아니라 복합적인 패턴도 가지고 있는 방대한 양의 레이블 되지 않은 데이터를 효율적으로 이용되게 한다.
우리의 대표 단어 학습 기반 명명 개체 인식 방법은 단백질과 유전자의 식별, 약물과 화학 또는 질병 진단을 적용할 수 있는 다분야 통합 개체명 인식 시스템으로 발전할 수 있다. 이 기법은 개체명 인식 모델 에서 특정 분야 지식을 포함하고 시스템의 성능에 영향을 주는 방대한 양의 레이블 되지 않은 데이터에 Deep neural network를 적용하여 생성된 신경 언어 모델이다. 제시 된 방법은 텍스트 전처리를 위한 자연어 처리 작업, 방대한 양의 텍스트 데이터로부터 특징 추출과 토큰 분류를 위한 조건부 랜덤 필드에 있어서 표출 된 학습된 단어들의 특징을 포함한다. 특정 분야의 다른 자유 텍스트 보다, 제시 된 방법은 어떠한 어휘에도, 어떠한 바이오 텍스트 데이터의 시스템에 해당하는 다른 개체명 인식 작업 사전에도 의존하지 않는다.
우리의 새로운 자가 학습 알고리즘과 그것의 학습 프레임 워크는 원래의 트레이닝, 레이블 되지 않은 데이터로부터 새롭고 균형적인 분포를 생성하여 기존의 생물의학 데이터의 불균형을 해결한다. 우리는 능동적 학습과 자가학습의 처리기법들을 결합하고 새로운 준지도 학습 기법을 개발하여 데이터 불균형 문제를 해결하고 생물의학 이벤트 추출에서 레이블 되지 않은 데이터를 사용할 수 있게 하였다. 제안한 알고리즘은 기존 분류 모델의 결정 경계를 넘는 중요한 지면의 구성의 바탕이 된다. 학습 데이터는 레이블 되지 않은 데이터로부터 얻어 진 새로운 예시 집합에 의해 증가 되고, 이것들을 기반으로 한 모델 학습은 특정 분야 배경에서 패턴을 찾아낸다. 그 새로운 예시 집합은 그것의 중요한 자가 레이블링을 위한 신뢰도를 기반으로 형성된다. 뿐만 아니라, 우리는 2단계의 학습 프레임 워크로 알고리즘을 확장했다. 첫 번째 단계에서는 초기 모델 유도를 위해 학습데이터의 균형을 이루고, 두 번째 단계는 레이블 되지 않은 데이터로부터 얻어진 질의 예시의 모델에 특정 분야 지식을 통합시키는 것을 포함한다. 각각의 단계에서, 일련의 방식으로 모델은 형성된다.
마지막으로 중요하게 발표하는 것은 딥러닝 기반의 반복 예측 모델이다. 파이프 라인 기반의 생명의학 이벤트 추출에서의 연속적인 에러의 문제를 해결하기 위해, 우리는 일반적이고 표현적인 반복 예측 모델이라 불리는 semi-joint model을 소개한다. 이 모델은 공동으로 트리거와 인수를 기록하고 이벤트 중첩을 해결하기 위한 이벤트 표현을 활용하여 이벤트 구조를 찾는다. Deep neural network 구조는 생물의학 이벤트에서 의미 있는 표현을 학습하고 예측하기 위해 설계 되었다. 레이블 되지 않은 데이터를 활용하고 시스템 성능을 보강하기 위해 단어 심기, 네트워크에서 사용된 단어 특징 들은 Recurrent Neural Network 를 통하여 수집된 커다란 문서 집합으로부터 학습된다. 우리의 기법은 한쪽 끝과 다른 한쪽 끝을 잇는 방식으로 최소한의 전처리와 기능 공학과 어떤 문장 분석 정보의 의존 없이 생물의학 이벤트를 추출한다. 우리가 아는 한, 이 방법은 이 문제에서 중요한 역할을 담당해왔던 파서의 의존 없이 BioEE를 해결하는 첫 번째 시도이다.
제시된 알고리즘을 NER의 배경과 BioEE의 기존시스템과 비교 분석하고benchmark data 평가 했을 때 좋은 성능을 얻었다. 실험결과, 우리는 화학(과 약물), 생물의학 과 질병 등의 개체명 인식의 평균 f-measure는 각각 86.47%, 87.04% 그리고 77.84%의 결과를 얻었다. 또한, BioEE에서 가장 좋은 결과는 GE’11 test corpora에서 f-measure 50.74% 결과를 얻을 수 있었다.
우리는 또한 다분야 NER과 이 연구의 일부로 개발된 BioEE툴을 오픈소스로 제공한다. 우리가 개발한 다분야 NER과 BioEE도구는 오픈소스(Open source)인 https://bitbucket.org/tsendeemts/banner-chemdner, https://bitbucket.org/tsendeemts/deeptext에 공개하였다. 그리고 연구한 결과를 PPI, 의미적 의료정보 검색, 연관추출 정보 등과 같은 바이오인포메틱스(Bioinformatics), 바이오메디칼(Biomedical) 분야에 활용할 수 있다.Natural language and its written form are the most common source of unstructured data. The vast majority of human knowledge is stored in unstructured text and is available electronically. The open database of life sciences literature contains more than 21 million references and approximately 2 are added per minute according MEDLINE fact sheet to the date. With such an information overload, now it is impossible for life scientists to explore all the papers, even in a particular single domain.
Therefore, the life scientists are actively using biomedical information extraction tools in order to advance and keep track of related work in their discipline. Research in Biomedical Information Extraction (BioIE) has focused on a wide variety of tasks starting from named entity recognition to complex event extraction. However, the BioIE has still been a challenge due the lack of appropriate resources that can be used as labeled training data. Fortunately, there is a large pool of unlabeled data containing much of the domain knowledge that one can exploit for a better BioIE.
In this dissertation, we present semi-supervised and deep learning-based machine learning methods that efficiently exploits unlabeled data in order to incorporate domain knowledge into BioIE and to leverage system performance. In particular, we explore Named Entity Recognition (NER) and event extraction tasks of BioIE. It is crucial to have an accurate NER system, because NER is an essential prerequisite task before effective text mining can begin for biomedical-text data. However, the named entities recognized, and pairwise relationships extracted, are insufficient for understanding biomolecular interactions. Therefore, extraction of complex relations (namely, biomedical events) that provide a high level insight for formalizing and understanding the biomolecular process in an organism has received increasing attention.
We first focus on Biomedical Named Entity Recognition (BioNER) and a multi-domain Named Entity Recognition in life sciences, that can be applied to identify chemical and drug, gene and protein or disease mentions from free text. We then investigate Biomedical Event Extraction (BioEE). Our purposed approaches aim not only to make use of unlabeled data in order to incorporate the domain knowledge into BioNER and BioEE, but also to solve the issues in existing systems. The proposed Active Co-Training (ACT) algorithm and candidate parsing model address a problem of name boundary of entity, whereas our representation learning approach built upon a deep neural network takes a step towards an integrated multi-domain NER system, which has not been attempted before to the best of our knowledge. Our novel self-training algorithm and its learning framework solve data imbalance issue in biomedical event data by dynamically drawing a new and balanced distribution from the original training and unlabeled data. The last but not least approach presented in our dissertation is recursive projection model based deep learning that we propose to fix the issue of cascade error of event extraction pipeline. Our approach extracts the biomedical events in an end-to-end fashion with minimal preprocessing and feature engineering and without any sentence dependency parsing information. To best of our knowledge, this method is the first attempt to solve BioEE without a dependency parser that has played a main role in this problem.
The proposed methods achieved the state-of-the-art performance, when evaluated on benchmark data and compared with existing systems in context of the NER and the BioEE. The average F-measures for chemical (and drug), biomedical and disease named entity recognitions were 86.47%, 87.04% and 77.84% respectively when evaluated on CHEMDNER chemical and drug mention, BC2GM gene and protein and AZDC disease corpus. The best result we achieved for the BioEE is a 50.74% F-measure on GE’11 corpora.
We also provide open source multi-domain NER (available at https://bitbucket.org/tsendeemts/banner-chemdner) and BioEE (available at https://bitbucket.org/tsendeemts/deeptext) tools that have been developed as part of this research. We expect that the approaches proposed in our dissertation can be applied to other real-world applications where unlabeled data is less expensive to collect, such as protein-protein interaction extraction, biomedical question answering and semantic search support, and generic relation extraction.Chapter 1 Introduction 1 1.1 Biomedical Information Extraction 2 1.2 Motivation 7 1.3 Aim and Objectives 9 1.4 Contributions 10

자연어처리에서 기계학습을 위한 학습 말뭉치는 매우 중요하다. 본 논문에서는 대량의 품사 부착 말뭉치의 오류를 자동으로 수정하는 새로운 방법을 제안한다. 자동으로 오류를 수정하기 위해서 RDR(Ripple-Down Rules)을 사용하였다. 기존의 RDR을 자연어처리에 맞게 커널을 사용할 수 있도록 확장하였다. 품사 부착의 오류 유형을 찾기 위하여 오류를 포함한 말뭉치와 이를 수정한 정답 말뭉치로 위키피디아와 블로그 문서를 사용하였다. 한국어 위키피디아 문서와 블로그 문서를 사용하여 수정 방법의 효과와 효율을 검증하였다. 이 방법은 대량의 품사 부착 말뭉치를 제작할 때 오류를 최소화하는 방법으로 사용이 가능하다.

컨볼루션 신경망은 이미지 처리를 위한 아키텍처였으나 최근에는 자연어 처리 분야에도 적용되고 있다. 하지만 자연어 처리와 관련된 기존 연구들은 대부분 영어를 대상으로 하고 있어, 교착어 특징으로 이루어진 한국어에서는 기존의 모델을 적용함으로써 성능을 기대하기에는 한계가 있다. 그리고 구어체 한국어의 맞춤법 오류, 음소나 음절의 축약과 탈락, 은어나 비속어의 남용, 의태어 사용 등 문법적 규칙에 어긋나는 다양한 표현들로 인해 단어 기반 컨볼루션 신경망으로 특징들을 추출 할 수 없는 문제점이 있다.
본 논문에서는 단어 기반 컨볼루션 신경망으로 추출할 수 없는 특징을 음절이나 자소에서 추출 할 수 있는 Multi-channel 모델과 정보를 밀집 전달하는 DenseNet 모델을 연결하여 한국어 구어체 문장 감성 분류를 개선할 수 있는 Multi-channel DenseNet 모델을 제안하였다.
본 논문에서 제안한 모델에서 덴스넷은 밀집 블록과 전이층이 반복적으로 연결되어 있다. 밀집 블록은 컨볼루션층의 출력이 이후에 오는 모든 컨볼루션층들의 입력에 연결이 되도록 밀집 연결을 구축함으로써, 더 깊게 층을 쌓을 수 있게 했다.
본 연구에서 제안한 Multi-channel DenseNet 모델의 성능을 평가하기 위해 네이버 영화 리뷰 데이터 v1.0과 네이버 영화 리뷰 코퍼스 30만를 대상으로 감성 분류 실험을 하였다. 네이버 영화 리뷰 데이터 v1.0에 대해서는 긍정과 부정 2극성으로 분류하여 85.96% 성능을 보였다. 타 방법들에 비해 1.45% 개선된 성능을 보였다. 네이버 영화 리뷰 코퍼스 30만에 대해서는 긍정, 중립, 부정 3극성으로 분류하여 81.61% 성능을 보였으며, 타 방법들에 비해 0.74% 개선된 성능을 보였다.The Convolution Neural Network was an architecture for imag processing, but recently it has also been applied to natural language processing. However, most of the existing researches related to natural language processing are aimed at English language. In Korean language composed of characteristics of agglutinative language, there is a limit to expect performance by applying existing models. There is a problem that the features can not be extracted by the word - based Convolution Neural Network due to miscellaneous expressions that are not in accordance with grammatical rules such as spelling errors in spoken Korean, reduction and elimination of phonemes and syllables, abuse of fluency and profanity, and use of idioms.
In this paper, we connect a Multi-channel model that can extract features that can not be extracted from word-based Convolutional Neural Networks from syllables and phrases and a DenseNet model that densely transmits information. Therefore, we proposed a Multi-channel DenseNet model which can improve sentence sentence classification in Korean.
In the model proposed in this paper, dense nets are repeatedly connected to dense blocks and transition layers. Dense blocks allow deeper layers to be built by establishing dense connections so that the output of the convolution layer is connected to the input of all subsequent convolution layers.
In order to evaluate the performance of the Multi-channel DenseNet model proposed in this research, sensibility classification experiments were conducted for the Naver movie review data v1.0 and the Naver movie review corpus 300,000 sentences. The Naver movie review data v1.0, it was classified as positive and negative two polarity, showing 85.96% performance. And improved by 1.45% compared to other methods. The Naver movie review showed 81.61% performance for corpus 300000 classified as positive, neutral, and negative three polarity. And improved by 0.74% compared to other methods.제1장 서론 1 1.1 연구배경 및 필요성 1 1.2 연구목적 2 1.3 논문의 구성 2

본 논문은 자연어 처리분야에 있어서 한국어 텍스트에 대한 자동처리에 대한 연구이다. 한국어는 굴절어의 하나로서 한국어 활용어미를 처리하는데에 음소단위의 정보는 필수이다. 한국어 텍스트를 컴퓨터상에 표기 하기위해 널리 쓰이고 있는 완성형코드에는 이러한 음소단위정보가 없으며, 따라서 완성형코드에서 음소단위의 처리를 쉽게 할 수 있는 코드처리시스템인 'Hangul Code Manager (HANCoM)' 을 개발 하였다.
논문의 전반부에서 다룬 한국어 텍스트에 대한 각종 통계자료는 자연어 처리 분야에서 유용하게 쓰일 수 있다. 여기서는 한국어 텍스트 코퍼스에 대해 음절(syllable) 및 단어의 빈도수등 각종 통계자료를 구하였으며, 이를 근거로하여 한국어 음절 단위의 mono-gram, di-gram, tri-gram 엔트로피 및 단어 단위의 mono-gram, di-gram 엔트로피를 구하였다. 또한 한국어 텍스트에 대해 자연어의 특징의 하나로 널리 알려져 있는 Zipf's 법칙에 대해 두가지 방법으로 모델링을 해봄으로서, 한국어도 유사하게 Zipf's 법칙을 따른다는 것을 보였다.
두 번째로는 한국어 전자사전에 대한 구현으로, Minimal acyclic automata를 사용하였다. Reverse transition table을 제안하여 오토마타의 구축 및 최적화 알고리즘을 개발하였다. 한국어 전자사전의 활용형태사전(DECOF)를 구축하기 위해, DECO-RA, DECO-FlexAV 라고 하는 사전을 구축하였다. 1억4천8백만개의 표제어(2.55 Gbyte)를 가지고 있는 DECOF 사전을 오토마타를 이용하여 구현하기 위해, 두 개의 오토마타와 하나의 Mapping Matrix 를 사용하는 알고리즘을 개발하였다. 이러한 방법을 사용하여 한국어 전자사전을 오토마타를 이용하여 구현할 수 있으며, 활용형태사전(DECOF)의 경우 데이터 압축은 괄목할만 하다.

Information retrieval where a large amount of information has accumulated from his need to search for information using various search methods. It is the typical search technology based on keyword search. The benefits of keyword-based search is quick and easy to implement, but the disadvantage is likely to provide meaningful information on the same keyword and do not contain the keyword entered by the user is not able to provide any information. Therefore, the semantic search that can provide information to identify a user's search intent, and accordingly research and development. The integrated policy information service system based on natural language processing techniques without the use of ontologies to create a Semantic Web-based technology and associated information knowledge maps and Key Words Extractor, related information manager and user browser. The integrated policy information service system to provide you with related information as well as information including keyword search, you can easily find the information you want.정보 검색이란 “대량의 정보가 축적되어 있는 곳에서 자기가 필요로 하는 정보를 여러 가지 검색방법을 사용하여 검색하는 것”을 말한다. 필요한 정보를 효율적으로 찾기 위한 여러 가지 검색기술로는 대표적으로 키워드 기반 검색이 있다. 키워드 기반 검색은 비교적 구현이 쉽고 빠르게 검색이 가능한 장점이 있지만, 동일한 키워드에서 의미가 다른 정보를 제공할 가능성이 크고 사용자가 입력한 키워드를 포함하고 있지는 않은 정보는 전혀 제공하지 못하는 단점이 있다. 따라서 사용자의 검색 의도를 파악하고 그에 따른 정보를 제공할 수 있는 의미기반 검색이 연구되고 발전하였다. 종합정책정보서비스 시스템은 온톨로지를 생성하는 시맨틱 웹 기반 기술을 사용하지 않고 자연어 처리기반기술을 이용하였으며 문서의 분류체계와 키워드로 구성된 지식 맵(정책용어집)과 문서에서 주제어를 자동으로 선정하기 위한 주제어 추출기, 문서간의 연관도를 계산하여 연관정보DB를 구축하고 관리하는 연관정보 관리기, 사용자가 검색키워드를 입력하면 결과를 제공하고 조회할 수 있는 사용자 검색기로 구성된 의미기반 검색시스템이다. 이러한 종합정책정보 서비스 시스템은 사용자가 검색한 키워드를 포함한 정보뿐만 아니라 연관정보를 함께 제공하여 사용자가 원하는 정보를 쉽게 찾을 수 있도록 하였다.목 차 國文抄錄 ⅳ 英文抄錄 ⅴ

최근 인터넷이 활성화되면서 상당한 잠재적 가치를 지니고 있는 텍스트 데이터들이 빠른 속도로 증가하고 있다. 텍스트 데이터는 기업의 환경이나 문서가 담고 있는 내용에 따라 다양한 포맷이 존재한다. 기업에서 사용되는 텍스트 데이터의 경우 20%만이 정형화된 데이터로 구성되어있고, 나머지 80%는 xls, ppt, doc, pdf, html등의 형태로 구성되어 있다. 이처럼 텍스트 데이터들은 일반적으로 정형화 되어있지 않아 분석하기가 쉽지 않고, 대부분 자연어로 쓰여진 문장 형태이기 때문에 함축된 정보를 추출하는 것조차 쉽지 않다. 그러나 마케팅 및 경영전략 수립에 활용할 수 있는 고급 정보를 내포하고 있는 까닭에 비정형적인 텍스트 문서로부터 유용한 정보를 추출하고 가공하는 기술인 텍스트 마이닝이 최근 많은 관심을 받고 있다. 본 논문에서는 그래픽이 뛰어난 소프트웨어 R을 이용하여 비정형화된 텍스트를 시각화하는 여러가지 방법을 살펴보면서, 문서 빈도 행렬 (term document matrix)에 주성분 분석, 군집 분석등의 다변량 기법들과 정보검색(Information Retrieval)을 실제 자료에 활용하여 텍스트 자료 분석에 필요한 통계기법을 검토하여 본다.1. 서론 7 2. 자연어처리(Natural Language Processing) 9 2.1 형태소 분석 9 2.2 용어 문서 행렬(Term Document Matrix) 11 3. 용어 문서 행렬에 다 변량 분석 응용하기 12

본 연구의 목적은 자연어 형태의 한국어 텍스트에 적합하며 실용적으로 응용될수 있는 자동색 인 시스템을 구현하는 것으로,본 연구는 한국어 텍스트로 부터주제를 대표할 수 있는 색인어(단윱�복합어)를 추출하는 구문. 통계적자동색인 시스템을 개발하였다. 구문. 통계적 자동색인 시스템소 분석과 단어가중 기법을 이용하여 단일어와명사구를 동시에 선택하는 자동색인 시스템을 말한, 문헌의 주된 내용을나타내는 명사. 명사구가 특정 구문범주에 속한다는 가정 아래, 자연어 형탭層�문장에서 미리 정의된 구문범주를이용하여 명사구를 추출한 다음, 통계적 방법으로 추출된 가중치를부여하여 가중치가 높은 용어를 색인어로 선정하는 시스템을 말한다. 시스템의성능을 측위하여 300개의 우리말 학술잡지 논문과 학위논문의 초록에서선택된 자동 색인어와 수작업 색인쓩냘臼낫� 본 시스템에서 추출된 자연어형태인 자동 색인어는 수작업 색인어에 비하여일반적으로 정성과 잉여성을 갖게 됨에 따라 수작업 색인어와의 일치율과유사계수의 값이 비교적 낮게 나타낮��구문 . 통계적 자동색인 시스템의 전반적인 성능은 다른 자동색인 시스템의성능과 비교해 볼비슷한 것으로 밝혀졌다. 따라서 본 연구에서 사용된 구문. 통계적 색인기법이 앞으로 유용하게 읊痔聆�것으로 보며, 본 연구는 아직 미흡한 연구상태에 있는 우리말 자동색인기개발에 있어서 풉輸各米搔�제시했다는 의의를 갖는다.

네트워크 기술과 인터넷 인프라 발전으로 인터넷 사용 인구가 세계적으로 약 35억 명에 달하고 있으며, 계속 증가하고 있다. 인터넷을 통해 정보를 빠르고 쉽게 얻는 것이 가능해졌지만 부작용 또한 발생한다. 인터넷의 부작용 중 대표적인 것은 성인물, 마약, 도박, 테러 등의 불법 컨텐츠가 존재하는 유해 사이트에 쉽게 접근이 가능하다는 것이다. 기존에는 사이트의 내부 컨텐츠를 분석하여 유해 사이트를 판별하는 방법과 사이트 간의 연결 관계를 분석하여 유해 사이트를 판별하는 방법이 존재했지만 두 방법 모두 한계점이 있다. 본 논문에서는 두 방법을 모두 이용하여 한계점을 상호 보완하여 성능을 향상시킨 유해 사이트 판별 시스템을 제안한다. 웹 사이트 간의 연결 관계를 이용하여 유해 사이트를 구별하고 구분한 유해 사이트의 텍스트 데이터를 학습하여 유해 사이트 판별 모델을 생성해 기존에 판별하지 못한 유해 사이트를 판별한다. 모델로 판별한 유해 사이트 중 가장 많이 링크를 받고, 유해할 확률이 가장 높은 웹 사이트를 관리자에게 추천한다. 관리자의 확인을 통해 새롭게 유해성을 판별 받은 유해사이트를 이용해 새로운 연결 관계를 구축하여 유해 사이트를 확장한다. 제안한 방법으로 유해 사이트를 더 많이 찾을 수 있으며, 웹 사이트 수집 속도 또한 향상된다.As the advancement of network technology and the development of internet infrastructure, the number of Internet users is about 3.5 billion worldwide, and it continues to increase. Through the Internet, it became possible to acquire information more faster and easier, but side effects are also occurring. One of the typical side effects of the Internet is that easy access to harmful sites containing illegal content such as pornography, drugs, gambling, and terrorism. Previously, there was a method of identifying the harmful sites by analyzing the internal content and a method of identifying the harmful sites by using the link relation of the websites. But there were limitations in both methods. We propose a harmful site discrimination system that improves performance by complementing each methods. We identify the harmful site by using the link relation of the websites and construct a harmful site discrimination model by learning the text data of the identified harmful sites, then identify the sites that have not been identified previously through the model. Among the site which is identified by the model with the highest number of links to be pointed is recommended to administrators. With newly identified harmful sites which are confirmed to the administrators, we find new link relations and expand harmful sites. Through the proposed method, we can find more harmful sites and collecting website speed increase.국문초록 ⅴ 영문초록 ⅶ 제 1 장 서론 1 1.1 연구 배경 및 목적 1

사이버폭력 피해 유형 중 욕설이나 비속어가 사용된 언어폭력으로 인한 정신적 피해가 심각한 수준이다. 실시간 채팅 검열 시스템을 위한 인공신경망 기반 금칙어 자동 필터링을 소개한다. 본 논문에서는 변형된 금칙어를 검출해내고 해당 채팅을 제재해야 하는지를 실시간으로 판단할 수 있는 개선된 금칙어 필터링 기법과 이를 이용하는 실시간 채팅 검열 시스템을 제안한다.
비슷한 발음의 자음과 모음을 가까운 거리에 표현하는 코드화 기법을 통하여 데이터의 사전 처리를 진행하며, 이를 통해 필터링의 정확도를 개선한다. 마할라노비스 거리 기반의 군집화 알고리즘과 인공신경망 기반의 알고리즘을 비교·분석한 결과 인공신경망을 이용한 알고리즘이 높은 성능을 보인다.
인터넷 채팅, 댓글이나 온라인 게임 등에 적용하면 기존의 필터링 방식보다 더 효과적인 필터링이 가능해질 것으로 기대되며, 기존의 무분별한 필터링으로 인한 의사소통에서의 불편을 해소할 수 있을 것으로 기대된다.제 1 장 서 론 1 1.1. 연구배경 1 1.2. 연구목적 1 1.3. 논문의 구성 2 제 2 장 관련 연구 3

목차 = ⅰ 표 및 그림의 목차 = ⅲ Abstract = ⅳ 요약 = ⅴ Ⅰ. 서론 = 1

구문분석은 문장 안에서 성분들의 관계를 찾는 과정이다. 자연어처리 분야에서 필수적인 단계 중 하나로서 처리속도, 성능, 강건함이 주요한 요소가 된다. 하지만 한국어 구문분석 연구는 다른 언어 연구에 비해 자원부족과 같은 문제로 연구 깊이와 결과가 좋지 못한 상황이다.
본 논문에서는 처리속도, 성능, 강건함을 가지면서 기존의 시스템에 비해 문장 성능을 높이기 위한 문장 구조 자질을 제안한다. 기존에는 다단계 구단위화(Cascaded Chunking) 방법을 사용하여 강건함을 획득하였고 어휘를 사용하지 않은 자질 학습으로 모델의 사이즈가 작으며, 고속으로 처리할 수 있었다.구문분석은 자연어처리의 기본 시스템이 되는 결과이기 때문에 상위 응용프로그램에서 적절한 데이터를 추출하려면 성능이 중요하다. 구문분석의 결과를 이용하게 되면 좀더 적은 데이터를 사용하여 사용자에게 맞는 데이터를 가공할 수 있기
구문분석의 성능은 어절(또는 형태소)성능과 문장 성능으로 평가한다. 문장의 성능을 높이기 위해서는 문장을 이루는 모든 어절의 아크가 올바르게 연결되어야 한다.
문장의 길이가 길어질수록 문장의 성능이 떨어지는데 이것은 문장의 큰 구조를 파악하기 힘들기 때문이다. 특히 트랜지션 기반의 구문분석 방법은 지역적 학습 방법과 오류전파 문제가 있으므로 그래프 기반 구문분석에 비해 문장구조 성능이 낮음을 기술하였다.
본 논문에서는 문장 성능을 향상시키기 위해 문장구조 자질을 적용하였다.
문장구조 자질은 문장에서 주요 요소가 되는 어절을 정의하는 것으로 자질로 정의하여 분석을 하는 방법이 성능향상에 도움을 준다는 것을 확인하였다. 즉, 지역적 학습 모델을 이용하여 분석을 하지만 문장구조 자질을 고려하여 전체 문장 구조 정보를 이용하는 것과 같다.
이외에도 기존 비어휘 다단계 구 단위화 시스템의 오류분석을 통하여 기존 문제점을 파악하고 구 패턴 묶음, 기능어 형태소 자질, 공기정보 자질을 추가하여 성능향상을 이룰 수 있었다.
또한 문장 구조 자질을 시스템 내부에서 정의하여 분석 시 성능 향상이 되는 것뿐만 아니라 정답 문장 구조자질을 이용하면 성능이 큰 폭으로 증가하는 것으로 보아 잠재적인 성능 향상을 확인 할 수 있다.
그것을 바탕으로 문장 구조 설정 성능 향상을 위해 넓은 문맥자질로서 학습한 모델을 실험하였다. 이로서 문장 구조 자질이 문장 성능 향상에 큰 영향을 주는 것을 확인 할 수 있다.
향후 연구로는 문장 구조 자질과 분석 가이드 방법을 통합한 모델로서 구문분석 성능 향상 연구와 확장 문장 구조 자질을 위한 효과적인 학습 방법을 추가적으로 연구할 예정이다.
또한 KIB(Korean Language Information Base) 코퍼스를 사용하여 평가하고 다른 시스템과 직접 평가를 해 볼 예정이다. KIB는 의존구조 구성단위가 ‘형태소’이므로 이를 ‘어절’단위로 변경하여야 한다. 그리고 세종코퍼스와 달리 구문태그의 기능태그가 없기 때문에 KIB 코퍼스에 맞는 자질을 추가적으로 연구할 예정이다.I. 서론 1 1. 구문분석을 위한 실용적인 모티브 2 2. 애매성(Ambiguity) 5 3. 의존문법 6 4. 한국어 특징 8

본 논문은 온톨로지 스키마 트리플의 의미를 포함하는 문장수집 방법을 제안한다. 온톨로지는 스키마 트리플을 따르는 다량의 인스턴스 트리플이 있고, 이들 인스턴스 트리플은 실세계에서 자연어 문장 형태로 등장할 수 있다. 본 논문에서는 이러한 인스턴스 트리플 정보를 활용하여 주어진 스키마 트리플의 의미를 포함 하는 문장을 수집한다. 인스턴스 트리플이 자연어 문장으로 표현될 때 특정 표현이 다양한 의미를 포함하거나 하나의 의미가 다양한 형태로 표현되는 모호성이 있다. 이러한 모호성 문제를 해결하기 위해 온톨로지와 문서간의 명시적인 연결정보와 다량의 인스턴스 트리플 정보를 활용한다. 실험에서는 DBpedia를 활용 하였으며, 제안하는 방법이 기존 연구에 비해 높은 정확률과 재현률을 보였다.This paper proposes a method for collecting sentences which entail ontology schema triple. Much of instance triples conforms to the schema triple of an ontology, these instance triples can present in the form of natural language sentences on the real world. In this paper, we use the information of instance triples for collecting sentences containing meaning of a schema triple. When instance triples are represented as the natural language form, there are several ambiguities which has multiple meanings, or the meaning of an expression is appeared in various forms. In order to resolve these ambiguities, our method uses the explicit connection information between an entity of an instance triple and a document and makes full use of instance triples information. According to experiment results with DBpedia, our proposed method outperforms than existing approaches.Ⅰ. 서 론=1 Ⅱ. 관련 연구=5 2.1 문장 코퍼스 자동 구축에 관한 연구=5 2.2 문장 코퍼스의 활용에 관한 연구=8 Ⅲ. 개체의 모호성 해결을 위한 접근=11

This study analyzes the differentiation of identities in the online community according to the level of anonymity and the effect of the differentiation on the life of the online community. Some scholars argues that anonymity makes an individual de-normative by making the individual atomize, dismantle his/her social identity, and make it hard to ask for ethical responsibility. On the other hand, the Social Identity model of Deindividuation Effects(SIDE) argues that the depersonalization caused by anonymity strengthens the sense of belonging to the group and makes it conform to the norm of the group by forming common identity.
Here, identity can be differentiated within the same group according to the level of anonymity, and an alternative model is proposed in which the interaction between differentiated identities affects group performance and lifecycle. On the one hand, Megalia is regarded as a pioneer of online feminism, but on the other hand it is seen as a hate site. The alternative model presented in this study helps to understand the process of forming different identities within Megalia and why the opposite interpretations are derived.
I analyzed the entire 162,893 posts and 709,768 replies throughout the lifecourse of Megalia from 2015 to 2016, in order to figure out how discourses were differentiated depending on whether the writer's ID was disclosed (pseudonym) or not (anonym), and to reconstruct the evolvement of collisions between the discourses. Natural language processing techniques (i.e., topic modeling and Word2Vec) were applied.
The results of the analysis are as follows. First, the topics discussed in Megalia with a high proportion were mostly related to common identity, and the proportion of topics that caused conflicts was not high. Second, according to the level of anonymity, there was a difference in the preferred topics. Most of the topics converged into the domain of common identity over time, but there were exclusively preferred core topics. Third, the exclusive identities formed according to the level of anonymity shows a distinct difference in the controversial situation and promotes differentiation among the members of the community. Fourth, strengthening the common identity in the community diminishes the boundaries between exclusive identities and leads to participation in collective action. Finally, the level of anonymity affects the frequency and use context of words in the common discourse and accelerates the differentiation of identities, but the final effect on the lifecycle of the community depends on the way in which the debate within the topic develops.
The analysis of this study shows that different identities can be formed within the same community due to structural factors such as anonymity level. It is also observed that the interaction between different identities changes the domain of common identity and also affects the life cycle of the community. I expect to contribute to the development of analytical frameworks that clarify how members in an online community form various identities in response to structural conditions such as levels of anonymity and to foster holistics understanding of the life-course of the community.본 연구는 익명성 수준에 따라 온라인 커뮤니티에서 나타나는 정체성의 분화와, 이러한 분화가 온라인 커뮤니티의 생애에 미치는 영향을 분석한다. 온라인 공간에서 익명성의 효과에 대해서는 두 가지 상반된 견해가 존재한다. 우선 부정론의 경우, 익명성은 사회적 정체성을 해체하고 윤리적 책임을 묻기 어렵게 만들어 원자화된 개인들이 탈규범적 행동을 하게 만든다고 주장한다. 이에 비해 탈개인화에 따른 사회적정체성 이론(Social Identity model of Deindividuation Effects)은 익명성이 야기하는 탈개인화가 오히려 집단에 대한 소속감을 강화하고 집단정체성을 형성하여 집단의 규범에 순응하도록 만든다고 주장한다.
사회적정체성 이론은 실제 온라인 공간에서 발생하는 다양한 집합행동의 존재를 설명해주고 있으나, 집단 내부에서도 정체성이 분화할 가능성과 이러한 정체성 분화가 집단의 생애에 미치는 영향력에 대해서는 충분히 분석하고 있지 않다. 따라서 본 연구에서는 익명성 수준에 따라 동일한 집단 내부에서도 정체성이 분화할 수 있으며, 분화한 정체성 사이의 상호작용이 집단의 성과와 생애주기에 영향을 미친다는 대안적 모형을 제시한다.
이러한 모형을 검증하기 위한 대상으로는 전투적 여성주의를 표방한 온라인 커뮤니티 ‘메갈리아’를 설정하였다. 메갈리아는 한편에서는 신세대 여성들에게 가부장적 현실에 대한 각성을 이끌어내고 집합행동에 나서게 한 온라인 여성주의의 효시로 파악하지만, 다른 한편에서는 익명 커뮤니티에서 나타나는 탈규범적 행위를 젠더 갈등이라는 형태에 맞추어 반복하는 혐오 사이트로 파악한다. 대립하는 두 가지 해석은 각각 온라인 공간에서 익명성이 수행하는 역할에 대한 사회적정체성 이론과 부정론에 상응하기 때문에, 본 연구에서 제시한 대안적 모형을 통해 상이한 정체성이 공존할 수 있는 이유 및 각각의 정체성이 상이한 해석으로 이어진 이유를 이해할 수 있다.
글쓴이 ID 공개 여부로 구분되는 익명성 수준(완전익명, 부분익명)에 따라 게시물에 나타난 담론 내용의 차이를 파악하고 내부갈등의 전개 양상을 재구성하기 위해 2015년에서 2016년에 이르는 메갈리아의 전체 생애에 걸친 게시물 162,893건 및 게시물에 작성된 댓글 709,768건을 전수 분석하였다. 분석에는 자연어처리 기법인 구조적 토픽 모형(Structural Topic Model)과 워드투벡터(Word2Vec)를 활용하였다. 구조적 토픽 모형을 활용하여 익명성 수준에 따라 게시물에서 나타나는 토픽 비중 차이를 분석한 후, 워드투벡터를 활용하여 동일 토픽 내에서 익명성 수준에 따른 맥락의 차이를 분석하였다. 또한 메갈리아의 생애에 중요한 영향을 미친 논쟁적 토픽들에 대해서는 추가적으로 구조적 토픽 모형을 통해 댓글에서 나타나는 반응의 차이를 분석하였다.
분석 결과는 다음과 같다. 첫째, 메갈리아에서 높은 비중으로 활발하게 논의된 토픽들은 대부분 메갈리아의 공동 정체성과 관련된 토픽들이었이며, 내부적으로 격렬한 갈등이 벌어진 논쟁적 토픽의 비중은 높지 않았다. 둘째, 익명성 수준에 따라 선호하는 토픽의 경향성에 차이가 있었으며, 시간에 따라 상당수의 토픽들이 공통담론의 영역으로 수렴하였으나 배타적으로 선호하는 핵심 토픽들이 존재하였다. 메갈리아에서는 익명성 수준에 따라 명확하게 선호하는 주제가 다르게 나타나며 이는 배타적 정체성의 형성을 시사한다. 셋째, 익명성 수준에 따라 형성된 배타적 정체성은 논쟁 상황에서 뚜렷한 차이를 보이며 커뮤니티의 구성원 사이의 분화를 촉진한다. 논쟁적 토픽에서는 뚜렷한 정체성 분화가 관측되었으며, 시간의 흐름에 따라 논쟁에 대한 반응 역시 분산되는 경향을 보인다. 넷째, 커뮤니티의 공동 정체성 강화는 익명성 수준에 따른 배타적 정체성을 완화하며 집합행동에의 참여를 이끌어낸다. 메갈리아에서 내부 논쟁이 성공적으로 봉합된 후, 금전적 후원과 같이 비용이 필요한 집합행동 참여 비중이 증가하였다. 마지막으로, 익명성 수준은 공통담론 내 단어의 사용 빈도와 활용 맥락에 영향을 미치고 정체성 분화를 가속하지만, 커뮤니티의 생애주기에 미치는 최종 효과는 토픽 내부의 논쟁 전개 방식에 따라 달라진다.
온라인 공간에서의 익명성의 역할에 대한 기존 입장들과, 메갈리아의 정체성을 둘러싼 기존 해석들은 모두 정체성을 사전에 주어진 고정적 실체로 파악하였기 때문에 합의에 도달할 수 없었다. 본 연구에서는 정체성을 고정된 실체가 아니라, 사회적 조건 속에서 변화하고 의식적 연출이 개입되는 수행적(performative) 개념으로 파악하여 정체성의 분화가 온라인 커뮤니티의 성과와 생애에 미치는 영향력을 파악하고자 하였다. 본 연구에서 수행한 분석을 통해 익명성 수준과 같은 구조적 요인에 의해 동일한 커뮤니티 내부에서도 상이한 정체성이 형성될 수 있으며, 상이한 정체성 사이의 상호작용에 의해 커뮤니티의 공동 정체성의 영역이 바뀌며 나아가 커뮤니티의 생애까지 영향을 받는다는 점을 관찰하였다. 이러한 연구결과를 통해 온라인 커뮤니티에서 사용자들이 익명성 수준과 같은 구조적 여건을 활용하여 다양한 정체성을 형성하고 상호작용하는 방식과, 커뮤니티의 생애과정 전개를 통합적으로 이해하는 분석틀의 발전에 기여하길 기대한다.

There are many people these days who want to make profits from real estate investments. This is because there are many cases and recognition that real estate investment products bring high profits. There are four ways to invest in real estate and find good areas. First of all, the more convenient the transportation is, the better. The second is selling hand and footwork. Thirdly, we need to take a close look at the development good. Finally, we need to be able to identify changes in the market through objective indicators. There is a basic urban plan in this objective index. The basic urban plan is a strategic plan that forms the basis of the relevant plans, such as the urban management plan, by presenting key indicators on the basis of the city and military administration, development and conservation of land, expansion of infrastructure and efficient urban management strategies. Therefore, it is very difficult for users to quickly grasp the vast amount of urban basic plans, although they want to select promising areas for real estate investment through the Urban Master Plan.[10].
In this paper, the purpose of this thesis is to select promising areas by analyzing the vast amount of urban basic plans in a short time by analyzing them in a short period of time by extracting regional information and related contents up to the top 10 list of urban basic plans that appear in the city's main plans, and visualizing them through Bar Chart and WordCloud.오늘날 사람들은 부동산 투자를 통해 수익을 창출하려는 분들이 적지 않다. 부동산이라는 투자 상품이 높은 수익을 가져다준다는 인식과 사례들이 많기 때문이다. 부동산투자, 좋은 지역 찾는 4가지 방법이 있다. 우선 첫 번째로 교통이 편리할수록 좋다. 두 번째로는 손품과 발품을 파는 것이다. 세 번째로 개발 호재를 유심히 살펴봐야 한다. 마지막으로 객관적인 지표로 시장의 변화를 파악할 줄 알아야 한다. 이 객관적인 지표에 도시기본계획 이 있다. 도시기본계획은 시·군 행정의 바탕이 되는 주요 지표와 토지의 개발·보전, 기반시설의 확충 및 효율적인 도시관리 전략을 제시하여 하위계획인 도시관리계획 등 관련 계획의 기본이 되는 전략계획이다. 이에, 도시기본계획 을 통해 부동산 투자 시 유망지역을 선정 하고는 싶으나 도시기본계획의 방대한 내용을 사용자가 단시간에 파악하기란 여간 힘든 일이 아닐 수 없다[10].
본 논문에서는 유망지역선정 프로그램을 설계 및 구현하여 도시기본계획 내용 중 출현 빈도수 상위 10위까지의 지역 정보와 관련 내용을 추출하고 이를 Bar Chart 와 WordCloud 를 통해 시각화 함 으로써 도시기본계획의 방대한 내용을 단시간에 분석하여 투자 유망지역을 선정 하는 것을 그 목적으로 한다.제 1 장 서론 1 1.1 연구의 배경 및 목적 1 1.2 논문의 구성 2 제 2 장 관련연구 4

최근 십여 년간 통계 기계번역 모델이 부단히 발전함에 따라 통계 기계번역은 자연어처리 분야에서 가장 활발히 연구되고 있는 분야 중 하나가 되었다. 통계 기계번역에는 단어 기반 통계 기계번역, 구 기반 통계 기계번역과 구문구조 기반 통계 기계번역이 있다. 세 가지 번역모델은 모두 단어를 정렬하고 단어정렬 결과로부터 번역규칙을 추출하고 추출된 번역규칙으로부터 번역확률을 추정하고 새로운 문장이 입력되면 번역하는 흐름으로 진행된다.

단어정렬이란 주어진 병렬 말뭉치의 정렬된 문장쌍 내에서 단어들의 대응관계를 찾는 과정이다. 단어정렬은 통계 기계번역 외에도 중의성 해소, 이중 언어 대역사전구축 등과 같은 자연어처리 분야에서 다양하게 활용할 수 있다. 단어정렬의 좋고 나쁨은 통계 기계번역시스템의 번역규칙 추출과 번역확률 추정에 직접적인 영향을 미치고 나아가서 기계번역 품질에도 영향을 미친다.

이 논문에서는 문자정렬을 이용해 중국어-한국어 통계적 단어정렬을 개선하는 전처리 방법을 제안한다. 문자 대역사전을 이용하여 사전기반으로 정렬한 결과와 통계적 단어정렬을 결합한 기존 연구와는 달리 제안하는 방법은 문자의 통계를 이용하여 단어정렬을 개선했다. 실험결과, 기존연구와 단어단위 정렬과 비교하였을 때 제안하는 방법의 단어정렬 정확률, 재현율과 F-measure에서 모두 향상된 것을 확인할 수 있었다. 또한 제안한 방법으로부터 얻은 정렬결과를 이용해 번역규칙을 추출하고 대역 확률을 추정하고 수행한 통계 기계번역 실험에서도 번역품질이 유의미하게 향상됨을 확인할 수 있었다.1. 서론 1.1 연구배경 1.2 단어정렬 2. 관련연구 2.1 문자 대역사전을 이용한 단어정렬

저자와 탐색자는 하나의 같은 의미를 여러가지의 다른 용어를 사용하여 표현하기 때문에 전문데이터베이스의 자연어 검색에서 검색효율이 저하된다. 이러한 문제점을 해결하기 위하여 여러학자들은 전문데이터베이스를 검색하는 보조수단으로 동의어, 계층어, 관련어를 제공하는 탐색시소러스의 이용을 제안하였다.
본 연구에서는 전문데이터베이스의 자연어 검색에서 검색효율을 향상시키기 위하여 전문데이터베이스의 하나인 한국경제신문사 ECONET의 기사 데이터베이스를 대상으로『경제신문 시소러스』와 퍼지시소러스의 퍼지관련어를 이용하여 기본 탐색어 검색, 계층어 확장검색, 관련어 확장검색, 종합검색의 검색효율을 측정하였다. 기본 탐색어 검색은 기자의 초기 질문에 포함된 용어만으로 구성하고 상대적인 재현율을 측정하기 위하여 종합검색에서 검색된 적합문헌을 100%로 간주하였다. 검색결과는 비모수통계방법인 Wilcoxon검증으로 검증하였다.
이 연구의 결과는 다음과 같다.
첫째, 신문기사의 용어는 신조어와 유행어 등으로 자주 갱신하여야 하며 탐색 시소러스의 갱신방법으로 통계방법인 퍼지관련어 생성공식을 적용할 수 있다.
둘째, 탐색시소러스를 이용한 확장검색은 기본 탐색어 검색과 비교하여 정확률은 거의 향상되지 않았으나 재현율은 많이 향상되었으므로 탐색시소러스를 이용한 확장검색이 재현장치임을 알 수 있다.
셋째, 기본 탐색어 검색과 확장검색의 중복도를 측정한 결과 각 탐색유형에서 중복도는 낮고 추가로 검색된 기사 중에서 고유한 기사가 많았으므로 모든 기사를 검색하기 위하여는 계층어와 관련어를 이용하여 확장검색하여야 한다.
넷째. Wilcoxon검증으로 관련어 확장검색과 종합검색이 정확률을 저하시키지 않고 재현율을 향상시키는 수단임이 증명되었다.
대부분의 신문기자는 정보요구로서 브라우징을 요구하기 때문에 신문기사 데이터베이스를 검색할 때에는 재현률의 향상이 중요한 요소이다. 따라서 전문데이터베이스의 검색을 효율적으로 수행하는 방법으로 탐색시소러스를 이용하여 초기 질문의 용어를 확장검색하는 것이 바람직하다.Authors and searchers usually express the same things in many different ways, which causes problems in free text searching of a full text database. Thus, search thesaurus that supplies synonyms, quasi-synonyms, narrower terms, related terms for given natural language terms have been suggested as a search aid to overcome these problems.
In this study, it was estimated and analyzed the retrieval effectiveness of the basic terms search, narrower terms expansion search, related terms expansion search and union of all previous searches using search thesaurus, 『Economic Newspaper Thesaurus』 and Fuzzy Related Terms on the newspaper database in ECONET of The Korean Economic Newspaper.
The basic terms searches contained only terms included in the original query statement and the searches were analyzed in terms of the relative recall and precision. : these were estimated by setting the recall of the union search to 100%. The statistical significance of the findings was tested with nonparametric statistical tests, Wilcoxon test.
The results of this study were as follows.
First, the words of newspapers should be renewed because of many new words and fashionable words. Fuzzy thesaurus which generates related terms can be used in the method for the rewal of related terms.
Second, in case of using the basic terms, narrower terms, related terms and union search, the extension searches were increased so much in the recall but the precision. Therefore, search thesaurus proved to be important as a recall device.
Third, the number of unique articles in the added retrivals was large, thus, the overlap between search modes was rather small. All the terms types were needed to reach the largest recall.
Fourth, it proved that the related terms, narrower terms and union search were the means of increasing the recall not decreasing the precision.
The recall is significant in searching newspaper databases because of most of journalists requires the browsing as information requests Therefore, use of search thesaurus is the desirable method to achieve the high retrieval effectiveness in full text databases.목차 도표목차 = iv 국문초록 = vi 1. 서론 = 1 1.1 연구의 필요성 = 1

정보 통신 기술의 발달로 인해 인터넷 법률, 인터넷 기사, 인터넷 상품리뷰 와 같은 인터넷 상의 문서가 증가하였고 그에 따라 자연어처리(Natural Language Processing) 분야에 대한 관심이 많아졌다. 그리고 최근에는 신경망(Neural Network)을 비롯한 머신러닝 알고리즘을 이용하여 자연어처리를 수행하는 태스크에 대한 수요가 높아졌다. 신경망을 비롯한 대부분의 머신러닝 알고리즘에서 텍스트를 적용하기 위해서는, 텍스트를 일련의 벡터의 형태로 표현해야 한다. 이와 같이 텍스트를 의미 있게 표현하는 방법을 워드 임베딩이라고 하는데, 워드 임베딩(word embedding)을 위한 연구로는Word2Vec, Glove,FastText 등이 있다. 그러나 리소스가 부족한 언어의 경우 단일 언어 임베딩(monolingual word embedding)을 적용하면 임베딩의 질이 떨어질 수 있다. 그리고 다중 언어 맥락에서 단어에 대한 의미추론을 하는 데는 어려움이 있다. 이러한 문제들은 교차언어 워드 임베딩을 기법을 적용하게 되면 리소스가 풍부한 언어(resource-rich)와 리소스가 부족한 언어(resource-lean) 간의 지식전달(knowledge transfer)을 가능하게 함으로 해결할 수 있으며, 다중언어 의미 (multilingual semantics)을 가능하게 함으로 해결할 수 있다. 위와 같은 이유들로 최근 이중 언어 임베딩(bilingual word embedding) 관련 연구들이 각광을 받고 있다. 그러나 한국어와 특정 언어로 구성된 병렬(parallel-aligned) 말뭉치로 이중 언어 워드 임베딩을 하는 연구는 질이 높은 많은 양의 말뭉치를 구하기 어려우므로 활발히 이루어지지 않고 있다. 특히, 특정 영역에 사용할 수 있는 로컬 이중 언어 워드 임베딩(local bilingual word embedding)의 경우는 상대적으로 더 희소하다. 또한 이중 언어 워드 임베딩을하는 경우 번역 쌍이 단어의 개수에서 일대일 대응을 이루지 못하는 경우가 많다. 본 논문에서는 로컬 워드 임베딩을 위해 한국어-영어로 구성된 한국 법률 단락 868,163개를 크롤링(crawling)하여 임베딩을 하였고 3가지 연결 전략을 제안하였다. 본 전략은 앞서 언급한 불규칙적 대응 문제를 해결하고 단락 정렬 말뭉치에서 번역 쌍의 질을 향상시켰으며 베이스라인인 글로벌 워드 임베딩 (global bilingual word embedding)과 비교하였을 때 2배의 성능을 확인하였다. 또 한 Word2Vec과 유사하지만 subword 개념이 추가된 임베딩 방법인 FastText를 적용하여 성능 차이를 비교하였다.1. 서론 ··········································································································· 1 1.1 연구배경 ········································································································ 1 1.2 연구내용 및 기여 ··························································································· 2 1.3 논문의 구성 ··································································································· 3 2. 관련 연구 ··································································································· 4

컴퓨터가 자연 언어를 이해하기 위해서는 [문장 입력, 형태소 분석, 구문 분석, 의미 분석, 문장의 의미파악]의 자연어처리과정을 거쳐야 하므로 언어의 구문(syntax)의 분석은 필수적인 과정이다. 자연어 질의응답 시스템, 자연어 인터페이스, 텍스트 요약 시스템, 기계번역 등 모든 자연언어 처리분야에서 자연언어의 구문 분석은 매우 중요한 역할을 한다. 또한, 자연언어는 인공언어와는 달리 중의성이 있는 문장이 많으므로 이를 적절하게 분석 할 수 있는 구문 분석 방법에 관한 연구가 필수적이다.
차트는 일반적인 파싱을 위한 자료구조로써 뿐만 아니라 특히 중의성이 있는 문자의 표현에 적합한 구조다. 차트를 이용한 구문분석은 중의적인 문장을 가능한 모든 구문 구조로 분석하는 경우에, 한 번 분석된 결과가 계속 유지되므로 이미 분석된 부분은 재분석 없이 이용할 수 있는 장점이 있다.
따라서, 본 논문에서는 부분 자유 어순을 반영하는 한국어 문맥 자유 문법을 제안하고 이를 기반으로 한 변형된 차트 파싱 알고리즘을 고안하여 한국어 구문분석기를 설계 및 구현하였다.In order to understand natural language, we have to follow the process "Input → Lexical Analysis → Syntactic Analysis → Semantic Analysis". Among those, "syntactic analysis" plays the most important role since it is able to be applied to various fields such as natural language query systems, natural language interface systems, text summary systems, and machine translation systems. It is also important to analysis a sentence containing ambiguity. In this paper, we propose an algorithm to parse the Korean language using chart. It is implemented using Prolog programming language.목차 제 1 장 서론 = 1 1.1 연구배경 및 필요성 = 1 1.2 본 논문의 구성 = 3 제 2 장 본론 = 4

인간 집단이 역사적으로 지나오는 동안에 의사 전달이나 의견 교환을 하기 위하여 자연적으로 발생한 언어를 자연 언어 혹은 자연어라 한다. 자연 언어는 인공언어와 달리 애매함이나 그때마다 여러 가지 생략이나 환언함이 있다. 더욱이 사회적인 지식 등도 필요하기 때문에 컴퓨터가 이해하는 것은 매우 곤란하다. 컴퓨터를 사용하여 자연 언어를 처리하는 것을 자연어 처리라고 하며, 컴퓨터 환경에서 자연 언어를 이해하고 모방하는 것이 자연 언어 처리 분야의 연구 목표 중 하나이다.

자연 언어 처리에서 말하는 형태소 분석이란 문장의 어절을 최소의 의미 단위인 형태소로 분석하는 것을 의미한다. 형태소는 언어학에서 일정한 의미가 있는 가장 작은 말의 단위로 더 분석하면 뜻이 없어지는 말의 단위이다. 단어 그 자체가 될 수도 있고, 일반적으로는 단어보다 작은 단위이다. 자연 언어를 분석하는데 가장 대표적인 문제점은 미등록어의 유사 단어를 찾는 부분이다.

사람은 미등록어가 포함된 문장을 이해할 때, 기존에 학습된 개념체계를 바탕으로 문맥 내 동시출현단어들의 의미를 이용해 대체 단어로 가장 접합한 의미를 판별한다. 이러한 개념의 핵심은 분포 가설에 기인하며, 이 가설은 언어학자 John Rupert Firth의 유명한 말 “You shall know a word by the company it keeps”로 설명되곤 한다. 즉 비슷한 맥락에서 함께 나타나는 경향이 있는 단어들은 비슷한 의미를 가지는 경향이 있다.

본 연구는 형태소 분석 오류인 미등록어를 유사 단어로 대체하는 Word2VnCR 알고리즘을 제안한다. 미등록어의 대체 후보 단어를 추출하기 위해 학습 데이터 셋을 워드 임베딩 학습한 뒤 대체 후보 단어를 추출한다. 추출된 대체 후보 단어들은 미등록어 주변의 인접 단어와 의미적 유사도를 측정해 높은 유사도 값을 가지는 대체 단어를 선정하고 이를 미등록어와 대체 한다.

제안된 Word2VnCR 알고리즘의 우수성을 입증하기 위해 NUS sms 말뭉치를 기반으로 미등록어의 유사 단어 대체를 Word2VnCR 알고리즘과 Word2Vec 알고리즘을 사용하여 비교 실험 하였다. 두 알고리즘을 이용한 미등록어의 유사 단어 대체 실험 결과 Word2VnCR 알고리즘이 미등록어를 의미가 유사한 단어로 대체하는데 있어 정확도면에서 Word2Vec 알고리즘 보다 높은 성능을 보이는 것이 확인되었다.

결과적으로 이 연구에서 제안하는 Word2VnCR 알고리즘은 미등록어를 의미가 유사한 단어로 대체하는데 있어 높은 정확도 나타내었다. 그러나 학습 데이터 셋을 어떻게 구축하느냐에 따라 실험의 결과는 영향을 받는다. 학습 데이터 셋의 워드 임베딩 학습이 재대로 이루어 지지 않는 다면 미등록어의 대체 후보 단어를 정확히 추출할 수 없기 때문이다. 따라서 Word2VnCR 알고리즘은 미등록어가 적게 출현하고 미등록어의 인접 단어가 의미적인 단어로 이루어진 텍스트를 학습 데이터에 추가하는 작업이 필요하다.Natural language refers to a language developed naturally to express intention or exchange opinions as a group of human passes through historically. Unlike artificial languages, natural languages are often ambiguous and have many omitted words or paraphrases. Furthermore, social knowledge is also required to properly comprehend a natural language; it is very difficult for computers to understand it. Processing a natural language using a computer is referred to as natural language processing; one of the primary research goals in the field of natural language processing is to understand and imitate natural languages in a computer environment.

Morphological analysis in natural language processing refers to the analysis of a word in a sentence in terms of morphemes, the smallest unit of meaning. A morpheme is the smallest unit of a word which has a certain meaning in linguistics; moreover, it is a unit of a word whereby the meaning disappears if analyzed further. Although it can be the word itself, in general, it is a unit smaller than the word. One of the most common problems in analyzing a natural language is finding a word that is similar to a out-of-vocabulary word.

When a person understands a sentence containing a out-of-vocabulary word, he/she determines its most appropriate meaning with a substituted word by using the context to determine the meanings of words based on the conventional concept system that has been learned. The core of such a concept originates from the distribution hypothesis; this hypothesis is explained by John Rupert Firth’s famous saying, “You shall know a word by the company it keeps.” In other words, words that tend to appear together in similar contexts tend to have similar meanings.

This study proposes the use of the Word2VnCR algorithm that substitutes a out-of-vocabulary word with a similar word. To extract similar candidates for out-of-vocabulary words, word-embedding is learned using a training dataset; afterwards, similar word candidates are extracted. For the similar word candidates that have been extracted, the sematic similarities of adjacent words around the out-of-vocabulary word are measured, and a similar word that has the highest similarity value is selected. This word replaces the out-of-vocabulary word.


To prove the excellence of the proposed Word2VnCR algorithm, a comparative experiment was performed using the Word2VnCR algorithm and the Word2Vec algorithm for similar word substitutions of out-of-vocabulary words from the NUS sms Corpus. The results showed that the Word2VnCR algorithm showed higher performance than the Word2Vec algorithm in terms of accuracy for the substitution of a out-of-vocabulary word with a similar word.

As the final outcome, the Word2VnCR algorithm proposed in this study showed high accuracy when substituting a out-of-vocabulary word with a similar word. However, the result of this experiment is affected depending on how the training dataset is built. Similar word candidates of out-of-vocabulary words cannot be accurately extracted because the word-embedding learning of training dataset is not properly done. Therefore, the Word2VnCR algorithm needs the task of adding texts having the following characteristics to the training data: few out-of-vocabulary words appear, and the words adjacent to these out-of-vocabulary words are composed based on sematic meanings.목 차 ABSTRACT Ⅰ. 서 론 1

1960년 중반에 프로그래밍 언어를 해석하기 위한 구구조 (phrase-structure) 구문분석 기법으로 처음 개발된 LR (Left-to-right scan, Rightmost derivation) 파싱 기법은, 그 자체가 지닌 훌륭한 처리 속도와 기본적인 문맥의존성 덕택에 자연어 파싱 연구자들도 관심을 보여왔다. 1980년 후반에 자연어 문법의 중의적 성질을 반영할 수 있는 일반화 LR 기법이 개발되면서, PCFG보다 나은 확률적 구구조 구문분석을 LR 기법을 통해 수행해보려는 노력이 다수 시도되었다. 1990년 후반으로 들어오면서 대용량의 구문분석말뭉치를 쓸 수 있게 되자, 말뭉치 기반의 문법 구축과 생성 모델에 의한 통계적 파싱 방법이 각광 받으면서 확률적 LR 모델의 연구는 활발히 이루어지지 않았다. 그러나, 말뭉치에서 학습한 규칙의 개수가 수천 개를 넘게 되면서 효율 문제는 여전히 남아있게 되었기 때문에, 실시간 처리가 가능한 효율을 필요로 하는 분야에서는 높은 문맥 의존성을 가질 수 있는 통계적 LR 파싱 모델의 연구가 필요하게 되었다.
기존에 발표된 확률적 LR 파싱 모델은 LR 파서 자체에 묶여 있어 자연어 파싱에 필요한 많은 문맥 정보를 고려하는 데에 많은 무리가 있을 뿐만 아니라, 부분파스 정보를 지닌 LR 스택을 상태정보 혹은 상태 시퀀스로 한정하고 있다. 이러한 모델은 모두 영어권을 대상으로 하고 있는데, 더 많은 정보를 고려해야 하는 한국어 파싱에서는 높은 정확도를 기대하기 힘들다.
본 논문에서는 풍부한 문맥 정보를 융통성 있게 사용할 수 있는 단순하고 직관적인 통계적 LR 파싱 모델을 제안한다. 제안한 모델은 문맥정보에 따라 Shift 혹의 Reduce 연산만을 수행하는 순위화 모델(ranking model)로서, LR 파싱 테이블 구축 시중의적인 연산(ambiguous action)을 가진 엔트리에만 연산 스코어를 설정한다. 또한, 본 모델은 효과적인 한국어 파싱을 위해 한국어 구문구조에 기반하여 구성한 문맥정보를 사용한다. LR 파서에서 기본적으로 주어지는 정보 이외에, 본 모델은 LR 스택 첨단에 위치한 서브트리 정보와 주변 단어의 정보를 사용한다. 이 정보는 기능어를 기반으로 만들어지는데, 서브트리 정보는 구문 구조를 반영하는 단말 기능어 열과 내용 중심어로 구성되며, 주변 단어의 정보는 서브트리에 주변의 구조 정보를 부분적으로 제공하는 기능어 열이다.
제안한 모델의 테스트를 위하여, 12000 여 문장으로 구성된 구구조 구문분석 말뭉치에 대해 본 모델을 적용하였다. 본 모델에서 사용한 기능어 기반의 문맥 정보는 파싱 정확도를 높이는 데에 효과적으로 기여했을 뿐만 아니라, 적은 학습 자료에도 파싱 성능을 크게 떨어뜨리지 않았다. 본 모델은 LR 파서에서 주어진 기본 문맥 정보만으로도 기존의 확률적 LR 파싱 모델보다 나은 성능을 보였다. 실험 집합에 대한 본 모델의 정확도는 86.71%의 F-score이다. 기존에 발표된 문맥 의존형 한국어 구구조 구문분석 모델과 비교했을 때, 약 2.5%의 F-score 향상을 보였다.LR (Left-to-right scan, Rightmost derivation) parsing techniques were originally developed for parsing programming language in the late 1960s, In the late 1980s, the generalized version of LR algorithm that can handle natural language grammars was published, and this algorithm were actively considered by natural language parsing researchers, because of its excellent efficiency and Inherent mild context-sensitivity. From the middle of 1990s, large amount of tree-annotated corpus were available. CFG rules extracted from the treebanks have been widely used for statistical parsing and so many researchers have focused on corpus-based generative parsing models. Meanwhile, the studies of the LR approach for natural language parsing was less popular than before. Still, LR approach would be needed in some application areas that require efficient parsers. However, incidentally or not, the previous LR parsing models has a deficiency in making available sufficient contextual information in the LR stack, being constrained by the LR parsing framework for CFGs itself.
The previous probabilistic LR parsing models are strictly restricted by the LR parser itself (a LR parsing table, a graph-structured stack, and a lookahead), thus they are not allowed to use other rich contextual information beyond it. They simplified the LR stack as a state alone or state sequence. The previous models are mainly for English. For more effective parsing for Korean, we need a more flexible model that can use rich contextual information.
In this thesis, we propose a statistical parsing model that can use contextual information more flexibly than the previous probabilistic LR parsing models. It is the ranking model that just performs Shift or Reduce action using the given contextual information of the stack and the input. For effective parsing of Korean, Our model uses the contextual information based on syntactic information. Let alone the basic information given by the LR parser (state and lookahead), our model utilizes the information of the sub-trees on the top of the LR stack and the surrounding words. This information is based on functional-morphemes; the sub-tree information consists of the functional morpheme sequence representing the syntactic structure and the content head morpheme, and the surrounding words information is the limited length of the functional morpheme sequence.
We tested the proposed model using the Korean treebank with 12,000 trees. Experimental results have shown that the functional morpheme based contextual information is effective in improving parsing accuracy, and does not decrease the parsing accuracy to much with a small training set. Moreover, our statistical ranking model outperforms the previous standard probabilistic LR parsing models even when using a state and a lookahead alone. The parsing accuracy of the proposed model is 86.71% F-score on the test corpus. When we compared our model with the previous context-sensitive PCFG parsing model for Korean, our model outperformed it by about 2.5% F-score.Abstract = ⅰ Contents = ⅲ Chapter 1 Introduction = 1 1.1 Practical Motivation for Parsing = 3 1.2 Background of the Researches on LR Method for Natural Language Parsing = 5

소프트웨어 개발에 있어서 요구 분석 단계는 그 중요성이 점차 부각되고 있 다. 요구 분석은 요구사항의 추출,정리,평가,그리고 모델링의 순서로 진행된 다. 이중 모델링 작업은 자연어 문서로된 요구 사항들을 형식에 맞는 모델로 전환시키는 작업이다. 기존의 모델링을 지원하는 CASE TOOl들이 있으나 이들 은 사용자로부터 모델 요소를 입력받아 모델링을 지원하는 기능을 제공하고 있다. 그러나 모델링에서 가장 필요한 요소는 요구 문서로부터 모델 요소를 찾아내는 것이다. 이 작업은 요구 분석가들의 주관적 이해에 의존하므로 시간 과 인력의 소모,요구사항들의 검증,요구 문장과 모델 요소간의 연결형성등에 문제점을 내포하고 있다. 이러한 문제점들에 기인하여 자연어 요구 문서의 개 념적모델을 자동 지원하는 방법및 시스템을 개발하게 되었다. 본 연구의 개념 적 모델은 객체,작용,관계,조건을 포함하고 있다. 본 연구는 소프트웨어 요구 사항문서로부터 위의 개념적 모델 요소들을 자동으로 분석함으로써 요구 분석 가들의 소프트웨어 모델 개발을 지원 함에 있다. 이를 실현하기 위하여 자연 어 요구 문서의 전산 처리를 위한 재구성 기술,다중 요구문장의 분해기술,요 구 문장의 의미 분석기술,요구 문장의 의미 표현으로부터 개념적 형상으로의 전환기술들이 개발되었다. 위의 기술들을 자동으로 지원하는 시스템이 개발되 었으며,본 시스템은 유닉스 시스템하에 오픈윈도우를 기본으로 하여 운영 되 고 있다. 본 시스템을 이용하여 두 가지의 각기 다른 영역의 요구 문서들을 처리한 후 분석한 결과 80%이상의 추출된 개념적 모델요소가 유용한 것으로 판명 되었다. 모델작업의 지원 외에 소프트웨어 요구 문서의 의미 분석 결과 는 요구 문서간의 의미적 연결과 요구 문장간의 불일치등을 파악하는데 사용 될 수 있다.

본 논문에서는 요구사항을 기반으로 구현된 소프트웨어가 요구사항을 만족하는 지를 시험하기 위해서 정형화된 요구사항을 기반으로 테스트 케이스를 생성하는 방안에 대해 제시한다. 군과 같은 특수 분야에서는 소프트웨어의 자연어 요구사항의 모호성을 줄이기 위하여 소프트웨어 요구사항에 약간의 제약을 가하여 정형화된 요구사항 작성이 가능하다. 본 논문에서 제시한 방안에서는 .docx 파일 포맷의 정형화된 소프트웨어 요구사항을 해석하고, 해석된 정보로부터 테스트 케이스 생성 전략에 따라 테스트 케이스를 생성한다. 제시한 방안으로 군에서 사용하는 요구사항을 정형화하여 작성하고 이를 기반으로 테스트 케이스를 생성이 가능함을 보여 그 유효성을 입증한다.제 1 장 서 론 1 제 2 장 관련 연구 3 제 1 절 CNL을 이용한 요구사항 표현 3 제 2 절 STE 4 제 3 절 STK 7

음성은 인간이 갖고 있는 기본적인 능력 중 가장 중요한 것의 하나로 인간과 인간사이의 의사소통은 물론, 최근에는 인간과 기계간의 정보 교환 수단으로 사용하기 위하여 음성 정보 및 자연어 처리에 대한 관심이 높아지고 있다. 특히, 연속음성 인식의 어려움을 극복할 수 있는 핵심어 검출을 기반으로 하는 대화 음성처리 시스템이 연구되고 있으며, 이를 지원할 수 있는 음성 인식방법과 유연한 자연어 처리의 연구가 요구되고 있다.
본 연구에서는 HMM기반 핵심어 음성 인식의 성능 향상 및 대화체 연속음성 처리의 유연성 강화를 위한 연구로 3가지 방법을 제안하였다. 첫 번째는 필러 모델링에 대한 연구로 필러 음소를 한국어 형태론에 근거하여 6/7개 음소군 필러 모델로 클러스터링하였다. 두 번째로는 금액 연속 숫자음 인식에 대한 연구로 반음절쌍 모델기반 숫자음 인식을 위해 두음절 FSN(finite state network)을 제안하였다. 세 번째로는 대화 음성처리 시스템에 필요한 하이브리드 의도분석 방안과 화행 관측을 통한 핵심어 전이 확률을 실증적 연구를 통해 제안하였다.
본 연구의 첫 번째 제안으로 필러 모델을 6/7개로 축소하는 방법은 하나의 음소는 잘 나타내주지 못하나 비슷한 음소 그룹의 표현에는 유용한 방법으로 핵심어 검출의 계산시간 단축을 목적으로 하였다. 필러 모델의 축소로 예상되는 인식률 하락은 필러 패스에 가중치를 부여함으로써 성능을 향상시켰다. 실험 결과 ROC는 7 필러 모델이 좋은 성능을 보였으며, FOM은 6 필러 모델이 88.3%로 가장 우수한 성능을 갖는 것으로 확인되었다. 계산시간에서도 6 필러 모델이 46 음소 모델의 0.97초보다 0.3초 단축된 0.67초가 소요되어 가장 우수한 성능을 보였다. 두 번째 제안인 금액 연속 숫자음 인식을 위한 두음절 FSN의 제안은 한국어 금액 숫자음의 구조가 “숫자음+단위음“의 두음절 가변적 구조인 점을 감안하여 두 음절을 “반음절+반음절쌍(n)+반음절” 단위로 분할하여 인식하기 위한 연구이다. 문장 단위 실험에서 본 연구에서 제안한 반음절쌍 음향 모델이 가장 양호한 성능을 보였으며, 단어 단위 실험에서는 한음절 음향 모델을 사용할 때 성능이 가장 양호함을 확인하였다. 세 번째는 대화 음성처리 시스템의 유연한 처리를 목적으로 발화자의 의도 분석을 위해 “의도종속 핵심어기반 + 지식기반 의도분석”을 통합한 하이브리드 의도분석 방법과 핵심어 전이 확률의 실증적 연구를 제안하였다. 의도 예측은 하이브리드 방법이 예측률 약 98.3%로 가장 우수한 성능을 보였으며, 처리시간은 핵심어 전이 확률을 적용한 경우 전이 확률을 적용하지 않은 경우의 평균 0.67~0.71초보다 약 0.14초의 단축을 확인하였다. 마지막으로는 제안한 3가지 방법들을 하나의 대화 음성처리 시스템에 결합하여 성능을 확인한 결과 6 필러 모델의 인식률이 평균 92.4%로 가장 우수한 것으로 나타났으며, 다음으로 7 필러 모델이 92.2%, 46 음소 모델이 90.9%의 순으로 나타났다.
결국 본 연구에서 제안한 방법들을 하나하나 개별적으로 사용하기보다는 이를 하나의 음성 인식기에 유기적으로 연결되어 상호 보완 관계를 가질 때 그 성능과 함께 실세계 적합성이 가장 우수한 음성 인식기임을 확인할 수 있었다.제 1 장 서 론 1 1.1 연구 목적 1 1.2 연구 배경 4 1.3 연구 방향 및 방법 6 1.4 연구의 구성 9

한국어 문장 생성 시스템은 컴퓨터가 사용자에게 전달하고자 하는 내용을 자연스러운 한국어 문장으로 표현하는 시스템으로, 기계번역이나 대화 시스템, 질의 응답 시스템 등 여러 응용 분야에서 자연어 인터페이스를 제공할 때 필수적인 시스템이다.
자연어 대화 시스템에서 사용자는 시스템이 생성해 발화하는 문장으로 시스템과 의사소통을 하게 된다. 이때 사용자와 시스템 사이에 친밀관계가 형성된다. 시스템이 딱딱한 문어체 문장만 생성해 발화하는 것 보다는 부드러운 구어체 문장으로 발화하거나 통신언어를 생성해 발화할 수 있다면 더 친숙함을 느낄 수 있을 것이다.
지금까지 한국어 생성에서 사용자와의 친밀도를 고려해 발화 문장의 문체를 선택하는 것에 관한 연구는 없었다. 본 논문에서는 한국어 생성기의 일부분으로 시스템이 형태소 단위로 생성해 낸 발화 문장의 형태소를 합성하는 과정에서 발화의 문체를 변환하는 시스템을 제안한다. 본 논문에서는 규칙 기반 문체 변환 시스템과 SVMs를 이용한 통계 기반 문체 변환 시스템을 제안한다. 문자 채팅 영역 말뭉치 10000만 문장을 ‘삼체’와 ‘요체’로 변환해 실험한 결과 SVMs를 사용한 통계 기반 문체 변환기가 ‘삼체’ 97.9%, ‘요체’ 94.3%의 정확률(accuracy)을 보여 규칙기반 문체 변환기보다 더 높은 성능을 보였다.
본 시스템은 한국어 문생 생성기의 일부로서 생성된 발화의 문체를 바꿔줄 수 있을 뿐만 아니라, 다량의 구어체나 통신체 말뭉치 구축에도 사용될 수 있다.A sentence-generation system for Korean is a software which generates natural Korean sentences. It is essential for the application systems such as a machine translation system which translates foreign language to Koreans and a dialogue interface system which enables to use natural language dialogues for user interface.
In natural language dialogue system, utterances that generated sentence by system is used in communication between user and system. So, system utterance is very important for familiarity. User will feel more familiar if system can make various sentence styles(ex. Internet language).
Till now, there is no system that make utterance considering familiar with user. In this paper, we suggest a sentence style conversion system as a part of Korean sentence generator. This system divided rule-based system and statistical-based system. We experiment with 10,000 sentence corpus in SMS massage domain. We experiment about ‘sam style’ and ‘yo style’. As a experiment result, ‘sam style’ has 92.82% and ‘yo style’ has 90.54% in F1-measure. This result came from statistical-based sentence style conversion.
This system not only convert the sentence style but also could use make a large corpus in as a part of Korean sentence generator.

패러프레이즈란 어떤 표현 혹은 문장을 같은 의미를 가지는 다른 단어들을 사용하여 표현한 것들을 의미한다. 이는 정보 검색, 다중 문서 요약, 질의응답 등 여러 자연어 처리 분야에서 중요한 역할을 한다. 특히, 양질의 패러프레이즈 코퍼스를 얻는 데는 높은 비용이 소요된다. 만약 패러프레이즈 문장 쌍을 대량의 원시 데이터로부터 자동으로 추출할 수 있다면 이러한 기법이 자연어 처리의 여러 분야에서 유용한 자원으로 사용될 것이다.
본 논문에서는 신문기사로부터 양질의 패러프레이즈 쌍을 추출하는 방법을 제안한다. 본 논문에서 제안하는 방법은 특정 날짜의 특정 사건에 대해 작성한 기사 사이에는 같은 사건을 다른 단어로 표현한 문장이 많을 것이라는 가정에서부터 비롯된다. 이 가정에 기반을 두어 원시 데이터의 문장 중에서 패러프레이즈가 될 수 있는 문장들을 선별하여 몇 가지 제약사항을 고안하였다. 우선 수집된 문장 중에서 특정 개체명이 비슷한 문장들을 1차적으로 선별한다. 이렇게 선별된 문장에서부터 유사도를 계산하여 최종 문장 쌍들을 결정한다. 실험을 통해 본 논문에서 제안하는 추출 방법의 정확도가 68∼85% 성능을 보였다. 이는 과거 비슷한 연구에서 보인 시스템의 성능 67%보다 향상된 수치를 보여주었고, 양질의 데이터를 자동으로 추출할 수 있다는 것을 확인하였다.Paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form. It plays important roles in various Natural Language Processing, such as Multi-Document Summarization, Question-Answering, Machine Translation, and so on. In particular, it is too expensive to obtain high-quality corpus of paraphrase. If a pair of paraphrase sentences, however, can be automatically extracted from the raw data, then these techniques will be used as a useful resource in many areas of Natural Language Processing.
In this paper, I propose a method for extracting these high-quality data form newspaper articles. News articles on the same day may deal about the events that happened same day. Based on this characteristic to collect news paper articles by date, and extract sentences which will possibly be as a paraphrase. Each sentence from the collection of articles will be tagged by Named Entity Tagger, and classify sentences that shares a high proportion of their Named Entities. Finally, extract final pair of sentences candidates from primary classified sentences, by calculating semantic similarity of them. The combination of specific Named Entity should be excluded from the final set.
According to the experimental results shows high performance of the accuracy. This showed improved performance levels than in the previous study, so the proposed extraction method was confirmed that it can automatically extract high-quality data from news articles.

생의학 분야의 발전으로 축적된 연구의 결과는 가공되지 않은 지식의 보고라고 할 수 있다. 그러나 정보의 과다 현상은 정보 습득의 효율성을 제한하는 문제점을 발생시켰다. 최근에는 자연어처리 기술을 이용하여 문헌으로부터 유전자나 단백질의 기능이나 상호 작용 관계를 지능적으로 찾아내는 정보 추출 시스템에 대한 관심이 높아지고 있다. 이러한 정보 추출 시스템을 구축하기 위해서는 우선적으로 생의학 문서로부터 단백질이나 유전자와 같은 생의학 개체명을 인식하는 연구가 선행되어야 한다. 생의학 개체명 인식은 문서에 나타난 개체명의 경계를 인식하고 그것의 의미를 결정하는 문제이다. 다시 말해서, 생의학 개체명 인식은 고차원적인 텍스트 분석을 위한 가장 기본적인 의미구조를 만들어 내는 작업이라고 할 수 있다. 그러나 생의학 개체명은 계속해서 새롭게 생성되고, 많은 철자 이형태와 별칭, 약어 등이 사용되기 때문에 인식이 특히 어렵다.
기종의 생의학 개체명 인식을 위한 연구로는 사전을 이용한 사전 기반 접근 방식, 전문가가 수작업으로 작성한 규칙을 사용하는 규칙을 사용하는 규칙 기반 접근 방식 그리고 개체명이 표시된 학습 문서로부터 학습을 수행하는 기계학습 방식이 사전이나 규칙 기반 접근 방식 보다 선호되고 있으며, 다양한 기계학습 방법이 생의학 개체명 인식 문제에 적용되어 왔다. 그 중에서도 문서 분류와 같은 자연어처리 분야에서 뛰어난 성능을 보여주었던 지지 벡터기계(SVM: Support Vector Machine)를 생의학 개체명 인식 문제에 적용하려는 시도가 있었다.
그러나 기존의 SVM을 사용하는 방식은 개체명의 경계 인식과 의미 분류를 동시에 수행하는 통합 모델(1-phase model)을 상용하였기 때문에 학습 복잡도가 매우 크고, 경계 인식에 유용한 자질과 의미 분류에 유용한 자질들을 구분 없이 사용하였다. 이에 본 논문에서는 경계 인식 의미 분류 단계를 분리한 분리 모델(2-phase model)을 사용하여 SVM 학습과정의 복잡도를 줄이고, 각 단계에 적합한 자질을 구분하여 사용함으로써 전체 인식 성능을 높일 수 있는 새로운 방법을 제안하고자 한다.
본 논문에서 제안하는 개체명 인식은 두 단계로 이루어진다. 첫 번째 단계에서는 개체명의 경계를 인식하고 사전을 이용하여 후처리를 수행한다. 두 번째 단계에서는 경계가 인식된 개체명을 대상으로 단백질, DNA, RNA와 같은 의미 클래스를 부여한다. 이 과정에서 개체명에 속하지 않은 단어는 경계 인식 단계에서 제거되기 때문에, SVM 학습 과정에서 불필요한 부담을 줄일 수 있고, 이는 전체적인 성능 향상과 연결된다.
CENIA 코퍼스의 2,000개의 논문 초록을 사용하여 실험한 결과, 경계 인식 성능은 74.8%, 전체 인식 성능은 66.7%를 보였다. 또한, 다른 개체명 인식 시스템과의 비교 실험을 통해 제안하는 방법의 유용성을 입증하였다.요약 Contents = iii Chapter 1 서론 = 1 1.1 연구 배경 = 1 1.2 생의학 개체명 인식의 문제점 = 2

21세기에 들어, 인터넷과 휴대폰이 빠르게 보급되면서 인터넷 통신 언어가 확산 되었다. 이 통신 언어로 쓰여진 문서는 많은 철자 오류를 포함하고 있는데, 현재 자연어 처리 기술로 이런 문서를 분석하였을 때 좋은 성능을 기대하기는 힘들다. 왜냐하면 이미 개발되어 있는 자연어 처리 기술은 입력에 오류가 없다고 가정하였기 때문이다.

이 문제를 해결하기 위해 본 논문에서는, 통신언어 원본과 교정본으로 구성된 철자 교정 말뭉치에서 자동으로 철자 교정 규칙을 학습하고, 이를 바탕으로 사용자 입력 문장을 교정하는 모델을 제안한다. 제안하는 방법은 교정 말뭉치를 이용하기 때문에 확장성이 높고, 자동으로 규칙을 추출하기 때문에 규칙의 구축 비용이 낮다. 그리고 간단하면서도 좋은 성능을 보이며, 규칙을 이용하기 때문에 수정이 용이하다.

제안하는 모형 규칙의 적절한 문맥 단위를 결정하기 위해서 어절, 음절, 자소 단위 모형을 각각 구현하여 비교 실험을 한 결과 음절이 교정 단위로 사용하기에 가장 적절하였다. 다른 기존 교정 모형과 비교해 봤을 때에도 제안하는 모델이 더 좋은 교정 성능을 보였다.

본 논문에서 제안한 교정기 모형은 앞으로 품사 부착기나 구문 분석기 같은 자연어 처리 기술을 실제 문서에 적용하기 위한 전처리기로써 유용할 것으로 기대한다.As cellular phone and the Internet are widely used nowadays, Internet communication language is also widely used, too.
Previous natural language processing techniques are not suitable for Internet communication language, because those techniques have limited functions to process new words, emoticons, and spelling errors.

To solve this problem, we propose a model that automatically extracts correction rules from a corrected corpus which consists of pairs of an original message and a corrected message, and corrects a user input message with the correction rules.
And in order to preserve both high precision and high recall, we devised a candidate-elimination algorithm that determines appropriate context size of each correction rule.

To decide proper correction unit, we conducted an experiment on three types of unit: word, syllable, and grapheme.
As a result, syllable is found to be the most reasonable correction unit.

Experimental results show the proposed model achieves better performance in terms of accuracy comparing to the previous correction models.
We expect our model can be used as a preprocessor to increase performance of various other natural language processing application such as part of speech tagging and parsing.

소프트웨어 시스템의 요구사항은 소프트웨어 시스템이 점차 복잡화, 대형화되면서 소프트웨어 프로젝트의 성패를 결정하는 중요한 문제로 인식되어 왔다. 특히, 시스템 수행 중 오작동의 원인이 되는 요구사항간의 충돌을 식별하고 분석하는 것은 소프트웨어 시스템의 성공을 위한 중요한 요소이다.

기존 연구에서는 요구사항 충돌 식별을 위해 정형화된 방법을 사용하여 요구사항을 기술하고, 요구사항 충돌을 식별하였다. 하지만 자연어로 기술된 요구사항에 익숙한 개발자들에게는 기존의 정형화된 방법은 요구사항 기술뿐만 아니라 분석 또한 어렵게 하였다. 현재 개발자들이 실제 도메인에서 요구사항 충돌을 관리하기 위해 요구사항 기술에서 충돌 식별까지 다루고 있는 프로세스 관련 연구가 미비한 실정이다.

본 논문에서는 시스템 수행에 있어 부정적인 영향을 줄 수 있는 요구사항 간의 상호 작용을 식별하고 분석하여 요구사항 충돌을 관리할 수 있는 프로세스와 지원도구를 제안한다. 특히 자연어로 기술된 요구사항을 위해 요구사항 충돌 타입을 정의하고 요구사항 분류를 기반으로 요구사항 충돌을 식별하고 관리하는 방법을 제안한다. 마지막으로 프로세스의 전 과정을 지원하는 요구사항 충돌 관리 도구를 가지고 주택 통합 시스템 (HIS, Home Integration System)에 적용하여 그 결과를 분석하였다.The more complicated and large-scaled software systems become, the more important software requirements become as a critical issue to succeed in software projects. To detect conflicts between requirements is one of the essential matters that must be considered for successful software projects. Formal methods have been proposed to tackle this problem by adding formality and removing ambiguity. However, they are hard to understand by non-experts, which limit their practical applications to some restricted domains. In addition, there is no systematic process to cover all steps for managing requirements conflicts. We propose a process for systematically identifying and managing requirements conflicts. The process is composed of four steps that include requirements authoring, partition, conflicts detection and conflicts management. Especially, in this approach, the detection and management of the conflicts are done based on the requirements partition in natural language and supported by a tool. Finally, to demonstrate its feasibility, the process proposed here has been applied to a home integration system (HIS) and the results are analyzed.

2007년 2월 통계에 따르면, 현재 웹에는 297억 개의 웹 페이지가존재하고 있으며, 2008년 현재 300억 개를 넘어섰을 것으로 추정하고 있다.

이렇듯 넘쳐나는 많은 전자문서들의 내용을 컴퓨터를 통하여 효과적으로 이해 및 전달을 위해서는 형태소분석, 구문분석, 문맥 및 의미분석 등의 자연어처리 기술이 필요한데, 이러한 자연어 처리의 가장 기본 단위인 ‘문장’을 구분하는 작업이 요구된다. 하지만 ‘문장’에 대한 정의만 해도 몇 백 가지가 넘고 문법적으로 정의되어 있지 않아, 문장을 인식하는 것 또한 어려움이 있다. 일반적으로는 문장을 인식하는 데에는 문장부호가 주로 사용되나, 그것 또한 생략되거나 잘못 표기된 경우가 많아 모든 문장을 파악하기에는 어려운 점이 있다.

본 논문에서는 문장부호 또는 문법적인 규칙을 통한 규칙기반의 문장경계 인식기를 구현하여 실험 및 검증을 하고, 규칙으로는 어려운 문제들을 해결하기 위하여 언어의 통계적 특징을 활용하여 보다 범용적인 문장경계 인식기를 제안한다. 이는 대량의 코퍼스 내에서 사용되고 있는 문장 경계를 기준으로 음절 및 어절 등의 자질을 이용하여 통계적 특징을 추출하고 기계학습 기법을 활용하여 문장경계를 인식하고자 하였으며, 특정 언어나 도메인에 제한적이지 않고 범용적인 자질만을 사용하려고 노력하였다.

언어의 특성상 문장의 구분이 애매한 경우, 문장부호로 문장이 종료되지 않은 경우 또는 잘못 사용 된 문장부호 등의 경우에도 적용 가능하도록 다양한 자질을 사용하여 실험하였으며, 한국어와 영문 코퍼스에 대해서 동일한 자질을 적용하여 실험하여 본 논문에서 제시한 자질들이 한국어 이외의 다른 언어에서도 적용될 수 있는 범용적인 자질임을 확인할 수 있었다.Web documents have grown to 29.7billions Feb. 2007. Probably more than 30billions documents are there at now.

We need part of speech detecting, context analysis and semantic analysis for using these electronic documents and also understanding efficiently. All these techniques are needed disambiguating sentence boundary or detecting end of sentence. But lots of definitions of sentence give us confusing while detecting sentence boundaries.
Generally we use punctuation mark for finding sentence boundary, but there are many sentences which does not have punctuation mark or using punctuation marks wrongly. So we need more sophisticated solution for solving sentence boundary disambiguation.

This paper suggests general purpose sentence boundary detection system which uses language statistical information gained from corpus like syllables or lengths around sentence boundary. Besides I tried to use general purpose features which is not related with special domain or language.

I tried to learn features through using machine learning techniques empirically. also experimented for two kinds of language Korean and English to confirm the general purpose system. Finally we found out that these features are applied to both languages.
There is little modifications for experiments with before, and reasonable result came out.요 약 4 ABSTRACT 5 그림 목차 8 표 목차 9 1. 서론 10

최근 소셜 네트워크 서비스(SNS, Social Networking Service)에서 생산되는 텍스트 데이터의 증가와 그래픽처리 장치의 발달로 딥러닝을 활용하여 텍스트 데이터를 자동으로 처리하는 텍스트 마이닝 연구가 증가하고 있다. 이중 텍스트 임베딩(text embedding)은 단어를 국소표현이나 분산표현하는 기법으로 단어를 실수차원의 벡터에 대응시키는 것이다. 자연어 처리 응용 분야가 텍스트 요약, 문서 분류, 기계 번역, 개체명 인식, 챗봇 시스템 등으로 확대됨에 따라 자연어 처리에서 가장 기본이 되는 텍스트 임베딩에 대한 연구 역시 활발히 진행되고 있다. 하지만 단어 임베딩과 관련된 기존의 연구들은 영어를 비롯해 대부분 고립어의 특성을 가진 언어를 대상으로 진행되어 왔고 교착어의 특성이 있는 한국어 임베딩에 대한 연구는 미비하여 기존의 연구 방식을 그대로 적용하기에는 한계가 존재한다. 또한 단일 임베딩 방식이 아닌 앙상블된 임베딩 방식이라면 기존의 텍스트 분류 문제에 있어 더 높은 성능을 얻을 것이라 기대된다.
본 연구에서는 교착어의 특성을 반영하여 임베딩 모델을 학습시키고 세 가지 임베딩 모델을 앙상블하여 한국어 문장 분류의 성능을 높이는 방법에 대해 소개한다. 각 세 가지 모델은 글자 단위의 임베딩과 Word2Vec, ELMo(Embedding from Language Model)가 활용되었다. 또한 교착어의 언어적 특성을 반영하기 위하여 단어의 구분을 띄어쓰기가 아닌 형태소 단위로 구분하여 사전 학습(pre-trained)을 진행하였다.
본 연구는 한국어 텍스트 분류에 대한 성능 평가를 위하여 NSMC(naver sentiment movie corpus v1.0)의 데이터를 사용하였으며 각 모델별로 사전 학습을 진행하였다. 최종적으로 모델의 성능 평가를 정확도(accuracy)를 통해 진행하였으며 교차 검증(k-fold cross validation) 결과 기존 모델에 비해 텍스트 분류 성능이 뛰어남을 입증하였다.Recently, due to the increase of text data produced by SNS(Social Networking Service) and the evolution of GPU(Graphics Processing Unit) computing, text mining researches have been increased using deep learning
to process text data automatically. Among these researches, text embedding is a technique of expressing a word by local representation or distributed representation, and mapping a word to a vector of a real number
dimension. As the applications of natural language processing are extended to text summarization, document classification, machine translation, object name recognition, and chatbot system, studies on text embedding, which is the most basic in natural language processing, are actively being carried out. However, previous researches related to word embedding has been conducted on languages such as English and there are limitations and insufficient studies on word embedding that reflects the characteristics of Korean. For these reasons, compares to the single embedding method, it is expected that the ensemble embedding method will achieve higher performance in text classification in Korean.
This study introduces a method of enhancing the performance of Korean sentence classification by ensuring three embedding models by learning the embedding model reflecting the characteristics of the agglutinative language. In each of the three models, word-based embedding, Word2Vec, and ELMo (Embedding from Language Model) were used for ensemble. In order to reflect the linguistic characteristics of agglutinative language, words were divided into morpheme units instead of spaces, and the pre-trained was conducted.
To show the usefulness of proposed approach, this study used data from NSMC (Naver Sentiment Movie Corpus v1.0) to evaluate the performance of Korean text classification. In conclusion, the proposed model was
evaluated through the accuracy and it has been proved that the text classification performs superior compare to the previous models by the result of K-fold cross validation.국문요지 ⅴ 제1장. 서론 1 제1절. 연구 필요성 1 제2절. 논문의 구성 2

현재 학생들은 인터넷 대중화에 따라 많은 정보를 웹을 통해 얻을 수 있게 되었다. 하지만 인터넷에는 유익한 정보 뿐만 아니라 왜곡된 불건전 정보들도 존재하기 때문에 학생들의 인터넷 접속 현황을 추적하는 것은 필수적인 요소가 되었다. 하지만 아직까지 학교와 가정에서는 학생들의 인터넷 현황을 모니터링 할 수 있는 소프트웨어나 시스템들이 구축되어 있지 않은 실정이고, 구축되어 있더라도 나날이 증가하는 불건전 정보 사이트들을 검색하고 차단할 수 없기 때문에 건전한 인터넷 문화를 조성할 수 없다는 단점을 가지고 있다.
본 연구에서는 학교와 가정에서 손쉽게 학생들의 인터넷 접속 현황을 모니터링 할 수 있고, 학생들이 접속한 사이트의 자동 분류를 통해 사이트 성향을 파악할 수 있는 웹 로그 분석 시스템을 설계하고 구현하였다. 이 시스템은 나날이 증가하는 사이트들을 사람의 손을 거치지 않고 오직 로그분석을 통해 자동으로 분류할 수 있기 때문에 교사나 학부모들이 손수 사이트들을 돌아다니면서 불건전 정보들을 검색하고 차단할 필요가 없게 된다. 더불어 컴퓨터 이용능력이 낮은 교사와 학부모도 손쉽게 본 시스템을 사용하여 인터넷 사용 현황을 모니터링 할 수 있다는 장점을 가지고 있다.목차 = i 논문개요 = iii I. 서론 = 1 II. 관련연구 = 3 2.1 학생들의 인터넷 사용 실태 및 문제점 = 3

정보 기술의 빠른 발전은 업무의 자동화를 촉진시켜 엄청난 양의 데이터를 전자적으로 수집하고 보관하는 것이 가능하게 되었다. 하지만, 이러한 기하급수적인 데이터의 증가로 인해 우리가 원하는 정보를 신속하고 정확하게 검색하기 어려운 실정이다. 이로 인해 검색욕구는 증대 되어 왔으며 검색하고자 하는 자료는 지속적으로 증가 하고 있다.
1990년대 웹 검색엔진을 사용하기 시작하면서 사용자는 원하는 자료, 사이트에 대한 정보를 확인하고자 검색 알고리즘을 활용하고 있다. 사용자의 서비스 욕구 만족을 위해 다양한 검색 알고리즘이 등장하였으며 자연어 처리와 의미 기반 검색 알고리즘까지 활용할 수 있게 되었다.
사용자들은 자신이 원하는 검색결과를 찾기 위해 수많은 검색서비스를 이용한다. 그러나 정작 사용자들이 원하는 결과를 제공해 주는 검색 알고리즘은 많지 않다.
사람들은 검색창에서 검색을 할 때 자신이 알고 있는 단어로 검색을 한다. 그러나 단어가 외래어거나 표준어가 아닌 경우에는 의도한 결과를 찾기가 어렵다. 특히 외국어나 다양한 표기법을 가지는 단어들은 특별한 기술이 없이는 여러 가지로 표기되는 검색어들을 같은 단어로 인식하지 못한다.
통합 검색 알고리즘이 적용된 형태소 분석 기능과 유사어 확장 기술을 적용하여 같은 의미를 갖고 있는 단어를 동일하게 보여줄 수 있도록 발전 되고 있다.
기관이나 기업의 경우 내부 데이터 통합 검색 구축을 통한 정보검색의 효율성을 증대함에 있어 중복데이터 및 권한, 보안에 대한 문제점에 대해 연구하고자 검색 알고리즘을 활용하여 통합 검색을 설계 하여 연계 성능의 향상 한다.
본 연구는 검색 알고리즘을 적용한 검색 알고리즘의 발전 과정과 검색 알고리즘에 대해 연구하였다. 이를 위해 검색 알고리즘에 대한 이론적 배경인 색인 및 구성 방법 연구에 대해 고찰 하였다.
대학교의 리포트 유사도 검색, G오픈마켓 상품 통합검색, Y정부기관 내부업무 검색, Y언론사의 기사 검색 사례 분석을 통해 효과분석을 측정하였다. 이론적인 배경과 사례연구를 통해 기존 검색 알고리즘 과 4세대 검색 알고리즘이 적용된 부분의 적용 효과에 대해 분석 하였다.The rapid development of information technology services by facilitating the automation of huge amounts of data to be collected and held electronically became available. However, this due to the exponential increase of data we want to retrieve information quickly and accurately is difficult Free.
This search has been increasing desire to search the data is constantly increasing.
1990s began to use a Web search engine users want data, to check information on the site are taking advantage of search algorithms.
Users of the service needs to satisfy a variety of search algorithms, was the emergence of natural language processing and semantic-based search algorithm was able to get up.
Users to find their desired results and uses many search services. However jeongjak people search algorithm that provides the desired results is limited.
When people search in the search word search, they should know. However, foreign words or Mandarin word is not hard to find the intended results. In particular, a variety of international conventions or special skills that have the words in different ways without the written word as the search query does not recognize them.
Search features and variations stemming algorithm is applied by applying scaling techniques have the same meaning for a word that has been developed to show the same.
If the internal data of the organization or company Search Search through the establishment of a redundant data to increase efficiency and permissions as security for a problem with a search algorithm utilized to study the integrated search is designed to improve the performance of the link. This study applied a search algorithm to the evolution of search algorithms and search algorithms investigated. To this end, a theoretical search algorithms and how to configure baegyeongin index were evaluated for study.
Report of the University of similarity search, G Open Market items, Y governmental organizations, business search, Y through the analysis of media stories in the article search to analyze the effects were measured. Through case studies and theoretical background search algorithms and fourth generation of the existing search algorithms for the analysis of the affected portion of the effect was applied.Ⅰ. 서 론 1 1. 연구의 배경 및 목적 1 2. 연구의 방법 및 구성 2 Ⅱ. 이론적 배경 3 1. 검색 알고리즘의 개요 및 구조 3

본 연구는 시맨틱 웹의 실현을 위해 필수적인 단계인 의미 태깅을 자동으로 할 수 있는 시스템에 관한 것이다. 웹 상의 방대한 자원을 일일이 사람이 수작업으로 의미를 태깅한다는 것은 사실상 불가능하기 때문에 본 연구가 반드시 필요한 것이다.
본 시스템에서는 한국어 웹 문서를 대상으로 의미 태깅을 하기 위해서 대량의 학습 데이터를 수집하여 학습을 시킨 후 새로운 입력에 대해서 자동으로 의미를 파악하여 의미 태깅하도록 설계 및 구현하였다.
한국어의 특징을 파악하여 필요한 정보를 추출한 다음 학습벡터를 만들어 기계학습을 시켜야 한다. 이를 위해서 본 시스템에서는 형태소 분석과 구문 분석 등의 자연어처리 기법을 이용하였고 이렇게 분석된 결과를 바탕으로 가도카와 시소러스의 의미코드(concept code)에 매핑하여 학습벡터를 만든다. 만들어진 학습벡터를 이용하여 기계학습을 시키면 개념별 분류기가 생성되고 이 개념별 분류기를 시스템에 적용한다. 구축된 시스템에 새로운 입력이 들어오면 확률적으로 분석하여 자동 의미 태깅을 하게 되는 것이다.
본 시스템의 학습 단계에서 구문 분석과정을 통한 직접 연관된 문맥을 파악할 수 있었기 때문에 좀 더 세밀하고 확실하게 기계학습을 할 수 있어서 정확률의 상승에 도움이 되었고 가도카와 시스러스를 이용하여 같거나 비슷한 단어들을 하나의 의미코드로 매핑함으로써 전반적인 시스템의 재현율을 상승시키는데 기여하였다. 적용 단계에서 학습에 쓰이지 않은 생소한 새로운 입력이 들어왔을 때에도 학습시에 가도카와 시소러스의 의미코드에 매핑하여 학습해 놓은 정보들을 바탕으로 확률적으로 해당 개념이 맞는지 아닌지를 분류하여 태깅하게 된다.
실험결과 전반적으로 좋은 성능을 나타내었으며, 향후에는 의미 분류의 수를 좀 더 늘리고 학습데이타의 양과 질을 높이고 가도카와 시소러스의 활용의 폭을 좀 더 넓힌다면 더 좋은 성능을 나타낼 것으로 생각된다.This study is about a system which can perform automatic semantic annotation to actualize Semantic Web. It is necessary to do this study since it is impossible to tag the documents manually in the web.
The system has been designed to expedite automatic recognition of new input through collecting a huge amount of training data and learning to take the meaning of a Korean web document.
It is necessary to gather information according to the Korean characteristics and to make training vectors to do machine training. For this reason, morphological analysis and syntax analysis were used in this system. Based on these analyses, the concept code is mapped with Kadokawa thesaurus to make training vectors. Categories based on each concept can be made by the training vectors and these can be applied to the system. Once there is new input into the system, the system will analyze it and perform automatic semantic annotation.
Thanks to the understanding of the directly related contents, more detailed and accurate machine learning was possible and helped to improve the accuracy. Kadokawa thesaurus made it possible to map similar words and phrase to one concept code. This contributed to rise in overall system reproduction rate. If there is an unfamiliar new input, the system can tag the proper web documents based on the information acquired by mapping Kadokawa thesaurus.
Results of the experiment show the system has a high level of performance. Even better performance will be possible with improved quality and quantity of training data, widen the utility of Kadokawa thesaurus.Ⅰ. 서론 = 1 1. 연구의 배경 = 1 1) 유비쿼터스 = 1 2) 시맨틱 웹 = 2 2. 연구의 목적과 방법 = 3

본 논문의 주요 목표는 자연언어자동처리를 위한 불어경제텍스트의 자료를 언어학적으로 분석하는 것이다. 이를 위한 기본 자료는 「르 몽드」,「르 피 가로」지등의 불어신문에서 추출된 것이다. 프랑스의 언론에서 자주 쓰이는 용어들을 통해 우리는 외국에서의 불어교육과 같은 불어교수법의 한 방법개발도 이 논문의 두번째 목적으로 고려한다. 본 논문의 주요 분야인 전산언어학은 여러가지 공동분야의 협력하에서 그 연구가 성립된다. 예를 들면 「자연어 처리」, 「언어학」,「용어학」과 「논리학」등이 그것이다. 이에 본 논문의 첫장은 자연어처리를 위한 프로그램의 설명으로 이루어진 다. 프로그램에서 자주 쓰이는 언어분석이론에 대한 소개도 이에 부연 설명된다. 두번째 장과 세번째 장은 불어경제용어연구에 초점을 두고, 파생어와 복합 어의 구성구조분석을 주로 한다. 네번째 장에서는 그 분석단위가 관용적 표현으로 확대된다. 프랑스신문 경 제기사에서 관용적으로 등장하는 명사구와 동사구의 표현을 문장결합관계의 측면에서 연구한다. 마지막 장은 불어 자동작문 도움기에 한국어 경제요어를 적용시도하는 것이다. 이 연구는 시작단계로서 앞으로 미래에 우리가 더욱 깊이있게 투자할 학 문분야가 될 것이다. 흔히 우리나라 불문학과 학생들은 문학텍스트에 익숙한 반면, 실제로 현 생활에 자주 쓰이는 불어에는 약하다. 이를 보완하기 위애 우리는 가능한 한 신문에서 자주 쓰이는 실용불어를 소개하려 노력하였다. 아울러 우리나라학계에도 자동언어처리에 쓰이는 언어학적 분석원리들을 소개하고 싶다.The investigation of laminar name structure has a significant role in the modeling ofturbulent combustion by introducing the namelet concept that is valid under cenain conditions.Several aspects of the lanlinar names are investigated experimentally and nunlehcally.
Systematically reduced reaction mechanisms are useful in reducing the computationaleffort with a decrease in the number of species, and for investigating narne structure byasymptotic methods. Such advantages are applicable in the simulation of turbulent reactingnows in which the magnitudes of the time scales of the nuctuating now field rapidly. reduced mechanism is derived Hames. The mechanism has beenvalidated by comparison of the Hame structures and burning velocities and fDund to k in goodagreement with experimental results.
Stretched laminar names have attracted considerable attention in the modeling ofturbulent combustion with laminar Hamelet concept in recent years. They become useful byconsidering the hct that the local Hamelets correspond to a distribution of strained rates·rrhename structure determines the distribution of heat release across a name front. Therebre,attention is focused on the impact of suppressants on the radiative heat loss from a name andgaining insight into the complex interactions that impact the global heat release in names.
Transient processes can have a beneficial influence on pollutant forITlation. For thisreason, we investigate the change of the NOx emission in a burner stabilized Hame under theefkct of acoustic waves. NO2 emission intensities are investigated at various hequenciesusing a nonintrusive measuren]ent technique. This diagnostic device can be made as pan of afeedback loop for automated control of combustion systems.
Flame-vortex interaction studies are perbrmed with a simplined model employing thefield or interface equation. The purpose is to exanline changes in name shape and structurecaused by a vodex, and its innuence on the NO enlission index.
XX1l

띄어쓰기는 한글의 한 가지 특징으로, 글을 쓸 때 어절 단위로 띄어서 쓰는 것을 말한다. 이러한 띄어쓰기는 문장을 구성하는 요소들의 구별을 명확하게 해주며, 정확한 뜻을 전달하는데 있어서 큰 역할을 한다. 전산적인 면에서는 띄어쓰기를 사용하지 않는 중국어나 일본어와 다르게 문장의 구성 요소를 구분하는 작업(Word Segmentation)을 필요로 하지 않아 상대적으로 처리가 용이하다고 할 수 있다. 또한 한글은 띄어쓰기를 하는 것이 원칙이기 때문에 많은 자연어처리 기법들이 입력 문장의 띄어쓰기가 옳다는 가정하에 동작하도록 개발되었다. 하지만 자연어처리 기술들을 실생활에서 사용하고자 한다면 띄어쓰기가 완벽한 입력을 기대할 수는 없다. 이러한 문제를 해결하기 위해서 지금까지 많은 종류의 자동 띄어쓰기 교정 모델들이 제안되었다.

자동 띄어쓰기 교정 모델들은 크게 규칙 기반 방법과 통계 기반 방법으로 나눌 수 있는데, 현재는 높은 정확률과 폭넓은 처리 범위 그리고 수작업을 덜 필요로 하는 점 때문에 통계 기반 접근 방법이 많이 사용되고 있다. 지금까지 제안된 통계 기반 띄어쓰기 교정 모델들은 대부분이 입력 문장의 띄어쓰기 상태를 고려하지 않는데, 이는 띄어쓰기 교정 문제에 적합한 학습 데이터가 존재하지 않기 때문이다. 띄어쓰기가 올바르게 된 학습 데이터는 세종계획 등에 의해서 만들어진 원시 문장 코퍼스를 사용할 수 있기 때문에, 입력 문장의 음절 정보만을 사용하는 통계적 접근 방법들이 주로 사용되어 온 것이다.

하지만 이렇게 입력 문장의 음절 정보만을 사용하는 모델들은 교정 대상 문장의 띄어쓰기가 잘 되어있는 경우에 그보다 좋지 못한 교정 결과를 만들어내는 단점이 있다. 띄어쓰기 교정 모델이 사용하는 문맥 정보가 특정 음절 사이의 띄어쓰기 상태를 추정하는데 불충분한 경우에, 띄어쓰기가 올바른 부분을 틀리게 고치는 오류가 발생하는 것이 가장 큰 원인이다.

본 논문에서는 맞았던 띄어쓰기를 틀리게 고치는 오류를 줄임으로써 전체적인 띄어쓰기 교정 성능을 향상시키는 방법을 제안하고자 한다. 제안하는 방법은 기존의 모델이 선택한 띄어쓰기 상태를 그대로 교정에 사용하지 않고, 모델이 제시한 교정 상태의 확률이 입력 문장의 띄어쓰기 상태가 올바를 확률보다 높아야지만 교정을 한다. 만약 그렇지 못한 경우에는 입력 문장의 띄어쓰기 상태를 그대로 따른다. 이렇게 함으로써 띄어 쓸 확률과 붙여 쓸 확률의 차가 크지 않은 경우에 주로 발생하는 맞은 띄어쓰기를 틀리게 고치는 오류를 감소시킬 수 있다. 실험 결과에 따르면 기존의 통계 기반 모델들과 다르게 입력 문장의 띄어쓰기 오류 비율이 적을수록 교정 성능이 향상되었으며, 입력 문장의 띄어쓰기 상태가 나빠질 경우에는 기존 모델로 성능이 회귀하였다. 또한 기존 모델들이 입력 문장의 띄어쓰기 오류가 적은 경우에 입력 문장보다 좋지 못한 교정 결과를 생성한 것과 반대로 항상 입력 문장보다 나은 교정 결과를 보였다.Contents 1. 서론 1.1 띄어쓰기의 정의 및 역할 1.2 띄어쓰기 오류가 발생하는 이유 1.3 띄어쓰기 오류의 유형과 영향

이 논문에서는 사람이 항공여행을 할 때 컴퓨터를 이용한다고 가정하고 이때, SQL과 같은 데이터베이스 질의보다는 사람이 매일 쓰고 있는 한글을 이용하여 여러 가지 정보를 검색할 수 있도록 하기 위해 개발된 일종의 정보 검색, 혹은 정보 추출 시스템에 관하여 기술하고 있다. 현재의 시스템은 영국 캠브리지에 있는 SRI연구소에서 개발한 Core Language Engine이라고 하는 자연어 처리 시스템을 바탕으로 이것을 한글을 처리할 수 있게 개조하였으며 여기에 자연언어에 대한 데이터베이스 인터페이스 모듈을 첨가한 것이다.

현재의 시스템은 크게 형태소 분석 (morphological analysis), 구문 분석 (syntactic analysis), 의미 분석 (semantic analysis), 문맥 처리 (contextual processing), 데이터베이스 인터페이스의 다섯가지 모듈로 구성되어 있으며 대부분의 코드가 프로로그 언어로 쓰여 졌다.

기존의 시스템은 자연어 데이터베이스 질의를 처리하는데 주로 문장단위의 인식을 위주로 한반면 본 논문에서는 한국어 질의 담화를 처리하는데 있어서의 여러 가지 어려운 점과 그 해결책을 제시하고 있다. 특히 자연어 처리나 음성인식 시스템에서의 문맥처리에 대한 중요성을 강조하고 있으며, 좀 더 실질적인 시스템개발에 근접할 수 있는 여러 가지 방법들을 기술하고 있다.The relationship between transient stress and molecular orientation has been investigated to elucidate the stress evolution mechanism in polyimide thin films and in patterned line structures. To better understand the major stress characteristics, a viscoelastic stress model is proposed based on the kinetics for chemical and physical reactions associated with the curing process. The stress behavior is monitored in-situ during thermal curing and subsequent cooling cycles using a bending beam technique, while the molecular orientation is characterized in terms of optical birefringence, dichroic ratio, and related mechanical properties. Two model polyimides, flexible PMDA-ODA and rigid rod-like BPDA-PDA, are selected to study the effect of molecular chain conformations. As the polyimide films are made thinner, to sub-micron dimensions, the initial stress increases, causing a high degree of in-plane molecular orientation. The induced molecular orientation continues!
d!
eveloping throughout curing and determines the mechanical properties as well as the final stress of the films. In the polyimide line structures, molecular backbones are aligned preferentially along the direction of lines as the line width becomes narrower. The transient stress behaviors are strongly related to the evaporation/plasticization effects of volatiles such as residual solvent, dissociated solvent and imidization by-products retained in the precursor films.

스마트폰을 비롯한 모바일 기기의 발달로 인터넷 이용 환경이 PC에서 모바일로 확대되었다. 이에 따라 웹을 통해 이동하면서 즐길 수 있는 콘텐츠, 이른바 웹 콘텐츠의 수가 증가하고 있다. 웹 콘텐츠는 웹에서 생성, 유통, 소비되는 모든 콘텐츠를 말하며, 다양한 모바일 기기에서 언제 어디서나 소비된다. 웹 콘텐츠의 형태는 동영상, 음악, 사진, 만화, 텍스트 등이 있다. 이러한 웹 콘텐츠의 범주화를 위한 다양한 연구들이 진행되었지만 링크를 이용해 페이지간의 관계성을 분석하거나 웹 페이지의 구조적인 내용에만 초점을 맞추었다. 또한 웹 콘텐츠의 감성을 긍정, 부정, 중립 세 가지로만 분류해왔다. 웹 페이지 자체를 구성하고 있는 텍스트를 분석하면 웹 페이지의 주제나 범주가 무엇인지 알 수 있다. 또한 웹 콘텐츠의 텍스트에서 기본감성과 차원감성을 추출한다면, 웹 콘텐츠의 범주와 더불어 콘텐츠 소비행태에서 패턴을 찾는 요소로 활용 될 수 있다.
일상생활에서의 콘텐츠 소비행태에 대한 연구는 단순히 웹 콘텐츠를 이용하는 기기와 이용시간, 이용빈도 등의 통계 분석에 그쳤다. 또한 콘텐츠 소비행태에 대한 데이터 수집은 주로 주관설문을 통해 이루어져왔다.
본 논문에서는 웹 콘텐츠의 텍스트를 이용해 카테고리와 기본감성, 차원감성을 자동으로 판별하는 콘텐츠 범주화 시스템을 구축했다. 또한 소비행태 데이터를 자동으로 수집하기 위해서 SociaL Browser라는 안드로이드 앱(App)을 만들었다. 콘텐츠 범주화 시스템은 사용자가 스마트폰으로 접속한 웹 페이지의 텍스트를 크롤링(Crawling)해 저장하고, 저장된 텍스트를 자연어처리(NLP)를 사용해 언어의 최소 의미 단위인 형태소로 분리했다. 분리된 어휘 각각을 미리 정의해 놓은 카테고리, 기본감성, 차원감성 각각을 대표하는 어휘 집합과 문서유사도를 비교해 판별했다. 콘텐츠 소비행태를 분석하고 동기화를 판별해 사용자들 간의 유의미한 연결성을 찾았다. 개개인의 웹 콘텐츠 소비행태를 모아 대중의 소비행태를 만들어 분석하면 대중의 패턴과 트렌드를 알 수 있다. 본 연구를 통해 콘텐츠 범주화에 바탕을 둔 패턴과 트렌드는 마케팅 요소로 활용 될 수 있으며, 여론조사를 대체할 수 있을 것으로 기대된다.

키워드 : 콘텐츠, 범주화, 소비행태, 동기화, 사회적 연결성The internet environment has been expanded from PC to mobile, due to the evolution of mobile devices, such as smartphone. Therefore, The new contents have been published such as cartoon, drama, and novel in the web for consuming at anytime and at anywhere via mobile device. The types of the web content are video, music, photo, cartoon, and text. Previous studies have been conducted to sort these web contents, however these studies have analyzed relation of the link between the web pages or have focused on the structure of the web pages. In addition, the emotion of the web contents only has been classified into three categories such as positive, negative, and neutral. The topic and category of the web contents were classified by analyzing the texts of the web pages. Moreover, the patterns of consumption behavior have been determined by discrete emotion and dimensional emotion extracted from the texts of the web pages.
The consumption behavior of contents in daily life has been determined by analyzing the mobile devices, usage time, and usage frequency. They were subjective measurements.
This study was to develop the contents categorization system and to determine category, discrete emotion, and dimensional emotion analyzing the texts of the web pages. Its android application named the ‘SociaL Browser’ has been developed to collect automatically data of the consumption behavior of contents. The contents categorization system has crawled and stored the texts in the web pages accessed through the smartphone. In addition, stored texts were divided into morphemes, which are the smallest unit of the language, using natural language processing(NLP). The categorization of morpheme set was determined by comparing document similarity with representative morpheme set of category, discrete emotion, and dimensional emotion, respectively. The meaningful connections between users were found using consumption behavior of contents and determined synchronization. Personal consumption behavior of the web contents become the consumption pattern of the public. Analysis of the consumption behavior of the public can grasp the trend. Pattern and trend based on the categorization can be utilized for marketing and can substituted for the poll.

Keyword : Contents, Categorization, Consumption Behavior, Synchronization, Social Connention1. 서론 1 1.1. 연구배경 1 1.2. 문헌연구 4 1.2.1. 콘텐츠 범주화 4 1.2.2. 기본감성, 차원감성, 사회감성 6

With the rapid development and spread of the internet and mobile devices, platforms such as Twitter, Facebook and YouTube have grown. As a result, a variety of unstructured data such as text, images, and videos are being generated at a high speed. Unstructured data has a different value from existing structured data. So the demand for analysis is increasing in many companies. Therefore, this study focuses text data.
In our daily lives, we should deal with a variety of classification problems such as customer group classification, fraud detection and party support prediction problems. In this study, we focus on multi-class classification problem among various classification problems. As a classification technique, we compared the performances of SVM, -nearest neighbor, decision tree and the ensemble methods such as bagging, random forest, boosting model.
As a real data example, the text data 'What's Cooking' provided by Kaggle is analyzed. The basic procedures for natural language processing are illustrated in detail. We compare the prediction performances of each classification algorithms.최근 인터넷과 모바일 기기의 빠른 발전과 보급으로 트위터, 페이스북, 유튜브와 같은 플랫폼들이 성장하였으며, 그로 인해 텍스트, 이미지, 영상 등 다양한 비정형 자료가 빠른 속도로 생성되고 있다. 비정형 자료는 기존의 정형자료와는 다른 가치를 지니고 있어 많은 기업에서 분석의 요구가 증가하고 있다. 본 연구에서는 텍스트 자료 사례 분석에 초점을 맞추었다.
고객 군 분류, 사기 감지, 정당지지 예측 등 일상생활에서 다양한 분류 문제들을 접할 수 있다. 본 연구에서는 다양한 분류 문제들 중 다중 클래스 분류에 대해 살펴보고자 한다. 분류 기법으로는 의사결정나무와 의사결정나무에 앙상블 기법을 더한 배깅, 랜덤 포레스트, 부스팅 그리고 SVM, -근접 이웃 모형을 고려하였으며, 기법들의 성능을 비교해 보았다.
자료는 Kaggle에서 제공하는 What's cooking 데이터를 사용하였다. What's cooking 데이터는 텍스트 자료로, 자료를 정형화하기 위해 자연어 처리 등의 기본적인 절차 진행하였으며, 이 과정과 분석 방법을 소개하고자 한다. 각 기법의 분류 예측 성능을 비교하였다.1. 서론 7 2. 분류 알고리즘 소개 8 2.1 의사결정나무(Decision Tree) 8 2.2 배깅(Bagging) 10 2.3 랜덤 포레스트(Random Forest) 11

최근 스마트폰의 보급으로 음악을 다운로드하여 소유하는 시대에서 듣고 싶은 음악의 권리를 사고 음악을 바로 듣는 스트리밍 중심의 시장으로 변하고 있다. 이런 시대 흐름에 맞춰 사용자가 원하는 음악을 찾기 위하여 음악 정보 기술에 대한 중요성이 강조되고 있다. 또한, 데이터를 분석하고 분석된 데이터를 학습하여 판단이나 예측을 하는 머신러닝과 음악 정보 검색 기술이 결합하면서 음악 추천 서비스에 대한 관심과 연구가 늘고 있다.
자연어 처리(NLP, Natural Language Processing)는 인간이 사용하는 언어를 컴퓨터가 이해하고 분석할 수 있게 하는 분야를 말한다. word2vec은 word embedding 학습 모형으로 인공신경망을 적용한 NNLM(Neural Net Language Model)과 RNNLM(Recurrent Neural Net Language model)의 성능을 개선해 자연어 처리에 큰 성능 향상을 가져왔다.
따라서 본 논문에서는 음악 메타데이터인 노래 가사를 word2vec을 이용하여 분석하고 이를 활용하여 노래 간의 거리를 측정한다. 측정된 거리에 따라 노래를 클러스터링하고 음악을 노래 가사의 내용에 따라 분류하고자 한다.Recently, a market has change focusing on streaming music from downloading music because of popularization of smartphone. Therefore, going with the times, the importance of music information retrieval is emphasized in order to find the music that users want. Also, the interest and research on music recommendation service is increasing with the combination of machine learning that analyzes data and learns the analyzed data and make judgements or predictions and music information retrieval technology.
Natural Language Processing(NLP) is a field that allows computers to understand and analyze human language. Word2vec has improved performance of natural language processing by improving performance of Neural Net Language Model (NNLM) and Recurrent Neural Net Language model (RNNLM) using artificial neural network as word embedding learning model.
In this paper, we use word2vec to analyze the song lyrics, which are music metadata, and to measure the distance between songs by using them. Then, cluster the songs according to the measured distance to classify the music according to the contents of the song lyrics.국문초록 ⅴ 영문초록 ⅶ 제 1 장 서론 1 1.1 연구의 배경 및 목적 1

이러닝에서 상호작용은 학습몰입도 및 학습효과 향상, 중도포기율 감소 등의 긍정적 요인으로 작용한다. 이러닝 상호작용의 중요한 기능으로 학습자가 학습 중 이해가 어려운 내용을 질의하면 교수자 또는 전문적인 답변교사가 해당 질의에 답변을 제공하는 질의응답 게시판을 제공한다. 하지만 질의에 대한 답변을 교수자가 직접 작성하기 때문에 답변 시간 지연, 답변 비용 발생, 교사의 업무 피로도 상승의 문제점이 있다.
본 연구는 학습자가 학습 중에 작성한 질의에 대해서 자동으로 답변을 검색하여 제공하는 질의응답 검색 시스템을 개발하여 답변 시간 단축, 답변 비용 절감의 목적을 달성하는데 있다.
이를 위하여 본 연구에서는 자연어 처리 기술인 Word Embedding 기술의 하나인 Doc2Vec 기반으로 질의와 답변을 분석하여 가장 유사한 질의의 답변을 학습자에게 제공하는 질의응답 검색 시스템을 설계하고 구현하였다. 연구 내용을 요약하면 다음과 같다.
첫째, 질의응답의 검색 정확도를 높이기 위해 한글형태소 분석기로 질의 및 답변 텍스트를 분석하여 불필요한 단어와 문장을 필터링하는 1단계 전처리 필터를 설계하고 구현하였다.
둘째, Word Embedding 기술인 Doc2Vec을 이용하여 질의 및 답변 텍스트의 벡터화 알고리즘을 설계하고 구현하였다.
셋째, 벡터화된 텍스트를 Cosine Similarity 알고리즘을 활용하여 유사한 질의를 검색하는 알고리즘을 구현하였다.
넷째, 실험용으로 준비된 30개의 질문에 대해서 개발된 시스템의 검색 정확도를 평가하였다.
본 연구결과는 다음과 같은 결론을 제시하고 있다.
첫째, 자연어 처리를 이용한 질의응답 검색 정확도가 70.0%로 평가되었다.
둘째, Doc2Vec 기반 질의응답 게시판을 도입하여 답변 시간을 크게 단축할 수 있을 것으로 기대한다.
본 연구의 성과를 질의응답 시스템에 적용함으로서 학습자의 질의에 대한 즉각적인 답변을 제공하여 학습효과를 높일 수 있을 것으로 기대한다. 추가적으로 한국교육방송공사와 같이 국가 예산을 통해 답변료를 지급하는 기관에서는 예산 절감을 통해 콘텐츠 품질 개선 등의 투자에 좀 더 힘쓸 수 있을 것이다.
본 연구는 질의응답 시스템에 저장된 내용을 검색하여 유사한 답변을 제공하는 검색 방식으로서 모든 질의에 대한 답변을 100% 제공할 수 없는 단점이 있다. 향후 교과 지식 뿐 아니라 문제풀이 과정에 대한 지식베이스를 활용한 인공지능 기반의 질의응답 시스템의 연구가 필요할 것으로 판단된다.제1장 서론 1 1. 연구의 필요성 1 2. 연구 목적 2 3. 연구 방법 4

인공지능은 4차 산업혁명에서 새로운 화두로 떠오르고 있는 기술 중 하나이며, 세계 각국의 기업들이 주목하고 있는 기술로 교육, 금융, 자동차 등 다양한 산업 분야에 적용되어 가고 있다. 챗봇은 정해진 응답규칙에 따라 사용자의 질문에 응답할 수 있도록 만들어진 시스템으로, 지능형 가상 도우미 서비스, 날씨, 교통, 일정 등에 대한 간단한 질의응답부터 사용자의 패턴 분석을 통한 서비스 제공까지 점차 그 활용 범위가 확대되며 생활 밀착형 서비스로 입지를 굳혀갈 전망이다. 이에 따라 인공 지능 챗봇에 대한 연구는 시대의 흐름에 따른 반드시 필요한 연구라고 볼 수 있다. 본 연구에서는 여러 기업에서 적용, 사용할 수 있는 인공지능 챗봇 플랫폼을 제시하고자 한다. 인공지능 챗봇 서비스를 도입하면, 자동 응답 서비스로 챗봇의 역할이 상당히 증대되어 상담 인원의 인건비 절감 등의 효과가 나타날 것으로 예상 된다.
영국 시장조사기관인 테크나비오(Technavio)는 인공지능 챗봇 플랫폼 관련 시장이 2017년부터 2021년까지 연평균 37%이상 성장할 것으로 전망했다. 특히 BFSI(Banking, Financial Services and Insurance) 분야와 유통 및 e-커머스 분야에서 집중적으로 사용될 것이며, 이 외에도 헬스케어, 항공, 여행 등 다양한 분야에 활용될 것으로 예상했다.[27]
챗봇(Chat-Bot)이란 사용자가 메신저를 이용해 친구와 대화하듯 자연스럽게 질문을 입력하면, 인공지능 기술 기반의 챗봇이 입력된 대화를 분석하여 마치 사람과 대화하는 것 같은 응답을 제공하는 서비스를 말한다. 최초의 챗봇은 1996년에 개발된 엘리자(ELIZA)로 환자의 심리치료를 목적으로 조셉 바이젠바움(Joseph Weizenbaum)에 의해 개발되었다[17]. 당시 개발된 챗봇은 입력된 값에 따라 정해진 답변만을 제공한 형태로 실제 인간과 유사한 대화 흐름을 유지시키기는 다소 어려운 점이 있었다. 그러나 오늘날 하드웨어 및 소프트웨어 기술이 발전함에 따라 챗봇에 머신러닝을 비롯한 인공지능 기술을 적용할 수 있게 되면서 자연어 처리 알고리즘의 강화를 통해 사용자와 보다 자연스럽고 정확한 대화를 할 수 있게 되었을 뿐만 아니라 사용자의 메시지를 기반으로 행동 패턴 분석 및 데이터 정보를 수집해 개인화된 맞춤형 서비스를 제공할 수 있게 되었다[35].

본 연구에서 다루는 인공지능 챗봇 서비스 플랫폼(Artificial Intelligence ChatBot Service Platform)은 전 세계 기업인들이 각자,의 성격에 맞는 인공지능 채팅 상담(정보 수령, 주문, 지불, 결제)이 가능한 챗봇/ 플랫폼을 쉽게 생성하고 운영할 수 있도록 제공해주는 것이 목적이다.
이로 인하여 얻을 수 있는 이점은 1)고객 서비스 채널로 사용함으로써 관련 비용을 절감 하고, 2)커머스 채널로 사용함으로써 직접적인 추가 매출을 창출 하고, 3)마케팅 채널로 사용함으로써 고객에게 호의적인 느낌을 가지게 하여 고객 충성도를 증대 시키는 효과를 갖고 있다. 또한 이를 이용하는 일반 사용자들은 복잡한 전화 ARS나 웹사이트를 통하여 서비스를 이용하는 대신, 인공지능 챗봇 서비스 플랫폼을 이용하며 상담, 주문, 예약 시간을 절약하는 등, 사용 고객의 만족도를 높이는 효과를 얻을 수 있다.Artificial Intelligence (AI) is one of the emerging technologies as a new topic in the fourth industrial revolution, a technology to which various companies from around the world pay attention, which is applied to various industries such as education, finance, and automobile. A ChatBot is a system designed to be able to respond to the user’s questions according to the set response rules. The range of its application gradually expands, including simple questions and answers concerning intelligent virtual assistant service, weather, traffic, and schedule and the provision of services through analyzing the user’s pattern. Also, it is expected that it will establish itself with a life-friendly type service. Thus, research on AI ChatBot is positively necessary research according to the stream of times. This study would present an AI ChatBot Platform that can be applied and used in various companies. It is expected that if an AI ChatBot service is introduced, the role of ChatBot will significantly increase with the auto-answering service, and there will result in the reduction of labor costs of the counseling personnel.
Market research institution in the UK, Technavio, predicted that AI ChatBot Platform-related markets would grow up by over 37% a year on average from 2017 through 2021. Especially, it will be used in Banking, Financial Services and Insurance (BFSI), distribution and e-commerce fields, intensively, and also in various fields, such as healthcare, airline, and travel.
ChatBot refers to a service in which a user naturally enters a question, using a messenger like talking to a friend, AI technology-based ChatBot analyzes the entered conversation and provides a response as if the user talks to a human being. The first ChatBot was developed by Joseph Weizenbaum, aiming to provide psychological therapy for patients with ELIZA developed in 1996. The ChatBot developed at the time provided only the answers fixed by the input values, so it was somewhat difficult to maintain the flow of conversation similar to that of real human beings. However, with the development of hardware and software technologies today, as AI technology including machine learning can be applied to ChatBot, it has become possible to talk to the user more naturally and more accurately through improving the algorithm of natural language, and personalized and customized services can be provided, analyzing behavioral patterns and collecting data information based on the user’s message.
AI ChatBot Service Platform dealt in this study aims to provide ChatBot Platform for businessmen from all over the world so that they can easily create and operate ChatBot Platform that can provide AI chatting counseling (Information receipt, order, payment, and settlement) for their own characteristics.
The advantages that can be obtained accordingly include: 1) To save related expenses by using it as a customer service channel; 2) to create direct additional sales by using it as a commerce channel; and 3) to increase customers’ loyalty, making them have a favorable feeling by using it as a marketing channel. In addition, it has an effect on the increase in the satisfaction of the customers who use it. For example, the general users can save time for counseling, order, and reservation, using AI ChatBot Service Platform, instead of using the service through complex phone ARS or website.목 차 목 차 i 그림목차 iv 표 목 차 vii

The media such as newspapers, televisions, radios and magazines played the important role in informing people about the facts that have occurred. Nowadays, the development of information and communication technology and the Internet makes it possible to access to various news more easily, and the influence of the media providing information that can form public opinion is larger and larger.
According to the statistics of registration notification for “Internet newspaper" published by the Ministry of Culture, Sports and Tourism, the number of Internet newspapers was 286 in 2005, 2,484 in 2010, and 6,605 in 2015. With the spread of the Internet, the emergence of mobile devices, and the development of networks, the number has increased more than 23 times in 10 years. It has been utilized as a research subject of many researchers as reflecting the use value of users. However, since most article analysis studies do not reflect up-to-date information, it is difficult to predict accurately the direction of public opinions that change over time. Therefore, it is necessary to study the method for quickly analyzing news data generated in real-time with the latest information.
This research constructed a real-time analysis system for 150 Internet newspaper companies in Korea as the objects of the study. Through the web mining process of desired Internet news articles, various visualizations were presented through real-time data collection and text mining analysis, and the Hadoop system, a distributed processing technology for rapid processing, was also applied. The opinion mining analysis of Netizen’s comments in real-time collected news and standard dictionaries will be opened to the public, which will be helpful for future researchers.
Listening to customers and analyzing consumer’s patterns and needs quickly will help the companies to establish operations and strategies. We expect that this study will contribute to the development of big data processing system that can deal with real-time processing not only simple frequency analysis but also association analysis, cluster analysis, classification analysis, and predictive analysis.제 1 장 서 론 1 제 1 절 연구의 배경 및 목적 1 1. 연구의 배경 1 2. 연구의 목적 3 제 2 절 연구의 범위 및 구성 4

현대 사회에서 소프트웨어가 차지하는 비중이 커지면서 소프트웨어의 정확성 검사는 필수이다. 소프트웨어의 작은 오류도 우리 사회의 심각한 문제가 될 수 있기 때문이다. 많은 인명 피해와 금전적 손실이 되었던 사고들이 소프트웨어 작은 오류에서 발생하였다. 따라서 소프트웨어의 정확성 검사는 반드시 이루어져야한다.
이러한 소프트웨어의 정확성 검사는 전장용 소프트웨어에서도 필요하다. 과거와 달리 자동차에 들어가는 소프트웨어의 비중이 커지며, 잘 개발된 소프트웨어가 자동차의 성능을 향상시킬 정도로 전장용 분야에서 소프트웨어 중요성은 커지고 있다. 또한 소프트웨어의 비중이 커지면서 소프트웨어의 작은 오류로 발생할 수 있는 문제들도 많아졌다. 따라서 이러한 문제를 해결하기 위해서 전장용 소프트웨어의 정확성 검사는 필수이다.
최근 전장용 소프트웨어 시장에서는 AUTOSAR라는 국제 공동 프로젝트를 만들어 전장용 소프트웨어의 표준을 규정하고 있다. 이는 날로 복잡해지는 전장용 소프트웨어의 개발과 빠른 시장 경제에 맞추어 소프트웨어 개발 속도를 향상시키기 위해 산업 표준을 규정해 소프트웨어 재사용 기법을 적용하기 위한 의도가 있다. 그런데 AUTOSAR 메타모델은 UML2.0의 클래스 다이어그램과 자연어 설명으로 기술된다. 따라서 엔지니어들이 메타모델의 구문 제약 사항을 준수하는 AUTOSAR 모델을 작성하는 것은 쉽지 않다. 또한 제약 사항들이 자연어로 기술되어 있고, 어플리케이션에서 시스템 수준가지 방대한 정보를 포함하고 있기 때문에 모델 작성자가 모델링 오류를 범할 가능성이 매우 높다.
본 논문에서는 전장용 소프트웨어의 정확성 검사를 위한 도구를 소개한다. 우리는 AUTOSAR 표준에 정의되어있는 규칙들 중에서 기존 방법으로 검사가 되지 않는 규칙들을 정형언어인 OCL을 이용하여 기술하고 이를 전장용 소프트웨어 모델을 검사하기 위해서 사용한다. 전장용 소프트웨어 모델은 전장용 소프트웨어를 생성하는 단계에서 생성되며 소프트웨어의 구조와 값들을 저장하고 있다. 이러한 모델들은 마크업 언어인 XML로 저장된다. 따라서 도구는 XML로 저장된 모델을 OCL로 기술된 제약사항으로 검사한다.제 1 장 서 론 1 제 1 절 연구 배경 1 제 2 절 연구의 목표 및 범위 2 제 3 절 논문의 구성 3

Speech act indicates the intention of speakers through utterance. In conversational Question Answering System, it's important to analyze the questions from the users to provide the right response to meet the intention of the users. Existing researches for the analysis of speech act have required lots of studying and engineering to analyze the information of utterance. but in the latest natural language process or research area, active researching is being done using AI neural network showing the extraordinary performance.
But most of the part of the natural language using AI neural network is based on the Latin language and the Korean language based ones are very less and there's been nothing for researches of Korean speech act based on Attention showing outstanding performances in AI neural network machine interpretation.
Therefore, in this paper, game related sentences posted on SNS up to 607,000 ones have been collected to analyze the speech act. 48,000 trained data have been created for Request, Complain, Question, Statement Speech Act. Korean Speech Act Analysis model has been proposed using neural network based on Attention LSTM.
Through the experiment, embedding parameters suitable for Korean language and performance comparison assessment by the Attention combination method have been done and the abstraction of the weight value from the trained morphological elements, I could confirm that it gives the more weights to the morphological elements of the feature of the speech act.화행(speech act)은 발화를 통해 전달되는 화자의 의도를 가리킨다. 대화형 질의응답 시스템에서는 사용자의 의도에 맞는 응답을 제공하기 위해 사용자의 질의를 분석하는 것이 중요하다. 화행 분석을 위한 기존 연구들은 발화의 정보를 분석하기 위해 과도한 자질 엔지니어링을 하였으나 최근 자연어 처리 분야 연구에서 우수한 성능을 보이는 인공신경망을 이용한 연구가 활발히 진행되고 있다.
그러나 인공신경망을 이용한 자연어처리 연구 대부분이 영어 및 라틴어 기반 언어로 진행되어 한국어에 관한 연구사례는 비교적 적으며 인공신경망 기계번역에서 우수한 성능을 보인 Attention 기반 한국어 화행 분석에 관한 선행 연구사례가 없었다.
이에 본 논문에서는 한국어 화행 분석을 위해 SNS에 작성된 게임 분야 한국어 문장 607,000개를 수집하였고 요청 화행, 항의 화행, 질문 화행, 진술 화행 4가지에 대해 총 48,000개의 학습데이터를 만들어 Attention 기반 LSTM 신경망을 이용한 한국어 화행 분석 모델을 제안하였다.
실험을 통하여 한국어에 적합한 임베딩 파라미터와 Attention 조합 방법에 따른 성능을 비교 평가하였으며 학습된 형태소의 가중치를 추출하여 각 화행의 자질이 되는 형태소에 더 높은 가중치가 학습되는 것을 확인하였다.

Schedule management agent performs common tasks including adding, modifying, and deleting schedules such as meetings and appointments of the user. Due to its characteristics, there are frequent modifications in the schedule, and therefore an easy and friendly interface and interactive services within the system are necessary. According to this, the necessity of conversational agents which receives input in natural languages, processes, and performs on the special domain has increased.Conventional conversational agents focus on pattern matching modules which choose the most appropriate answer based on a matching technique between natural language queries and answers. However, processing and analyzing the perception based linguistic expressions of humans cause an increase in the knowledgebase, and make it hard to suggest the answers which exactly reflect user’s intentions.Computing with words can solve these problems, and makes it possible to manage the quantitative linguistic and perception based expressions which include calculation and deduction. However, it also has limitations in complexity of processing natural languages and adapting real world problems.In this paper, to complement these problems, we propose the CW based dialogue management technique for quantitative linguistic expressions based on the perception on schedule retrieval domains in conversational scheduling agents.The proposed methods are validated through performance and process evaluation based on designed scenarios. The proposed system has shown its better efficiency in performance and user satisfaction than the rule and Bayesian theory based conversational system.회의, 약속 등에 대한 일정에 대해 등록, 변경, 삭제 등의 작업을 대행해주는 일정관리 에이전트는, 그 특성 상 일정에 대한 변경이나 조정이 잦아 보다 쉽고 친숙한 인터페이스 환경과 시스템과 상호작용 가능한 양방향의 서비스를 필요로 한다. 이에 따라 사용자로부터 자연어로 된 문장을 입력받고, 이를 처리하여 실제 도메인에서 특정 기능을 수행하게 하는 대화형 에이전트의 필요성이 대두되고 있다.하지만 기존의 대화형 에이전트는 사용자의 질의를 분석하여 질의와 답변의 쌍으로 구축된 지식을 기반으로 가장 적절한 답변을 선택하는 패턴매칭 모듈에 초점을 맞추고 있는데, 인간의 지각에 기반한 언어학적 정보에 대한 질의를 분석하기 위해서는 지식베이스의 크기가 커질 뿐 아니라 사용자의 의도를 정확히 반영한 답변을 제시하기가 힘들다. 이 같은 문제를 해결하기 위해 등장한 Computing with words는 언어적 표현 및 지각적 정보의 정량적 표현을 가능하게 하고, 이를 통한 연산 및 추론을 가능하게 한다. 하지만 이 역시, 자연어 처리의 복잡성과 CW의 이론적 배경을 실제 문제에 적용하기 어렵다는 한계를 가지고 있다.따라서 본 논문에서는 CW를 실제 도메인에 적용 시의 복잡한 자연어 처리 문제와, 단순 패턴 매칭의 융통성 및 적용성 문제를 보완하고자 한다. 이를 위해 기존의 일정관리를 위한 대화형 에이전트에서, 일정 검색 도메인에서 발생하는 지각적 정보에 기반한 언어학적 표현의 처리에 실제 CW를 적용하여 지각적 정보에 대한 연산을 수행하는 방법을 제안한다.제안하는 방법은 규칙, Bayesian 이론을 적용한 대화 시스템과의 성능 비교 평가와, 프로세스 평가를 설계된 일정관리 시나리오를 기반으로 수행함으로써 그 유용성을 보였다.



Semantic transforming a natural language question to its corresponding logical form is central to the knowledge-based question answering system. Most previous methods have tried to achieve this goal by using syntax-based grammar formalisms and rule-based logical inferences. However, such approaches are usually limited in coverage of the lexical trigger which performs a mapping task from words to logical properties of the knowledge base, and easy to ignore implicit and broken relations between the properties, not interpreting the full knowledge base.
In this thesis, our goal is to answer questions in any domains via semantic embedding space in which the embeddings are encoded the semantics of words and logical properties. In the latent space, semantic associations between the existing features can be leveraged through their embeddings without using any hand-craft lexicon and rules. The embedding-based inference for question answering provides the ability to map factoid questions posed in natural language and the logical representations of the correct answers guided by the knowledge base. Our method is organized as follows: 1) semantic embedding space construction and 2) embedding-based question answering.
The first stage involves learning low-dimensional embeddings of a word and a logical property, both of which are a semantic-associated pair extracted from unstructured textual data with distant supervision, so that vector representations of semantically similar features are close to each other in the semantic embedding space. That is, the meaning representations of words can be collaboratively specified using their relations with the logical properties as regards the knowledge base consisting of conceptual data in the hierarchical and multi-relational structure.
The second stage focuses on ranking potential answers based on semantic similarities between embeddings of bag-of-words represented in the given question and those of logical representations of the potential answers. Here, the set of candidate answers is generated by the facts, as the knowledge base constituents, of possible entities appearing in the question statement. We then formulate an answer statement according to the expected answer type.
In terms of the overall performance of question answering, experimental results and case examples demonstrate that our proposed method outperforms previous KB-QA baseline methods on a publicly released QA evaluation dataset: WebQuestions.자연어 질문을 이에 상응하는 정규화 형태로 바꿔주는 의미적인 변환 과정은 지식 기반 질의 응답 시스템에서 중요한 부분이다. 최근 연구들은 이를 위해 구문 기반 문법적 형식화 기법이나 규칙 기반 논리적 추론 기법 등을 이용한 방법들을 제안하고 있다. 하지만 위 연구들은 어휘 단계 정규화 작업에서 다양한 어휘를 지식 베이스 내 논리적 자질로 연결하지 못하는 문제가 있고, 또한 지식 베이스 내 자질 간의 내포되거나 생략된 관계를 발견하지 못하므로, 지식 베이스를 충분히 파악하지 못하는 단점이 있다.
본 연구의 목표는 의미적인 임베딩 공간을 통하여 모든 분야의 질문에 대하여 답변하는 것이다. 여기서 임베딩은 단어들과 논리적 자질들의 의미적인 정보가 수치화된다. 임베딩 공간에서는 자질 간 의미적인 연관성이 사람에 의한 어휘 목록이나 규칙 없이도 자질들의 임베딩에 의해 계량화 된다. 이런 질의 응답을 위한 임베딩 기반 추론은 자연어로 표현된 주어진 질문과 이에 상응하는 답변의 지식 베이스로부터 제공되는 논리적 표현들을 연결시켜 줄 수 있다. 본 방법은 1) 의미적인 임베딩 구축과 2) 임베딩 기반 질의 응답으로 구성된다.
첫번째 단계는 의미적인 임베딩 공간에서 의미적으로 유사한 단어와 논리적 자질의 저차원 임베딩이 서로 비슷한 값을 가진 벡터가 되도록 학습한다. 의미적으로 연결된 단어와 논리적 자질 쌍은 distant supervision 기법을 이용하여 비정형 텍스트 데이터로부터 자동으로 추출된다. 즉, 지식베이스는 계층적이고 다단계로 구조화 된 개념을 표현하는 데이터로 구성되어 있기 때문에, 단어들의 의미적인 표현은 이들의 논리적 자질 간 관계를 통해 협력적으로 구체화된다.
두번째 단계는 주어진 질문을 표현하는 단어 묶음들의 임베딩들과 후보 답변의 논리적 표현들의 임베딩들 간 의미적 유사도를 통해 답변들을 순위화하는데 초점을 둔다. 여기서 후보 답변들의 집합은 질문에서 나타나는 가능한 개체들의 지식 베이스 정보에 의해 생성된다. 그 다음, 예상되는 답변 유형에 따라 답변을 표현화한다.
질의 응답 성능 관점에서, 본 논문에서 보고하는 실험 결과와 구체적인 사례 등은 본 연구가 기존의 지식 기반 질의 응답 연구들보다 효과적임을 증명하고 있다. 성능 평가는 공개된 질의 응답 평가 데이터인 WebQuestions를 이용하였다.1. Introduction 1 1.1 Question Answering 1 1.2 Knowledge-based Question Answering 3 1.3 Embedding Model-based Open Question Answering 6 1.4 Structure of Thesis 10

Advances in computer vision and natural language processing accelerate the studies of artificial general intelligence. Since both vision and natural language are the major and most interactive modalities of human, understanding and reasoning grounded on both vision and language became the key challenge for the artificial general intelligence. Visual question answering (VQA) is an instance of Visual Turing Test (VTT), which is aligned with this direction on top of the prestigious seminal work, Turing test [Turing, 1950]. In the VQA dataset [Agrawal et al., 2017], with large image datasets, question-answer pairs are collected for supervised learning. For instance, a machine answers for a given image and question, such as "Who is wearing glasses?", "Is the umbrella upside down?", or "How many children are in the bed?".
In this dissertation, having that the visual question answering task is general- ized as multimodal learning, the advances in multimodal learning are studied in deep learning where hierarchical representations are learned with various forms of multiple layers in neural networks, called multimodal deep learning. First, multimodal deep learning is introduced with three categorization: multimodal fusion, cross modality, and shared representation learning. After that, based on our previous works Kim et al. [2016b, 2017a, 2018], three major studies are discussed – multimodal residual learning, multimodal low-rank bilinear pooling, and bilinear attention networks.
Multimodal residual learning finds the joint representation of vision-language multimodality based on the idea of residual learning, which imposes a constraint that a part of neural networks must learn residual errors of a fitting function represented by a previous part of the neural networks. Whereas multimodal low- rank bilinear pooling gives a mathematical ground for the use of element-wise multiplication (a.k.a Hadamard product) as a joint function, since it can be interpreted as a low-rank bilinear pooling in the condition of that each modality is linearly transformed with appropriate model parameters. Bilinear attention networks unify the previous two works. Using the interpretation of low-rank bilinear pooling, it successfully generalizes unitary attention mechanism into bilinear attention by matrix chain multiplication. This is so efficient that the computational cost is the same with the counter, unitary attention networks. Moreover, residual learning of attention is proposed to exploit up to eight bilinear attention maps in reasoning processes, which prevents the over-fitting that usually comes from multi-layer attention networks.
As a result, Multimodal Residual Networks (MRN) achieved the 4th place in the VQA Challenge 2016, and Multimodal Low-rank Bilinear Attention Net- works (MLB) achieves a new state-of-the-art with a significantly less number of parameters in November 2016, at the time of publication. Moreover, Bilinear Attention Networks (BAN) placed the 2nd place (shared with the other team) in the VQA Challenge 2018 achieving the best single model among the entries, and invited as a speaker in CVPR 2018 workshop (Salt Lake City, USA) on June 18. Since each vision or natural language processing is still an evolving area of studies, the multimodal deep learning can take advantage of the progressed results from computer vision and natural language processing studies in the future.컴퓨터 시각과 자연어 처리 기술의 발달은 일반 인공 지능에 대한 연구를 가속화 하였다. 시각과 자연어는 인간이 사용하는 가장 상호 작용적인 양태이므로 시각과 언어에 모두 기반한 이해와 추론은 일반 인공 지능의 핵심 과제가 된다. 시각 질의 응답(VQA)은 시각 튜링 테스트의 한 예로서, 초석이 되는 튜링 테스트 [Turing, 1950] 연구에 기반한다. VQA 데이터셋 [Agrawal et al., 2017]은 대용량의 이미지 데이터셋을 이용해 지도 학습을 위한 질문-답 쌍을 수집하였다. 예를 들면 "누가 안경을 쓰고 있나?", "우산이 뒤집어져 있나?", "침대에는 몇 명의 아이들이 있는 가?"와 같은 질문에 기계는 수집한 답들을 이용해 학습한 후 이미지와 질문만을 보고 답을 내어야 한다.
본 연구에서는 시각 질의 응답 과제를 다중 양태 학습 문제로 일반화하고, 다중 양태 학습의 발전을 다층 구조 신경망의 다양한 형태를 활용하여 계층적 표상을 학습하는깊은학습,다중양태깊은학습 관점에서살펴본다.다중양태깊은학 습을 세 가지 분류 기준, 다중 양태 융합, 교차 양태, 공유 표상 학습으로 나누어 소개한다. 또, 이전 연구들 Kim et al. [2016b, 2017a, 2018]를 바탕으로 세 가지 주요 연구, 다중 양태 잔차 학습, 다중 양태 저계수 쌍일차 추출, 쌍일차 주의 망 의 내용들을 논의한다.
다중 양태 잔차 학습은 잔차 학습을 기반으로 시각-언어 다중 양태의 결합 표상 을 찾는다. 여기에서 신경망의 일부는 앞 부분의 신경망이 표현하는 목적 함수의 잔차 오류를 학습하도록 강제한다. 반면, 다중 양태 저계수 쌍일차 추출은 각 양태 가 적절하게 선형 사영된 조건에서 원소곱이 결합 함수로서 가지는 수학적 의미를 설명할 수 있게 한다. 쌍일차 주의 망은 이전 두 연구를 통합한다. 저계수 쌍일차 추출에 대한 해석을 바탕으로 행렬 연결 곱을 이용해 단일 주의 기제를 쌍일차 주의로 성공적으로 일반화하여 계산 비용은 단일 주의 망과 비슷한 수준으로 효율적이다. 더 나아가, 주의 잔차 학습을 제안하여 여덟 개의 쌍일차 주의 지도를 추론 과정에서 활용할 수 있게 하여 다층 주의 망에서 발생하는 과조정을 방지한다.
그 결과, 다중 양태 잔차 망 (MRN)은 VQA 챌린지 2016에서 4위를 기록하였 고, 2016년 11월 출판 시점에는 보다 적은 파라미터를 이용하여 다중 양태 저계수 쌍일차 주의 망 (MLB)을 제안하고 세계 최고 성능을 갱신하였다. 쌍일차 주의 망
(BAN)은 VQA 챌린지 2018에서 준우승(공동 2위)를 하였으나 단일 모델로는 최고 성능을 보였다. 이 결과는 2018년 6월 18일, CVPR 2018 학회(미국 솔트레이크 시티) 워크샵에 초청되어 구두 발표하였다.
시각 또는 자연어 처리는 계속 발전 중인 분야이므로 제안하는 다중 양태 깊은 학습 방법들은 컴퓨터 시각과 자연어 처리 기술의 발달과 더불어 더 향상될 수 있는 가능성이 있다.Abstract i Chapter 1 Introduction 1 Chapter 2 Multimodal Deep Learning 6 2.1 Introduction 6 2.2 Linear Model 8

본 연구는 의료 지식이 사회적으로 구성되는 과정과 건강 행동에 미치는 영향을 분석하려는 시도이다. 의료 지식이 중요한 까닭은 그것이 사람들이 주목하는 대상과 그것에 대한 판단에 영향을 미치는 것은 물론, 개인들 사이의 연결망과 상호작용하는 중요한 구조적 변수이기 때문이다. 본 연구는 지식을 고려함으로써 기존 연구가 건강 행동을 설명할 때 사용했던 개인 수준의 변수와 개인 간 수준의 변수들의 한계점을 보충할 수 있다고 주장한다.
본 연구는 유방암 의료 지식을 주요 대상으로 삼고, 그것의 형성과 영향을 분석한다. 여러 질병 중 유방암을 선택한 까닭은, 유방암에서 의료 지식이 사회적으로 구성되는 양상과 그것이 지니는 광범위한 영향을 관찰하기 용이하기 때문이다. 세 가지 요소가 여기에 기여하고 있다. 1) 우선 암 의료 지식에 대한 대중적 관심이 높다. 2) 유방암은 그 중에서도 발생률이 가장 높으며 국가적/사회적 관심이 집중되어 있다. 3) 유방암의 유병 기간이 긴 편이라 의료 지식의 영향이 장기적이고 광범위하다.
유방암 의료 지식을 탐구하기 위해, 1975년부터 2016년까지 제출된 유방암 논문 초록 48448건과 community.breastcancer.org라는 유명한 유방암 온라인 환자 포럼의 게시글 약 14만 건과 부속 정보를 자료로 삼았다. 유방암 의료 지식은 다양한 수준에서 형성되고 작동하는데, 공식적 의료 지식의 경우 연구 논문을 통해 접근하였고, 비공식적 의료 지식은 온라인 환자 포럼을 통해 접근하였다. 이들을 분석하기 위해 토픽 모델링을 중심으로 단어 네트워크 분석과 라소 회귀 분석을 활용하였다. 토픽 모델링은 문서 안에 존재하는 복수의 토픽을 추정하여 문서 집합 이면에 잠재하는 지식의 구성을 측정하는데 용이한 방법인데, 단점이 없는 것은 아니다. 이에 단어 네트워크와 라소 회귀분석을 통해 토픽 모델링이 가진 한계를 극복하려 시도하였다.
분석 결과 밝혀낸 점은 크게 두 가지이다. 첫째, 의료 지식은 일정 부분 사회적으로 구성된다. 질환, 즉 환자의 주관적 경험 만이 아니라, 질병, 즉 신체의 생물학적 변화에 대한 논의 역시 그러하다. 본 연구는 논문의 펀드 출처가 논문의 핵심 대상과 그것을 다루는 방식이 설정되는데 미치는 영향을 분석함으로써, 집단의 이해관계에 따라 탐구의 초점과 단위가 달라지는 현상을 확인하였다. 유방암 논문이 정부 펀드를 받았을 경우, 해당 논문은 유방암 환자보다 다른 단위에 더 관심을 가진다. 유방암 세포나 유전자 등 아예 미세한 대상에 주목하거나(본 연구는 이를 ‘분자적 대상’이라고 지칭하였다), 인종이나 특정 지역의 환자 집단 등에 주목한다. 반면 민간 펀드를 받았을 경우, 해당 논문은 유방암 환자에 주목한다. 유방암 연구에서 가능한 여러 주목 대상이 존재하는데(환자, 환자 집단, 유방암 세포, 세포 내 신호 전달 물질 등) 펀드 출처에 따라 주목 대상의 단위가 체계적으로 변화한다는 말이다.
이는 국가와 민간의 상이한 관심사에서 기인한다. 국가는 전체 인구의 건강 상태를 관리하고 의료비를 억제하는데 관심을 가진다. 그것이 예산을 절약하는 방법이자 국가를 부강하게 만드는 길이기 때문이다. 개별 환자가 아니라 환자 집단에 주목하는 것은 이를 고려하면 자연스럽다. 분자적 수준의 미세한 대상에 집중하는 것은, 1) 그것이 개별 환자의 특수성을 넘어 전체 인구에 적용 가능한 앎을 만들어 내고, 2) 전체 인구에서 위험 집단을 선별하고 여기에 자원을 집중함으로써 효율적인 건강 관리를 가능하게 만들기 때문이다. 본 연구는 이를 국가의 ‘분자적 관리 기술’이 ‘탈-개체적 프레임’을 유도한다고 정리했다. 반면 민간 영역의 지원을 받은 지식이 개별 환자에 집중하는 것은, 개별 환자의 치료 성과를 향상시키는 것이 민간 단체의 인도적 관심사에 부합하거나 제약 회사의 이윤 창출에 도움이 되기 때문이다.
둘째, 의료 지식이 질병의 생물학적 측면에 지나치게 집중할 때, 질병의 구체적 경험에서 발생하는 불만과 요구를 억제하는 효과가 발생한다. 본 연구는 유방 재건술을 둘러싼 상이한 논의 구조를 통해 이를 확인하였다. 유방암 환자 포럼에서, 유방 재건술에 호의적인 태도를 지닌 사용자는 초점이 다양한 비공식적 유방암 의료 지식에 노출되어 있다. 이 지식은 사용자들이 전문 정보 만이 아니라 자신의 경험을 다양한 타인과 교류하고 논의하면서 발생한 것으로 추정된다. 반면 유방 재건술에 거리를 두는 사용자는 유방암의 생물학적 정보에 집중하는 비공식적 유방암 의료 지식에 노출되어 있다. 이 사용자들은 주로 제 3자가 만들어낸 전문 정보를 공유하는 용도로 온라인 포럼을 이용하며, 다른 사용자와 한정적인 관계만을 맺고 있다.
왜 초점이 다양한 비공식 지식 구조는 유방 재건술에 호의적인 태도를 유도하고, 생물학적 측면에만 집중하는 지식 구조는 유방 재건술에 유보적인 태도를 유도하는가? 유방 재건술은 생존 보장을 위한 처치가 아니라 사회적 관계 회복과 심리적 이익을 위한 처치이다. 그런데 생물학적 측면에 집중하는 지식은, 비-생물학적 영역에 존재하는 불만과 불편함으로부터 눈을 돌리게 만든다. 거꾸로 다양한 초점을 지닌 지식은 이런 불편함을 환기한다. 전자에 비해 후자는 사회적/심리적 불편을 극복하기 위한 적극적인 움직임을 유도한다. 유방 재건술에 대한 상이한 태도가 이를 보여준다. 이는 유방 재건술이라는 한정된 대상에 대한 결과이지만, 대부분의 공식적 의료 지식이 질병의 생물학적 특징에 주목하고 있는 현황에 비추어볼 때, 함의가 크다.
이 연구는 의료 지식이 권력과 밀접한 관련이 있음을 보여준다. 우선 객관적이라 여겼던 의료 지식의 형성 과정에 사회적 세력의 이해관계가 일정 부분 개입한다. 때로 그들 사이의 경쟁과 각축이 일어나기도 한다. 나아가 의료 지식은 사람들의 주목과 판단 양상 그리고 네트워크에 영향을 미침으로써, 전통적인 방식의 권력은 아니지만 역시 중요한 권력 현상을(미시 권력) 만들어낸다.
의료 지식에 주목한 본 연구의 전략과 구도는 감염병을 비롯한 다른 질병에도 적용할 수 있다. 또한 자연어 처리를 통해 지식에 접근한 본 연구의 방법적 시도는, 폭증하고 있는 자연어 자료를 활용한 연구에 주요한 참조점이 될 것이다.제 1장 건강 행동과 지식 구조 1 1. 유방암 지식과 건강 행동 1 2. 합리적 의사 결정 이론과 건강 행동 12 3. 네트워크와 건강 행동 20 4. 지식 구조와 건강 행동 26

자연어 이해에 필요한 개연규칙은 다양한 방법으로 축적이 시도되고 있다. 본 논문에서는 천자문 중에서 회의문자의 자소결합 사례를 이용하여 개연규칙을 발견하였다. 회의문자의 의미는 그 글자를 구성하는 자소의 개별적인 의미가 결합한 것이다. 이러한 원리에 착안하여, 자소간 의미 결합 개연성을 발견하였다. 이를 위해, 회의문자 100자를 선정하여 원래 글자의 해설을 참고해서 자연어 이해에 필요한 개연규칙을 유도하였다.
100자 중 60자는 ‘한자를 이용한 개연규칙 발견’[34]에서 참고 하였고, 나머지 천자문중 회의문자 40자에 대해 개연규칙을 유도하였다. 또한 개연규칙 유도뿐만 아니라 그 적절성을 3명에게 평가하게 하였다.
그 결과 69개의 규칙에 대해서는 모두 적절하다고 생각하였고, 세 명 모두 적절하지 않다고 생각하는 규칙은 없었다. 그러므로 유도한 개연규칙을 사람이 보기에는 적절했다고 판단하였다.

자연어 이해 분야에서, 화자의 발화 의도를 예측한다는 것은 화자가 생각 혹은 하고자 하는 것이 무엇인지 파악하는 것을 의미하며 청자는 화자의 발화를 듣고 그 의도에 맞는 행동을 취한다. 발화 의도를 정확하게 예측하는 것은 자연어 기반 대화 시스템에서 중요한 역할을 한다.
본 논문에서는 발화 의도 예측을 위한 계층구조 주목 메카니즘 기반 순환 신경망 모델을 제시한다. 제안하는 모델은 단어를 입력으로 하여 발화를 벡터로 표현하는 단어 단위 인식기와 벡터로 표현된 발화를 입력으로 하여 대화를 표현하는 벡터를 구성하는 발화 단위 인식기의 계층 구조를 가진다. 각 단계의 인식기에는 주목 메카니즘이 적용되어 대화로부터 발화의 의도를 예측함에 있어 중요한 정보를 갖는 단어 및 문장에 더 집중할 수 있도록 한다. 본 논문에서 제안하는 모델은 기존 모델들에 비해 두 가지의 이점을 갖는다. 첫 번째, 제안하는 모델은 이전 발화의 문맥을 고려하여 현재 발화의 의도를 예측하기 때문에 기존의 모델에 비해 높은 성능을 보인다. 두 번째, 주목 메카니즘을 통해 어떠한 단어와 문장이 발화 의도를 결정함에 있어 중요한 의미를 갖는 지 시각화 할 수 있다.
본 논문에서는 한국어 자막 데이터와 DailyDialog 데이터를 이용하여 제안한 모델의 성능을 평가하였다. 그 결과, 정확도에 있어서 비교 모델에 비해 근사하거나 우월한 성능을 보였으며 Macro Average F1-Score에 있어서 눈에 띄는 성능 차이를 보였다. 또한 실험의 결과를 통해 본 모델이 낮은 비율을 차지하는 발화 의도 클래스의 자질을 다른 모델에 비해 잘 추출해내는 것을 확인할 수 있었다.In natural language understanding, it is important to predict intent of utterance because we can know what the speaker wants through the intent of utterance, and then we do some action or reply to the speaker.
In this paper, we propose hierarchical attention-based recurrent neural network for intent classification. The proposed model consists of word level encoder and utterance level encoder that make utterance representation vector and dialogue representation vector, respectively. In each encoder, we apply attention mechanism to pay attention to informative words and utterances. We have two advantages by using this model for intent classification. First, the performance of the model has improved by using the contextual information which is obtained from the previous utterances in dialogue. Second, by applying attention mechanism, we visualize what words and utterances are important to the decision of the intent. We evaluate the model with Korean Subtitle dataset and DailyDialog dataset. The accuracy of proposed model is on par with or outperform baselines and the macro average F1-score of proposed model has an outstanding gap with other models. Furthermore, the experiment shows that our proposed model could extract the feature of the utterance which has the intent of minority class. Through the empirical analysis, we demonstrate our proposed model achieves success in intent classification.제 1장 서론 1 제 1절 연구 배경 1 제 2절 연구 내용 2 제 2장 관련 연구 4

자연어를 분석하는데 하향식(top-down) 파싱(parsing) 알고리즘과 상향식(bottom-up) 파싱 알고리즘이 있다. Earley 파싱 알고리즘은 자연어 분석에서 가장 많이 이용되는 기법중의 하나이다. 이 알고리즘은 하향식 정보를 효과적으로 이용하기 때문에 상향식 접근 보다 매우 효율적이다. 그러나 Earley 파싱 알고리즘은 입력 문장에 에러(error)가 있으면 즉시 종료되기 때문에 견고한(robust) 파싱을 구현하기 어렵다. 본 논문에서 우리는 Earley 파싱 알고리즘을 보다 견고한 파싱 기법으로 만드는 방법을 제안한다. 주 파싱 모드(mode)인 Earley 파싱으로 분석하다가 에러 때문에 중지되면 파싱 모드를 지역적 양방향 분석으로 전환시킨다. 에러 위치 이후에 나타나는 임의의 단어를 아일랜드(island)로 정한다. 아일랜드를 지역적으로 좌우 양방향으로 확장 시켜서 에러 위치까지 도달하게 한 다음 에러의 종류를 파악하고 이를 복구하는 기법을 제안한다. 이렇게 하므로써 하향식 정보를 이용하는 효율성을 유지하면서 상향식 파싱 기법의 큰 특징인 견고성도 얻을 수 있다.For natural language analysis either top-down or bottom-up parsing algorithm is used. Earley's parsing algorithm is a representative top-down method which is widely used for natural language analysis. Top-down methods take advantage of prediction by utilizing top-down information. This makes top-down methods more efficient than the bottom-up methods. However, the Earley algorithm is too fragile in case of parsing ill-formed input. It can not continue parsing if scanning is not possible at a word. The breakdown in the middle of the input can be avoided in case of the bottom-up approaches.
In this paper we will develop a method that makes the Earley algorithm robust. The major mode of parsing is carried out by the Earley algorithm. When the algorithm encounters an error, the parsing is switched to the local bi-directional analysis temporarily. One word is selected as an island among the words after the error. The local bi-directional analysis extends the island in both directions. As soon as the island reaches the position of the error, the type of the error is identified. Then the error can be corrected by comparing the analysis results of Earley's and local bi-directional process. By this approach we can achieve robustness of bottom-up methods as well as efficiency of Earley's algorithm.차례 그림 차례 = iii 국문 요약 = iv 제1장 서론 = 1 1.1 연구 배경 = 1

자연어 문장을 분석하여 구문 구조를 알아내는 것은 매우 중요하다. 왜냐하면 기계 번역, 질의응답 시스템, 정보검색시스템 등 모든 언어 처리를 필요로 하는 시스템들은 필수적으로 구문 분석이 잘 되기를 요구하고 있기 때문이다. 현재까지 대부분의 시스템들은 구 구조 문법을 이용하고 있다. 그러나 여기에서의 문제는 주어진 자연어에 대한 모든 완전한 구문 규칙을 알아낼 수 없다는 것이다. 따라서 항상 불완전한 시스템에 그치고 말게 된다.
본 연구에서는 구문 규칙이 필요 없는 문법 체계인 의존 문법을 기반으로 한 자연어 구문 분석(파싱) 기법을 연구하였다. 기존에 수행된 의존 문법 기반 파싱에 관한 연구는 단지 품사 정보나 자질 정보만을 이용하였다. 따라서 문장에 대하여 구한 구문 트리의 정확도가 그리 높지 못하였다. 따라서 본 연구에서는 의존문법 기반 구문 분석을 하는 데 있어서 품사 정보나 자질 정보 뿐만 아니라 구문트리가 부착된 말뭉치에서 추출한 통계 정보와 어휘 정보를 주된 정보원으로 이용하도록 하였다. 이와 같은 통계/어휘 정보를 이용하는 의존 문법 기반 구문 분석 시스템을 구현하고 그 성능을 측정하였다. 실험 결과 과거의 기법보다 훨씬 더 정확도가 높은 시스템을 만들 수 있음을 알게 되었다.It is important to obtain the syntactic structures of natural language sentences because the systems for machine translation, question-answering, information retrieval, etc. require to utilize the syntactic structures. Most of the recent systems use the phrase-structured grammar for syntactic analysis. But those systems encounter a problem which is that we do not have the complete grammar for any natural language. Thus these systems fail to analyze some sentences.
In this paper, we studied a natural language syntactic analysis method that is based on dependency grammar. The motivation is that dependency grammar does not require to have grammar rules. The existing studies for parsing based on dependency grammar have used only part-of-speech and feature information. Thus precision of the sentence analysis was not high.
The main characteristic of our dependency grammar parsing is that the major source of knowledge for parsing is from statistical and lexical information that is extracted from treetagged corpus. We have implemented a syntactic analysis system following this line and evaluated its performance. The experimental results show that we can achieve a system with higher precision than the existing systems.차 례 그림 차례 = iii 표 차례 = v 국문 요약 = vi 제1장 서론 = 1

감성분석은 문장에 담긴 사람의 긍정, 부정을 나타내는 주관을 탐지하고 분석하는 일이다. 주로 판매 상품에 대한 평가와 영화 감상평 등 특정 사물에 대해 이루어지며, 개인마다 다른 감성과 표현의 다양성을 넘어 여론을 분석하기 위해 쓰인다. 문장생성은 대화형 시스템, 기계 번역, 언어모델링 등 다양한 분야에서 목적에 맞는 문장을 생성하는 작업의 총칭이다. 생성되는 문장은 의미, 문법 더 나아가서는 화용까지 맞아야 하므로 문장생성은 굉장히 어려운 분야이기도 하다. 본 논문에서는 딥러닝을 이용한 감성분석과 문장생성에 관해 기술한다. 합성곱 신경망을 이용한 감성분석과 다양한 재귀 신경망을 이용한 문장생성, 더 나아가 원하는 감성이 담긴 문장을 생성하는 모델까지 제안하고 이를 한국어에 응용한다.Sentiment analysis is the task of detecting and analyzing the subject that shows the positive and negative of the person in the sentence. It is mainly used for evaluating sales items and movies, and it is used to analyze public opinion beyond the diversity of sentiment and expressions of individuals. Text generation is a generic term for the task of generating sentences for various purposes, such as interactive systems, machine translation, and language modeling. Text generation is a very difficult field because the sentence to be generated must be suitable for semantics, syntax and even pragmatics. In this paper, we describe sentiment analysis and text generation using deep learning. We propose text generation using recurrent neural network, sentiment analysis using convolutional neural network, and a deep learning model that generates a text containing desired sentiment, and apply it to Korean.Ⅰ. 서론 1 Ⅱ. 관련 연구 3 1. 감성 분석 3 2. 문장 생성 4

Recently, as the internet and mobile communication have developed, a lot of people can leave their opinions regardless of time and place, and it is easily seen by other users. Among them, the easiest thing to do is product evaluation. The evaluation made by the user becomes the result of emotion about the product, and it is divided into positive or negative opinions. Product reviews can be useful information for potential buyers. In order to get the information you want, you have to check each one manually.
It takes a lot of time to accurately find the desired information among various evaluation texts freely left by the users. Text mining is a technique for finding opinions that are helpful for making decisions. It is a technique to automate a series of processes for obtaining necessary information from various texts. It is called opinion mining or sentiment analysis.
Recently, a method of analyzing morphemes for text composed of natural language and then analyzing words by vectorizing them through various machine learning algorithms has been studied. In recent years, as deep learning techniques have been developed, researches using neural network algorithms have been actively carried out. A recurrent neural network model, which is an algorithm to process sequential data, and text processing using efficient Convolution neural networks, which are efficient in feature extraction and image recognition, have been studied.
In this study, emotion of evaluation sentence is analyzed by evaluation data of movie among user 's various evaluation. The evaluation text data is divided into morpheme units through morphological analysis and converted into distributed expression data using Word2Vec, which is a popular word embedding method in recent years. Based on the transformed data, we combine the ability of the Convolutional neural network to recognize the local pattern and the LSTM, which is one of the Recurrent neural network models that can utilize the text order, to create the optimal emotion classification model.최근 인터넷과 이동통신이 발달함에 따라 수많은 사람들이 시간과 장소에 구애받지 않고 자신의 의견을 남길 수 있고 다른 이용자가 쉽게 볼 수 있게 되었다. 그중에서도 가장 쉽게 접할 수 있는 것이 상품에 대한 평가이다. 사용자가 작성한 평가는 하나의 상품에 대해 실제 사용자의 좋고 나쁨에 대해 감정을 표현한 결과가 되고, 개개인에 따라서 긍정 또는 부정적인 의견으로 나뉘게 된다. 상품 리뷰는 잠재적 구매자들에게 정말 유용한 정보로 활용될 수 있지만. 원하는 정보를 얻기 위해서는 일일이 수작업으로 확인해야 할 것이다.
사용자들이 자유롭게 남긴 다양한 평가 텍스트 중에서 원하는 정보를 정확하게 찾아내기 위해서는 많은 시간이 필요하다. 의사 결정을 하기 위해 도움이 되는 의견들을 찾는 기법을 텍스트 마이닝(text mining) 이라고 하며, 다양한 텍스트에서 필요한 정보를 얻기 위한 일련의 과정들을 자동화하는 연구로써 텍스트 데이터를 이용해 사용자의 극성을 판단하는 기법을 오피니언 마이닝(opinion mining) 혹은 감성 분석(sentiment analysis)이라고 한다. 자연어로 구성된 텍스트에 대하여 형태소를 분석한 후 단어들을 벡터화 하여 다양한 기계학습 알고리즘을 통해 분석 하는 방법들이 최근 많이 연구되고 있으며, 최근에는 딥러닝(deep learning) 기법이 발달함에 따라서 신경망 알고리즘을 이용한 연구도 활발히 진행되어지고 있다.
대표적으로 순차적인 데이터를 처리하는 알고리즘인 순환신경망 모델과 특징추출에 효율적이고, 이미지 및 영상 인식에서 많이 쓰이는 합성곱신경망을 활용하여 텍스트 처리를 하는 연구가 진행되었다. 하지만 현재 많이 사용되는 방법들은 한글, 특수문자, 숫자, 영어 등이 섞여있는 문장들에 대해서 분석하는데 어려움을 겪고 있다.
본 연구에서는 한글 텍스트로 된 사용자의 평가에 대하여 기존에 방법 보다 더 높은 성능으로 분석하기 위해 현지 패턴을 인식하는 합성곱신경망의 능력과 텍스트 주문을 활용할 수 있는 순환신경망 모델중 하나인 LSTM모델을 조합하여 감성분석을 수행하는 방법을 제안하고 최적의 감성분류 모델을 만들어 기존의 기계학습 알고리즘과 비교한다. 결과적으로 두 신경망을 조합한 모델이 기존의 방법 보다 약 5% 이상의 성능 향상을 보였다.제1장 서론 제2장 배경 및 관련 연구 2.1 감성분석 2.2 순환신경망을 이용한 감성분석



본 논문은 자연어 처리과정에서 난제로 대두되는 Multiword Expression (MWE)을 언어학적 접근을 통해 자동으로 인식하도록 하는 연구이다. MWE는 두 단어 이상으로 구성된 하나의 어휘소로, 각 구성단어 사이에 띄어쓰기나 수식어 등을 포함하고 있어 별개의 학습 없이는 하나의 어휘소로 인식되기가 어렵다. 더욱이 구성단어의 합성성(compositionality)을 통한 유추가 불가능하다는 idiomaticity를 가지고 있어 언어처리 과정에서 반드시 하나의 어휘소로 식별되어야만 한다. MWE가 문장 내에서 하나의 어휘소로 적절하게 인식되지 않아 MWE의 구성단어에 합성성의 원리가 적용되면, 자연어처리의 문장분석에 오류를 발생 시킬 수 있고 잘못된 의미전달이 일어날 수 있다.
본 논문은 MWE의 자동인식과 인식의 정확도 향상을 위해, [POM] ([Part Of Multiword expression]) 태깅과 MWE의 언어학적 분류(classification)를 적용시켰다. [POM] 태깅 모델은 프랑스어의 형용사-명사 어구에만 한정되어 있었던 Wehrli et al. (2010)의 태깅 모델을 모든 종류의 영어 MWE의 구성요소에 적용할 수 있도록 확장시킨 것이다. 더불어 MWE의 언어학적 특성을 좀 더 세밀하게 반영하고 있는 MWE의 개정 분류법을 제안하였다. 개정 분류법은 Sag et al. (2002) 과 Baldwin et al. (2010)이 만든 기존 분류법을 바탕으로 하였으나, 언어학에 기반을 둔 추가적인 검토를 통해 MWE를 기존의 3가지에서 5가지 종류로 세분화하였고, 분류 기준이 되는 통사적 특징들도 재정립하였다.
본 논문의 가장 중요한 요지는 위의 두 가지 접근법을 동시에 적용하여 MWE를 인식하도록 설계한 점이다. 두 접근법의 조합이 MWE의 인식률 향상에 기여할 수 있다는 것을 증명하기 위해, 오직 [POM]만 태깅하여 MWE를 인식하도록 하는 모델과, 개정 분류법이 아닌 기존 분류법을 통해 MWE를 자동인식하게 하는 두 개의 추가적인 평가모델을 만들었다. 첫 번째 평가모델은 MWE의 언어학적 특성이 반영된 분류법을 적용하는 것이 자동인식에 얼마나 중요한지를 증명하고자 한 것이다. 두 번째 평가모델은 MWE 분류법을 적용한다 하더라도 그 분류가 정교할수록 인식의 정확도가 향상되며, MWE의 언어학적 특성과 분류에 대한 계속적인 노력이 필요하다는 것을 시사하고자 한 것이다.
본 논문이 제안한 MWE 인식 모델은 MWE 분류에 따라 서로 다른 규칙의 설정하여 MWE의 구성요소들이 적절하게 사용되었을 때에만 MWE로 인식하도록 설계되었다. 그 결과, 본 논문이 제안한 모델을 데이터 전체에 적용하여 처리하였을 때 92.85%의 MWE 인식률을 보였고, [POM] 태깅만 이용하는 모델과 기존 분류법을 적용한 모델은 각각 85.84%과 90.02%의 인식률이 도출되었다. 이는 언어학적 특성이 더욱 정교하게 반영될수록 MWE의 자동인식률이 향상될 것 이라는 본 논문의 주장을 뒷받침할 만한 결과값이라고 할 수 있겠다.
각 모델간의 인식률 차이를 좀 더 명확하게 하기 위해, 문장 내에서 MWE의 구성요소가 규칙을 위반하고 있는 데이터만을 대상으로 하여 추가적인 결과값을 도출해 보았다. 그 결과 [POM]만 태깅한 경우 15.82%, 기존 분류법을 사용한 경우 64.84%의 인식률이 도출되어, 전체 데이터에 적용하였을 때 보다 각 모델간의 인식률의 차이를 좀 더 명확하게 나타냈다. 이는 MWE에 적용된 규칙을 위반한 경우에 대해, 본 논문이 제시한 인식모델에서는 MWE로 인식되지 않았지만, 평가모델은 상대적으로 정교함이 떨어지는 규칙이 적용되었기 때문에 규칙을 위반한 경우도 MWE로 인식되는 오류를 발생 시키기 때문이다. 뿐만 아니라 전체 데이터 내에서 규칙을 위반하고 있는 문장과 그렇지 않은 문장의 비율이 동일하지 않기 때문에, 각 모델간에 차이를 발생 시킬 수 있는 데이터만을 중점으로 하여 도출한 결과값이 각 모델간 차이를 보는데 좀 더 유의미할 것이라고 생각되었다.
종합하자면, MWE의 인식률 향상을 위해서는 MWE의 언어학적 특성에 대한 지속적인 연구가 우선되어야 한다. 이러한 연구 결과를 통해 그 특성을 더욱 정교하게 시스템에 적용할수록 MWE의 인식률은 물론 전체 언어처리의 정확도도 함께 향상될 것이다.Chapter 1 Introduction 1 Chapter 2 Linguistic Properties of MWEs 4 2.1 Idiomaticity of MWEs 5 2.2 Idioms and Collocations Related to MWEs 8

감성 분석은 주어진 글이 가리키는 대상이나 화자의 의견 또는 평가를 분석하는 분야로 글의 주체가 가진 의견이나 감성은 수치적으로 파악하기 어렵기 때문에 자연어 처리에서도 어려운 분야로 여겨져 왔다. 감성 분석 분야는 최근 급부상한 빅데이터의 열풍과 함께 선거나 광고, 마케팅 등의 영역에서 사람의 직관에 의존하던 부분들을 대체하여 체계적이고 정밀한 분석을 하는 데에 큰 성과를 이루고 있다.
본 연구는 네이버 영화 리뷰 데이터와 Rotten Tomato Movie Review 데이터를 이용하여 순환신경망 모델을 중심으로 글이 긍적적인 판단을 내리고 있는지 부정적인 판단을 내리고 있는지를 효과적으로 분류하는 방법에 대한 내용을 다룬다. 텍스트를 컴퓨터가 이해할 수 있는 형태인 벡터로 변환하는 과정에서 문서를 자모, 글자, 단어, 형태소로 나누는 방법을 제안하고 각각을 감성 분석 모델의 입력으로 사용할 때 어떠한 변환 방법이 가장 좋은 성능을 내는지를 비교해 본다.
감성 분석 모델로는 기존의 순환신경망을 개선한 LSTM, Bi-directional LSTM 과 여기에 Attention Mechanism 를 적용한 모델을 사용하였고 이것을 비 신경망 모델인 Naïve Bayes Classifier 와 비교해 보았다.
네이버 영화 리뷰 데이터에서는 형태소 기반으로 문서를 나누었을 때 Bi- directional LSTM 에 Attention Mechanism 을 적용한 모델이 가장 우수한 성능을 보였고 Rotten Tomato Movie Review 데이터에서는 단어기반으로 Bi- directional LSTM 에 Attention Mechanism 을 적용한 모델이 가장 우수한 성능을 보이는 것을 확인하였다. 성능의 차이가 발생하는것은 훈련 데이터에 등록되지 않은 검증 데이터의 미등록어 비율이 주요한 원인으로 작용하는 것을 확인하였고 특히 한글의 경우 단어 단위로 문서를 나누었을 때 미등록어 비율이 현저하게 높아지는 것을 확인할 수 있었다.
분류와 더불어 Attention Mechanism 모델에서 각 문서에 대해 시퀀스의 어떤 시간 스텝에서 Attention Vector 가 활성화되는지를 분석하였고 이를 바탕으로 어떠한 형태소가 감정 판단을 하는데 영향을 끼치는지 정량적으로 분석할 수 있었다.제 1 장 서 론 1 제 2 장 관련 연구 4 2.1. 순환신경망 4 2.2. LSTM 7 2.3. Bi-directional LSTM 9

본 논문에서는 자연어 처리를 위한 여러 가지 툴 중에서 그 효용성이 날로 증대되고 있는 품사 태킹 시스템 (Part of Speech Tagging System)에 관하여 기술한다. 품사 태킹 시스템의 주 기능은 영어 문장의 각 단어에 대해서 각 단 어에 대해서 각 단어가 해당 문장에서 가질 수 있는 최적의 품사를 결정하는 것이다. 품사 태킹은 Corpus Annotation의 가장 기본적이고도 전형적인 형태 이다. Corpus는 어떤 정해진 원칙에 근거한 택스트나 텍스트들의 샘플들의 집합으로 정의된다. 최근 On-Line Corpus의 등장에 힘입어 Corpus Annotation의 필요성이 급격히 증대되고 있는 실정이다. Annotated Corpus가 요구되는 기본적인 정보를 제공할 수 있기 때문이다. 우리가 생각 할수 있는 대표적인 응용의 예로서는 음성합성, 음성인식, Spelling Correction, 교정 (Proof-reading), 질의응답, 기계어 번역, 정보검색 등이 있다. 본 논문에서는 일련의 자연어 처리를 위한 툴의 필요성을 충족시킴과 아울러 태킹된 Corpus의 잠정적인 응용 분야들을 위한 기초를 다지기 위한 강력한 성능의 품사 태킹 시스템 구축을 위한 제반 고려 사항들을 기술하고 이의 구현에 대해서 설명한다. IITagger는 기본적으로 규칙기반 (Rule-based)시스템으로, 태킹을 위한 기초 판단 기준으로 Context-Free Grammar를 사용하며 가능한 문법의 집합과 Corp us의 크기를 줄이기 위하여 Suffixes 정보를 최대한 이용한다. 그리고 Ambiguity 문제를 해결하는 방법으로는 기본적인 Top-down 파싱과 Bottomup 파싱기 법을 혼합한 Hybrid 파싱기법을 이용하였다. Ambiguity 해소를 위하여 파싱기법을 사용하지만 구조적인 Ambigutity는 전혀 고려하지 않는다. 왜냐하면 우 리가 필요로 하는 정보는 품사에 관한 것이지 구문 구조에 관한 것이 아니기때문이다. 즉 단순히 파싱의 개념만을 이용한다고 할수 있다.

단어 임베딩은 자연어로 이루어진 단어를 실수 차원의 벡터에 대응시키는 과정으로, 개체명 인식, 기계 번역, 문장 분류 등 다양한 인공지능 및 자연어 처리 분야의 기반으로 활용된다. 하지만 단어 임베딩과 관련한 기존 연구들은 대부분 영어를 비롯한 라틴어 기반 언어만을 대상으로 진행되어 왔고, 한국어와 같이 단어의 형태가 단어의 의미와 연관되어지는 교착어의 특성을 가진 언어들은 기존 연구를 그대로 적용하기에 한계가 존재한다.
본 연구에서는 분산 가정을 바탕으로 한 기존 단어 임베딩 학습 방식의 토대를 그대로 유지하면서도, 음절 단위 기반의 컨벌루션 신경망을 적용하여 단어의 형태를 임베딩 학습 과정에 반영하는 새로운 단어 임베딩 학습 모델을 제시한다. 또한 모델의 복잡성 증대로 인해 상승하는 학습 비용을 줄이고, 계산 효율성을 높이는 중복단어 동시 처리와 배치 재배열 방법에 대해 소개한다.
본 연구는 단어 임베딩의 일반적인 평가지표인 단어 유사도, 이웃 단어 비교 테스트에서도 기존 모델에 비해 뒤지지 않는 모습을 보여주는 한편, 오타나 신조어등 말뭉치에 존재하지 않는 단어 벡터에 대한 학습을 가능하게 하였고, 실제 실험 결과를 통해 그 성능 역시 상당히 뛰어남을 입증하였다. 그리고 기존의 모델들이 단어와 결합되는 조사의 의미를 제대로 학습하지 못하는 단점 또한 해결하였다.제 1 장 서론 1 1.1 연구의 배경 및 내용 1 1.2 논문의 구성 2 제 2 장 배경 지식 4

본 논문은 의미 네트워크를 이용한 자동 문서 요약 방법에 대해서 기술한다. 문서 내에 쓰여진 모든 단어들에 대해 의미적으로 연관 관계에 있는 단어들을 집합으로 묶어 어휘 사슬을 생성하고, 주제어 구를 추출하기까지 문서를 요약하는 과정은 최초, 원문 내의 모든 단어를 토큰(token)으로 분리하고, 토큰들에 대해 단어의 품사 중 키워드가 될 수 있는 명사만을 추출하는 전처리 과정으로부터 시작한다. 전처리 과정을 통하여 걸러진 명사들은 의미 네트워크로 널리 알려진 wordnet을 이용하여 의미적으로 관련이 있는 단어들에 대해 어휘 사슬(Lexical Chain) 집합을 구성하고, 이렇게 생성된 어휘 사슬들에 대해 스코어링(scoring) 알고리즘을 적용함으로써 주제어 구를 추출하게 된다. 여기서 스코어링 알고리즘이라 함은 의미 분석을 통해 생성된 어휘 사슬에 대해 빈도수, 어휘들 간의 의미적 관계를 고려하여 가중치를 부여함으로서 단어가 갖는 중의성을 해소하고, 주요한 어휘 사슬만을 추출해내는 것으로 Barzilay와 Elhadad는 직관에 의한 실험을 기반으로 적용하였다. 그러나, 이러한 스코어링 알고리즘은 어휘 사슬을 형성하는 대표 단어의 빈도수에 의해 요약문이 길어질 수 있다는 단점이 있다. 따라서 본 논문에서는 어휘 사슬에 대해 가중치를 주지 않고, 어휘 사슬을 형성하는 대표 단어를 포함하는 문장에 대해 핵심 문장 추출 알고리즘을 적용함으로 보다 효과적으로 요약문을 생성할 수 있는 방안을 제시한다. 최종적으로 생성되어진 요약문은 기존의 스코어링 알고리즘에 의한 요약문과 비교하고, 최종적으로 사람이 수동으로 추출한 요약문과의 일치도를 이용하여 정확성을 평가하였다.This paper describes the method of automatic summarization by using semantic networks. Lexical chain is created from combination of semantically related words that is used in the document. Document summarization procedure to retrieve key phrase starts from preprocessing that divides all the words in a document as tokens and extraction of nouns to be a keyword among part of speech of words. The nouns that is filtered from the preprocessing procedure creates lexical chain from semantically related words. Application of scoring algorithm to the generated lexical chains generates key phrases. This scoring algorithm gives weights on lexical chain by semantic analysis based on frequency and the semantic relation in the vocabulary. It solves ambiguity of words and retrieves important lexical chain. Experimentally, Barzilay and Elhadad applied this. Scoring algorithm has defects on the truth that it may create long summarization by the frequency of keyword which is used for lexical chain. This thesis suggests a method to create a summarization based on statical algorithm for a sentence which includes keyword that is used for lexical chain.
Finally generated summarization is compared to the summarization from the scoring algorithm. It is compared to manually generated summarization to check accuracy.목차 Ⅰ. 서론 = 1 Ⅱ. 관련 연구 = 3 1. 의미 네트워크 (wordnet) = 5 2. 어휘 사슬 (Lexical Chain) = 6

모바일 기기가 발전함에 따라 소셜 네트워크 서비스는 생활에 깊게 파고들어 사회 이슈 및 여론의 조성에 까지 영향을 끼치며, 개인의 의견 표출의 장이 되기도 한다. 특히 오프라인의 인간관계와 영향이 적은 트위터는 각종 사회 정치 이슈 및 특정 제품 또는 인물에 대한 언급이 잦다. 트위터를 통한 트렌드 분석, 제품 수요의 분석 등은 지속적으로 연구되어 사용자에게 서비스로 제공되고 있다.
이처럼 트위터의 데이터를 활용하고자 하는 움직임은 계속 있어왔으나, 트위터를 통한 감성분석에 대한 연구는 활발하지 않은 실정이다. 감성 분석은 데이터의 통계화에 그치지 않고 패턴을 파악하거나 극성을 계산하여 사용자의 인식을 파악할 수 있는 기술이다. 국내에는 이런 감성 분석 알고리즘에 대한 연구가 부족할 뿐 아니라 감성 사전의 구축이 미비하고 대부분 기계학습을 통한 감성 분석에 치중하고 있기 때문에 한국어 문법에 기초한 감성 분석 알고리즘의 연구는 미흡하고 부족한 실정이다.
본 논문에서는 이러한 부분을 고려하여 트위터 데이터의 활용과 한국어 감성 분석을 위한 알고리즘 및 감성분석 시스템을 제안한다. 주제어와 관련된 트윗의 문장의 극성을 문장과 형태소, 단어 단위로 분석하고 각 극성을 계산함으로서 문장 구성이 단순하고 짧은 트위터 데이터 분석에 최적화한 감성 분석 알고리즘을 제시하였다. 극성 분석을 위하여 한국어 감성 사전의 구축이 요구되었기에, SentiWordNet이라는 영어 감성사전의 데이터를 추출하여 제시한 알고리즘에 맞게 재구성하고 경량화 한 뒤 한국어로 번역하여 특정 분야에 특화되지 않은 범용적인 한국어 감성 사전을 구축하였다.As the development of mobile devices, social networks are affecting the composition of example to dig deeply into social life issues and public opinion must also be the head of an expression of personal opinion. Twitter in particular the relationships and the influence of offline is much less mention of the various socio-political issues and the specific product or person. The sentiment analysis of Twitter data is huge active in foreign country, but this Korean sentiment analysis based on Twitter data is a study on the inactive situation.
The sentiment analysis algorithms to control the lack of pre-deployment and are focused on the sentiment analysis with machine learning research, Sentiment analysis is one of the most important technique on web marketing and political society, but sentiment analysis algorithm based on Korean grammar is inadequate and insufficient state.
In this paper, we proposed an algorithm for Korean sensitivity analysis using Twitter data . By analyzing and calculating the polarity of the main sentence of tweets related to the proposed unit ‘morpheme’, sentiment analysis algorithms are optimized for simple and short sentence configurations such as Twitter data analysis.I. 서 론 1 1. 연구 배경 및 목적 1 2. 연구 범위 및 구성 3 II. 관련 연구 4 1. 감성 분석 관련 연구 및 동향 4

텍스트 마이닝(Text Mining)이란 자연어로 구성된 비정형 텍스트 데이터 에서 패턴 또는 관계를 알아내어 의미 있는 정보를 찾아내는 것으로써 사람들이 말하는 언어를 이해할 수 있는 자연어 처리(Natural Language Processing)에 기반을 둔다(Wikipedia, 2014).
본 연구에서는 텍스트 마이닝을 이용하여 영화의 흥행여부에 대한 내용으로 주로 다루고 있고, 사용된 데이터는 포털 사이트 ‘D’사와 영화진흥위원회의 영화에 대한 리뷰 데이터, 평점평균 그리고 스크린 수이다.
텍스트 마이닝과 관련된 기존의 연구는 영화흥행 여부를 종속변수로 하고, 영화를 본 소감(즉, 댓글 또는 리뷰)을 정제하여 종속변수에 영향을 줄 수 있는 단어들을 추출하였다. 그리고 영화와 각 영화에서 추출된 단어의 빈도로 구성된 행렬을 SVD(Singular Value Decomposition)하여 얻어진 변수를 설명변수로 사용하여 로지스틱 회귀분석을 실시하였다(김상화 외 3명, 2012).
본 연구에서 사용하고 있는 기존 방법의 데이터 구조는 47편의 영화에 대해서 10개의 변수로 의 행렬구조로 구성되어 있다. 여기서 47편의 영화는 2013년부터 2014년까지 흥행영화 34편과 비 흥행영화 13편을 선정하였고, 10개의 변수는 각 영화에서 나오는 단어들의 빈도가 100개 이상인 단어들로 구성하였다. 본 연구에서 제안하고자 하는 방법에 대한 데이터 구조는 기존 방법의 데이터 구조에 2개의 변수(평점평균과 스크린 수)를 추가 한 의 행렬구조이다.
본 연구의 목적은 기존 방법의 SVD에 종속변수에 영향을 줄 것이라고 판단되는 변수(평점평균과 스크린 수)를 추가하여 로지스틱 회귀분석을 실시한 결과와 기존의 방법을 정확도 측면에서 비교해 보는데 있다. 로지스틱 회귀분석에 의한 변수선택법은 단계별변수선택법을 사용하였다. 변수선택 결과 기존의 방법은 SVD1과 SVD3이 선택되었고, 제안방법은 기존의 방법에 의해 선택된 변수들 SVD1과 SVD3과 평점평균이 선택되었다.
로지스틱 회귀분석에 의한 두 방법의 우위성 비교결과, 기존의 방법에 대한 정분류율은 89,36%, 본 연구에서 제안하는 방법에 대한 정분류율은 93.62%로 얻어져 본 연구에서 제안하는 방법이 기존의 방법보다 더 좋은 결과를 보였다.Ⅰ. 서 론 1 1.1 연구 배경 및 연구 목적 2 Ⅱ. 분석방법의 이론적 배경 3 2.1 텍스트 마이닝(Text Mining, TM) 3 2.1.1 텍스트 마이닝 과정 4

자연언어처리는 컴퓨터로 언어를 분석하는 분야이다. 자연어처리 분야에서는 인간이 사용하는 언어를 처리하기 위하여 다양한 언어 정보를 이용한다. 따라서 자연어 처리를 위해서는 언어 현상을 관찰하고 이로부터 언어 정보를 추출하는 작업이 요구된다.
언어 모델링은 언어에 존재하는 규칙성을 찾고 특성화하는 과정을 의미하며 이를 통해 구축된 언어모델은 음성인식(speech recognition), 문자인식(character recognition), 기계번역(machine translation)의 분야에서 이용된다. 언어모델은 현재의 문맥에서 다음 단어가 올 확률을 줌으로써 가장 적절한 다음 단어를 선택할 수 있는 수단을 제공한다.
언어 모델의 성능은 교차 퍼플렉시티(cross-perplexity)를 이용하여 측정하는데 이는 정보 이론에서 말하는 교차 엔트로피(cross-entropy)의 변형이며 Shannon-McMillan-Brieman의 정리에 근거하여 계산된다. 쉽게 말하면 퍼플렉시티는 문장을 만들어 나가는 과정에서 다음에 출현할 수 있는 단어들의 종류가 얼마나 되느냐를 나타내는 것으로 이 값이 낮을 수록 좋은 언어모델이라 할 수 있다.
이러한 퍼플렉시티를 낮추기 위한 방법으로 그동안 많은 연구가 있었다. 하지만 아직까지는 뾰족한 해결방안이 나오지 못한 채 여러 방법을 계속 시도하고 있는 상황이다. 국내 자연언어처리 분야는 아직 많이 개척되지 못한 분야로 한국어가 "교착어"라는 특성으로 인하여 영어에 비해 매우 어려워 많은 방법이 시도될 수 있다.
이런 언어모델의 활용분야 중 하나인 소규모 음성 인식, 특히 대 어휘 연속 음성 인식의 경우에는 음향 모델도 중요하지만 언어 모델 또한 이에 못지않게 중요하다. 그러나 현재 음성 인식을 위한 언어 모델에 사용할 수 있는 충분한 크기의 양질의 말뭉치가 없다는 것이 음성 인식의 큰 걸림돌이 되고 있다. 또한 음성 인식은 도메인에 따라서 언어 모델이 차이가 있기 때문에 수많은 종류의 음성 언어 DB가 필요하다. 음성 DB의 구축을 추진한다 하여도 완료까지는 상당한 시일과 비용이 소요되며 또한 구축된다고 하더라도 음성 데이터에 의한 말뭉치의 구축은 그 규모에 있어서 한계를 가지고 있다. 또한 대규모의 말뭉치가 없는 것은 아니지만 일반적으로 존재하는 대규모의 말뭉치는 문어체의 말뭉치이기 때문에 그 통계적 특성이 매우 상이하다는 문제점을 가지고 있어 좋은 언어 모델의 구축이 불가능하다. 따라서 두 종류의 말뭉치, 즉 소규모의 동일 도메인 말뭉치와 대규모의 타 도메인 말뭉치의 장점만을 살릴 수 있는 말뭉치 처리 기술을 개발하여 문제들을 해결할 수 있다. 이렇게 두개의 말뭉치를 가지고 이들의 장점을 살리는 말뭉치 처리 기술을 dual-source backoff라고 명명한다. 이를 이용하여 1차 실험을 해본 결과 대략 20% 정도의 퍼플렉시티 감소(향상) 효과를 거두었다. 이는 매우 성공적인 결과이다. 그러나 아직 많은 개선의 여지가 있다.
본 논문에서는 이러한 여러 개선 가능성중 discounting과 관련된 부분에 대해 생각해보고 이에 대한 실험으로 과연 어떤 결과를 얻을 수 있는지 알아보고자 한다.This thesis describes the research results that explored the possibility of using two different corpora for reducing the cross perplexity of the language model. One corpus is assumed to be in the same domain as the test corpus. However, the size of the corpus is unacceptably small, roughly in the range of several millions of words at most. The other corpus is rather huge - in the order of 10 to 100 billion; but from different domain. The two domains that were used in the experiments are broadcast news scripts and newspaper articles. In order to find ways for combining these corpora, statistical analysis is performed first. In doing so, we found many interesting features. Based on the statistical observations, we designed a host of different methods of combining two corpora. Among them two methods were successful. One method, called the dual-source backoff, resulted in about 20% reduction in perplexity. The other, called over-discounting, resulted in small reduction in perplexity. The result is a huge success in that the original aim was 10% reduction with respect to the original corpus, in which measure our enhancement would be translated to more than 30%.목차 국문초록 제1장. 서론 = 1 제1절. 자연언어 처리 단계 = 1 제2절. 언어모델 기술 동향 분석 = 2

딥 러닝의 대한 관심이 높아지면서 자연어 처리 연구에도 딥 러닝을 활용한 연구들이 활발하게 진행되고 있다. 자연어 처리에서 딥 러닝을 사용하려면 문자인 자연어를 어떻게 컴퓨터가 이해하는 심볼로 표현하는 것이다. 단어를 하나의 심볼로 표현하는 방법을 단어 표현이라 한다. 그 중에 벡터 공간에 단어 표현하는 방법에 대해 많은 연구가 진행되고 있다.
벡터 공간에 단어를 표현하기 위해 말뭉치와 인공신경망을 사용한다. 말뭉치에서 단어의 주변 단어를 이용하여 학습 데이터를 생성하며 인공신경망을 통해 단어를 벡터 공간에 표현한다. 말뭉치를 이용한 단어 임베딩은 학습에 사용된 말뭉치에 영향을 받는다. 학습에 사용하는 말뭉치의 크기에 따라 학습할 수 있는 단어의 수가 결정된다. 또한 말뭉치에서 단어의 등장 빈도수에 따라 학습의 결과가 달라지는 문제점을 가지고 있다.
본 논문에서는 사전과 한국어 어휘 지도(UWordMap)을 활용하여 한국어 단어 임베딩을 실시한다. 사전에 등장하는 단어의 뜻풀이에서 일반명사, 고유명사, 형용사, 동사를 추출하여 학습에 사용한다. 어휘 지도에서는 명사 단어의 상위어 정보와 반의어 정보를 사용한다.
사전과 어휘 지도를 통해 생성한 학습데이터는 자질 거울 모델을 사용하여 학습한다. 자질 거울 모델은 인간이 알지 못하는 단어를 이해하기 위해 사전을 찾아 보는 것에 영감을 받은 모델이다. 인간은 사전에 등재된 단어의 뜻풀이를 통해 단어의 의미를 파악하며 뜻풀이 안에 등장하는 단어에 대해서도 사전의 정보를 이용하면 이해할 수 있다. 이러한 인간이 처음 보는 단어를 이해하는 방식을 학습 모델로 만든 것이 자질 거울 모델이다. 자질 거울 모델을 사용해 인간이 학습하는 방식처럼 단어를 벡터화 한다.
사전과 어휘 지도를 이용하여 동형이의어 수준의 한국어 단어 임베딩을 실시했다. 표준국어대사전에 등재된 명사, 동사, 형용사, 부사, 조사 단어를 대상으로 학습하며 총 408,739개의 단어를 학습한다. 단어에 대한 유의어 검색 실험에서는 단어의 사전적 의미와 유사한 단어들이 높은 코사인 유사도를 가지는 것을 볼 수 있다.
동형이의어 수준에서 단어가 가지는 다의어로 인해 유의어 검색에 어려움을 가진다. 뜻풀이가 많아질수록 단어의 의미 분별이 어렵기 때문에 해결을 위해 다의어 수준 단어 임베딩을 실시했다. 다의어 수준으로 명사, 동사, 형용사, 부사, 조사 단어를 대상으로 총 475,058개의 단어를 학습했다. 다의어 수준의 단어 임베딩 실험은 각 단어의 유의어 검색을 통해 동형이의어 단어 임베딩과 비교했다. 비교 결과 동형이의어 수준의 단어 임베딩의 유의어가 복합적으로 나타나는 반면 다의어 수준 단어 임베딩은 다의어 단어 하나가 가진 의미에 대한 유의어만 등장했다. 다의어 수준 단어 임베딩을 통해 단어에 대해 세분화된 벡터로 표현이 가능했다. 하지만 사전 기반 단어 임베딩에서 유의어로 등장하는 단어들은 실생활에서 쓰는 단어가 아니며 실제 사용 빈도수가 매우 낮거나 적은 단어들이 등장하는 문제점을 가지고 있다.
말뭉치 기반에서 반의어 관계에 있는 단어 쌍은 문장에서 위치상 비슷하기 때문에 벡터가 유사하게 표시된다. 반의어 단어 쌍의 주변 단어들도 유사하게 표시되는 문제를 해결하기 위해 어휘 지도에 포함된 단어의 반의어 정보를 사용한다. 반의어 실험은 말뭉치 기반의 Skip-gram과 반의어를 학습에 포함하지 않은 모델과 비교한다. 반의어 학습을 통해 두 단어의 하위 집단에 대해 분별이 어려운 문제를 해결이 가능했다. 그리고 부정형 뜻풀이의 개선을 통해 뜻풀이에서 추출하지 않은 부정형 보조 용언에 대한 고려가 가능하며 정확한 단어의 의미를 벡터에 내재하는 것이 가능했다.Word representation is a method of representing each of words in a text with distinguishable symbols. One of the Word representation is to represent in vector space. The Vector space word representation uses corpus and neural network to express words. Corpus-based word embedding employed many corpora to ensure that words that were positioned nearby in text would also be in close proximity in vector space. However, corpus-bases word embedding is affected by the frequency of word occurrence and has a problem learning.
In this paper, word embedding is done using dictionary and Korean Word Map(UWordMap).
Word learning is done using dictionary definitions and semantic relationship information (hypernyms and hyponyms) in UWordMap. Word definitions and semantic relation information rather than corpora to solve the problem of word representation learning in relation to low-frequency words or polysemy: a typical problem in corpus-based models. Words are trained using the feature mirror model, a modified Skip-Gram.
Since words from dictionary are represented via vectors, homograph of words was distinguished so that 408,739 words of four parts of speech (noun, verb, adverb, and adjective) were represented via a vector. In the test of finding the synonyms of words, words with similar dictionary definitions appeared.
To solve the difficulty of semantic discrimination at the homograph word embedding, the polysemy level of word embedding was done. Polysemy of words was distinguished so that 475,058 words of four parts of speech (noun, verb, adverb, and adjective) were represented via a vector. In homograph word embedding, the surrounding words of various meanings appeared. However, in polysemy word embedding, a word containing one meaning appeared around.
The antonyms were not considered in the corpus-based word embedding. To solve the problem of antonyms, using antonym information in Korean Word Map (UWordMap). Since antonyms of words are also learned, words can be distinguished whose definitions or hypernyms and the same. Improved word definition of negative form, the vector could have the meaning of the correct word.목차 1 서론 1 1.1 연구 배경 1 1.2 관련 연구 4 1.2.1 벡터 공간 단어 임베딩 4

딥러닝과 클라우드 컴퓨팅 기술의 발전으로 인간의 자연어 의미를 이해할 수 있는 기술이 발전하였고, 사용자의 음성 명령을 인식하여 다양한 서비스를 제공하는 음성인식 스피커 시장이 성장하고 있다. 사용자는 음성인식 스피커에 자연어 명령을 하기 위해 먼저 디바이스에 정의된 키워드를 입력하여야 하며, 이것을 키워드 인식(Keyword Spotting) 기술이라 한다. 이 기술은 음성인식 스피커의 대기 소비 전력과 자연어 처리를 위한 클라우드와의 통신량을 줄일 수 있어 대부분의 음성인식 기기에 적용되고 있다. 특정 키워드를 인식한 이후 음악 감상, 검색, 홈 제어, 쇼핑 등의 다양한 서비스가 제공된다.

현재의 키워드 인식 기술은 해당 키워드의 인식률을 높이기 위한 연구들이 주로 이루어졌고, 이를 통해 다양한 노이즈 환경에서도 높은 성능을 보이며 제품에 적용되고 있다. 하지만, 음성인식 스피커에는 하나의 사용자 계정이 등록되어 패밀리 디바이스로 사용됨에 따라 다른 사용자도 키워드 인식만으로 개인 데이터와 정보에 쉽게 접근할 수 있어 보안 취약의 문제점이 발생하고 있다. 이는 개인정보를 기반으로 하는 쇼핑, 금융 등의 서비스 확대에 문제점으로 지적된다. 이러한 음성인식 스피커의 개인정보 보호와 개인화 서비스를 위해서는 사용자 인식(Speaker Recognition) 기술이 필요하게 되었다.

최근 임베디드 디바이스에서도 딥러닝 연산의 최적화를 위한 하드웨어와 소프트웨어 기술들이 발전하였다. 이러한 기술의 발전으로 디바이스 기반의 사용자 인식 시스템은 GPU 가속기를 지원하는 환경에서 모델 학습이 디바이스 내에서 직접 이루어 지고, 완성된 모델을 이용하여 사용자 여부를 판단(inference)한다. 이것은 외부의 네트워크 연결로 인한 제약이 배제되어 사용자 인증의 지연(latency)을 최소화 할 수 있다. 하지만, 임베디드 시스템의 한정된 시스템 자원과 학습을 위한 데이터가 희소함에 따른 한계가 존재한다. 이러한 한계를 해결하기 위하여 본 학위 논문에서는 딥러닝 모델 최적화를 위한 전이학습(Transfer Learning) 기법과 전 처리를 수행하는 CPU와 모델 학습을 수행하는 GPU의 데이터 병렬화 기법인 데이터 입력 파이프라인(input pipeline) 구조를 적용함으로써 디바이스 기반 사용자 인식 모델 학습 시 전체 학습 시간을 단축하고자 하였다.

딥러닝 학습의 데이터 입력 파이프라인(Data Input Pipeline) 구조를 설계함에 있어, 전 처리 과정을 수행하는 다중 쓰레드(Thread) 수가 정의되어야 한다. 하지만, 이는 딥러닝 기반 사용자 인식 모델 학습의 end-to-end latency 분석이 현실적으로 어렵기 때문에 어느 정도의 다중 Thread를 정의하여야 할지에 대한 예측이 불가능하다. 만약 너무 적은 수의 Thread로 전 처리 과정을 수행한다면 GPU 학습을 위한 데이터 셋 전달에 병목현상(bottleneck)이 발생하게 되어 GPU 가속기의 idle 상태가 증가한다. 이로 인하여 GPU utilization을 낮아지게 되고, 결과적으로 전체 학습 시간은 늘어나게 된다. 반면 너무 많은 수의 다중 Thread를 전 처리에 선정하게 되면, 전 처리 과정을 빠르게 진행하여 GPU 학습을 위한 충분한 데이터를 전달하게 되지만, 과도한 다중 Thread로 인한 빈번한 컨텍스트 스위칭(Context switch)와 학습 데이터 큐(Training Data Queue)에 접근 경합 등의 문제를 발생시켜 시스템 자원의 오버헤드(Overhead)를 유발하고 이것은 GPU 가속기의 학습 지연 문제를 일으킨다.

본 학위논문에서는 제시된 문제를 해결하기 위해, 적응형 쓰레드 큐잉(Adaptive Thread Queuing) 알고리즘을 제안한다. 이 알고리즘은 한정된 시스템 자원을 효율적으로 관리하면서 최적의 학습 속도를 유지하고자, 학습 과정의 GPU utilization 예측 정보를 기반으로 런타임에 병목현상과 시스템 오버헤드가 발생할 것을 예측하고, 알고리즘 수행 정책에 따라 전 처리를 수행하는 다중 Thread의 수를 동적으로 변경하여 병목현상과 시스템 오버헤드로 인한 학습 과정의 지연을 단축하는 방법이다. 실험을 통해 제안 알고리즘을 검증한 결과, 학습 과정에서 CPU의 자원을 효율적으로 사용하면서 모델 학습을 위한 GPU utilization은 최대로 유지되었다. 그 결과, 제안된 Adaptive Thread Queuing 알고리즘은 고정된 수의 다중 Thread 학습법 대비, 전 처리 과정에서 발생할 수 있는 지연의 문제를 개선하면서 모델의 전체 학습 속도가 향상되는 것을 확인하였다.With the development of deep learning and cloud computing technology, the technology to understand the meaning of human natural language has developed, and the speech recognition speaker market which provides various services by recognizing voice commands of users is growing. The user should firstly input a keyword defined in the device in order to perform a natural language command on the speech recognition speaker, and this is called keyword spotting technology. This technology is applied to most voice recognition devices because it can reduce standby power consumption of voice recognition speakers and communication with cloud for natural language processing. After recognizing a specific keyword, various services such as music appreciation, search, home control, and shopping are provided.

Currently, keyword recognition technology is mainly used to increase the recognition rate of the keyword, and it is applied to products showing high performance in various noise environments. However, since a single user account is registered in a voice recognition speaker and is used as a family device, other users can easily access personal data and information only by recognizing a keyword, thereby causing a problem of security vulnerability. This is pointed out as a problem in expanding services such as shopping and finance based on personal information. Speaker Recognition technology is required for personal information protection and personalization service of these speech recognition speakers.

Recently, hardware and software technologies have been developed for the optimization of deep learning operations in embedded devices. With the development of these technologies, device-based user recognition systems are model training directly in a device that supports GPU accelerators, and inference is made using the completed model. This can minimize the latency of user authentication by eliminating the restriction due to external network connection. However, there are limits to the limited system resources of the embedded system and the scarcity of data for model training. To solve these limitations, this thesis proposes a transfer learning method for optimizing the deep learning model, a data input pipeline, which is a data parallelization technique of the GPU that performs preprocessing CPU and model training, We have tried to shorten the total training time when device-based user recognition model.

In designing the data input pipeline structure of the deep learning training, the number of multiple threads performing the preprocessing process should be defined. However, it is impossible to predict how many threads should be defined because end-to-end latency analysis of deep learning based user awareness model training is practically impossible. If the preprocessing process is performed with too few threads, a bottleneck occurs in the delivery of the data set for GPU training, which increases the idle state of the GPU accelerator. This lowers the GPU utilization, which in turn increases the overall training time. On the other hand, if too many threads are selected for preprocessing, the preprocessing process proceeds quickly and delivers sufficient data for GPU training. However, because of frequent context switching due to excessive multithreading and synchronization contention to the Training Data Queue those causes overhead of system resources, which causes training delay problem of GPU accelerator.

In this thesis, we propose an Adaptive Thread Queuing algorithm to solve the presented problems. This algorithm predicts bottleneck and system overhead to occur at runtime based on GPU utilization prediction information of training process in order to maintain optimal training speed while efficiently managing limited system resources. To reduce the delay of training process due to bottleneck and system overhead. Experimental results show that GPU utilization for model training is maximized while CPU resources are used efficiently in the training process. As a result, the proposed Adaptive Thread Queuing algorithm improves the overall training speed of the model while improving the problem of delay that can occur in the preprocessing process, compared to a fixed number of multi - thread training methods.제 1 장 서 론 1 제 1 절 연구 동기 4 제 2 절 연구 내용 6 제 3 절 논문 구성 8

자연언어처리 분야가 차세대 사용자 인터페이스로서 각광받으면 많이 연구되고 있는 실정이다. 자연어처리를 올바르게 분석하고 이해하기 위해서는 자연어에서 발생하는 모호성을 해결하는 과정이 반드시 요구된다. 이 과정을 태깅이라하고 태깅은 주로 형태소단위의 품사 태깅을 한다. 품사 태깅 방법으로는 규칙을 이용하는 방법, 확률을 이용하는 방법 그리고 규칙과 확률을 이용한 혼합형 방법 있다.
말뭉치에 의존하지 않고 태깅의 정확률을 높이기 위해서 통계정보에서 높은 정확률을 갖는 품사 태깅 시스템이 필요하다 통계정보로 태깅의 정확률을 높이려는 시도가 드문 이유 중에 하나는 양질의 대용량 말뭉치가 없었기 때문이다.
본 논문에서는 21세기 세종계획에서 2001년도에 만든 말뭉치를 이용하여 통계 정보를 보다 효율적으로 이용하는 품사 태깅 시스템 만들었다. 품사 태깅 시스템에는 기존 Viterbi알고리즘을 사용하고 bigram을 확률값을 이용했다. 본 논문에서 제안하는 확장 Viterbi알고리즘은 trigram을 이용하는데 단순히 bigram을 trigram으로 대치한 것이 아닌 trigram을 이용한 새로운 Viterbi알고리즘을 제안한다. 이번 실험에는 세종 말뭉치를 이용했고 실험결과 확장 Viterbi알고리즘 기존 Viterbi알고리즘 보다 40%의 에러률을 감소시킬 수 있었다.There has been two paradigms for part-of-speech(POS) tagging: Rule-based lagging and Stochastic tagging. The rule-based tagger has advantages of high accuracy and consistency of tagging results. Furthermore, it is easy for human to understand and analyze tagging results. But it has several serious problems. It needs intensive labor to acquire disambiguation rules and its coverage of tagging is limited to core linguistic phenomena. We cannot make every rule to cover all the various and exceptional linguistic phenomena. In the contrary, the stochastic tagger is robust to the various linguistic phenomena whereas the accuracy of tagging results is lower than tile rule-based tagger. This is because the stochastic tagger selects tile most probable result by using probabilities estimated from the corpus which rarely represents the real world language.
In this paper, we propose Viterbi algorithm of trigram. Viterbi algorithm of trigram show about 97% accurate results of tagging on sejong corpus but a existing Viterbi algorithm show about 95% accurate results.목차 = ⅰ 제1장 서론 = 1 제1절 연구의 목적 = 1 제2절 품사 태깅 = 5 제1항 개요 = 5

한글을 초성, 중성, 종성으로 분리하여, 여러 줄에 걸쳐 적어도 의미가 전달될 수 있으나 이 점은 기존 텍스트 필터링 기술로는 인식하기 어렵다는 문제점이 있다. 본 연구에서는 해당 문제를 해결하기 위해 문자를 텍스트가 아닌 이미지 형태로 처리하여 해결하고자 한다. 채팅창에 쓴 글에 금칙어가 존재하는지 검출하기 위한 선행 연구로 금칙어 이미지 분류 실험을 진행한다.
변형 금칙어 이미지를 분류하기 위해서는 각 클래스별 대량의 변형된 금칙어 데이터가 필요하다. 변형된 금칙어를 수집하는 데 한계가 있고, 수집된 데이터로 학습을 할 경우 새로운 금칙어에 대한 적용이 쉽지 않다고 판단했다. 그래서 변형 알고리즘을 연구하여, 입력된 단어에 대해 1만 개 이상의 변형된 단어를 출력하는 프로그램을 개발했다. 이후 각 클래스별 생성된 이미지로 CNN(Convolutional Neural Networks)을 사용하여 클래스를 분류하는 실험을 진행했다.
금칙어 이미지 분류 실험은 한국어 금칙어 140개, 영어 금칙어 69개의 클래스로 실험을 진행했고, 각 클래스에는 1만 개 이상의 변형된 금칙어 이미지 파일이 있다. 또한 변형되지 않은 일반 단어 클래스를 추가한 실험을 진행하여, 금칙어가 아닌 단어는 어떻게 분류하는지 실험했다.
본 연구에서 제시하는 이미지 학습 기반의 텍스트 필터링 기법은 속도가 중요한 온라인 채팅창에 또는 어플리케이션에 바로 적용하기에 무리가 있다. 따라서, 필터링 강도에 따라 기존 필터링 시스템에 추가적인 기법으로 사용한다면, 기존 필터링 시스템의 한계를 부분적으로 보완하기 위한 도구로 사용될 수 있을 것으로 판단된다.제1장 서론 1 1. 연구의 배경 및 목적 1 제2장 관련 연구 4 1. 자연어 처리 방식의 필터링 기법 4 2. 기계학습을 사용한 필터링 기법 8

딥 러닝(Deep learning)은 다 계층 인공신경망 기반의 기계학습 기술로서 최근 컴퓨터 비전, 음성인식, 자연어 처리 분야에서 인식 성능을 높이는 데 중요한 역할을 하고 있다. 딥 러닝을 이끄는 알고리즘에는 이미지 인식에 주로 사용되는 CNN(Convolutional Neural Networks)과 음성인식 및 자연어 처리에 주로 사용되는 RNN(Recurrent Neural Networks) 등을 이야기할 수 있다. 그중 CNN은 데이터로부터 자동으로 피쳐를 학습하는 대표적 이미지 인식 알고리즘이다. 딥 러닝은 GPU 성능과 함께 딥 러닝 알고리즘의 발전으로 기존 컴퓨터 비전 알고리즘에 비해 높은 성능을 보인다. 하지만 딥 러닝 기반 알고리즘으로만 사용하기에는 높은 연산의 복잡도가 필요하고, 성능이 제대로 나오지 않을 수 있다.
본 논문에서는 딥 러닝 모델 중 YOLO v2를 이용하여 무인 비행체를 탐지하고, 이에 더해 칼만 필터를 이용하여 성능을 향상하기 위해 추가로 결합하는 방법을 이용하였다. 또한, YOLO v2에서 검출이 실패하는 경우 칼만 필터를 이용하여 궤적 정보를 사용해 개선 방안을 추가해 성능을 개선했다. 이 알고리즘은 시간에 따라 진행한 측정을 기반으로 한다. 해당 순간에만 측정한 결과만 사용한 것보다는 좀 더 정확한 결과를 기대할 수 있다. 실험 결과 제안 방법과 YOLO v2와 비교하면 7.7%의 성능 향상 효과가 있었고, 속도는 4 fps 감소한 20 fps 결과를 보였다. 따라서 YOLO v2에서 검출이 실패해도 제안 방법은 실험 결과를 통해 성능이 향상됨을 확인할 수 있었다.목 차 Ⅰ. 서 론 Ⅱ. 관련 연구 2.1 CNN 기반 객체 검출 방법

소프트웨어를 테스트하는 방법에는 크게 블랙 박스 검사와 화이트박스 검사 두 가지로 설명할 수 있다. 블랙 박스 테스트는 소프트웨어 내부 구조나 동작에 신경 쓰지 않고 응용 프로그램의 기능을 검사하는 소프트웨어 테스트 방법이다. 화이트 박스 테스트는 애플리케이션의 내부 구조 또는 작동을 테스트 하는 소프트웨어 테스트 방법으로 소스 코드 전체 또는 일부에 접근해야 한다.
제조업체 또는 소프트웨어 업체에서는 제품 출시 전 외부에 테스트를 맡기는 경우가 많다. 그리고 대부분의 경우 소스 코드 액세스가 불가능하거나 매우 어렵다. 따라서 자연어로 된 테스트 케이스(TC, Test Case)를 활용한 블랙박스 검사를 진행하게 된다.
제품 출시 전 진행되는 블랙박스 검사의 경우 전체 시스템을 다양한 시각으로 시험하여 제품의 안정성을 높여야 하므로 매우 많은 수의 테스트 케이스가 존재한다. 하지만 테스트 환경과 같은 제한된 자원과 시간 제약으로 인해 모든 테스트 실행이 불가능하다. 따라서 테스트 케이스 선택하기 위한 방법들이 많이 연구되고 있다.
본 논문에서는 블랙박스 검사의 오류 검출과 동시에 다양한 테스트 케이스를 수행 할 수 있는 새로운 방법을 제시한다. 기존 시험 내역을 활용한 테스트 케이스 우선순위(점수), 유사한 테스트 케이스 제거 (중복 제거), 테스트 케이스 절차 유사도를 활용한 비슷한 테스트 케이스를 그룹화 (분류)를 통해 다양한 테스트 케이스를 선택하는 방법을 제시하였으며, 이를 이용하여 테스트 케이스 선택을 수행하였다. 실제 사용되는 네트워크 기지국 장비 테스트 케이스 및 테스트 이력 정보 데이터를 이용한 실험을 통하여 기존의 기존 점수 기반 모델 및 기존 연구보다 향상된 성능을 보임을 확인하였다. 또한 제품 출시 후 발견된 오류 정보를 테스트 케이스에 반영하는 반 자동 에코 시스템을 보여준다.제1장 서 론 1 제2장 이론적 배경 및 관련 연구 3 2-1 배경 지식 3 2-1-1 올바른 블랙박스 검사의 필요성 3 2-1-2 자연어 처리 (Natural Language Processing) 4

인터넷의 발달로 정보의 양이 예전과는 비교할 수 없을 정도로 많아지고 변화의 속도 또한 매우 빠르다. 이러한 막대한 정보량은 한 사람의 정보처리 능력범위를 초과하고 있다. 효과적인 정보처리를 위한 개연규칙은 자연어 이해분야의 필수적인 연구자원으로서 다양한 방법으로 축적이 시도되고 있다.
본 논문에서는 한자 중에서 회의문자의 자소결합 사례를 이용하여 개연규칙을 발견하였다. 회의문자의 경우에는 개별적으로 뜻을 갖고 있는 글자들을 결합하여 새로운 의미의 글자를 만든다. 결국, 회의문자의 의미는 그 글자를 구성하는 자소의 개별적인 의미가 결합한 것이다. 이러한 원리에 착안하여, 자소간 의미 결합 개연성을 발견하였다. 이를 위해, 회의문자 60자를 선정하여 원래 글자의 해설을 참고해서 자연어 이해에 필요한 개연규칙을 유도하였다.
유도과정은 다음과 같이 7단계로 이루어졌다. 1) 음, 훈, 부수, 육서분류 등의 데이터를 가진 한자 마스터 데이터베이스를 구축한다. 2) 기초한자 위주로 회의 문자 60개 선정한다. 3) 자소분해 및 한자의 의미를 저장한다. 4) 한자와 자소와의 개연성을 고려하여 개연의미를 정리한다. 5) 중영사전을 이용하여 회의문자와 자소의 영어단어를 서로 맵핑하고 저장한다. 6) 한자와 자소에 대한 영어단어의 OfN 범주를 결정한다. 7) 선정된 OfN범주에서 대표 개념을 정하여 개연규칙을 만든다.
이렇게 유도된 개연규칙은 문장 간 의미결속을 파악하는 용도로 이용할 수 있으며 온톨로지 OfN에 추가하여 글의 화제문이나 주제문을 선정함으로써 글의 이해도를 높이는데 활용할 수 있다.Rapid advancement of the Internet has led not only to a big outbreak of information in its quantity but also to a fast shift in its diffusion speed, with the result that this huge amount of information is usually far beyond the scope a person is able to deal with. With regard to this, many studies on the abductive rules to effectively process information have been cumulatively conducted as a research source essential for understanding natural languages.
In this study, some abductive rules have been discovered by making use of the cases where graphemes are combined in ideogrammic compounds of Chinese Characters. In ideogrammic compounds, new words are coined by combining the words, each of which has its own peculiar meaning. In addition, its meaning is composed of the totalities of the particular meaning of each of graphemes which are included in the word. Based upon this principle, we could find the probability of combining the meanings of each grapheme. To this end, 60 ideogrammic compounds were selected, glossaries on original words were taken into account, and some abductive rules finally came to derived with a view to helping to understand natural languages.
The process consists of 7 phases as follows. 1) Master database for Chinese Characters is constructed with data including sounds, meanings, radicals, classifications of Chinese Characters, etc. 2) 60 ideogrammic compounds are selected based on basic Chinese Characters. 3) Decomposition of graphemes and meanings of Chinese Characters are stored. 4) The meaning of probability is defined by considering the probability of Chinese Characters and graphemes. 5) ideogrammic compounds and English words of graphemes are mutually mapped and stored by using a Chinese-English dictionary. 6) OfN categories are determined for is categorized on Chinese Characters and English words of graphemes. 7) Some abductive rules are made by deciding a typical concept of selected OfN categories.
These abductive rules induced so far can be largely helpful not just in grasping the combination of meanings between sentences, but in making an easy understanding of articles by choosing topic or subject sentences with the augumented OfN.Ⅰ. 서론 1 1. 연구의 필요성 1 2. 연구의 동기와 목적 2 3. 연구 방법 4

인터넷이 발전하면서 교육분야에서도 많은 변화가 있어 왔다. 자료를 검색하는 수준에서 동영상 강의 특히 사이버 강좌, 및 사이버 대학으로 발전하는 등 다양하게 발전해 왔다. 그러나 상대적으로 학습 평가를 측정할 수 있는 방법은 채점의 어려움으로 인하여 객관식으로 한정되어 왔다.
본 논문이 벡터유사도와 시소러스를 이용한 주관식 답안을 자동 채점함으로써 학습자는 창의성이나 문제 해결력 등 고등 기능을 길러주는 다양한 문제를 접할 수 있고, 교수는 채점으로 인한 업무의 부담을 줄일 수 있어 보다 효율적인 원격 학습-평가 시스템이 될 것이다.
본 논문은 교수가 문제를 출제한 후 그에 해당하는 답을 여러 유형으로 입력하고, 동의어가 있을 경우 동의어 사전에 등록한다. 학습자가 주어진 문제의 답을 입력하면, 형태소 분석기 프로그램인 'HAM'을 이용하여 키워드 매칭 방법으로 정답과 학습자의 답안을 비교하여 자동으로 채점되어진다.
향후 주관식 즉 자연어 처리에서 오는 문제점 즉, 구문 분석 의미 분석을 통하여 보다 정확한 채점의 결과를 얻을 수 있는 전문가 시스템이 도입되어야 할 것이다.The development of Internet has brought forth changes in the education field. Internet has presented a cyber world for people to search for information, listen to lectures and even attend college. However, this type of education system has a downfall since multiple choice questions limits the ability to evaluate one's academic performance.
This essay will introduce a performance evaluating system, which enables automatic marking for short and long essay questions. This method vector similarity and thesaurus will provide students to enhance their creativity and problem solving techniques through various types of test questions.
This system allows the professor to store various types of answers to the questions. Therefore, once there is a synonym or similar type of phrases, analyzing program 'HAM' will process the keyword matching system to compare the student's answers with the professor's given answers.
There is now a need for a system that has the ability to analyze sentence structure and different word phrases. This will eliminate errors that currently exist in the automatic marking system.목차 = ⅰ 제1장 서론 = 1 1.1. 연구의 목적 및 필요성 = 1 1.2. 연구의 내용 및 방법 = 2 제2장 연구 배경 = 4

트위터와 같은 소셜 미디어에서 발생하는 비정형 데이터는 하루하루 방대한 양이 생성되고 있다. 비정형 데이터는 최신 데이터들이 계속해서 생겨남에 따라 뉴스와 더불어 새로운 형태의 데이터로 떠오르고 있다. 이러한 데이터는 방대하기 때문에, 그것을 처리하기 위한 최신의 자연어 처리 도구가 필요하다. 그러나 어느정도 좋은 성능을 내는 자연어 처리 도구를 만들기 위해서는 트위터에 관련된 레이블 데이터를 구축하는 것이 필요하다. 하지만 레이블 데이터를 구축하는 것은 많은 시간과 돈을 요구한다. 표지부착에 드는 노력과 비용을 줄이기 위한 명백한 방법은 사전이나 임베딩과 같이 구애받지 않고 얻을 수 있는 추가적인 데이터를 사용하는 것이다. 그러므로 본 논문은 이러한 유용한 데이터를 효과적으로 구축하고 사용하는 방법과 관련하여 dropout conditional random fields (CRFs) 와 여러 mbedding 을 조합하여 사용하는 방법에 대해 제안한다The noisy and informal texts from social media such as Twitter have been tremendously generated everyday. It has become new sources of important information in addition to standard channels such as news, because the up-to-date data are generated in a unprecedented rate from social media such as Twitter. Since this data is massive, we need off-the-shelf natural language processing (NLP) tools for processing them. However, to construct reasonably performed tools, it is necessary to build twitter-specific labeled data, but constructing labeled data requires time
and budget. An obvious solution to reduce the annotation effort and cost is to utilize freely obtainable additional resources such as gazetteers and embeddings. Therefore, this thesis focus on how to build and use these useful resources effectively: 1) dropout conditional random fields
(CRFs) and 2)combinations of multiple embeddings.LIST OF TABLES iii LIST OF FIGURES iv ABSTRACT v 1 Introduction 1 1.1 Outlines 1

인터넷의 발달로 인한 수많은 정보의 공유는 지식 정보사회의 발전을 가져왔다. 이러한 정보사회의 발전과 동시에 표절과 같은 새로운 지식 범죄도 급증하고 있다. 표절은 연구의 정직성과 창의성을 떨어뜨리고 학문의 발전을 저해하는 요소이다. 이러한 표절을 근절하기 위해서 그동안 많은 방법들과 시스템들이 제시되었다. 이중 자연어로 구성된 구조가 없는 일반 문서의 표절을 검사하는 방법은 지문법을 이용하였다. 지문법과 같이 통계적인 방법을 이용한 유사도 검사 방법은 문서 대 문서 전체를 비교하기 때문에 부분적 유사성, 즉 문장이나 문단 단위의 비교를 할 수 없는 단점이 있다. 본 논문에서 제시하는 시스템은 자연어로 이루어진 일반문서 중 특별한 문서의 구조 정보를 가지고 있는 논문을 대상으로 유사도를 측정 하였다. 즉 논문의 구조를 AST 형태의 자료구조로 표시하고 이를 이용하여 사용자가 원하는 부분 또는 전체 유사도 측정 방법을 제시한다.The sharing of a lot of information have improved information society, because of the advance of Internet. Like this, development of information society have rapidly increased simultaneously new knowledge crime like plagiarism, too. The plagiarism drops honesty and originality of research, it is an element which hinders the development of study. For these reasons, in the meantime, many methods and systems were presented to uproot this plagiarism. Fingerprint had been used that general document consisted of natural language without structure make an inspection of plagiarism on the document. Similarity checking method which uses statistical method such as the fingerprint has a weak point which can’t compare a sentence or a literary world unit, because this method compare the document versus document whole. The system which presented in this paper measured similarity of a paper which have structural information of document among general documents which consisted of natural language. Namely, structure of paper represents data structure of AST form and uses itself for presentation of similarity-measurement-method of part or whole of paper.제 1 장 서론 = 1 1.1 연구배경 및 목적 = 1 1.2 연구범위 및 논문의 구성 = 2 제 2 장 관련 연구 = 3 2.1 일반 문서의 복제 검사 방법 = 3

최근 IT융합 차량이 급속도로 발전하며 OBD-Ⅱ 스캐너와 ECU 모듈로 차량에 대한 다양한 데이터를 수집하고 있다. 그러나 수집 되는 차량 데이터의 활용 사례가 매우 적으며 몇몇의 사례 또한 데이터의 수집과 출력만의 기능을 하고 있다. 단순히 스캐닝 된 차량 데이터는 일반 사용자가 해석하기 매우 어려운 부분이다. 차량 데이터의 활용 가치를 보다 높이기 위해서는 일반 사용자가 이해할 수 있는 가치 있는 정보로 제공이 되어야 한다.
따라서 본 논문에서는 위의 문제점을 보완하여 규칙 기반 알고리즘을 사용하여 차량 상태를 분석 하고 진단 해주는 시스템을 제안한다. 더 나아가 유저인터페이스로 머신러닝 기반 Chatbot을 함께 구축하여 차량 진단 요청과 결과를 자연어를 통하여 인터랙션이 가능한 시스템을 제안한다.
본 논문에서 제안하는 시스템을 설계하기 위해 사용자가 요청과 결과를 친근한 채팅 형태의 인터페이스로 주고받을 수 있도록 머신러닝 기반 Chatbot을 설계 하고, 차량의 데이터를 수집하고 상태를 분석하기 위하여 Find-S Algorithm을 적용하여 규칙 기반의 차량 진단 시스템을 설계하였다. 또한 차량 데이터를 제어하기 위하여 가상 차량 운행 시뮬레이터를 설계하였다.
본 논문에서 설계한 시스템을 구현하기 위해 Intel Core i7 @2.5GHz 환경 PC의 운영체제 Windows 10에서 JAVA, JSON, Spring외 Eclipse 툴을 이용하였다. 또한 IBM사의 BlueMix Platform에서 Chatbot Frame Work로 Watson Conversation을 이용하여 Chatbot 서버를 구축하여 필요한 데이터를 API통신으로 주고받을 수 있도록 구현 하였다.
본 논문에서 제안하는 시스템의 성능을 평가하기 위해 두 가지 항목을 테스트 하였다. 먼저 블랙박스 테스트로 Chatbot과 사용자가 정상적인 입력에 대한 대화가 가능한가에 대한 부분을 테스트 하였으며, 반대로 비정상적인 대화를 요청 할 시 비정상적인 대화를 감지하는지 테스트 하였다. 테스트 결과 학습 되지 않은 요청을 사용자가 할 경우 차량 진단을 진행 하지 않으며, 진단 가능한 CASE에 대해서는 사용자가 원하는 진단 항목의 결과를 출력해주는 것을 확인 하였다. 두 번째 테스트 항목은 차량 점검 결과 신뢰도에 대한 테스트를 진행하였다. 먼저, 차량 초기 데이터로 진단을 하고 가상 머신 차량 운행 시뮬레이터를 통하여 운행을 총 3회 진행하였다. 각각 운행이 끝날 때 마다 차량의 상태를 진단하였으며, 진단 결과와 차량 데이터를 실제 차량 점검 기준 표와 비교하여 차량 진단 정확도를 확인하였다. 테스트 결과 운행 횟수에 따른 진단 결과와 차량 데이터가 실제 차량 점검 기준 표와 제안 시스템의 결과가 동일한 것으로 확인 하였다.I. 서 론 1 1.1 연구 배경 1 1.2 연구 필요성 2 1.3 연구 목적 2 1.4 기존 연구와의 차별성 및 독창성 3

개방, 참여, 공유의 특징을 가진 웹 2.0 시대가 시작된 이후, 정보 수요자였던 인터넷 사용자들의 데이터 생산 및 공유가 쉬워졌다. 이런 편의성으로 인해 데이터의 양은 기하급수적으로 증가하게 되었고, 인터넷에 존재하는 디지털 정보 중 대부분을 차지하는 형식인 비정형 데이터(Unstructured Data)의 양도 증가하고 있다. 비정형 데이터란 정해진 형식 없이 자연어 형태로 이루어진 데이터를 뜻한다. 인터넷에 생성된 비정형 데이터 중, 상품에 대해 소비자가 평가한 리뷰들은 잠재적 고객과 해당 상품을 생산하는 기업이 의사결정을 내리는데 도움을 준다. 잠재적 고객에게는 상품의 특징을 파악하여 구매에 판단을 내릴 수 있는 정보를 주고, 생산 기업에게는 자사의 상품에 대해 고객들이 생각하는 의견을 파악하여 제품을 개편할 때 사용하기 때문이다. 많은 양의 리뷰 데이터에서 해당 상품에 대한 유용한 정보를 얻기 위해서는 리뷰 데이터 수집, 저장, 전처리, 분석 및 결론 도출의 과정이 필요하다. 따라서 본 연구는 R을 이용하여 앞에서 언급한 전 과정을 처리하였다. 본 연구는 R을 이용한 텍스트 마이닝 기법을 사용하여 텍스트로 이루어진 비정형 데이터에서 자연어 처리 기술 및 문서 처리 기술을 적용하여 데이터베이스에 저장할 정형화된 정보를 산출하는 방법을 소개하였다. 그 후, 도출된 정형화 리뷰 정보에 데이터 마이닝 기법을 적용하여 필요한 목적에 맞춤화된 시각적 정보를 도출시키는 방안을 제시하였다.In the era of the Web 2.0, characterized by the openness, participation and sharing, it is easy for Internet users who were information consumers to produce and share the data. According to this convenience, not only the amount of the Internet data, but also the amount of the unstructured data which occupies most of the digital world's data has increased exponentially. The unstructured data means the data which is composed of natural language without a set of format. One of the kinds of the unstructured data called personal product review is necessary for both the potential consumer and the company that produces those products. In the point of potential consumers' view, they understand the product's characteristic and decide whether buy or not. In the point of company's view, they apprehend the customer's opinion and reflect the opinion in the renewal product. In order to extract useful information from lots of scattered review data, the process of collecting data, storing preprocessing, analyzing and drawing a conclusion is needed. Therefore, we introduce the text mining methodology for applying the natural language process technology to the text format data like product review in order to carry out extracting structured data by using R programming. Also, we introduce the data mining to derive the purpose-specific customized information from the structured review information drawn by the text mining.국문초록 ⅶ 영문초록 ⅸ 제 1 장 서론 1 1.1 연구의 배경 1

휴대용 통신기기의 상용화 이후 실시간으로 자신의 상황을 알리거나 생각을 표현할 수 있는 소셜 미디어의 사용은 나날이 증가하고 있다. 그에 따라, 소셜 미디어에서 작성되는 소셜 텍스트의 양 역시 크게 늘고 있다. 하지만 이런 소셜 텍스트들은 많은 철자 오류를 포함하고 있으며, 이러한 철자 오류들은 소셜 텍스트에 자연어 처리 기술 및 응용 시스템을 사용할 때의 성능에 악영향을 끼친다.

자연어 처리 기술의 성능 저하 문제를 해결하기 위한 대표적인 전처리 수단이 철자 오류 교정이며, 기존의 철자 오류 교정은 크게 규칙 기반 철자 오류 교정 방법과 통계 기반 철자 오류 교정 방법으로 나눌 수 있다. 두 철자 오류 교정 방법은 각각의 장단점이 존재하는데, 규칙 기반 철자 오류 교정 방법은 교정의 정확률이 높으나 교정 규칙의 수가 많지 않아 교정이 이루어지는 경우가 적고, 통계 기반 철자 오류 방법은 교정이 많이 이루어지지만, 교정의 정확률이 낮다는 단점이 있다. 이러한 상반된 두 철자 오류 교정 방식들의 단점은 두 교정 모델의 혼합으로 개선이 가능하다.

본 논문에서는 규칙 기반 철자 오류 교정 방법론과 통계 기반 철자 오류 교정 방법론을 혼합하여 두 철자 오류 교정 모델보다 더 좋은 성능을 보여줄 수 있는 교정 모델 혼합 방법을 제안한다. 두 교정 모델의 시스템적인 혼합에는 어려움이 있으므로 본 논문에서는 순차적인 방식의 교정 혼합 방법론들을 제시하였으며, 선 규칙 후 통계 방법이 그것이다. 제시한 방법론은 규칙 기반 철자 오류 교정의 높은 정확률을 최대한 유지하는 선에서 통계 기반 철자 오류 교정으로 보완하는 것을 전략으로 삼았다. 또한, 교정 모델 혼합에 사용된 두 종류의 철자 오류 교정 모델 모두 같은 교정 말뭉치를 사용하기 때문에 확장성도 거의 유지되었다.

제안하는 교정 모델 혼합 방법에 대해 SMS 말뭉치와 트윗 말뭉치로 실험을 하였고, 규칙 기반 철자 오류 교정 모델과 통계 기반 철자 오류 교정 모델, 교정 모델 혼합 모델의 성능을 비교 분석하여 선 규칙 후 통계 방법이 기존 철자 오류 교정 모델들에 비해 성능이 많이 향상되었음을 보였다. 또한 형태소 분석 및 품사 부착기에 대한 응용 평가를 실시하여 자연어 처리 기술의 전처리 수단으로서도 유용함을 입증하였다.1 서론 1 2 관련 연구 4 2.1 규칙 기반 철자 오류 교정 . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 통계 기반 철자 오류 교정 . . . . . . . . . . . . . . . . . . . . . . . . 5 3 철자 오류 교정 모델 혼합 방법 7

제약기반의 문법형식(Constraint-based Grammar Formalisms)은 1980년대부 터 현재에 이르기까지 종전의 문법형식을 대체할 수 있는 이론으로서 이론언 어학 및 전산언어학 분야에서 활발히 연구되어 오고 있다. 최근의 이론언어 학(Lexical-Functional Grammar, Head-driven Phrase Structure Grammar 등) 및 전산언어학 연구는 이러한 문법형식에 근거한 다양한 문법이론의 개발 및 자연어처리에 적용하려는 연구결과를 낳게 했다. 본 논문의 목적은 제약기반문법이 어떻게 효율적인 자연어 파싱에 적용될 수 있는 지를 연구하는 것이다. 이를 위해 확장 LR 파싱 알고리듬을 개발에 제약기반문법에 적용시키고, LR 알고리듬의 적용이 야기시키는 비결정성과 비 종결성의 문제가 어떻게 해결될 수 있는 지를 보인다. 또한 이러한 파싱 알 고리듬이 인간언어처리의 모델로 사용될 수 있는 지를 밝힌다. 인간언어처리 의 모델링을 위해 확장 범주문법(Extended Categorial Grammar)를 문법형식으 로 삼아 범주문법과 확장 LR 알고리듬이 인간언어처리과정을 모델화할 수 있 음을 보인다. 끝으로 본 논문의 확장 LR 알고리듬이 형태처리(Morphological Processing) 를 위해 일관되게 적용될 수 있음을 보인다. 이를 위해 두단계 모델(Two-lev el Model)의 형태규칙을 문맥자유문법(Context-free Grammar) 형식으로 변환 하고, 본 논문에서 개발된 확장 LR 알고리듬을 적용한다. 이러한 방법의 적 용으로 형태규칙 개발의 범용성과 사전탐색의 효율성을 높이는 형태소분석 방 법을 제시한다.The goal underlying this study is to provide a variety of types of information to
assist planners and managers with the difficult task of managing national parks. Another goal of the study is to suggest a conceptual land spectrum model (LSM) that reflects current scientific understanding of the zoning concepts and identified problems from existing systems, including the Korean land-use system. A model was constructed through a comprehensive literature survey which included a review of relevant published and unpublished literature. A trial application of the model was used to illustrate the LSM concepts and process, and to examine its feasibility in a case study of Sorak Mt National Park, Korea.
The LSM presented here was principally based on a zoning approach which deals
with resource values and recreation opportunities for conservation purposes and
appropriate uses in the Korean national park system. It also tried to take into account the dual goals of national parks, which are preservation and use.
Due to the complex legal constraints in the Korean national park system, the LSM
is not a legal tool, but rather a comprehensive zoning framework for resource values and recreation opportunities. As such, the LSM can be used both as a guide to allocations of resources and management actions, and as data input to the decision-making processes related to other resource management in the national park system. A trial application, using GIS as a tool, in Sorak Mt. National Park demonstrated that the LSM procedure can be successfully adopted to a wide range of Korean national parks. GIS allowed some functional advances for computerized analysis, data management, and presentation of mapped information in the case study.

소프트웨어 개발에는 코드의 작성, 테스트, 문서화와 같은 작업들이 수행된다. 현재 소프트웨어 개발을 돕기 위해, 소프트웨어 생성 및 테스트 과정을 위한 여러 가지 기법 및 도구가 개발되었지만 코드의 주석이나 설명을 작성하는 것과 같은 문서화 작업은 여전히 개발자에게 많은 시간과 비용을 요구한다. 기존의 주석 자동 생성 및 요약 기법들은 주로, 소스코드의 의미 및 구조 정보를 파악하기 위한 규칙을 생성하는 규칙 기반의 기법들이 연구되어왔다. 하지만 규칙 기반의 기법은 모델의 이식성과 유연성이 부족하다는 문제점이 존재한다. 최근 Deep Leaning 기반의 자연어처리 기법들이 발전하면서, 이를 소프트웨어 개발에 관련한 문제에 적용해 기존 기법의 문제점을 해결하려는 연구가 진행되고 있다. 하지만 자연어와 비교하면 소스코드는 좀 더 구조적인 정보를 가지고 있으며 소스코드에 대한 해석의 다양성이 존재하여, 기존 기법의 단순한 적용을 통한 소스코드의 설명문 생성은 한계가 존재한다.
본 논문에서는 Convolutional neural network를 통해 소스코드의 구조를 파악하고, 이를 기반으로 소스코드의 주제를 예측해 memory network를 생성하는 모델과 소스코드의 설명문을 생성하는 모델을 결합한 새로운 설명문 자동 생성 기법을 제안한다. 이를 통해 각각의 소스코드에 대해 고정된 범위의 주제를 찾고 이에 집중한 소스코드의 설명문을 생성하였다. 제안 기법에 대한 평가를 위해서, 다양한 Java project로부터 함수 단위의 코드-주석 쌍을 수집하여 생성한 GitHub 데이터셋과 C# 프로그래밍에 대한 질문과 이에 관련된 code로 이루어진 StackOverflow 데이터셋에 제안기법을 적용하였다. 실험을 통해 서로 다른 두 데이터셋의 소스코드에 대하여 설명문을 생성 및 분석한 결과, 제안 기법이 언어의 종류나 형태에 큰 영향을 받지 않으면서 소스코드의 내용을 잘 내포하고 있는 설명문을 생성함을 보였다.제1장 서론 1 제2장 배경 지식 5 2-1 Deep learning 5 2-2 Convolutional Neural Network 6 2-3 Recurrent Neural Network 9

소프트웨어 공개 저장소에서 응용 프로그램의 카테고리는 소프트웨어의 체계적 관리와 효율적인 검색을 위하여 매우 중요한 항목이다. 하지만 카테고리의 수동 분류는 응용 프로그램의 관리자가 수동으로 수행하여야 하므로 비용이 많이 들고 지루하며 때로는 잘못된 분류가 수행되기도 한다. 이러한 수동 분류의 대안으로 기계학습 알고리즘을 이용한 자동 분류 연구가 지속적으로 제안되고 있다. 본 논문에서는 컴퓨터 비전, 음성 인식 및 자연어 문장 처리에서 좋은 성능을 보이고 있는 CNN 알고리즘을 응용 프로그램 카테고리 자동 분류에 적용하여 그 성능을 평가하고 추가적인 성능 향상 방법에 대하여 제안한다. 실험 결과 구글에서 제공하는 자연어 벡터 파일을 이용한 CNN-base 모델이 기존의 기계학습 알고리즘 중 가장 좋은 성능을 보인 DNN 모델과 비교하여 더 높은 정확도를 달성하였고, 입력 데이터에서 프로그램 전용 불용어를 제거할 경우 전체 평가 항목(정확도, 정밀도, 재현율, F-Measure)에 대하여 DNN 모델 대비 최대 8.0%, 13.4%, 8.0%, 9.6% 성능이 더 향상되었다. 또한 프로그램 소스 코드를 학습하여 생성한 벡터 파일을 이용한 CNN-code 모델의 경우 본 연구의 실험에서 테스트된 모든 모델 가운데 가장 좋은 성능을 보여주었으며 DNN 모델과 비교할 경우 각각 15.1%, 18.4%, 15.1%, 17.4% 향상된 성능을 보였다.The category of an application in public software repositories is crucial information for the management and retrieval of the application. However, manual categorization is costly and tedious, and the repository administrators sometimes classify applications into incorrect categories. As an alternative to the manual approaches, several studies on automatic classification of applications using machine learning algorithms have been proposed.
In this paper, we apply the Convolutional Neural Network (CNN) to the automatic categorization of applications and evaluate its performance. The CNN has recently shown good performance in many areas such as computer vision, speech recognition, and sentiment classification of natural language sentence. We also propose an additional performance improvement method.
The experimental results show that the CNN-base model using the pre-trained word vectors file provided by Google for natural language processing achieves a higher accuracy than the DNN model which shows the best performance among the machine learning algorithms other than CNN. In addition, the performance of the CNN-base model is improved by 8.0%, 13.4%, 8.0%, and 9.6%, respectively, compared with the DNN model over the four evaluation items(accuracy, precision, recall, f-measure) when the program-specific stop-words are removed from input data. The CNN-code model using the code token vectors generated by learning the program source code shows the best performance among all the models tested in the experiments of this study, improved by 15.1%, 18.4%, 15.1%, 17.4%, respectively, compared with the DNN model.제1장 서론 1 제2장 관련 연구 및 기술 3 2.1 정보 검색을 이용한 카테고리 자동 분류 연구 3 2.2 기계학습을 이용한 카테고리 자동 분류 연구 4 2.3 딥 러닝을 이용한 카테고리 자동 분류 연구 5

목차 =ⅰ 국문요약 = ⅲ ABSTRACT = ⅳ 제1장 서론 = 1 1.1 연구 배경 및 목적 = 1

시맨틱 웹에서 온톨로지는 용어에 시맨틱을 부여하여 기계 가독성을 높일 수 있다는 점에서 여러 분야에 활용될 수 있다. 이에 따라, 전자 상거래에서도 정확한 상품 정보의 교환을 위해 상품 도메인에 온톨로지를 적용하려 하고 있다. 본 논문에서는 구축된 상품 온톨로지를 효과적으로 검색하여 보여주기 위해서는 기존의 방식이 아닌 상품 온톨로지의 특색에 맞는 검색 방식과 시각화 기법이 필요하다고 판단하여, 이를 반영한 상품 온톨로지 비주얼라이제이션 시스템 PROVIS를 소개한다.
이 시스템에서는 효과적 상품 검색을 위해 우선 카테고리 검색을 속성 검색과 분리하여 수행하고, 자연어로 입력받은 카테고리명에 맵넷 기법을 이용해 ‘시맨틱’을 부여한다. 이후, 선택된 카테고리 내의 상품 인스턴스를 사용자가 관심을 가지는 속성으로 나누어준 결과를 클러스터 맵 기법으로 보여주고 사용자가 직관적으로 판단하여 결과를 얻는 것이 본 논문에서 제시한 비주얼라이제이션 기법이다. 이때, 다양한 속성을 이용하여 상품을 좀 더 세부적으로 나눌 수 있게 하고 사용자가 최소한의 검색 결과를 보고 최소한의 클릭으로 원하는 상품을 찾아내게 하는 것이 시스템의 목표이다. 또한 시스템에서는 사용자의 잘못된 질의어로 원하는 결과를 얻지 못했을 경우를 위해, 질의 수정을 위한 추천을 제공한다. 이는 상품에 대한 정보가 부족한 사람이라도 검색을 통해 원하는 상품을 쉽게 찾게 하기 위한 것이다.Ontology, which is a key word on Semantic Web, can be used in many other fields because it improves machine-readability by giving terms semantics. Therefore, it is expected to exchange the product information clearly and accurately by applying Ontology in E-commerce. In this paper, visualization techniques for product ontology and a system which reflects these techniques, PROVIS, are introduced.
The system PROVIS separates the category searching from the product property searching for streamlining the product retrieval. In the category searching, users write terms which describe the products' category that they want to buy and give terms semantics by selecting a category through the visualization of the MapNet technique. Then, users select several product properties which they are interested in. After that, the product instances in the selected category are divided into clusters of those properties through the Cluster Map visualization. Users can interpret the Cluster Map by intuition and find the products they want.
The goal of PROVIS is to help users find that products with the minimum clicks. It also provides recommendations for query refinement in case users made wrong queries. Therefore, even users with rack understanding of the products can rewrite their queries for better results by using the recommendations.목차 = ⅰ 국문요약 = ⅲ 제1장 서론 = 1 제2장 관련연구 = 5 2.1 온톨로지 = 5

웹2.0 시대를 맞아 인터넷 상의 블로그 및 커뮤니티 공간에 일반 사용자들이 자신의 의견 및 생각을 표현하게 되었다. 이러한 의견 및 생각의 표현은 서비스 및 상품에 대한 의견도 다수 포함되어 있다. 상품 구매 시, 다수의 사람들이 이러한 의견을 참조하는데, 사용자들은 소수의 의견들만을 참조하고 전체적인 의견은 참조하지 못하고 있다. 의견 분석 시스템은 상품 및 서비스에 대한 인터넷 상의 글들을 분석하여 상품의 긍정, 부정을 평가하는 시스템으로, 자연어 검색에서 발전한 의견 검색이라 할 수 있다.
본 연구에서는 의견 분석 서비스에서 핵심이 되는 문장의 긍정, 부정을 파악하기 위하여 ‘긍정’, ‘부정’, ‘중립’의 극성 정보 외에 ‘반전’의 정보를 추가로 학습하고, ‘반전’에 대한 처리를 하기 위하여 구문 분석 및 반전 처리를 제안한다.
본 연구에서 제안한 모델에서는 기존의 연구보다 더 나은 성능을 보이는 것을 확인 하였다. 이에 본 논문에서는 기본적인 의견 분석 외에 추가로 한국어에 맞는 자연어 처리 기법 및 의미 분석, 문맥 파악 등의 처리를 통하여 더 나은 성능의 의견 분석 시스템을 기대 할 수 있을 것 이라는데 의의를 둔다.Web 2.0 of blogs and community spaces on the Internet to the end users to express their opinions and thoughts were. These opinions and thoughts expressed in the comments on a number of services and products are included. Product purchase, many people refer to this feedback. Howerber users only see a few of the comments and do not see the overall opinion is not. Opinion Analysis System is on the Internet for products and services for analysis of the positive and negative to evaluate the system, developed in the natural language search opinions search can be called.
In this research, Opinion Analysis System a statement from the core of the positive and negative to determine the 'Positive', 'Negative', 'Neutral' information, in addition to the polarity of the 'Inverse' of information to learn more. 'Inverse' for the simple syntactic analysis processing to conduct the reverse process is suggested.
In this research, Opinion Analysis System a statement from the core of the positive and negative to determine the 'Positive', 'Negative', 'Neutral' information, in addition to the polarity of the 'Inverse' of information to learn more. 'Inverse' for the simple syntactic analysis processing to conduct the reverse process is suggested.
In this research, the existing research on the proposed model, showing better performance. In this research, analyzing the opinion in addition to basic natural language processing techniques for Korean and semantic analysis, context and process through a betterunderstanding of the performance opinion analysis system says he might be able to look forward to significant places.Ⅰ. 서론 1 가. 연구동기와 목적 1 나. 연구방법과 구성 4 Ⅱ. 관련 연구 5 가. 정보검색 5

옥스퍼드 사전은 2016년 올해의 단어로 Post-Truth (탈 진실)을 선정했다. 가짜 뉴스는 진실을 위협하는 대표적 현상으로 뉴스 형식을 빌려 권위를 가장한 허위를 유포한다. 학교에서 배운 지식으로 사회 문제의 해결방안을 고민해보고자 이 연구를 시작하게 되었다. 이 연구에서 가장 고민이 되었던 부분은 ‘무엇’이 참이고 거짓인가라는 ‘기준’이었다. 따라서, 서울대 팩트체크 센터에서 분류해놓은 정치 기사 자료들만을 대상으로 분석하였다. 통계적 방법으로는 Multinomial Naive Bayes(MNB), L2-regularized Logistic Regression, Linear Support Vector Machine(SVM) 등을 사용하였다. 분석 과정은 자연어 처리, 문서 특성 추출, 초기 모델 구축, 모델 성능 향상 및 최종 모델 선택 순서이다. 자연어 처리로 명사 11326개가 추출되었고, 문서 특성 추출은 TFM(Term Frequency Matrix)과 TF-IDF(Term Frequency Inverse Document Matrix)로 하여 초기 모델을 구축했다. 다음으로 18가지 변수 선택(변수 제거) 방법과, 매개변수 조절을 통해 모델 성능을 향상시켰다. 모델 성능은 Accuracy 와 AUC(Area under the curve) 모두 교차 타당성 검증(10-fold cross validation)을 실시했으며, 최종 모형은 MNB로 Accuracy가 0.730, AUC가 0.767로 도출되었다. 분류기 성능 향상을 위한 추후 연구 과제로는 먼저 문서 특성 추출 방법 및 분류기의 다양화 이다. 다음은 모델에 신문사, 신문기자, 정당, 정치인 별로 다양하게 가중치를 적용시켜 보는 것이다.The Oxford Dictionary selected Post-Truth as the word of the year in 2016. Fake news is a representative phenomenon that threatens the truth. This study intends to explore the statistical solution for sorting fake news. Clear criteria is crucial to distinguish truth and false. Therefore, this study analyzed the political articles classified by the Fact Check Center of Seoul National University. Author used Multinomial Naive Bayes (MNB), L2-regularized logistic regression, and Linear Support Vector Machine (SVM) as statistical methods.
First, author analyzed natural language processing,document feature extraction, initial model construction, model performance improvement, and final model selection. 11,326 nouns were extracted by natural language processing, and document characteristics were drawn by using TFM (Term Frequency Matrix) and TF-IDF (Term Frequency Inverse Document Matrix). Second, author tried to improve the model performance by using 18 variables selection methods (removing variables) and controlling the parameters. The performance of the model was verified by 10-fold cross validation for both Accuracy and Area under the curve (AUC). The final selection model was MNB with an accuracy of 0.730 and an AUC of 0.767.
(The further studies of improving classifier performance are to diversify document characteristic extraction method and classifier. The next is to apply various weights to the model for newspaper, newspaper reporter, political party, and politician, and so on.)제 1 장 서 론 제 2 장 통계적 분석 방법론 2.1 단어 주머니(Bag of words) 2.2 벡터 공간 모형(Vector Space Model 2.3 분류 모형(Classification)

기계학습을 이용하는 자동문서분류시스템은 분류모델의 구성을 위해 단어를 특징으로 사용한다. 자동문서분류시스템의 성능을 높이기 위해 보다 의미 있는 특징을 선택하여 좋은 분류 모델을 구성하기 위한 여러 연구가 진행되고 있다. 본 논문에서는 좋은 분류 모델을 구성하기 위해 적절한 특징을 선택하고, 선택된 특징의 가중치를 계산하여 부여하는 방법을 제안한다.
본 논문에서는 문서를 두 가지의 개념으로 나누어 방안을 제시하고자 한다. 먼저 링크를 고려하지 않는 문서의 분류에서는 의미 있는 특징을 선택하기 위하여 특징의 중의성을 해소하는 방안을 제시한다. 자연어 처리 기법인 청킹을 통하여 특징의 모호함을 배제하고 특성 있는 특징으로 가공하고 적절한 특징을 선택한다. 선택된 특징의 정보에 따라 가중치를 부여한다. 링크를 고려하는 문서의 분류에서는 링크정보와 태그정보를 이용하여 분류 모델을 개선한다. 링크정보를 통하여 인접한 문서의 특징을 차용하고, 문서의 중요도를 계산하여 가중치를 부여한다. 태그 정보를 이용하여 특징을 선택하고, 특징의 중요도에 따라 가중치를 부여 한다. 제시 방안은 나이브 베이즈 알고리즘을 통하여 성능을 평가하였다.제 1 장 서 론 1 제 1 절 연구의 배경 및 목적 1 제 2 절 연구 내용 및 범위 2 제 3 절 논문의 구성 3 제 2 장 배경 연구 및 관련 연구 5

실생활에서 만들어지는 대부분의 자료는 문서 형태로 이루어져 있다. 매년 여러 분야에서 많은 논문들이 발표되고 있으며, 주위에서 일어나는 사건들에 대한 기사들은 매일같이 발행된다. 이러한 문서들은 인터넷의 발달로 디지털 형태로도 쉽게 접할 수 있다. 그러나 이러한 자료들은 기존의 통계 분석이나 데이터 마이닝에 적합하지 않은 자료로 구성되어 있어 자료 내의 숨겨진 의미를 찾아내기가 어렵다.
자연어로 이루어진 비정형 자료에서 자연어 처리 과정과 데이터 마이닝 기법을 통하여 의미 있는 정보를 추출하고 가공하는 기법을 텍스트 마이닝 기법이라고 한다. 텍스트 마이닝 기법을 적용하면 검색 시스템을 통해 사용자가 원하는 정보를 손쉽게 찾을 수 있으며 주어진 범주에 따라 문서들을 분류할 수도 있다.
본 논문은 텍스트 마이닝 기법을 이용하여 기후 관련 논문에서 용어들의 출현빈도를 분석하였다. 이를 위하여 용어-문헌 행렬을 만들고, 용어들 간의 비유사성 측도를 바탕으로 계층적 군집분석기법을 적용하여 문서들을 군집화 하였다. 군집화 된 문서들 간의 상호 연관성과 군집별로 특정용어의 빈도를 파악하여 문서군집을 특정주제별로 분류하였다. 이러한 연구를 통하여 식품분야의 기후변화 관련 논문들의 추세와 관심주제어를 파악할 수 있었으며, 향후 기후변화 적응 및 대응 체계 로드맵 작성 시 연구 개발 기초 자료로 활용할 수 있을 것이다.Most data we encounter in everyday life exist in the form of documents. Every year, a number of papers are being published in each of every fields, so are journals on the events happening around us. At the moment, all of these documents are available in digital form, and can be accessed through the internet. However, it is difficult to extract the hidden informations in these documents without reading them all, for they consist of the data not in the best form to be analyzed with the statistical tools or with the data mining methods.
Text mining is a method that extracts meaningful informations from atypical data composed with the natural languages, using the data mining method and the method of processing natural languages. With the use of a searching system which applies the text mining method, it is more convenient to identify the specific informations that users seek and to categorize documents.
Research articles related to climate change were analyzed by implementing a text mining algorithm, which is one of nonstructural data analysis tools with a focus on frequencies of terms appearing in the abstracts. As a first step, a term-document matrix was established, followed by implementing a hierarchical clustering algorithm based on dissimilarities among the selected terms and expertise in the field to classify the documents under consideration into a few labeled groups. Through this research, we were able to find out important topics appearing in the field of food related to climate change and their trends over past years. It is expected that the results of the article can be utilized for future research to make systematic responses and adaptation to climate change.Ⅰ. 서론 1 Ⅱ. 본론 4 1. 비정형 자료의 자료 변환 4 2. 군집분석 5 2.1 계층적 군집분석 6

온라인에서 판매되는 상품은 점점 다양해지고 많은 상품들이 신규로 등록되며 소비되고 있다. 쇼핑몰에 상품 등록은 판매자에 의해서 이루어지며 대부분 텍스트 정보로 이루어져 있다. 이 중에서 카테고리는 시스템에 의해 상품의 메타 정보로부터 추론 될 수 있는 부분이다.

이번 연구에서는 상품명으로부터 상품 카테고리를 매칭시켜주는 방법에 대해 연구하였다.

종합쇼핑몰에 등록되는 상품명의 특성상 상품 고유명 뿐만 아니라 다양한 정보를 포함하게 되는데, 이는 자연어에 가까울 정도로 많은 단어들과 긴 길이를 갖는다. 이러한 특성을 수용할 수 있도록 딥러닝 중 자연어 처리에 많이 사용되는 RNN 모델을 제안한다.

상품명으로 카테고리를 추론하는 과정은 텍스트 분류 문제로 재정의 될 수 있는데, 텍스트 분류에서 보편적으로 사용되는 전처리 과정에서 일반적인 구두점 제거와 상품의 카테고리와는 직접적인 연관이 없는 배송정보나 프로모션 여부 등의 텍스트를 제거하는 방법과 함께 고도화된 입력 값 정제를 위하여 형태소 분석을 통한 명사 축출 방법을 제안한다.

사용된 딥러닝 프레임 워크는 google의 tensorflow 이며, python 언어를 사용하였다. tensorflow 중 RNN으로 구성된 encoder 와 decoder를 가진 sequence to sequnce 모델을 사용하였다.

3개의 1차 카테고리를 성능평가의 지표로 활용하였다. 고도화된 입력 값 정제를 위한 형태소 분석 적용 여부를 각각의 1차 카테고리 상품에 적용하여 비교하였을 때 정확도가 평균 6.75% 증가하였고, 3개의 1차 카테고리를 하나의 모집단으로 한 실험의 결과에서는 정확도가 8.59% 증가하였다.

제안된 방법 중 형태소 분석을 통한 고도화된 입력 데이터 정제의 여부에 따라 상품명으로 상품 카테고리 분류 정확도를 비교하였을 때 개선됨을 실험적으로 증명할 수 있었다.국문요지 i 차 례 iii 그림차례 v 표 차례 vi

시맨틱 웹의 기반이 되는 온톨로지를 표현하기 위한 언어로 RDF(Resource Description Framework)와 OWL(Web Ontology Language) 등 여러 표준이 제안 되었다. 제안된 표준에 대한 검색어로서 일반 관계형 DB에서 사용되는 SQL과는 달리 SPARQL이라는 질의어가 사용된다. 하지만 제안된 표준과 SPARQL 질의어는 전문 지식이 없는 일반 사용자가 온톨로지 데이터베이스로부터 원하는 결과를 얻기 위해서 질의를 작성하여 수행하는 것이 용이하지 않다.
본 논문은 제시된 문제점을 해결하기 위해 사용자 인터페이스를 통해 한글질의를 입력받고 시스템에서는 해당 한글질의로부터 SPARQL질의를 생성하여 검색을 수행하는 방법을 제안한다. 생성된 SPARQL 질의는 온톨로지 저장소에 수행되어 검색 결과를 받아 사용자에게 제공된다. 이러한 방법을 통해 사용자는 온톨로지, SPARQL에 대한 전문적인 지식 없이도 온톨로지 시스템의 데이터를 쉽게 검색하고 이용할 수 있는 장점이 있다.
자연어 처리기법을 사용하지 않고 사용자로부터 생성되는 한글질의에 일정한 패턴을 두어 해당 패턴으로만 한글질의가 생성되도록 대화형 GUI를 제공하며, 제안하는 시스템에서는 사용자가 생성한 한글질의는 시스템이 처리 가능한 한글질의만으로 생성되게 된다. 이런 방법을 통해 자연어처리의 단점을 극복하고 사용자가 원하는 정보를 제공하는 시스템을 제안 한다.
본 연구는 NTIS 시스템에서 사용되는 스키마를 기반으로 하여 대화형 한글 GUI를 개발하였으며, 다양한 예제를 통해서 생성된 한글질의와 SPARQL 질의를 NTIS 시스템에서 제공되는 N-Triple 데이터에 대하여 질의를 수행하여 결과를 검증하였다.

본 논문은 ‘컴퓨터가 이야기를 만들 수 있을까’라는 단순한 질문에서 시작되었다. 이 질문은 90년대 중반부터 등장한 ‘디지털 스토리텔링’이라는 광범위한 영역에 속해있는 것이었지만, 국내 디지털 스토리텔링 연구에서는 거의 다루어지지 않았던 주제였다. 리서치 과정에서 인공지능 연구와 맞물려 활발히 진행되고 있는 ‘스토리 생성 시스템’ 연구를 접할 수 있었고 이를 논문의 형식으로 정리하고 싶은 생각이 들었다.
스토리 생성 시스템 연구는 서사성을 갖춘 완성도 높은 스토리를 만들기 위하여 디지털 스토리텔링의 기술이 창작과정과 어떻게 만나야 하는가에 대한 연구이다. 여기서 스토리는 비선형성, 다중매체성, 네트워크성, 상호작용성 등으로 특징져지는 ‘새로운’ 스토리가 아니라 기존의 문자매체와 영상매체에서 만들어져 온 서사성을 갖는 전통적ㆍ고전적 스토리이다. 디지털 스토리텔링의 특성을 최종적으로 만들어 진 스토리 ‘결과’에서가 아니라 스토리 창작 ‘과정’에서 찾으려는 시도라고 할 수 있다.
스토리 생성 시스템은 컴퓨터가 자동적으로 어떤 스토리를 만들어내는 것을 의미하지 않는다. 인간 작가에게 체화되어 있는 스토리 창작과정의 노하우(암묵적 지식)를 명시적 지식으로 전환한다. 여기에 기존 문자서사와 영상서사의 구성 방식에 대한 서사학과 기호학의 연구를 더해서 알고리듬화하고 이를 컴퓨터 ‘스토리 엔진’으로 구성한다. 인간 작가가 이 컴퓨터 프로그램을 활용하여 새롭고 창의적인 방식으로 스토리를 창작하는 인간-컴퓨터 상호작용(HCI) 시스템을 스토리 생성 시스템이라고 할 수 있다. 여기서 컴퓨터는 스토리 창작과정에 대한 집합적 지식을 갖추고 있으며 인간 작가의 행동에 탄력적으로 반응하는 일종의 새로운 스토리텔러이다.
그렇다면 스토리 생성 시스템의 핵심이라고 할 수 있는 컴퓨터 스토리 엔진은 어떻게 만들어질 수 있을까? 일반적인 컴퓨터 어플리케이션 개발 과정과 같이 실행,평가, 보완의 피드백으로 연결되는 분석-설계-개발의 복잡한 단계를 거쳐야 할 것이고, 본 논문은 그 첫 번째 단계(분석가의 역할)에 집중한다. 첫 번째 단계의 주된 내용은 스토리 창작과 관련된 명시적 지식을 체계적으로 분석ㆍ정리하여 모델링(다이어그램 시각화)하는 것이다.
본 논문에서는 시스템 다이내믹스를 중심 방법론으로 활용한다. 시스템 다이내믹스는 피드백 루프와 저량/유량 변수라는 개념으로 복잡계 시스템을 동태적으로 파악하는 시뮬레이션 방법론이다. 스토리는 인물ㆍ사건ㆍ시공간이라는 요소가 복잡하고 역동적인 비선형 상호작용을 하며 전체를 변화시키는 복잡계 시스템의 한 유형이고, 시스템 다이내믹스는 이런 스토리의 구조와 요소, 작동방식을 시각화하여 모델로 만드는 데 적합한 방법론이다.
본 논문은 크게 두 부분으로 나누어진다. 첫 번째는 선행 연구들을 중심으로 스토리 생성 시스템의 역사, 분류체계, 성과와 한계에 대해서 다루는 부분이다. 여기서는 제작 프로세스와 스토리 생성 주체라는 기준으로 스토리 생성 시스템을 세분화하고, 연구의 핵심 방법론을 네 가지 카테고리(데이터베이스, 자연어 문맥인지 및 생성, 플롯 플래닝, 지능형 에이전트)로 구분하여 성과와 한계를 살펴본다. 두 번째는 대안적 스토리 엔진 설계를 위한 방법들을 모색해보는 부분이다. 디자인 타임 모델과 런 타임 모델을 비교ㆍ분석하고, 스토리 시각화의 방법들을 방법을 살펴본다. 선행 연구의 한계와 문제점을 보완하여 시스템 다이내믹스를 적용한 스토리 엔진 모델을 가설적으로 제시한다.
스토리 생성 시스템에 대한 본격적인 연구가 없는 국내 상황에서, 본 논문은 실험적인 시도로서 의미를 가질 수 있을 것이다. 논문을 쓰면서 HCI의 의미에 대해서 다시금 생각해볼 수 있었다. 컴퓨터가 인과성과 합리성에 근거해서 인간에게 질문을 던지고 제약을 가하면, 인간은 이 질문의 의미를 곱씹으면서 문제를 해결하고 돌파구를 마련하는 것. 이것이 역동적이고 창의적인 HCI의 진정한 의미일 것이다.This thesis began with a simple question : can a computer create stories? This question initially belonged to 'digital storytelling,' a broad area which emerged in the mid-1990's, yet has rarely been asked by Korean digital storytelling researchers. This thesis concerns the studies of 'story generation systems,' a currently very active area of research closely related to artificial intelligence.
Story generation system research has focused on how to utilize technology of digital storytelling in the process of creating stories with quality narrativity. Here, a story is not a 'new' story characterized by elements such as non-linearity, multimedia, networks, and interactivity, but a traditional and classical story having narrativity based on established textual and visual media. The research mainly attempts to discover the characteristics of digital storytelling in the process of story creation itself, rather than its outcome.
Story generation system should not be mistaken with a computer automatically generating a certain story. It transforms tacit knowledge about story creation, which is internalized within a human author, into explicit knowledge, which is then algorithmized along with research from narratology and semiotics and composed into a computer 'story engine.' Story generation system is a human computer interaction system in which a human author utilizing the computer program to make stories in a new and creative way. Here a computer is a new kind of storyteller, equipped with collective intelligence regarding the story-making process and a flexible responsiveness to the human author's actions.
How then can such a computer story engine be made, which is the core of the story generation system? It requires a complex process of analysis-design-development followed by execution-assessment-complement, just as in the normal development process for computer applications. This thesis focuses on the first step, which is the role of an analyst. The main objective of the first step is to systematically analyze and arrange explicit knowledge related to storytelling and create visual models based upon it.
System Dynamics is utilized as the primary methodology of this thesis. System Dynamics is a simulation methodology that dynamically detects the complex system with feedback loops and stock/flow variables. Story is a type of the complex system which changes the whole system through the dynamic and non-linear interactions of elements such as characters, events, and time-space. System Dynamics is an appropriate methodology in visualizing the story's structure, elements, and working mechanism.
This thesis can be divided into two main parts. The first part concerns the history of the story generation system, as well as its classification schemes, achievements and limitations. The system is examined by subdividing the system on the basis of the production process and the subject of story generation and by dividing the main research methodologies into the following four categories : database, natural language context and creation, plot planning, and intelligent agent. The second part of the thesis then investigates ways to design alternative story engines by comparing and contrasting design-time models with run-time models and seeking various ways to visualize stories. A tentative story engine model designed by fixing previous models' limitations and applying system dynamics will then be provided.
This thesis is an experimental yet meaningful attempt in Korea, where no full-scale research on the story generation system has ever been done. Writing the thesis also provided opportunities to re-consider the meaning of HCI. If a computer asks questions to humans and imposes limits based on causality and rationality, humans may have better chances to solve problems by pondering upon the meaning of such questions, which I believe is what HCI is all about.Ⅰ. 서론 = 1 1. 연구 배경 및 문제제기 = 1 2. 연구 범위 및 방법 = 4 Ⅱ. 본론 1 - 스토리 생성 시스템 연구의 역사와 분류 = 9 1. 스토리 생성에 대한 역사적 고찰 = 9

With development of information technologies, advances in Web technologies produced a large amount of information, made network such as Internet a critical element inevitable for daily life and brought improvement in quality of information like free information exchange over Internet.
Conventional Web has put emphasis on convenience to facilitate use of HTML so that everyone can easily access Hypertext information, resulting in a drastic increase in the volume of information.
As a result, users are now confronted with challenges in searching desired information.
Typical approaches that have been used till now to search information are usually based on keywords using frequency of words or word information. Therefore, since many documents irrelevant to user's query are included in the searched results, user has to search the represented results further to obtain a desired result.
To resolve this problem, interests have been focused on a semantic Web in which a search engine itself searches the information required for user through conceptualization, and ontology is at the center of it.
Ontology can be regarded as a kind of dictionary that defines relations between words. It explicitly expresses the conceptual architecture in a specific domain to enable modeling of concepts closer to a real world around domain as a platform for building a search system.
To build an ontology, we collected product related documents of each manufacturer, analyzed the documents and built a product domain ontology in OWL based on classification and features frequently found in those documents. In addition, synonym list, deduction and natural language processing functions were added to the ontology to allow expansion and deduction in searching.
A database was built with the product data of about 40,000 items of Mart A operating on/off-line discount stores nationwide, and the user interface was developed using JSP and PowerBuilder9.0.
We applied two techniques for ontology-based product search. One is searching by class in which users directly select a class of desired product from the ontology classes. The other is the natural language search method in which the keyword entered by user is searched through natural language processing.
From the experiments on the two ontology-based searching methods, we could see that, unlike the conventional keyword search methods, it is possible to implement precise searching by giving meaning to the keyword itself using the concepts of classes and attributes, synonym list and deduction function defined in the ontology. Moreover, it is also feasible to search products with similar characters or those unexpected by user by semantic expansion of the searched products.
By building an ontology-based intelligent product search system with this search architecture, we could obtain searching results that is impossible with the existing keyword-based searching methods and implement more precise searching.목차 = ⅰ 표목차 = ⅲ 그림목차 = ⅳ Abstract = ⅵ Ⅰ. 서론 = 1

많은 기업 또는 공공기관(단체)에서는 제품 개발에 대한 기초 자료로 사용하거나 또는 특정한 의사결정을 위해서 고객의 의견을 청취하길 원하고 있으며, 이를 위해서 SNS에 의해서 사용자가 직접 생산한 콘텐츠를 확보하여 고객의 의견을 파악하려고 노력을 하고 있다.
하지만 SNS 데이터를 확보하고 그 속에 숨어 있는 고객의 의견을 찾기 위해서는 큰 노력이 필요하고 데이터를 어떻게 확보하고 분석해야 하는지 방법을 찾기가 쉽지 않은 이유로 기업 또는 공공기관(단체)에서는 SNS 데이터를 활용하여 고객의 의견을 파악하는데 어려움이 있었다.
한편 인터넷이 널리 보급되고 IT기술의 발전으로 컴퓨터뿐만 아니라 모바일기기, IPTV, IOT(Internet Of Things) 등 다양한 장비(Device)의 출현으로 하루에 생성되는 소셜 네트워크 데이터가 급증하고 있으며, 이렇게 방대하게 생성되고 있는 데이터들을 분석할 수 있는 빅데이터 관련 기술이 지속해서 발전하면서 빅데이터 분석 기술을 이용, 필요한 고객의 소리를 청취할 수 있는 환경이 되었다.
본 연구에서는 기업의 제품개발에 기초자료로 활용하거나 공공기관(단체)의 의사결정을 위해 활용할 수 있도록 SNS 데이터를 수집하고 자동으로 분석하여 서비스하는 시스템을 연구하였다.
방대한 SNS 기반의 SNS 데이터는 일일 수집 데이터 2,000만(R社 Crawler 기준) 건으로 수작업으로 분석하는 것이 불가능한 빅데이터이다. 빅데이터의 특징은 3V로 요약하는 것이 일반적이다. 즉 데이터의 양(Volume), 데이터의 생성 속도(Velocity), 형태의 다양성(Variety)을 의미한다(O’Reilly Radar Team, 2012). 기존의 빅데이터 분석은 수집(저장), 분류, 분석, 인사이트 발굴, 리포트 생성의 절차를 거쳐서 미래를 예측하는 분야에 이용해 왔다.
이러한 빅데이터 자동 분류 기술이 적용되기 이전에는 데이터를 분류하기 위해 센싱 반자동 분류 시스템이 활용되어왔다. 센싱 반자동 분류 시스템이란 수집된 데이터를 온톨로지 사전 기반으로 자동으로 일차적으로 분류하고 분류된 데이터 중에서 분석 목적에 부합하는 데이터를 작업자가 수작업으로 선별하여 지식에 해당하는 정보를 추가하여 시스템에 저장하는 방식의 시스템이다. 센싱 반자동 분류 시스템에서는 작업자가 수작업으로 정제하고 분류한 데이터를 활용하여 SNS에 표출되는 사람들의 의견을 청취할 수 있는 환경을 제공한다.
하지만 센싱 반자동 분류 시스템은 수집된 SNS 데이터에서 특정 조건에 해당하는 데이터를 검색한 이후에 작업자가 수작업으로 데이터를 정제하기 때문에 모든 데이터를 정제하는 것은 불가능에 가깝다. 따라서 특정 이슈나 특정 제품을 미리 지정한 이후에 관련된 데이터만 선별하여 수작업으로 정제하여야 한다. 따라서 전방위 산업에 적용하는 것은 불가능에 가깝다. 또한 특정 제품이나 분야로 한정을 하여도 작업자가 수작업으로 데이터를 선별하고 분류하기 위해 엄청난 시간과 노력이 필요하므로 높은 비용이 발생하는 문제점이 있다.
본 연구에서는 이러한 반자동 센싱 시스템의 문제점인 데이터 분류의 한계를 극복하고 시간을 단축하여 비용을 절감하는 것을 목적으로 센싱 자동 분류 모델을 연구하였다.
수집된 모든 SNS 데이터를 자연어 처리 및 기계학습을 통하여 자동 분류하고, 사용자 의견이 담긴 문장을 분석한 후 인구통계적 프로필 정보를 매시업(Mashup)해서 다양한 분석 결과를 제공함으로써 보다 많은 데이터에서 보다 신속하게 다양한 분야의 사용자 의견을 청취할 수 있는 SNS 데이터 기반 센싱 자동 분류 모델(SACM: SNS Data_based Sensing Automatic Classification Model)을 제시하고 SACM 기반의 서비스 시스템을 연구하였다.
본 연구에서 제안하는 SNS 데이터 기반 센싱 자동 분류 모델은 기존의 반자동 센싱 시스템의 문제점인 전방위 산업의 데이터를 분류할 수 없었던 문제점과 수작업으로 데이터를 분류함으로써 나타나는 주관적 견해로 인한 평가 오류의 문제, 그리고 분류 시간의 증가 문제, 또한 수작업 분류를 담당하는 직원의 업무 효율 저하 문제를 모두 해결할 수 있는 시스템이며, 이를 위해 자연어 처리 및 기계학습에 의한 자동 분류와 문장 분석 기법을 제시하였다.
본 연구에서 제안하는 모델이 기존의 비효율적인 반자동 센싱 시스템을 대체하여 효율적이고 정확한 고객의 목소리를 찾아내어 산업의 많은 분야와 공공기관(단체)에서 사용할 정책 발굴의 기초 자료가 되는 효과를 줄 수 있기를 기대한다.A lot of companies or government offices want to listen to public opinion to leverage the information in the process of developing products or making decisions. They are trying to acquire contents which users post on social media so that they can understand their opinion.
Companies or government offices have been facing challenges understanding customer’s thoughts from SNS data because it requires so much effort to get SNS data and find the meaning behind it. Besides, it is not easy to find a way to collect and analyze the data.
Meanwhile, social data has been increasing rapidly due to such widespread access to internet and development of Information Technology which has created a variety of devices such as computers, mobile devices, IPTV and IOT(Internet of Things). Big data technology has been constantly developing to keep up with the significant quantities of data. It has set the stage for listening voice of customer based on big data technology.
This study is about a system which collects and analyzes SNS data automatically for companies to use the results as a basis for developing products or for government offices to make decisions upon the results.
SNS data runs to average 20 million posts per day (according to Company R’s crawler). It is impossible to analyze that much of big data manually. It is common to summarize the properties of big data into 3Vs (O’Reilly Radar Team, 2012). Volume refers to the amount of data, velocity refers to the speed of data processing and variety refers to the number of types of data. The conventional big data analysis has been applied to predicting the future through the process of collecting(storing), classifying, analyzing data, finding insight and producing report.
Sensing semi-automatic data classification system has been used for classifying data before this automatic big data classification technology was applied. Sensing semi-automatic data classification system conducts the initial classification automatically based on ontology dictionary. Workers select the data manually which fit for the analysis purposes, add their own knowledge and store the data in the system. Sensing semi-automatic data classification system provides the environment to listen to public opinion expressed on social media by cleansing and classifying the data manually.
However, it is nearly impossible for sensing semi-automatic data classification system to cleanse every single data because workers cleanse the data manually after searching under a certain condition. It is required to specify a particular issue or product in advance and cleanse the related data manually. Therefore, it is almost impossible to apply to the entire industries. Besides, it takes significant amount of time and effort of workers to select and categorize the data manually even though the coverage of the products or fields is limited. It causes the problem of high cost.
This study focuses on sensing automatic data classification system which is aimed to overcome the shortcomings of sensing semi-automatic data classification system and reduce the cost by saving time.
This study suggests SACM(Sensing Automatic Classification Model for SNS Data) and covers service system based on SACM which enables users to listen to public opinion in various fields from much more data by categorizing SNS data automatically through natural language processing and machine learning, analyzing sentences with public opinion in and providing various analysis results after mashing up with demographic information.
The conventional sensing semi-automatic data classification system has a few problems. It is not able to classify the data of the entire industries. Categorizing data manually might cause errors in the results sometimes because the workers cannot stay objective all the time. Moreover, it takes so much time and causes decline of efficiency of the workers with the course of time. SACM is a system which can solve all the aforementioned problems. This study suggests natural language processing and automatic classification and sentence analysis technique based on machine learning.
This model is expected to replace the ineffective sensing semi-automatic data classification system and find the accurate voice of customer so that many industries and government offices can use the information from the system to have the basis for developing strategies or polices.제 1 장 서 론 1 1.1 연구의 배경 1 1.2 연구의 목적 6 1.3 연구의 내용 및 범위 9 1.4 연구의 방법 11

BIM을 이용한 설계 검토는 시공의 정확성 등의 이유로 전 세계적으로 활발하게 연구되고 있고, 실제 건설 현장에서 폭 넓게 활용 되고 있다. 건축 시설물의 유형 중 병원 건축물은 타 시설물과 비교를 했을 때, 설계 요구사항이 복잡하고 다양하기 때문에 여러 객체정보를 추출 할 수 있다고 판단하였다. BIM을 이용한 룰 기반 설계 검토를 위해서 필요한 객체정보를 국제병원설계가이드로 꼽히는 FGI 병원시설물 설계 가이드 분석을 통하여 필요한 객체 정보를 추출하고 국내 병원의 설계요구사항 RFP에서의 객체정보와 IFC 엔티티(entities)와 비교분석을 함으로써 병원 시설물의 BIM을 이용한 룰 기반 설계검토에 필요한 최소 객체 정보들을 알아보고 IFC스키마에서의 적용 범위를 분석하고자 한다.
텍스트기반의 병원설계가이드와 설계요구사항요청서들은 룰 기반 설계검토를 위해서는 자연어문장에서 컴퓨터가 인식할 수 있는 규칙언어로 변환을 해주어야 한다. 이를 위해서는 자연어 문장을 단순화하고 컴퓨터가 인식할 수 있는 서술어(method)와 주어, 목적어(object)로 분해를 해야 한다. 본 연구에서는 룰로 사용할 수 있는 주요 객체정보 즉, 주어, 목적어의 오브젝트로 정의하여 객체정보를 분류하였다. 분석결과 FGI 병원설계 가이드에서는 공간객체의 정보가 가장 많이 제공되고 있으며, 국내 병원 세 곳의 설계요청서를 분석하여 얻은 결과 역시 공간객체의 정보가 가장 많이 언급되었다. 이를 통하여 BIM을 활용한 규칙 검토를 위해서는 최소한 공간객체의 구성이 우선 필요하다는 것을 확인 할 수 있었다. 추가적으로 본 연구에서는 IFC2x3와 IFC4의 엔티티들을 추출 된 객체정보들과 비교한 결과 IfcSpace 객체 중심으로 모델이 구성되고 이를 바탕으로 BIM을 이용한 룰 기반의 설계 검토에서 대상이 되는 BIM 모델의 구현 수준을 제안하였다.

We propose a novel memory network model named Read-Write Memory Network (RWMN) to perform question and answering tasks for large-scale, multimodal movie story understanding.
The key focus of our RWMN model is to propose the read network and the write network that consist of multiple convolutional layers, which enable memory read and write operations to have high capacity and flexibility.
While existing memory augmented network models treat each memory slot as an independent block, our use of multi-layered CNNs enables the model to read and write sequential memory cells as chunks, which is more reasonable to represent a sequential story because adjacent memory blocks often have strong correlations.
For evaluation, we apply our model on the MovieQA benchmark, and achieve the best accuracies on several tasks, especially significantly on the visual QA task.
Our model shows a potential to better understand not only the facts in the story, but also more abstract information, such as relationships between characters and the reasons for their actions. Code is available on our project page: http://github.com/seilna/RWMN딥러닝이 컴퓨터 비전 및 자연어처리와 관련된 여러 문제들에서 뛰어난 성능을 보임에 따라, 그와 더불어 시각 및 언어 정보를 통합적으로 활용하는 연구들도 빠르게 발전하게 되었다.
이에 이야기 요소가 포함된 비디오의 내용을 이해하고, 이와 관련된 자연어 질문을 이해하여 알맞은 정답을 도출하는 Movie Question Answering (MovieQA) 문제가 제시되었으며, 현재까지도 이를 풀기 위한 많은 연구가 진행되고 있다.

본 연구에서는 Movie Question Answering 문제를 풀기 위해서, External Memory 구조를 기반으로 한 새로운 모델 구조를 제안하였으며, 이를 Read-Write Memory Network (RWMN) 라고 지칭한다.
기존의 External Memory 모델들이 각 메모리 블록들을 독립적으로 취급한 반면에, RWMN은 메모리 블록들 사이에 내재하고 있는 시간적인 상관관계 (temporal correlation) 를 활용하는 Convolutional Read/Write operation을 이용하는 것이 핵심이다.
Movie Question Answering 문제에서 각 메모리 블록들이 주어진 영화의 내용을 순차적으로 인코딩하고 있다는 점을 고려했을 때, 시간적인 상관관계를 모델링하는 것은 MovieQA 문제를 푸는데 매우 중요한 역할을 하며, 이를 활용한 RWMN은 MovieQA 공식 벤치마크의 6개 subtask중 4개 task에서 가장 높은 성능을 보였다.

또한, 본 연구에서는 MovieQA에 포함된 질문들 중에서 주로 고차원적 이해가 필요한 질문들에 대해 제시한 RWMN이 기존 모델들보다 높은 성능을 내는 것을 보임으로써, 시간적인 상관관계를 잘 모델링하여 Question Answering 문제를 풀고 있음을 실험적으로 보였다.

RWMN의 구현 코드에 관한 정보는 프로젝트 페이지에서 확인할 수 있다. (http://github.com/seilna/RWMN)Chapter 1 Introduction 1 Chapter 2 Related Work 5 2.1 Neural Memory Networks . . . . . . . . . . . . . . . . . . 5 2.2 Models for MovieQA . . . . . . . . . . . . . . . . . . . . . 6 Chapter 3 Read-Write Memory Network (RWMN) 8



The construction safety sector is the area where the most data is accumulated within the construction industry and where the data is expected to be fully utilized. Automatic classification of data into desired categories is the basis for smoothly extracting information from within vast amounts of data and enabling interaction with data from different areas. Recently, many scholars have been conducting research on selecting neural network models and optimizing model parameters to enhance classification performance, but there is a lack of consideration on the quality and processing of available data that has been accumulated so far.
In this study, experiments for classifying the text data of construction severe accident cases into five categories are conducted: falls, electric shock, strikes, collapses, and crushes using CNN (Convolution Neural Network) algorithm. The initial classification performance was 29.44%, but the reasons for the low classification accuracy were analyzed and 1) keyword and number pre-treatment, 2) detailed classification of the falling data, 3) classification of the primary cause in the composite accident, 4) Accuracy Change experiment were conducted.
The above experiment resulted in performance improvements in three experiments, excluding detailed classifications of falls data, and the classification performance was improved to 50.42% when complex accidents were eliminated. In the process of recording complex accidents during construction, it was found that various disaster properties were mixed and recorded within one accident, thus disturbing accurate classification in existing categories.
The results of this study suggest that pre-analysis and processing of data from multiple accidents, such as complex accidents, are necessary to categorize the construction disaster cases into desired categories and extract accurate information from them. It also contributes to the usability of research by presenting a general-purpose text classification model in documents in the construction industry as well as construction cases.건설 안전 분야는 건설산업 내에서도 데이터 축적이 가장 활발하게 일어나는 분야중 하나이며 사고 사례로 축적된 텍스트 데이터의 활용이 기대되는 분야이다. 해당 데이터들을 보다 충분히 학습하고 활용하여 기존에 담고 있던 지식 그 이상의 통찰을 얻어내기 위해서는 자연어 처리에 기반한 문서 및 문장 분류 기술이 필요하다.
최근 학계에서는 텍스트 분류 성능을 높이기 위한 신경망 모델 선정 및 모델 파라미터 최적화 연구가 다수 진행되고 있으나 지금까지 축적되어온 활용가능한 데이터의 질과 처리에 관한 고찰은 부족한 실정이며 특히 한국어로 축적된 데이터에 대한 분석은 언어 자체의 특수성과 분류 모델이 활발히 개발되지 못하는 특성으로 인하여 영문 텍스트 데이터에 대한 연구보다 활발히 진행되고 있지 못한 실정이다.
이에 본 논문은 1) 한국어 비정형 텍스트 데이터를 문서 내부의 정보를 학습하여 특정 범주로 분류하는 모델을 구현하고, 2) 한국어로 축적된 건설재해사례 데이터를 재해유형별로 분류하는 실험을 통하여 분류성능을 개선할 수 있는 방안을 데이터 관리 및 분석의 측면에서 제시하는 것을 목적으로 한다.
본 연구에서는 건설 중대재해 사례 텍스트 데이터를 CNN(Convolutional Neural Network)알고리즘을 사용하여 추락, 감전, 낙하, 붕괴, 협착 5가지의 범주로 분류하는 실험을 진행하였다. 실험의 초기 분류 성능은 29.44%였으나 분류 정확도가 낮게 나오는 이유를 분석하여 1) 키워드 및 숫자 전처리 여부, 2) 추락데이터의 세부 분류, 3) 복합사고에서 1차 원인으로의 분류, 4) 복합사고 제거 여부에 따른 정확도 성능 변화 실험을 진행하였다.
위 실험 결과 추락데이터에 대한 세부 분류를 제외한 세 가지 실험에서는 성능 향상 결과가 도출되었으며 복합사고를 제거할 경우 분류 성능이 50.42%까지 향상되었다. 건설 중대재해 중 복합사고를 기록하는 과정에서 한 사고 안에 여러가지 재해 속성이 혼재되어 기록되면서 기존 분류 범주에서의 정확한 분류를 방해하는 것으로 밝혀졌다.
본 연구 결과는 건설 재해 사례를 원하는 범주로 분류하고, 궁극적으로 그 속에서 정확한 정보를 추출하고 데이터를 원하는 곳에 활용하기 위해서는 복합사고와 같이 여러 사고가 함께 기인한 데이터에 대한 사전적인 분석과 처리가 필요함을 시사한다. 따라서 공개된 건설재해 데이터들을 활용한 적절한 예방대책과 데이터 분석 기반의 예측시스템, 위험 요소 감지 시스템 등의 개발을 위해서는 정부 및 기관에서 재해 사례를 기록함에 있어 복합사고의 경우 사고의 원인이 된 1차사고를 기준으로 데이터를 축적해야 한다. 또한 연구자들이 우리나라의 건설재해 사례 데이터를 분석함에 있어서 복합사고의 정제 및 처리에 주의를 기울인다면 연구의 결과 및 활용성에 있어서 저 나은 결과를 기대할 수 있을 것이다.Ⅰ. 서론 1 A. 연구의 배경 및 목적 1 1. 사회적 배경 1 2. 학술적 배경 3 3. 연구의 목표 5

본 논문에서는 한국어 대명사들의 의미 해석에 관여하는 여러 층위의 제약 들과 특히 통사적 제약들(syntactic constraints)을 체계화하는 것을 그 목표로 하였다. 분석과 기술(description)은 Unification-based Grammar 중의 하나인 LFG(Lexical-Functional Grammar)와 Mary Dalrymple의 결속 이론(結束 理論 / Binding Theory)에 의거하였다. 본 논문은, 자료체의 제시와 그 분석을 통하여, 한국어 대명사들은 그것들이 결속되거나 자유로워야 하는 결속 혹은 비결속 영역(binding domain / disjoint domain)에 대해서 뿐만 아니라 그것들의 선행사(先行詞 / antecedent)가 될 수 있는 (혹은 선행사가 되어서는 않되는) 성분들의 조건에 대해서도 상호간에 구분이 됨을 보였다. 이러한 사실은 한국어 대명사들은 "재귀대명사"와 "비재귀대명사"로 구분이 될 수 있으며 각 부류의 대명사들의 해석은 보편적인 규칙들에 따른다는 전통적이며 일 반적인 견해의 경험적인 문제점을 명확히 드러내었다. 그와 같은 문제점을 피하기 위해, 본 논문에서는 결속 요건들(binding requirement)을, 대명사류 전체에 적용되는 일반적이고도 보편적인 통사적 규칙으로서가 아니라, 각 대 명사의 어휘부(lexical entry)에 등재되어야 할 개별적인 어휘 정보로 간주하였다. 각 대명사의 어휘부에 등재될 결속 요건들은 Per-Kristian Halvorsen & Ronald M. Kaplan(1988)과 Mary Dalrymple(1993) 등이 제안한 Inside-Out Functional Uncertainty와 LFG의 formal language를 사용한 Binding Equation 의 형태로 제시되었다. 자연어에서 대명사의 사용은 필수적이다. 한국어 대명사들의 binding equation 들은 다양한 자연어 처리 과정에서, 예를 들어, text parsing, text generation 혹은 machine translation의 과정 중에 긴요 하게 사용될 수 있는 정보를 구성한다.

본 연구는 CD-ROM 데이터베이스의 탐색을 하는 이용자들의 탐색행태 및 탐색환경을 조사하여 CD-ROM 데이터베이스 탐색행위에 영향을 미치는 요인을 분석하였다. 그리고 CD-ROM 데이터베이스를 운용하는 시스템의 탐색 환경에 대한 일반적이고 종합적인 특성을 알아보기 위하여 한국의학도서관협의회 소속 의과대학 도서관과 병원도서실을 대상으로 이용자의 탐색행태를 조사하였다.
CD-ROM 탐색은 일반적으로 이용자에 의해서 행해지지만 의학분야에서는 각 도서관이 특성상 담당사서에 의한 대리탐색을 부분적으로 하고 있다는 점과 적극적인 봉사의 형태로 탐색서비스를 수행하고 있는 바 담당사서의 대리탐색은 계속될 것이다. 따라서 담당사서와 이용자의 CD-ROM 데이터베이스 탐색행태를 비교분석하고 시스템의 주변환경과 제공하는 탐색서비스의 유형이 탐색행태에 영향을 주고 있는가에 대한 분석을 하였다.
분석결과를 요약하면 다음과 같다.
1. CD-ROM 네트워크를 구축한 도서관은 6개 기관으로 모두 의과대학 도서관이다. 가장 많이 보급된 형태는 2-4개의 CD-ROM 드라이브를 사용한 일인 전용형 시스템으로 49%를 차지하였다. 시스템이 제공하는 정규교육과정은 전무한 상태이고 담당사서는 CD-ROM 탐색 당시 이용자의 요구에 의한 안내를 주로 하고 있다. CD-ROM 검색과 관련된 이용자의 질문은 즉답형 질의로 CD-ROM 사용시 발생하는 컴퓨터의 조작상의 문제가 대부분이다.
2. CD-ROM 탐색의 이용연령층은 20-30대에 90%가 집중되어 있다. 전공과목 별로는 임상의학분야의 이용자가 많이 이용하고 있고(75%), 직위별로는 전공의, 임상강사, 조교등이 61%를 차지하였다. 정보가 필요할 때 이용하는 채널은 소속 도서관으로, 도서관에 설치된 CD-ROM 검색이 35%로 가장 높았다. 주로 이용하는 정보원은 학술잡지, CD-ROM 검색, 고급교과서 순으로 CD-ROM 도입에 따라 이용자의 정보이용행태가 변화하고 있음을 알 수 있었다. 이용자의 소속 기관에 따라 정보이용과 CD-ROM탐색의 목적이 유의한 차이를 보이고 있는데 의과대학 도서관의 이용자보다는 병원도서실의 이용자가 논문작성과 임상검토 를 목적으로 CD-ROM을 주로 활용하고 있는 반면, 의과대학 도서관의 이용자는 학문적 연구를 위한 목적에 CD-ROM을 이용하였다. CD-ROM 검색 후 원문 제공의 여부가 관련성이 깊었다.
3. 담당사서와 이용자의 탐색방식에 대한 분석 결과 담당사서에 비하여 이용자는 메뉴방식을 선호하였다. 담당사서와 이용자의 자연어 및 통제어 사용은 이용자는 주로 자연어 탐색을 하고 있는 반면 담당사서는 통제어와 자연어를 병행하여 사용하고 있는데 경험과 교육정도가 높은 이용자는 담당사서의 탐색 방법과 유사한 경향을 보였다. 제한탐색의 경우 담당사서는 5%가 사용해본 경험이 없는 반면 이용자의 56%가 전혀 사용하지 않았는데 반면 탐색경험과 이용자 교육정도가 높은 이용자는 사서의 행태와 유사하게 제한검색의 빈도가 높았다.
CD-ROM 검색시스템을 사용하고 있는 이용자의 만족은 다음과 같이 분석되었다.
1. 이용자가 정보자료를 이용할 때의 만족도는 담당사서가 도서관 근무 경력이 많은 도서관일수록 높았고, 이용자가 CD-ROM 데이터베이스의 사용을 많이 할수록, 시스템이 제공하는 보조도구의 수가 많을수록 정보이용에 만족하고 있다. 즉, 이용자의 도서관 이용에 사서와 시스템이 제공하는 서비스가 영향을 주고 있다.
2. 원문의 입수시간이 빠른 도서관일수록 이용자의 연구수행에 주는 만족도는 높았다. CD-ROM 탐색 후 원문의 제공을 이용자가 원하고 있고 이는 의학도서관 협의회내에 정보유통을 활성화하는 요인으로 작용하고 있다.
3. CD-ROM의 탐색의 경험이 많아질수록 도서관의 이미지는 향상되었다.
4. 검색결과의 만족도에 영향을 주는 요인은 시스템이 제공하는 CD-ROM 이용에 대한 교육정도와 원문입수 시간이다.
5. CD-ROM의 유용성은 탐색 속도가 빠르고, 다수의 CD-ROM을 사용할 수 있는 드라이브를 갖춘 도서관과 원문의 입수가 빠른 도서관에서 긍정적 반응을 보였다.
연구 결과 다음과 같은 제언이 가능할 것이다.
도서관이 이용자에게 제공하는 탐색서비스의 방법을 개선해야 한다. 소극적으로 행해온 이용교육 방법에서 벗어나 도서관이 정책적으로 CD-ROM 및 도서관 이용 교육을 확대하여야 한다. 담당사서가 CD-ROM이 가진 장점을 활용하여 탐색서비스를 개선한다면 이용자의 전체적인 만족도는 향상될 것이다
또한 CD-ROM 검색시스템이 제공하는 모든 탐색기법을 사서가 숙지하여 시스템을 효율적으로 이용해야 한다. 또한 보다 능동적인 임상의학 정보봉사를 위하여 의학도서관의 전문사서제도가 도입되어야 한다.The purpose of this study was to identify the factors affecting user's searching behavior with CD-ROM MEDLINE database, and survey searching environment and behavior of users.
In regard to identify general and composite characteristics of CD-ROM MEDLINE system, It made a general survey of the user's searching behavior in medical college libraries and hospitals libraries, They all belonged to Korean Medical Libraries Association.
CD-ROM MEDLINE searching was usually done by end user, but partly, by intermediary, in Medical Science. Accordingly, analysized searching behavior of intermediary and maybe end user will be done continuously intermediary searching.
Results of this paper were summarized as follows:
1. Six libraries constructed CD-ROM Network. They were all medical college libraries. It was most extensive figures that stand alone system including two or four CD-ROM drive. The formal education related CD-ROM MEDLINE searching was wholly lacking in medical libraries. Librarian introduced information about user's problems. User's question related CD-ROM searching was ready reference. It was mostly computer handling methods.
2. An age group of CD-ROM MEDLINE users was between twenties and thirties and it occupied 90 percent wholly. It was the majority of the search from faculty related Internal medicine and then surgery, basic medicine (pharmacology, biochemistry, biology), pediatrics and clinical pathology were next in this order.
3. The type of the objectives of CD-ROM MEDLINE searching were as follows:
There were significant difference between medical college libraries and hospital libraries. The users of medical college libraries utilized for the purpose of clinical treatment, but the users of hospital libraries used in order to research a subject.
The users of CD-ROM MEDLINE felt that recognition on library was improved using CD-ROM, and there were significant difference between groups by utility frequency. The facter affecting in the degree of satisfaction for retrieval result of CD-ROM MEDLINE was the education of CD-ROM MEDLINE searching and the acquired time of requested orignal article by the library.
In conclusion, the user's impression of CD-ROM MEDLINE was positive, So users were readied for orientation courses on the use of the CD-ROM MEDLINE system. Although users expressed considerable satisfaction with intermediary search, they should be educated of CD-ROM MEDLINE searching.
From this point of view, Librarian must always evaluated some different advantages and disadvantage in searching information among several CD-ROM MEDLINE versions.목차 국문초록 = v 1. 서론 = 1 1.1. 연구의 배경 = 1 1.2. 연구의 목적 = 2

이 연구는 자동차 내에서 음성 인터페이스의 오류 발생 시 효과적으로 오류를 회복하는 전략에 대해 알아보고자 하였다. 이를 위해 음성 인터페이스의 두 가지 오류 유형인 불가해성 오류와 오인식 오류를 구분하여 각각의 오류 상황에서 제시할 수 있는 오류 회복 전략을 알아보았다.
첫 번째 연구는 불가해성 오류에 대한 음성 오류 회복 전략에 대한 내용이었다. 이 오류는 시스템이 사용자의 발화에 대해 어떠한 결과도 제시할 수 없는 경우를 뜻한다. 자동차 시뮬레이터를 활용한 실험을 통해 주행을 하며 음성 인터페이스를 사용하게 하였고 선행 연구를 바탕으로 선정한 3가지 오류 회복 전략을 비교하였다. 이를 통해 시스템이 제시한 오류 회복 전략에 따라 사용자가 인식하는 오류의 발생 원인과 참가자의 재발화 방식이 달라지는지 확인하였다. 참가자들은 재발화 요청(“다시 말씀해주시겠어요?”)에 해당하는 피드백을 들었을 때, 오류가 발생한 원인이 ASR Error(소리는 입력이 되었으나 발음상의 문제로 시스템이 어떠한 처리도 할 수 없는 상황)때문이라고 생각했다. 이를 해결하기 위해 ‘천천히 & 또박또박 말하기’와 ‘그대로 반복’을 하는 재발화 패턴을 보였다. 재질문(시스템의 이전 질문이 다시 재생됨)을 하는 회복 전략을 들었을 때 참가자들은 Input Error, 즉 시스템 자체에 소리가 입력이 되지 않아 오류가 발생한 것이라 생각했다. 이를 회복하기 위해 ‘그대로 반복’과 ‘크게 말하기’를 사용하여 재발화 하는 것을 알 수 있었다. 마지막으로 예시를 제공해주는 오류회복 전략을 들었을 때에는 Out of grammar(시스템에서 제공하는 기능이지만 말하는 방식이 잘못되거나 시스템이 이해하기 어려운 방식으로 명령하여 발생한 오류)가 오류 원인이라고 생각했다. 따라서 오류를 회복하기 위해 ‘발화 바꾸기’와 ‘키워드만 말하기’를 활용하여 오류를 회복하려는 것을 보였다.
두 번째는 오인식 오류에 대한 음성 오류 회복 전략에 대한 내용이었다. 이 오류는 사용자의 명령을 잘못 처리하여 의도와는 다른 답변을 제공해주는 오류이다. 이와 같은 오류는 오류발생을 바로 인식하지 못할 수 있다는 문제가 있기 때문에 오류 발생 후에 회복하는 것 보다 오류를 미리 방지하는 것이 중요하다. 따라서 오류를 예방하는 회복 전략 중 자연어 처리 결과를 제시하여 확인하는 명시적 확인 전략을 중심으로, 주행 중 어떠한 모달리티로 오류를 회복하는 것이 효과적인지 알아보았다. 그 결과 시청각 멀티 모달리티로 오류를 회복했을 때 운전자의 인지부하가 가장 낮았으며, 시각으로 제공할 경우 인지부하가 가장 높았다. 선호도는 시각보다는 청각을 더 선호하는 것으로 나타났다. 명시적 확인 전략은 시스템이 사용자에게 자연어 처리 결과를 제공하며 그 중 원하는 것을 선택하도록 하기 때문에 제공되는 선택지의 개수 또한 중요한 변수가 될 수 있다. 실험 결과 선택지의 개수가 클수록 인지부하가 높아지고 선호도가 낮아지는 것을 확인하였다.
이 연구는 음성 인터페이스에서 발생하는 오류를 해결하는 오류 회복 전략을 자동차 주행 맥락에서 알아보고자 하였다. 시스템에서 제공한 오류 회복 전략에 따라 오류 원인의 인식과 재발화 방식이 달라진다는 것을 확인하였으며, 오류 회복 상황에서 운전자의 인지부하를 줄이는 모달리티에 대해 알아보았다. 결론적으로 자동차용 음성 인터페이스 사용 시, 오류회복에 적절한 재발화 전략과 모달리티를 확인하여 실제 상용 자동차에 적용 가능한 오류 회복 방식을 제시할 수 있을 것으로 보인다.Voice user interface is considered commonly used interaction methods in automotive vehicle system since that interface can be handled without direct contact, and also no need to effort on learning the operation procedure. Despite the advantages stated above, when the voice error occurs, drivers may have distraction to treat the voice interface error. There are two ways to handle the voice errors-prevent before the error occurs or clearly recover the occurred errors.
In this paper, error recovery strategies were treated focusing on two types of 'understanding error'; non-understanding and misunderstanding. First, to study the non-understanding error, each 3 error recovery strategies consists of (1)Ask-repeat, (2)Re-prompt and (3)You can say; YCS(give some examples with utterance and apologize process) were used when the errors occurred. The results were shown that each error recovery strategies can be perceived to different cause about why the error is occurred: (1)ASR(automatic speech recognition) error, (2)End-point error and (3)Out of grammar. Furthermore, user used different tactics to handle the voice obstacles depending on the error strategies because they felt different reason of failure from: (1)repeat and simplification, (2)repeat and slow & clipped speech and (3)speak aloud and use more info, chosen two from seven tactic categories.
The second study is related on misunderstanding error, focus on the modality of error recovery strategy of explicit confirmation: (1)auditory, (2)visual and (3)multi-modality; auditory &visual. That means system verify the result of natural language processing and ask user that is correct. This study examines whether the type of modality or the number of word buttons which is the NLP(natural language processing)processing result, can affect drives’ workload and preference. The result is that drivers showed the lowest workload with multi-modality when they simultaneously tasked, recovering obstacles while driving and overcoming with visual way were shown as the highest. They prefer auditory way more than the one with visual modality. The increasing number of word buttons, drivers’ workload and thus, not favourable. Users responded that the errors they perceived were occurred because of the ASR error or input error.
This study has practical implications of the system feedback to overcome the voice error when it happens during driving. Also, this paper covers both types of ‘understanding errors’. The result may suggest the guidelines for the voice interfaces design in vehicle to handle each types of the error recovery strategies and the types of modality strategies would be chosen with a certain type of conditions.

오피니언 마이닝 분야 중 감성분석은 텍스트에 포함된 내용이 주관적인지 객관적인지 판별하고 작성자의 주관이 드러난 내용에 대해 감성의 극성을 분석하여 긍정(positive), 부정(negative), 중립(neutral) 중 하나로 분석하는 연구 분야이다. 초기 감성분석 연구는 대부분 웹사이트와 소셜 미디어 서비스에 나타난 의견들을 자동으로 분석하여 ‘긍정/부정’ 또는 ‘좋다/싫다’의 분석 결과를 제공하였다면, 최근 연구에서는 데이터로부터 단순한 긍/부정이 아닌 기쁨, 슬픔, 기대, 공포 등 소비자의 다양한 감정을 인식하는 감정분석에 대한 연구가 시도되고 있다. 단순한 긍정, 부정의 감성분석은 “핸드폰 액정 사이즈가 작아서 손에 쏙 들어오니 글을 입력하기가 편하다”, “핸드폰 액정 사이즈가 작아서 앙증맞고 귀엽다”와 같은 문장에서 긍정 이외의 정보를 추출하기는 어렵다. 그러나 보다 세분화된 감정분석에서는 위의 문장에서 ‘편하다’, ‘귀엽다’ 등과 같은 보다 고차원의 감정 추출이 가능하며, 이는 보다 정확한 정보 추출 및 이러한 의견을 분석하여 상품의 품질을 높이는데 기여할 수 있을 것이다. 본 연구에서 ‘긍정/부정’ 또는 ‘좋다/싫다’와 같이 극성 분류가 아닌, 텍스트 데이터로부터 세분화된 감정을 분류 하는데 목적이 있다. 따라서 본 연구에서는 긍/,부정의 극성 분류를 감성분석이라 정의하고, 세분화된 다양한 감정을 분류하는 것을 감정분석으로 정의하였고, 한국어 기반 텍스트 데이터에서 세분화된 감정분류를 위한 표층적 자질 집합과 의미적 자질 집합을 설계하고, 감정분류의 정확도를 높이고자 하였다.
단순한 긍정, 부정이 아닌 ‘기쁨’, ‘편안’, ‘슬픔’, ‘불안’ 등과 같은 다양한 감정에 대해 인식하는 분류체계는 적용하는 응용 시스템에 따라 감점 범주의 종류가 제각각인 경우가 많다. 특히 국내의 경우, 감정에 대한 극성 분류 연구가 대부분이었으며, 여러 감정으로 분류한 연구는 아직 미비한 상태이다. 국외 감정에 대한 대표적인 분류체계는 플러칙(Plutchik)의 8 개의 분류체계(기쁨, 신뢰, 두려움, 놀람, 슬품, 혐오, 화남, 기대)이다. 플러칙은 인간의 기본 감정을 8개로 구분하였으며, 우리가 일반적으로 경험하는 대부분의 감정들은 이 여덟 가지 감정들이 서로 혼합하여 나타난다고 주장하였다. 기분상태검사(profIle of mood States, POMS)의 6개 분류체계(tension, depression, anger, vigor, fatigue, confusion)도 자주 사용되는 감정 범주이다. 그러나 이와 같은 감정 분류체계는 영어로 작성되어 있기 때문에 그 의미에 맞게 한국어로 번역해서 접근해야 하는 한계점이 존재한다. 본 연구에서는 이러한 문제점을 해결하기 위해 한국어 감정분류를 위해 한국어 감정어휘 목록을 통해 한국어 감정범주 25개를 선별하였다. 한국인들의 실정에 맞는 434개의 한국어 감정어휘 목록 중 친숙성 평가기준을 근거로 상위 25개의 감정단어를 선별하여 사용하고, 이를 토대로 학습 말뭉치를 구축하였다.
감정분류를 위한 접근방식은 최근 연구에서도 자주 사용되는 기계학습 기법을 사용하였다. 25개의 세분화된 감정들에 대해 지도학습을 수행하기 위해서는 충분한 학습 말뭉치가 필요하다. 그러나 한국어 감정범주에 대한 학습 말뭉치의 부재는 세분화된 감정분류를 어렵게 하고 있으며, 공개된 한국어 감정 말뭉치는 긍정, 부정에 대한 극성 정보만 담고 있는 한계점이 있었다. 따라서 본 연구에서는 다양한 감정분류를 위해 25개 감정범주에 대한 학습 말뭉치를 구축하였고, 표층적 자질과 의미적 자질을 설계하였다. 표층적 자질은 텍스트 데이터에서 감정분류를 위한 즉각적이고 일차원적인 근거를 제공한다. 그러나 텍스트 데이터가 내포하고 있는 의미를 담아내기에는 한계가 있다. 기존 연구에서는 이러한 의미를 표현하기 위해 다양한 자질들을 조합하거나 새로운 자질을 개발하여 학습에 사용하기도 하였다. 그러나 자연어 처리 분야에서, 단어를 ‘의미’ 벡터 공간으로 임베딩하는 기법들이 소개되면서 감정분류 분야에서도 이러한 기법들을 사용하여 자질을 자동으로 학습하는 접근법이 시도되고 있다. ‘의미’는 단어 뿐만을 대상으로 하지 않고 구, 문장, 문서 등 자연어처리 과정에서 발생하는 모든 처리 단위가 대상이 된다. 대량의 데이터에서 단어나 문장을 의미 벡터 공간으로 임베딩하여 주변 단어와 문장과의 문맥 정보를 활용하여 학습모델을 만들고, 기계학습 알고리즘을 통해 감정을 분류한다. 따라서 본 연구에서는 표면적 자질 뿐만 아니라 워드임베딩 기법을 활용한 doc2vec, skip-thought vector와 같은 의미적 자질을 사용하였으며, 표면적 자질과의 성능을 비교 분석하였다.
감정분류에 대한 또 다른 문제로 학습에 사용되는 어휘 자질의 과도한 사용으로 인해 데이터 부족 문제를 유발한 다는 것이다. 예를 들면, “나 정말 화가나(화남)”, “진짜 재밌다(재미)” 등에 대해서는 학습 말뭉치에 나타난 고빈도 감정 어휘를 사용하였기 때문에 잘 동작하지만, 학습 말뭉치에 나타나지 않은 “드디어 성공했어(기쁨)” 등의 표현에 대해서는 적절한 감정을 결정할 수 없게 된다. 본 연구에서는 25개의 세분화된 감정범주를 사용하기 때문에 이처럼 특정 감정범주를 위한 데이터 부족 문제가 발생한다. 감정범주의 개수가 많아질수록 해당 감정의 데이터를 수집하기는 점점 어려질 것이다. 따라서 본 연구에서는 데이터 오버샘플링 기법을 통해 이 문제를 해결하여 학습 데이터 부족 문제를 해결하고자 하였다.
본 연구는 한국어 문장에서 25개의 다양한 감정범주로 감정을 분류한 그 자체로도 의미가 있지만 다음과 같은 이유에서 더욱 중요하다.
첫째, 본 연구는 한국어로 작성된 텍스트 데이터를 분석하여 세분화된 25개의 감정범주를 분류하였고 높은 정확도를 제공한다. 세분화된 25개의 감정범주는 한국인 실정에 맞는 감정범주로 정의하였고, 다양한 감정범주를 분류하였다.
둘째, 최신 기술인 워드 임베딩 학습 기법을 사용하여 의미적 자질을 설계하고, 이를 한국어 감정분류에 적용하였다. 워드 임베딩 학습 기법은 학습 속도가 매우 빠르며, 표면적 자질을 사용한 학습결과와 비슷한 성능을 제공한다.
셋째, 감정분류를 위해 사용된 표면적 자질과 의미적 자질에 성능평가를 통해 각각의 자질에 대한 성능을 비교 분석하였으며, 이는 감정분류에 있어서 표면적 자질과 의미적 자질에 대한 실증적 실험결과를 제공하고 이를 통해 한국어 감정분류에 있어서 앞으로 연구방향을 설정함에 있어 기초자료로 사용될 수 있다.
넷째, 본 연구에서는 25개의 감정을 분류하기 위한 학습 말뭉치를 전문가를 통해 직접 구축하였고, 감정분석 연구와 실무에 활용이 가능한 자원을 공유한다는 차별성을 가진다.제 1 장 서 론 1 제 1 절 연구의 배경 1 제 2 절 연구의 목적 6 제 2 장 관련연구 8

Over the last decade, we have witnessed an explosion of information available on the Web in the era of Big Data. Today, multiple data sources, from the user generated content to scientific literature, produce massive amounts of valuable data mostly in text format. However, the processing capabilities of humans cannot keep up with the speed of data growth, making it increasingly difficult to convert information to knowledge. As a result, efforts have been made to automatically extract and structure information from many text corpora in the data mining and natural language processing research communities. In recent years, due to the development of artificial intelligence technology especially deep learning, existing text mining and Natural Language Processing (NLP) technology has developed beyond simple knowledge extraction to a level where a machine can understand the meaning of a text and perform various tasks through it.

This dissertation proposes machine reading methods for solving machine comprehension tasks on multi-domain text corpora. Whereas existing systems focus solely on performance evaluation using benchmark datasets, we first define a concept of machine understanding from cognitive informatics perspective. Then, we introduce a new machine understanding method using concept graph. Finally, unlike the existing systems proposed for machine understanding tasks in a limited domain, we applied our methods to various text domain sources from user-generated content on the Web to biomedical literature.

We achieve our research objectives through exploiting semantic tools resources such as FrameNet and Abstract Meaning Representation with conventional machine learning models and recent deep neural architecture. Using quantitative experiments and qualitative analysis, we show that our proposed machine understanding methods are scalable and effective in solving a wide range of NLP tasks including content classification and Question-Answering.지난 10여 년간 정보기술의 발전에 따라 빅데이터 시대가 열리면서 웹을 중심으로 다양한 분야에서 정보의 생산 및 유통량이 폭발적으로 증가하였다. 이러한 빅데이터는 다양한 형태를 띠고 있으며, 그중의 대부분은 텍스트 형식으로 이루어져 있다. 사용자가 대량의 데이터에서 지식을 얻기 위해서는 텍스트를 읽고 그 안에 포함된 정보를 추출해야 한다. 그러나 전문가가 다수의 텍스트를 일일이 검토하기에는 시간적, 비용적 한계점이 존재하며 폭증하는 데이터에 효과적으로 대응하기 어렵다. 따라서 기계적인 방법을 통하여 텍스트에서 자동으로 지식을 추출하는 연구들이 지난 수년간 수행됐다. 특히, 해당 연구는 기계가 텍스트를 읽고 그 안에 포함된 내용을 처리하는 연구는 자연어 처리, 정보 추출 및 검색 분야에서 `기계 이해'라는 주제로 주목받고 있다. 한편, 최근에는 딥러닝을 중심으로 하는 인공지능 기술의 발전으로 인해 종래의 단순한 지식 추출을 넘어서서 기계가 텍스트의 의미를 이해하고, 이를 바탕으로 다양한 작업을 수행할 수 있는 수준으로 발전하였다. 그러나 기존의 `기계 이해'연구들은 처리 가능한 텍스트의 도메인이 한정적이며, `이해한다'라는 행위와 개념의 기계적 접근방법에 대한 충분한 논의 없이 벤치마크 데이터셋을 사용한 질의-응답 태스크의 성능 향상에만 집중하는 한계점이 있었다.

본 논문에서는 다중 도메인 텍스트에서 `기계 이해'를 효과적으로 수행할 수 있는 방법론 및 이에 필요한 텍스트 데이터셋을 제안함으로써 기존의 '기계 이해'연구의 한계를 보완하고자 한다. 구체적인 접근 방법은 다음과 같다. 첫째, 정보추출 기반의 기계 이해 시스템은 객체 간 관계 추출을 주목적으로 하고 있으며, 사실 정보가 담긴 지식베이스에 의존하기 때문에 뉴스나 웹 페이지 같은 사실적 정보가 포함된 텍스트만 처리할 수 있는 한계가 있었다. 본 연구에서는 외부 지식의 도움 없이 텍스트 자체에 내포된 의미를 파악하고 추출하기 위하여 프레임넷을 사용하여 문장의 의미를 정형화된 형태로 추출하고, 이를 기반으로 텍스트의 의미적 차이를 규명할 수 있는 방법론을 소개한다. 둘째로, 기계가 문장의 의미를 이해한다는 개념을 재정의 하기 위하여 추상 의미 표현 그래프를 사용하여 인지과학적 관점에서 사람의 인지 과정을 모델링하는 연구를 수행한다. 마지막으로, 제한된 텍스트 영역에서만 수행되었던 딥러닝 기반의 `기계 이해' 모델을 생명의료 분야의 텍스트로 확장하는 데 필요한 데이터셋 생성 방법과 모델 성능 향상 방안을 제안한다. 아울러 본 논문에서 제안된 `기계 이해' 방법들을 다양한 실험을 통해 검증하고 사용자 생성문서 및 생명의료 문헌 빅데이터를 포함하는 다중 도메인 텍스트 데이터셋에 적용하여 분류, 분석 및 질의-응답과 같은 자연어 처리 태스크를 효과적으로 수행할 수 있음을 보인다.Abstract Contents i List of Figures v List of Tables viii 1 Introduction 1

Recognizing Textual Entailment (RTE) has the objective of judging whether the meaning of one piece of short text, normally a single sentence, can be inferred from another piece of longer text. Modeling inference in human language is very challenging but has been shown to be fundamental to Natural language understanding and many other natural language processing tasks. Over the last decade, there have been promising developments in Natural Language Processing (NLP) with several investigations of approaches focusing on Recognizing Textual Entailment (RTE). These models include models based on lexical similarities, models based on formal reasoning, and most recently deep neural models.
Motivated by our previous work on sentence-to-sentence relation network for recognizing textual entailment, as well as the success of Residual Architectures and Attentive pooling/other forms of neural network attention mechanism. In this dissertation, we present a network that exploits the strengths of the above stated networks effective enough for the task of Recognizing Textual Entailment. Our model basically combines two attentive representations of the sentences for the classification task.
Given two sentences P and H, we first pass each sentence through a sequential encoder to extract a higher level sentence representation. We then pass these higher level sequential representations through another network which computes the inter-sentence alignment representations. The inter-sentence alignment computation consist of two different sub-modules which independently applies different attention computation technique to align the higher level sequential representation from the sequential encoders. Finally, based on the newly generated representations, the decision on entailment is made through a fully connected layer. Experiments on the Stanford Natural Language Inference(SNLI) corpus suggests that this is a promising technique for RTE.텍스트 함의 인식(Recognizing Textual Entailment, RTE)은 주어진 긴 텍스트로부터 주로 한 문장으로 이루어진 짧은 텍스트의 의미를 유추해 낼 수 있는지를 판단하는 것을 목표로 하는 태스크이다. 인간의 언어로 추론을 모델링하는 것은 매우 어려운 일이지만, 자연어 이해와 다른 다양한 자연어처리 태스크의 기본 요소로 제시되었다. 지난 10년간, 텍스트 함의 인식 문제를 풀고자 한 연구가 많이 존재했다. 기존 연구들은 어휘적 유사도, 형식 추론을 기반으로 한 모델, 최근 들어서는 심층 뉴럴 모델을 이용하여 텍스트 함의 인식을 위한 모델을 제안하였다. 본 논문에서는 기존의 텍스트 함의 인식을 위한 문장-문장 관계 네트워크 뿐만 아니라 레지듀얼(Residual) 아키텍처, 주의 집중(attention) 메커니즘 신경망의 주의 집중 풀링 등의 구조에서의 장점을 모은 네트워크 모델을 제안한다. 제안하는 모델은 기본적으로 분류 태스크를 위한 두 개의 문장의 주의 집중 표현(attentive representation)을 결합한다. 먼저, 두 개의 문장 P와 H가 주어졌을 때 두 문장을 순차 인코더를 이용하여 고차원의 문장 표현으로 나타낸다. 그런 다음 문장 간 정렬 표현을 계산하기 위하여 두 개의 고차원 문장 표현은 다른 네트워크를 통과하게 된다. 문장 간 정렬 계산은 순차적 인코더로부터 얻은 고차원 문장 표현을 정렬하기 위하여 독립적으로 다른 주의 집중 계산 테크닉을 적용하는 두 개의 서브 모듈로 이루어진다. 최종적으로, 새롭게 생성된 표현들은 완전 연결 레이어(fully connected layer)에 의해 텍스트의 함의가 결정된다. Stanford Natural Language Inference (SNLI) 코퍼스를 이용한 실험을 통해 본 논문에서 제안한 모델은 텍스트 함의 인식 문제에서 유망한 기술임을 증명하였다.1 Introduction 1 1.1 Overview 1 1.2 Motivation 4 1.3 Approach of the Dissertation 6 1.4 Organization of the Thesis 6

인간과 컴퓨터의 예전 상호작용 방식은 많은 시간과 사용자의 노력을 요구하였으며 훈련되지 않은 사용자가 보다 많은 어려움을 겪게 하였다. 에이전트에 대한 최근의 연구들은 소프트컴퓨팅 기법을 이용하여 이러한 문제를 해결하는 새로운 길을 제시하였다.
소프트컴퓨팅 기법을 적용한 에이전트 구축에는 두가지 주요한 방법론이 이용되어 왔다. 하나는 각 소프트컴퓨팅 기법을 개별적으로 적용하는 것이다. 이 방법은 적용된 기법의 내부 파라미터를 결정하기가 어렵다는 단점이 있다. 다른 하나는 두개 또는 그 이상의 소프트컴퓨팅 기법들을 결합하여 내부 파라미터조정의 어려움을 완화시키는 것인데, 특히 진화알고리즘과 다른 기법의 결합이 깊이있게 시도되었다.
비록 기존 방법론이 성공적이긴 하였지만 적어도 다음과 같은 세가지 문제점을 지니고 있다. 첫째로, 진화현상에는 적응성뿐만 아니라 우연, 필요, 그리고 임의의 유전적 표류같은 다른 현상들을 포함하고 있기 때문에 진화결과 얻어진 해가 적응성에서 얻어진 결과임을 보장하지 못한다. 둘째로, 기존 방법론은 무엇이, 언제, 어떻게, 그리고 왜 어떤 유전자는 좋은 해로 진화하고 다른 것들을 그러지 못하는지를 보이지 못한다. 이 질문들은 적응과 진화의 원인을 보인다는 점에서 중요한다. 마지막으로 기존의 방법론은 진화된 해에 대한 분석을 수행할 수 없다. 필연적으로 진화는 단순한 구조에서 창발적 행동이나 특성으로 파악될 수 있는 복잡한 구조로의 변화를 초래한다. 그러나, 기존의 방법론은 창발성에 대한 엄격한 정의가 없었기 때문에 어떤 행동이 창발적인지 여부를 구분할 수 없었다.
이 논문에서는 기존 방법론의 단점을 극복하기 위하여 지능형 에이전트를 위한 소프트컴퓨팅 방법론을 제안한다. 이 방법론은 구축과 분석이라는 두 부분으로 이루어져 잇다. 구축부분에서는 규칙기반 시스템과 진화 알고리즘을 결합하여 지능형 에이전트를 구축한다. 분석 부분에서는 진화활동성, 스키마분석, 그리고 관찰적 창발을 적용하여 진화와 행동적 관점에서 진화된 에이전트를 분석한다.
진화활동성 통계는 적응적 진화를 수치적으로 측정하기 위한 방법으로서 진화가 적응적인지 아닌지를 객관적이고 수치적으로 판별할 수 있다. 스키마 분석을 최종해로 가는 진화과정의 분석에 이용되며 관찰적 창발은 진화된 제어기의 창벌적 행동들을 밝히는데 적용된다.
본 논문에서는 제안된 프레임워크의 유용성을 알아보기 위하여 에이전트 연구의 두 가지 주요 흐름인 소프트웨어 에이전트와 하드웨어 에이전트의 두 가지 사례에 적용한다.
첫 번째 사례연구에서는 제안한 프레임워크를 하드웨어 에이전트를 위한 행동 제어기의 구축에 적용한다. 이 사례연구에서 사용한 하드웨어 에이전트로는 케프라라고 하는 일종의 행동기반 이동로봇을 이용하였다. 케프라의 여덟 개 거리감지 센서 값이 행동제어기의 입력으로 이용되고 이로부터 나온 두 가지 제어 값이 로봇의 두 모터를 구동하기 위하여 이용된다. 행동 제어기로는 규칙기반 시스템의 일종인 퍼지 제어기를 이용하고 유전자 알고리즘을 이용하여 퍼지 행동 제어기의 내부 파라미터를 조정한다. 제안한 프레임워크를 이용하여 진화된 제어기에 대한 진화 및 행동 분석을 수행한다.
두 번째 사례연구에서는 제안한 프레임워크를 소프트웨어 에이전트를 위한 행동 제어기의 구축에 적용한다. 이 사례연구에서 이용되는 소프트웨어 에이전트는 웹 사이트의 가상 대리자로서 자연어를 이용하여 방문자와 상호작용을 수행하는 대화형 에이전트이다. 이 대화형 에이전트는 화행분류, 구조적 패턴 매칭, 그리고 지식 구축과 표현의 세가지 주요 구성요소로 이루어져 있으며 사용자의 의도를 파악하기 위하여 오토마타를 이용하여 일련의 키워드들을 인식함으로써 화행을 분류한다. 또한 보다 효율적인 화행분류를 위하여 화행분류 모듈 사이의 상호작용을 포섭구조를 이용하여 제어한다. 사용자의 질의를 지식 데이터 베이스에 있는 답변과 매칭하기 위해서 이 논문에서는 전통적인 자연어 처리기법보다는 패턴매칭 기법을 적용하였다.
두 가지 사례연구를 통하여 제안한 프레임워크가 지능형 에이전트를 위한 제어기의 구축에 효율적으로 적용될 수 있음을 확인하였다. 또한 진화분석을 통하여 최종해가 우연이나 다른 진화현상의 산물이 아니라 각 구성요소의 적응성에서 비롯된 산물임을 확인할 수 있었다. 진화된 제어기에 대한 행동분석을 통하여 제어기가 진화과정에서 창발적인 행동을 획득하였으며, 특히 이러한 창발성이 보다 낮은 단계의 구성 요소들의 상호작용에 의한 것임을 알 수 있었다.
결론적으로, 제안한 프레임워크가 에이전트의 제어기를 구축하는데 효과적으로 적용될 수 있음을 알 수 있었으며, 특히 진화결과 구성된 제어기의 행동분석과 진화과정에 대한 분석에 장점을 지니고 있음을 알 수 있었다.The old ways of interactions between human and computer require more time and effort of users and make untrained users to meet more difficulties. Recent research on agents has opened a new way for solving those problems, especially utilizing soft computing techniques.
Two main frameworks are used to apply soft computing techniques to the construction of agents. One is to apply each soft computing technique independently. It has some drawbacks in determining the internal parameters of the technique used. The other one is to combine two or more soft computing techniques to relieve the difficulties of tuning internal parameters. Especially, the combination of an evolutionary algorithm and other techniques has been extensively attempted.
Despite the success of the conventional framework, it has at least three shortcomings. First, it cannot guarantee that evolved solutions are from adaptation, since evolutionary phenomena contain not only adaptation but also others such as chances, necessity, and random genetic drift. Second, it cannot show what, when, how, and why some genes have successfully evolved into good solutions while the others not. These questions are critical to show causes of adaptation and evolution. Finally, it is not able to analyze the evolved solutions. Evolution necessarily causes the changes of simple structures into complex ones that may be viewed as emergent behaviors or emergent properties. However, the conventional frameworks cannot tell whether certain behaviors are emergent because they do not have any formal definition of emergence.
This dissertation proposes a soft computing framework for intelligent agents to overcome the shortcomings of the conventional frameworks. The framework consists of two parts: construction and analysis part. In the construction part, the combination of rule-based systems and evolutionary algorithms is used to construct an intelligent agent. In the analysis part, evolutionary activity statistics, schema analysis, and observational emergence are applied to the analysis of the evolved agent in both evolutionary and behavioral perspectives.
Evolutionary activity statistics is a quantitative measure for adaptive evolution and can tell objectively and quantitatively whether an evolution is adaptive. Schema analysis is to identify evolutionary pathways to solutions. Observational emergence identifies emergent behaviors of the evolved controller.
To show the usefulness of the framework, this dissertation applies it to two agents: software and hardware agents, which are the two main fields of agent research.
In the first case study, the framework is applied to the construction of a behavior controller for a hardware agent. The hardware agent called Khepera is a kind of behavior-based mobile robot. Eight proximity sensors are used as input to the behavior controller and two control values are produced from the controller to drive the two motors of the robot. Fuzzy logic, a kind of rule-based systems, is used for the behavior controller, and a genetic algorithm tunes the internal parameters of the fuzzy behavior controller. Analysis is performed on the evolved controller and the evolutionary process using the framework.
In the second case study, the framework is applied to the construction of a behavior controller for a software agent. The software agent is a conversation agent that can act as a virtual representative of a web site interacting with visitors using natural language. The agent consists of three main components: dialogue act categorization, structured pattern matching, and knowledge construction and representation. Dialogue acts (DAs) are classi fied by automata that accept a sequence of keywords to identify user's intentions. In addition, subsumption architecture controls the interactions among DA analysis modules to make the classification of DA more effective. We utilize pattern matching techniques rather than conventional natural language processing techniques to match users' queries with a knowledge base.
The two case studies show that the framework can be effectively applied to the construction of controllers for intelligent agents. In addition, the analysis on the evolutionary process presents that the solutions have resulted from the adaptability of each component that constitutes the solution rather than from chances or other evolutionary phenomena. The analysis on the behaviors of the evolved controllers shows that the controller has obtained emergent behaviors through the evolutionary process. Especially, the emergent behaviors have resulted from the interactions of lower level components of behaviors.
Consequently, we have confirmed that the framework can be effectively applied to the construction of controllers for intelligent agents. Particularly, it has advantages in analyzing the behaviors of an evolved controller and in analyzing the evolution.CONTENTS = v ABSTRACT = xii 1 INTRODUCTION = 1 1.1 Background and Motivation = 2 1.1.1 Fundamental Concepts of Intelligent Agents = 3

Recently, the information technology (IT) is significantly developed and plays an important role in many areas. When IT is applied to solve energy and environmental issues and to help the sustainable development of human beings, it is called as “Green IT”. Green IT was often classified into two categories: one is Green IT 1.0 and the other is Green IT 2.0. Green IT 1.0, also called as Green of IT, implies the IT that is used to improve the efficiency of the energy or to solve the environmental problems caused by the IT itself. Whereas, Green IT 2.0 also called as Green by IT, means the IT that helps enhancing sustainability for all areas of business. Though Green IT 2.0 can be useful in many areas of business, up until now, policies related with Green IT 2.0 are limited partly because it encompasses too large scope to focus on a certain purpose.
The goal of this study is to reclassify Green IT to increase the usage of it through establishing policies linked with Green IT. Green IT is reclassified into four categories based on the usage of big data technology: green IT by environmental big data green IT by bio big data, green IT by things big data and green IT by human big data.
Projects with a high priority were investigated for each categories of green IT and this investigation performed using Delphi method. Results indicated that developing environmental data integration and sharing system, pollution prevention and surveillance system, and human diseases prevention system are among the high priority needed for green IT by environmental big data. In addition, developing health insurance review & assessment service, hospital diagnosis, treatment integration and sharing system, natural language diagnosis through NLP(Natural Language Processing) technology and utilization system are among high priority for green IT by bio big data. Furthermore, developing data security technology, policies associated with M2M(Machine to Machine), and energy consumption monitoring system are highly prioritized for green IT by things big data. Finally, strengthening national security through analyzing human crime data, establishing process and counter plan for national disaster using SNS(Social Networking Services) data, developing sharing and utilizing system of security and traffic data using public data like national emergency management agency are highly prioritized for green IT by human big data.
The result of this study can be used to develop policies that are needed to be developed in many areas of business using green IT. The policies based on this study can help develop green IT as well.그린IT는 IT기술을 활용해서 보다 합리적이고 정확하게 빠른 방식으로 에너지와 환경 문제를 해결하는데 도움을 주고 나아가 인류의 지속 가능한 발전을 위해서 직/간접 적으로 매우 중요한 역할을 수행하는 기술이다. 그러나 현재의 그린IT 분류체계는 그린IT 1.0 (Green of IT) 즉, ‘IT 기기 자체의 에너지 효율화’와 그린IT 2.0 (Green by IT) 즉, ‘IT 부문을 넘어 기업 전체 운영과 사회 전반의 지속 가능성을 향상 시키는데 IT를 활용’하는 두 가지 분류체계가 여전히 지배적으로 사용되고 있다. 하지만 그린IT 2.0 의 범위가 너무 방대해서 분류구분으로서의 변별력이 떨어짐에 따라 그린IT 관련 정책을 구체적으로 수립하는 데에 어려움이 존재한다.

본 연구의 목적은 그린IT의 활용도를 높이기 위해 그린IT의 분류체계를 재정립하는 것이다. 본 연구에서는 그린IT를 분류함에 있어서 빅데이터 기술에 기반하여 환경기반 빅데이터, 바이오기반 빅데이터, 사물기반 빅데이터, 인간기반 빅데이터 와 같이 4가지로 분류하고, 각각의 분류영역별로 우선순위가 높은 선행과제들을 델파이기법에 의거하여 전문가들로부터 도출 하였다. 또한 이 과정에서 국가 주도로 진행해야 하는 과제 와 정책적인 접근이 필요한 과제를 따로 선별하여 그린IT를 활성화 하기 위한 정책으로 도출 하였다.
환경기반 빅데이터를 활용한 그린IT는, 선행되어야 할 과제로 정부주도의 환경관련 데이터 통합 및 공유 체제 구축, 환경 오염 예방 및 감시 시스템 구축 과 인간의 질병예방에 환경관련 데이터 활용 등이 과제로 도출되었고, 정부주도의 환경관련 데이터 통합 및 공유 기반 인프라 구축 과 거버넌스 체제 구축이 국가적 차원의 과제로 도출 되었다.
생명기반 빅데이터를 활용한 그린IT는, 국민 건강보험 심사평가원 과 병원의 진료 관련 데이터의 통합 및 활용, 자연어분석 기술을 활용한 수십 년간의 자연어 형태로 저장된 진료기록 분석 및 활용, 바이오 빅데이터 관리와 활용을 위한 정책, 법안 수립 등이 선행과제로 도출되었고, 개인 민감 데이터의 관리체제 수립이 국가적 차원의 과제로 도출 되었다.
사물기반 빅데이터를 활용한 그린IT는, M2M 데이터 보안기술, M2M 관련 정책 수립, M2M 개발 에코환경 구축 및 에너지 소비현황 관리 환경구축이 선행과제로, 정부주도의 M2M 표준수립, 글로벌 표준 참여, 중소규모 M2M 디바이스 기업의 소프트역량 강화 지원, M2M 데이터 보안을 위한 정책마련 등이 국가적 차원의 과제로 도출 되었다.
인간기반 빅데이터를 활용한 그린IT는, 범죄 데이터 분석으로 국가차원의 보안강화, 국가 재난 시의 프로세스 및 대응방안 수립, 소방방재청 등의 안전 및 교통관련 데이터 공유 와 활용이 선행과제로 도출되고, 재난 재해 관련 데이터의 민간활용을 위한 정책, 개인위치 정보와 같은 통신사 가입자위치 정보, SNS 정보와 같은 민간보유 데이터의 활용을 위한 정책 수립 등이 국가적 차원의 과제로 도출 되었다.

본 연구의 결과는 정부와 기업의 그린IT 전략 사업, 혹은 시범사업 추진 시 우선적으로 시행해야 할 과제의 도출 시 기초자료로 사용될 수 있다. 이와 더불어 국가 차원의 주도적인 진행이 필요하거나 국가차원의 정책이 필요한 과제는 정책수립 과제로서 제언하여 정부의 정책 수립 시 우선시 될 수 있도록 하여, 최종적으로는 향후 그린IT 의 발전에 중요한 역할을 할 수 있다.
그린IT는 국가나 기업의 이미지 개선을 위한 홍보 목적, 친환경 기술 자체를 위한 핵심기술 개발 목적 등과 같이, 명시적으로 널리 활용되어야 하는 인류의 지속 가능한 발전을 위한 중요한 영역이다. 그러므로 국가와 기업이 그린IT 에 투자하고 기술을 개발할 수 있도록 환경을 조성하여, 그린IT 와 관련한 부문의 연구가 더욱더 활발히 이루어 지도록 모든 관련 조직이 지속적인 노력을 해야 할 것이다.1.서론 1)빅데이터의 정의 2)그린IT의 정의 3)연구목표 2.빅데이터 기술에 기반한 그린IT 분류체계 수립

Based on the fusion of IT and automobile technology, a vehicle is being connected to home, office and surrounding infrastructure as well as other vehicles. In near future, a completely connected vehicle will appear in the form of vehicle equipped with powerful sensors and networking and communication devices. Connected vehicle will open up a new world of possibilities that make road transport safer, cleaner and more secure. In future, an autonomous or self-driving vehicle will make the driving itself be less stressful, more interesting and even productive. It will offer more interesting alternatives in the vehicle and increase the travel demand as well. It will make our life much brighter by linking home, vehicle, and office, which have the most important meaning in personal life.
In this paper, a concept of a voice-based virtual connecting agent is proposed for a home connected vehicle. It is a personal IoT partner which is always accompanying a driver to give instant assistance during driving a vehicle, staying at home, and working at the office. It provides the consistent speech interface supporting the voice recognition, the natural language understanding and the personalized voice activation at any time. It also offers personalized services by controlling and monitoring the status of home appliances at anytime and anywhere. In order to provide the same interface in all environments, a smartphone is used as a central hub. A cloud-based intelligent home platform is used as a core medium to be able to interconnect between smart homes and connected vehicles.
In order to strengthen lifestyle changes and practical user experience, we have installed home IoT devices in a real house located in North America and embedded the smartphone linked in-vehicle infotainment system into Honda CIVIC vehicle. Four kinds of home connected vehicle interaction scenarios are defined and scenario-based experiments are conducted through the virtual connecting agent.
Experimental results show that users want to connect the vehicle and all digital devices through the same interface and communicate with them using a natural language interface as if they are talking to people. They feel more comfortable and familiar when they make use of their own smartphone in the vehicle, at home, and other places. Users expect the virtual connecting agent to predict and execute the next things the driver should do according to the schedule information rather than receiving the schedule information in the vehicle.
It is needed to automate routine and repetitive tasks that users must do in daily routine. It is important to preempt users’ needs and provide the personalized services on time. When moving to another place, users want to monitor the situation of the house accurately and gain a sense of security. They really want to feel less stressed not to worry about a lot of activities performed before leaving home.
In the era of autonomous driving, future technology development should proceed in a way to satisfy user 's hidden needs. By strengthening the current IoT technology and cybersecurity technology, more devices will need to be connected seamlessly. The useful information will need to be exchanged securely. Ultimately, the future is going to be one that a learning-based personalized agent understands human naturally and feels considerate and emotionally intelligent like human.IT기술과 자동차 기술이 융합되고 발전하여 자동차가 자동차 및 주변환경과 통신을 하는 커넥티드 카와 인지 센서를 기반으로 주변환경을 스스로 인지하고 주행상황 및 경로를 종합적으로 판단하여 스스로 운전을 하는 자율주행 카의 시대가 도래하고 있다. 자율주행이 실현되면 자동차는 상시 네트워크에 연결되어 실시간 정보 교환을 할 수 있게 되어 안전성을 강화하게 될 뿐만 아니라 운전자의 편의와 흥미를 충족시켜 주기 위한 다양한 서비스를 제공하게 될 것이고 사람들은 차 안에서 다양한 일을 할 수 있게 될 것이다.
자율주행 기술과 커넥티드 기술의 발전은 개인의 삶에서 가장 중요한 의미를 가지는 공간인 집, 자동차, 그리고 회사를 보다 강하게 연결시켜 삶을 보다 윤택하게 만들어 갈 것이고, 운전이라는 부담으로부터 자유롭게 되어 삶의 여가를 즐기기 위한 여행도 점차 늘어나게 될 것이다. 본 논문의 주제는 스마트폰을 기반으로 하여 자동차와 집을 음성을 통해 자연스럽게 연결하는 음성기반 가상비서에 관한 것이다. 본 연구에서는 클라우드 기반의 지능형 홈 플랫폼을 활용하여 자동차에서는 인포테인먼트 시스템을 연결하고, 집에서는 사물인터넷 기반의 스마트 기기들을 연결하였다. 모든 환경에서 동일한 인터페이스를 제공하기 위해 스마트폰을 허브로 사용하였고, 자연어처리가 지원되는 음성인식 기술과 개인화된 음성 구동 기술을 적용하였다.
라이프 스타일의 변화와 실질적인 사용자 경험을 강화하기 위해 북미에 위치한 리얼 하우스에 사물인터넷 기반의 디지털 기기들을 설치하였고, 스마트폰 연동이 가능한 인포테인먼트 시스템을 설계하여 혼다 CIVIC차량에 장착하였다. 집과 자동차가 긴밀하게 연결되어 실제 생활에 유용하게 사용될 수 있는 4개의 홈-카 연동 시나리오를 선정하였고, 스마트폰 기반의 음성 비서 서비스를 함께 정의하여 실제 삶에 적용해보는 시나리오 기반의 실험 테스트를 진행하였다.
실험 결과 운전자는 차량 안에서 집안에 위치한 조명과 음향 기기를 원격으로 제어하거나 단순한 정보를 제공받는 것이 아닌 사용자의 숨어있는 요구를 제공받기를 원했다. 사용자들은 현재 온도를 모니터링 하기보다는 차량 스스로 상황을 판단하여 최적의 온도를 제어해 주길 원했고, 차량 안에서 스케줄 정보를 제공받는 것보다는 스케줄 정보에 따라 운전자가 해야 할 다음 일들을 예측하여 실행해주길 원했다. 매일같이 반복되는 일상에서 운전자가 반드시 해야 하는 일들이면서 단순 반복적인 일들을 자동화하여 제공해주는 경우 가장 큰 사용자 만족도를 얻어낼 수 있었다. 사용자들은 동일한 인터페이스를 통해 자동차와 모든 디지털 디바이스들을 연결하기 원했고 마치 사람과 대화하듯이 자연스러운 언어로 연결되기를 원했다. 다른 곳으로 이동하는 경우 집안의 상황을 정확하게 모니터링하여 안정감을 얻길 원했고, 상황을 스스로 판단하여 필요한 일들을 적시에 실행해 주길 원했다.
본 논문에서는 다기준 분석기법의 일종인 계층형 분석기법을 활용하여 가상 비서에 대한 정량적인 평가도 수행하여 보았다. 평가항목으로는 유용한 정보 제공 활용성, 단순 반복적인 일들에 대한 자동화 수준 그리고 다양한 기기들에 대한 원격제어 편의성을 정의하였고, 평가지표로는 차량탑승, 집 도착, 집 거주 그리고 외출로 구성된 홈 커넥티드 카 상호작용 시나리오를 정의하여 활용하였다. 평가 항목 간 그리고 평가 지표간 이원 비교를 통해 상대적 중요도를 구하였고, 이를 가중치 화하여 평가를 수행하였다. 유용한 정보 제공 활용성 측면에서는 차량 탑승 시나리오, 단순 반복적인 일들에 대한 자동화 수준 측면에서는 집 도착 시나리오 그리고 다양한 기기들에 대한 원격제어 편의성 측면에서는 집 거주 시나리오가 상대적으로 높은 평가를 받았고, 4가지 평가항목을 모두 고려할 경우 집 도착 시나리오가 좋은 평가를 받았다.
자율주행차 시대가 되면 자동차와 집의 연결을 통한 보다 다양한 서비스를 필요로 하게 될 것이다. 따라서 향후의 기술 개발은 사용자의 숨어있는 니즈를 만족시킬 수 있는 방향으로 진행되어야 할 것이다. 고성능 컴퓨팅 및 인공지능 기술을 빠르게 적용하여 자연어 처리 기반의 음성인식 기술을 발전시키고 학습 기반의 개인화 서비스를 확장시켜 나가야 할 것이다. 사물인터넷 기술과 보안기술을 강화하여 좀 더 다양하고 많은 기기들이 지금보다 더 편리하게 연결될 수 있고, 유용한 정보들을 안전하게 주고 받을 수 있도록 해야 할 것이다.



This paper deals with constructional method of Korean machine dictory in focus synonym processing. It is aid to solve that former method's problems, such as searching speed and memory capacity.
On the basis of Korean thesaurus, analysis about semantic property of synonym, which is linked each word with pointer. According to the analyzed, it is useful in natural language processing and expert system.목차 I. 서론 = 1 I.1 연구의 배경 = 1 I.2 연구의 목적 = 2 II. 이론적 배경 = 4

In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods can be applied especially useful to a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model.1 INTRODUCTION 1 2 RELATED WORK 3 3 METHODOLOGY 5 3.1 Language Model 5 3.1.1 Sentence Completion Language Model 6

자연어 처리 분야에서도 심층 신경망 기술이 주목되고 있으며, 최근에는 convolution neural network(CNN)기반의 심층 신경망 구조가 이미지 분류뿐만 아니라 자연어 처리의 문서 분류에서도 좋은 성능이 입증되었다. 하지만 convolution neural network(CNN)을 이용한 문서 분류 연구에서는 대체로 문장의 평균 단어수가 적은 짧은 문장에 한하여 적용되었으며, 의미론적 관계가 복잡한 긴 문장은 다루기 어렵다는 단점을 가지고 있다. 본 논문은 기존의 연구의 한계점을 극복하고 더 정확한 문서 분류 성능을 위하여 word2vec을 활용한 recurrent neural network(RNN)기반의 심층 신경망의 접근법을 새롭게 제안한다. 이를 위해 장기 의존성 문제를 극복한 long short-term memory(LSTM)을 사용하여 긴 시퀀스의 입력에서도 효과적인 문서 분류가 가능하도록 하였고, 제안 방식의 효율성을 검증하기 위해 영문 데이터 분 아니라 한국어 영화 리뷰 데이터에 대해서도 실험을 수행하였다. 그 결과 장문을 포함하고 있는 영문 신문 기사에서는 87%, 단문으로 구성된 영문 리뷰 문서에서는 90%, 한국어 영화 리뷰에서는 88%의 문서 분류 정확도를 보였다.제 1장 서론 ......................................................1 1.1 연구의 배경 및 필요성..............................1 1.2 연구의 목표 및 구성.................................3 제 2장 이론적인 배경..........................................5

Long time has passed since computers which used to be a means of research were commercialized and available for the general public. People used writing instruments to write before computer was commercialized. However, today a growing number of them are using computers to write instead. Computerized word processing helps write faster and reduces fatigue of hands than writing instruments, making it better fit to making long texts. However, word processing programs are more likely to cause spelling errors by the mistake of users. Spelling errors distort the shape of words, making it easy for the writer to find and correct directly, but those caused due to users’ lack of knowledge or those hard to find may make it almost impossible to produce a document free of spelling errors. Even though people often write for chatting or on their SNS pages on the Internet, there are no set of spelling rules and manners for cyberspace and as a result, serious issues have not be raised to date. However, spelling errors in important documents such as theses or business proposals may lead to falling reliability. These spelling errors should be completely avoided, but it is not easy even for experts. Consequently, it is necessary to conduct research on high-level spelling error correction programs for the general public. This study was designed to produce a system to correct sentence-level spelling errors to normal words with Korean alphabet similarity algorithm. On the basis of findings reported in related literatures that corrected words are significantly similar to misspelled words in form, spelling errors were extracted from a corpus. Extracted corrected words were replaced with misspelled ones to correct spelling errors with spelling error detection algorithm.계산기에서 발전하여 탄생하게 된 컴퓨터는 인간이 직접 계산하는데 시간이 오래 걸리는 문제들을 빠른 시간 내에 처리할 수 있도록 개발되어졌다. 초기의 컴퓨터는 방 하나를 차지할 정도로 크기가 거대하였고, 거대한 크기만큼 개인이 함부로 소유할 수 없을 정도로 구매 비용이 막대하였다. 하지만 기술이 발전함에 따라 컴퓨터의 크기는 점점 더 소형화 되어졌으며 그로 인해 컴퓨터의 가격이 하락하면서 일반인들도 쉽게 구매할 수 있도록 사용화 되어졌다. 기존에 사람들은 문서를 작성하기 위하여 종이와 볼펜 같은 필기도구를 이용하여 문서를 작성하였으나, 컴퓨터가 상용화 된지 오랜 시간이 지난 현재에는 컴퓨터를 이용하여 문서를 작성하는 경우가 많아졌다. 컴퓨터를 이용하여 문서를 작성하는 방식은 필기도구를 이용하여 문서를 작성하는 방식에 비해 문서의 작성 속도가 월등히 빠르며 손에 가해지는 피로 또한 적기 때문에 작성에 시간이 오래 걸리는 장문의 문서를 작성할 경우 사람들은 일반적으로 필기도구를 이용하기보다 컴퓨터를 이용하여 문서를 작성하는 경우가 많다. 하지만 컴퓨터를 이용해 문서를 작성하는 방식은 작성자가 팬을 이용해 철자를 그리는 형태의 직접적인 방식이 아닌, 키보드의 자판을 누름으로서 철자를 입력하는 간접적인 방식으로 문서를 작성하기 때문에 철자오류가 발생할 확률이 높다. 보통 철자오류는 문서를 작성하는 작성자의 눈에 쉽게 발견되기 때문에 발생 즉시 교정되어진다. 하지만 철자오류의 종류에 따라 쉽게 발견하기 힘든 부류의 철자오류가 존재하며, 또한 작성자의 지식부족으로 인해 발생한 철자오류도 존재하기 때문에 철자오류가 존재하지 않는 문서는 작성하기 어렵다. 철자오류가 발생할 경우 문장의 내용을 곡해할 수 있기 때문에 작성이 완료된 문서는 읽는 대상에게 정확한 내용을 전달 위하여 반복적으로 철자오류 교정을 수행하지만, 그럼에도 불구하고 철자오류가 존재하는 경우가 많다. 철자오류는 문장의 내용을 곡해할 뿐만이 아니라 논문이라 사업 제안서와 같은 중요 문서에서 발견될 경우 문서와 연관된 연구나 실험 혹은 사업에 대한 신뢰도를 하락시키는 문제를 발생시킬 수 있다. 문제 발생을 막기 위해 철자오류는 철저히 교정되어야 하지만 철자오류를 완벽히 교정하는 것은 교정 지식이 풍부한 교열 전문가들 또한 수행하기 어렵다. 철자오류 교정은 교정 지식이 미비한 일반인들이 수행하기에는 큰 어려움을 겪고 있으며, 더군다나 일반인들이 철자오류를 완벽히 교정한다는 것은 불가능에 가깝다. 일반인들의 철자오류 교정을 보조하고자 문서를 작성하기 위해 제작된 워드프로세서와 같은 문서 작성 프로그램은 작성자의 철자오류 교정 보조 시스템을 제공하고 있으나, 문서 작성 프로그램에서 제공하는 한국어 철자오류 교정 시스템은 사전을 기반으로 하여 규칙에 맞지 않는 철자오류를 교정하고 사전에 존재하지 않는 단어들을 표시할 뿐 문맥에 맞지 않는 철자오류는 교정하지 못하고 있다. 이 때문에 문서 작성 프로그램 사용 중 발생한 철자오류의 일부는 교정 시스템에 발견되지 못한 채 문서에 남는 경우가 존재한다. 이와 같은 문제를 해결하기 위하여 철자오류 교정 시스템에 대한 추가적인 연구가 필요하다.
본 논문은 철자오류 교정을 수행하기 위해 기존의 코사인 유사도를 이용하여 철자오류를 검출하는 알고리즘을 개량하여 철자오류를 검출할 뿐만이 아니라 추가적으로 철자오류를 교정하는 방법에 대해 연구하는 논문이다. 기존의 철자오류 교정 방식은 철자오류가 발생한 단어의 철자를 하나씩 치환한 다음 주변 단어들과 동시등장빈도를 구함으로서 철자오류를 교정한다. 하지만 이런 방식은 철자오류 교정을 완료하는데 소모되는 시간이 길어질 수 있다. 그렇기 때문에 철자오류를 교정하기 위한 방법 중 하나로 한글 편집거리 알고리즘을 사용하여 철자오류 교정 단어를 추출하게 된다. 본 논문에서는 제안하는 철자오류 교정 알고리즘의 성능을 향상시키기 위해 코사인 유사도의 임계값을 탐색하며, 최종적으로 철자오류 교정에 대한 성능을 제시한다.Ⅰ. 서론 1 A. 연구 배경 및 목적 1 B. 연구 내용 및 구성 3 Ⅱ. 관련연구 4

자연어에 있어서 표현형태와 그 의미는 일 대 일로 대응하지 않는다. 이 형태와 의미가 고유하지(proper) 않은 양상을 동의성, 모호성, 중의성 등에서 찾을 수 있다. 동의성은 두 개 이상의 표현이 한 가지 의미를 갖는, 즉 여럿이 하나로 연결되는(many to one) 관계를, 그리고 중의성은 그 역(逆)으로서 하나의 표현이 두 가지 이상의 의미를 갖는, 즉 하나가 여럿으로 연결되는(one to many) 관계를 맺는다. 모호성은 어떤 표현이 한 가지 의미로 분명해지도록 의미 국면을 결정해 주지 못하는 상태이다. 반면에 중의성은 뜻이 두 가지 이상의 국면으로 해석이 되지만 의미가 명료한 양상을 말한다.
자연언어의 한 특성인 이 중의성은 여러 가지 요인에서 기인된다. 둘 이상의 의미를 지닌 한 어휘가 문장에 사용되면 그 문장은 흔히 중의성을 띠게 된다. 두 가지 이상의 범주나 결합구조로 분석될 수 있는 통사적 구조에서 중의성이 발생한다. 그리고 한 문장이 전제할(presuppose) 수 있는 둘 이상의 상황이 그 문장에서 중의성을 야기한다.
한 표현이 두 가지 이상의 음운형태로 발화되어 의미의 차이를 가져올 수 있다는 종래의 음운적 중의성에 대해 우리는 설명의 방식을 달리할 수 있다. 즉 다른 각도에서 보면, 음운적 중의성은 "동일한 문어형태를 가진 한 표현이 그 문어적 중의성을 해소하기 위해 음운적으로 다른 형태를 취한 결과이다"라고 파악되는 것이다. 그래서 음운적 중의성 역시 중의성 해소책의 한 수단으로 간주될 수 있다. 이 관점에서 본 논문은 광범위한 자연어의 중의적 현상들을 어휘적 요인, 통사적 요인 그리고 전제에 의한 중의성으로 분류한다.
문장의 중의성을 해소시키기 위해서 단어를 교체하거나 단어를 추가하는 어휘적 해소방법이 있다. 또 문장 안에서 어순을 바꾸거나 특수구문으로 옮기는 통사적 해소방법이 있다 그리고 또 다른 중의성 해소책으로, 그 중의적 문장의 전제를 상기(想起)시키거나 그 중의적 문장이 속해 있는 문맥의 상황을 적용시켜주는 방법이 있다. 어휘적인 그리고 통사적인 중의성 해소책은 보통, 그 중의적인 문장이 맥락에 합치(合致)되어 갖고 있는 독특한 언어표현의 느낌을 변색시킨다. 그리고 이러한 해소책은, 중의성이 해소된 문장이 원래 문장에 비하여 - 중의성이 해소되었다고 하더라도 - 부자연스럽고, 장황하게 보이도록 한다. 반면에 문맥과 전제에 의해 중의성이 해소된 문장은 자연스러워서 그 맥락에 필연적으로 동참되는 한 부분처럼 인식된다.
문맥은 인간의 언어 수행과정 속에 실재하며 크게 기능하고 있다. 그러나 문맥은 우리의 눈에 잘 띄지 않게 숨어 있으면서, 문장들이 그 안에서 결속되어지는 장(場)으로서의 한 역할을 허용한다. 문맥 속에서, 해당 표현 자체가 지닌 전제, 그 해당 표현에 동반하는 언명이 지닌 전제, 선행 표현이 이루는 전제 그리고 후행 표현이 이루는 전제가 필요한 정보를 공급하여 줌으로써 그 해당 표현이 지닌 중의성이 해소되는 것으로 보인다.
여기에서 우리는 "문맥은 같은 전제를 공유하는 문장들이 그 공통의 전제를 끈으로 하여 의미적으로 응집되고 통일성을 이룬 화용론적인 한 영향권이다"라고 본다. 그래서 한 문맥 속의 문장들은 공통의 전제가 허용하는 범위 내에서 자유롭다. 생략형의 문장이나 대명사의 사용 등이 가능한 것은 문맥 내부에 이러한 문장들의 결속이 존재한다는 증거이다. 결과적으로 문맥을 벗어난 단일문장은 대개 중의성을 띠게 된다. 반대로 그러한 중의적인 문장은, 문맥의 상황을 적용시켜 주면 쉽고 자연스럽게 그 중의성이 해소된다.
자연언어 표현이 갖는 중의성이 전제에 의해서 해소되는 것은 의미합성의 견지에서 볼 때 시사되는 바가 크다. 한 표현의 의미는 자체 언명이 지닌 의미뿐만 아니라, 그 표현의 전제가 관여하여 이루는 의미까지도 합쳐진 것으로 규정되기 때문이다. 다시 말하면 어떤 표현의 의미를 안다는 것은 그 표현 자체가 언명하는 의미와 그 표현이 전제하는 의미 - 즉 문맥의 부분 구성 표현들 간에 공유되는 전제, 동시에 발화되는 동반 표현에 의한 전제, 선후 맥락에 의해 형성된 전제 - 모두를 알아야 한다는 것을 의미하는 것이다.It is generally accepted that there is no one-to-one correspondence between form and meaning in natural languages. Such an aspect that a meaning is not proper to a certain form is recognized in synonymy, vagueness, ambiguity, etc.
Synonymy is a many-to-one relation wherein two or more surface structures have one single meaning. In contrast Ambiguity is a one-to-many relation wherein a single surface structure has two or more meanings.
Vagueness has to do with lack of specificity, so vague terms are only dubiously applicable to marginal objects. Whereas ambiguity involves two or more distinct meanings for one word, phrase, or sentence, and an ambiguous term may be at once clearly true of various objects and clearly false of them.
Being a characteristic feature of natural languages, ambiguity comes from various causes. When a word with two or more meanings is used in a sentence, the sentence is preponderantly ambiguous. Those syntactic structures which are analyzed to have two or more categories or combinations are generally ambiguous. And two or more situations presupposed from a sentence make the sentence itself ambiguous.
Phonological ambiguity is traditionally believed to be a linguistic phenomenon that if a presentation is verbalized in two or more phonological patterns, it can make a difference in meaning. But we consider changing the explanation of the phonological ambiguity, because a written form which is ambiguous is mostly to be pronounced in different phonological patterns in order to disambiguate itself. So it seems to be a means for disambiguation rather than a sort of ambiguity. Accordingly, the widely varying ambiguous units are divided into three groups, viz. the ambiguities from lexical and syntactical factors, and the ambiguity from presuppositions.
For disambiguation, we can replace ambiguous words with single semantic words or append some words. Or we can also change the order of words in the sentence or modify tile structure of the sentence. The former can be called the lexical method and the latter the syntactical method. And we have another method for disambiguation, in which its presupposition is brought back to the ambiguous sentence or the situation where the ambiguous sentence lies is applied to it.
We can't help judging that those disambiguated sentences through the lexical or syntactical method become faded, losing their fresh feeling in the context, or unnatural and tedious. Whereas the other sentences, disambiguated by means of presupposition, appear natural and indispensable to the context. That is because the context, existing in every discourse, functioning as a field where sentences are binding, is a hidden stretch of language.
In a context, the ambiguity of a sentence within is believed to be disambiguated by the application of presuppositions, which have a coherent relation with the very sentence, accompanied clauses and leading or following sentences.
Here we conclude that a context is a find of pragmatic domain of sentences, among which common presuppositions are shared, and is linguistically coherent as the common presuppositions are making a concatenation of sentences which comprise a unity in it. So the sentences within a context are free of their forms as far as presupposition allows. It is evidence for the existing of the binding inside the set that we are able to use ellipses or pronouns In the context.
Consequently, a single sentence out of context mostly becomes ambiguous. Contrarily, if we put such an ambiguous sentence Into the situation provided by the context, we can easily disambiguate it.
It is very suggestive from the compositionality principle point of view that the ambiguity found in the presentation of natural languages is disambiguated by using the application of its presupposition. That is because the meaning of a sentence consists of not only the intention of its assertion but also such suggestions produced by the presupposition of the sentence. In other words, that we know the meaning of a sentence means that we know the meaning of the sentence's assertion and the sentence's presupposition, namely, the common presupposition between all the parts of the context, the presuppositions of accompanied clauses and the presuppositions of leading or following sentences.초록 = 1 제 1 장 서론 = 3 제 2 장 중의성의 개념 = 5 2. 1 언어표현의 형태와 그 의미 = 5 2. 2 모호성, 동의성과 중의성 = 6

자연어처리(NLP: Natural Language Processing)에 머신러닝(Machine Learning) 기술을 적용한 가상 에이전트 챗봇은 이전보다 더 유용해지고 우리 생활에서 자주 사용하게 되었다. 또한 전 세계적으로 메신저의 사용이 늘어나면서 메신저 상에서의 챗봇 서비스가 주목받고 있다.
챗봇의 사용 목적이 다양해지면서 이를 간단하게 개발하기 위한 챗봇 빌더가 국내외로 출시되고 있으며, 본 연구에서는 챗봇 빌더에 따른 사용자 사용성을 조사해보려 한다. 본 연구는 누적된 데이터에 의한 차이가 없도록 진행하기 위해 챗봇 개발이 필요했다. 그래서 교내 도서관의 관련 정보를 제공해주는 챗봇을 각각의 빌더를 사용해 개발했으며, 빌더 내 UI와 메뉴들을 간단하게 소개한다.
연구에 사용할 챗봇 빌더는 의도 파악 방법에 따라 선정하였는데, Google의 Dialogflow는 통계기반의 챗봇 빌더이며 TF-IDF 알고리즘과 나이브 베이즈 분류를 사용하여 의도 파악을 한다. Kakao의 i open builder는 유사도 기반의 챗봇 빌더로, 저장되어 있는 문장과 사용자의 입력 문장을 형태소 분석 후 토큰으로 나누어 비교하고 의도 파악을 진행한다.
사용성 조사를 통해 빌더의 인식률 차이와 인터페이스가 사용성에 기여한다는 것으로 나타났다. 이 연구는 특정 목적에 의해 완성된 챗봇의 성능이 아닌 제작 과정을 통해 챗봇 빌더의 성능을 비교 진행했다는 점에서 기존의 챗봇 연구들과 차별점을 지닌다. 또한 정체성과 인터페이스, 대화 디자인을 고려해 완성도 높은 챗봇을 제작하는 것에 대한 방향성을 제안한다.Machine Learning and Natural Language Processing-based virtual agent, namely chatbot, became more useful than ever before and was frequently used in our daily life. Also, the use of the text-based messaging service is increasing worldwide; the chatbot as one type of messaging service is getting more attention as well.
There are chatbot builders developed by both Korean and US companies. In this study, we investigated the usability of two chatbot builders. We developed the chatbots to provide information about a library.
The chatbot builder to be used in the study was selected according to the intention identification method. Google's Dialogflow is a statistics-based chatbot builder, uses the TF-IDF algorithm and the Naive Bayes algorithm to determine the intent. Kakao's i - open builder is a similarity-based chatbot builder. It compares stored sentences with user's input sentences by morpheme analysis and then compares them by the token.
The empirical usability study conducted as a part of this work, showed that recognition rates were different by the chatbot builders and the messenger interface generated by chatbot builder contributed to the usability. This research differs from the existing chatbot studies in that it does not compare the performance of the chatbots but the performance of the chatbot builders. In the future, the comparatively more complete chatbot should be developed and compared by including the persona, interface, and dialog scenario design.목 차 LIST OF TABLE ⅲ LIST OF FIGURES ⅳ ABSTRACT ⅴ

자연어처리에서 학습 말뭉치는 매우 중요한 부분이다. 본 논문에서는 RDR(Ripple-Down Rules)을 이용하여 개체명 태그의 오류를 수정하는 방법을 제안한다. 규칙 생성에 자질 정보를 사용하기 위해 RDR을 확장하여 템플릿을 사용 가능하게 확장하였다. 학습과 테스트에 블로그 문서를 수집하여 제작한 말뭉치를 사용하며, 소량의 문서를 학습하고 대량의 문서를 평가함으로 일반적인 상황에서도 오류 수정의 효과와 효율을 검증하였다.그림 목차 표 목차 I. 서론 II. 관련 연구 1.개체명 부착 말뭉치의 종류

본 논문은 AQP-WKSP(Analysis & Question-Answering Processor Using What Korean Sentence Pattern)를 활용하여 자동화된 사이버 교육 질의응답 시스템을 구축하기 위하여 AQP-WKSP를 제안하고 구현하였다.
질의응답 시스템(Question Answering System)은 주어진 질의에 대해 응답을 추출하는 시스템을 총칭하는 용어로 사용자의 질의 의도를 파악하여 질문과 매칭 되는 답변을 보여주는 시스템이다. 본 논문은 사이버 교육 시스템을 운영하고 관리하는데 큰 비중을 차지하고 있는 Q/A 게시판의 답변을 자동화하여 실시간으로 제공함으로써 한 차원 높은 사이버 교육이 될 수 있도록 구축한 질의응답 시스템에 관한 것으로, 기존 질의응답 시스템의 순차적인 프로세스에서 나타나는 문제점을 해결하기 위하여 통합적인 프로세서를 구현하였고, 모든 분석과정에서 나타나는 중의성 문제를 해결할 수 있는 새로운 질의응답 시스템을 제안하였다.
사이버 교육 시스템 게시판의 질의문을 ‘한국어 질의문 문장패턴(WKSP)’를 활용하여 기존의 질의응답 시스템의 여러 분석과정에서 발생하는 문제들을 AQP-WKSP를 구축하여 해결하였다. 그리고 생성된 질의문에서 정확률을 향상시키기 위하여 기 구축된 사이버 교육 시스템 KSP기반 교재를 활용하여 추출된 질의문 문장패턴과 대응되는 응답대상 문장을 검색하는 방법을 제안하여 기존의 질의응답 시스템에 비해 사용자에게 효율성을 증가 시키고 응답문을 생성하는 과정을 최소화함으로써 분석과정에서 발생하는 여러 가지 문제점을 해결하여 질의응답의 정확률을 향상시키는 질의응답 시스템을 구현하였다.
시험 방법 및 결과는 사이버 교육 게시판으로부터 질의문 2,300개중 단문으로 이루어진 1,610개의 질의문을 추출하여 동사형, 형용사형, 지정사형으로 분리하여 각각 94.1%, 82.2%, 80.4%의 정확률로 효율성과 활용성을 입증하였다.

향후 제안된 질의응답 시스템은 사이버 교육 시스템 뿐 아니라 웹 정보를 대상으로 한 웹 질의응답, 멀티미디어 정보에 대한 멀티미디어 질의응답, 온톨로지 질의응답, 전자상거래 질의응답 등으로 계속 연구가 진행되어 대형 정보 관리 시스템과 같은 다양한 분야에 활용된다면 차세대 정보검색기술로서 사용자에게 보다 큰 만족을 제공할 것으로 기대된다.This paper studies on the cyber education question-answering system using AQP-WKSP. The question answering system is what shows answer matched to question with grasping the intention of user's as the general term of system answering to given questions. This paper is on the question answering system that enables cyber education to be one step high with supporting in realtime by automatizing answer to Q/A notice board given a great deal of weight on in operation and management of cyber education, and solves the problem of a sequential process in a existing question answering system with configuring synthetic process and proposes a novel question answering system solving ambiguity problem appeared at all analysis process.
It solves the problems happened in any analysis processes by establishing AQP-WKSP, utilizing 'Korean question sentence pattern' from questions in notice board of a cyber education system. In second step, it gives the more efficiency to user rather than the existing system by proposing a method searching answering sentence corresponding to question sentence pattern extracted with utilizing a textbook based on KSP in the existing cyber education system to increase the rate of accuracy, and implements the question answering system that increases accuracy in question and answer with solving many problems in analysis processing by minimizing making steps.
In the future, the proposed question answering system will have studied not only for cyber education but also that for multimedia, that for ontology, that for E-business and if it is used in many field such as a mass information management system, it will be expected to give users more satisfaction by the oncoming generation of Information Searching Technology.제 1 장 서론 1 제 1 절 연구배경 1 제 2 절 연구 목적 및 내용 1 제 2 장 이론적 배경 4



This paper presents a new hybrid dialog management framework that integrates a statistical ranking algorithm into an example-based dialog management approach for chat-like dialogs. The proposed model uses ranking features that consider various aspects of dialogs, including the relative importance of speech acts, dialog history sequences, and the causal relationships among speech acts and slot-filling states. The ranking algorithm enables one to aggregate these feature scores systematically and to generate diverse system responses. Additionally, the model provides detailed feedback by analyzing the causal relationships among speech acts and predicting the user’s possible intentions associated with a given dialog states. Simulated experimental results demonstrate that our approach is effective for task-oriented dialogs and chat-like dialogs. Additionally, a case study using elementary school students implies that the proposed system can be used for language learning purposes in addition to task-oriented services.I. Introduction 1 1.1. Background 1 1.2. Previous Technologies 5 1.3. Problem Statement 9 1.4. Motivation 14

This paper proposes a new method for automatic mapping of KorLexNoun(KLN) and Sejong semantic classes(SJSC) by considering semantic similarity. KLN is a part of Korean Lexico-semantic network(KorLex) and also is a Korean WordNet which contains rich words with semantic relationships. However, it has not enough information for semantic analysis and syntactic analysis.

Sejong electronic dictionary(SJD) is made for general purpose of Korean natural language processing with specific semantic and syntactic information and is based on SJSC. But it relatively contains very few words compared with KorLex.

Therefore, For the purpose of improving the technology of semantic analysis and syntactic analysis, the research about manual mapping between KorLex and SJD has been conducted, but the manual mapping conducted by people consumes very large cost, time, and high-quality human resources. Furthermore, it also has a weakness such as complicate maintenance for the relationship between two language resources because the two language resources, KorLex and SJD, have been constantly developed and expanded.

This paper presents two methods for automatic mapping for KLN and SJSC to overcome the weaknesses of manual mapping by considering semantic similarity between two resources with using linguistic information and statistical information. First, the method using linguistic information for automatic mapping considers semantic similarity based on monosemy/polysemy in KLN, nouns of SJD and word senses of KLN, semantically related words or synonym set in SJD, KLN. Second, the method using statistical information for automatic mapping considers semantic similarity with calculating , Mutual information, Information gain between words belonged SJSC and word senses of synonym sets in KLN.

The experiment on the ground of result by manual mapping as correct answer showed better performance than existing automatic mapping between two language resources. This research obtained Recall 0.838, Precision 0.718 and F1-measeure 0.774 with using linguistic information and Recall 0.826, Precision 0.712 and F1-measure 0.765 with using statistical information and heuristic.제1장 서론 1 1.1. 연구의 배경과 필요성 1 1.2. 연구 범위 4 1.3. 논문의 구성 4



최근 온라인 리뷰에 대한 기업들의 관심이 높다. 소비자들이 온라인 쇼핑을 이용할 때뿐만 아니라 오프라인 매장에서도 스마트폰으로 리뷰를 검색하여, 의사결정을 하기 때문이다. 소비자들은 전문 판매원보다 실구매자가 남긴 온라인 리뷰를 더욱 신뢰한다. 본연구는 감성분석에서 가장 효율적인 속성 선택 방법과 분류기의 조합을 찾는 것을 목표로 한다. 이를 위해 텍스트 마이닝 관점에서 텍스트에 포함된 긍정, 부정을 분류하는 오피니언 마이닝과 화남, 놀람, 슬픔 등을 분류하는 이모션 마이닝을 수행하였다. 연구의 목적을 실증하기 위해 온라인 상에 공개된 apparel, book, DVD, electronic, kitchen 총 5가지 상품군의 리뷰 텍스트를 수집하고, 텍스트의 TF-IDF를 계산하고 Bag-of-Words형태로 구성하여 상품 리뷰 속성 셋을 구성하였다. 이후 속성 선택을 사용하여 텍스트의 오피니언과 이모션을 잘 설명할 수 있는 속성을 고르고, 선택된 속성을 분류기에 학습하여 분류 성과를 측정하였다. 오피니언 마이닝의 결과 오피니언 마이닝의 분류 정확도는 Information Gain 방법으로 속성을 선택하고, 앙상블 분류기의 한 종류인 Stacking을 사용하는 것이 5가지 상품에 대해 가장 높은 성능을 보였다. Information Gain 속성 선택은 오피니언 분류 성능은 Stacking 외의 다른 분류기에서도 더 높은 성능을 달성하였다.목 차 제1장 서 론 1 1. 연구 배경 1

딥러닝을 활용한 여러 분야 중 텍스트 분류 과제에서는 순환 신경망 기반 Sequence model의 사용이 주를 이루고 있다. 그러나 여러 연구에서 통상 이미지 처리에 유용하다고 알려진 합성곱 신경망을 활용하여 텍스트 분류 성능을 입증하고 있다. 그런데 한글 데이터 기반 연구의 경우 많은 연구에서 여전히 Sequence model을 사용하고 있으며 합성곱 신경망 및 최신 알고리즘을 활용한 연구가 부족한 실정이다.
이에 본 연구는 한글 리뷰 데이터를 중심으로 합성곱 신경망 기반 가장 최신 알고리즘인 Capsule Network을 활용한 감성 분류 모델을 제안하였다. 또한 Capsule Network에 텍스트를 적용하는 과정에서 ‘문장 캡슐’ 개념을 도입하였고 이에 따라 Sequence model에서 가장 성능이 좋다고 알려진 Hierarchical Attention Network에서와 같이 텍스트 데이터를 문서-문장-단어 단위로 적용 가능하도록 하였다. 추가적으로 Hierarchical Attention Network 기반 감성 분류 모델을 학습 하였고 Capsule Network와 성능 비교를 하였다. 그 결과, 각각 약 93%, 94%의 높은 정확도를 기록하였다. 따라서 Capsule Network가 텍스트에서도 유용함을 확인할 수 있었고 한글 데이터의 감성 분류에서 합성곱 신경망 기반의 Capsule Network가 Sequence 기반 모델인 Hierarchical Attention Network 보다 더 높은 정확도를 보임을 확인하였다.

핵심되는 말 : 딥러닝, 자연어 처리, 감성 분류, Capsule Network

국내 택배시장 규모는 매출 3조원 이상, 물량 13억 상자 이상을 처리하고 있다. 2000년 6천억원에서 불과 10년 사이에 500% 이상 확대되었다. 그에 반해 소비자들의 불만 역시 증가하였다. 따라서 현재의 수작업 V.O.C(voice of customer) 분류 방식으로는 적절한 대응에 한계가 있을 수 밖에 없다.
이 논문에서는 효율적인 택배불만 처리를 위해서 불만의 종류와 정도를 기계학습을 이용하여 자동분류 하는 과정 및 결과를 기술한다. 약93,000건의 V.O.C를 대상으로 학습 데이터를 구축하고 여러 자질 선택 기법을 비교하였으며, 기존의 다양한 문서 자동 분류 방법들을 적용해 보았다. 실험결과 사전기반 분류보다 기계학습모델이 더 좋은 성능을 보였고, 기계학습모델 중에서는 지지벡터기계가 가장 좋은 성능 보였다. 불만의 정도에서 지지벡터기계의 Precision 결과는 평균 82.4%, Recall 결과는 평균 83.9%로 나타났고, 사전기반에서 Precision 결과는 평균 79%, Recall 결과는 평균 67.9%로 나타났다. 불만의 종류에서는 지지벡터기계의 Precision 결과는 평균 75.9%, Recall 결과는 평균 76.4%로 나타났고, 사전기반에서 Precision 결과는 평균 76.4%, Recall 결과는 평균 65%로 나타났다.For the size of the Korean door to door delivery service market, sales reach more than KRW 1 trillion and more than 1.3 billion boxes are being treated. More than 500% of KRW 6 hundred billion in 2000 was expanded for 10 years only. In contrast, complaints of consumers have also increased. Therefore, the current classification method of V.O.C(voice of customer) by hand is a limit to respond to them properly.
This paper describes the process and result of automatic classification about kinds and degree of complaints using machine learning techniques to handle complaints of the door to door delivery service efficiently. We built a training set in V.O.C. of around 93,000 compared the several feature selection methods with each other, and applied the existing various documents’ automatic classification to it. The test result has found that the machine learning model showed better performance than dictionary-based classification and the support vector machine did the best performance in the machine learning model. The degree of complaints of the support vector machine showed precision of 82.4% and recall of 83.9%. And the dictionary-based result did precision of 79% and recall of 67.9%. The kinds of complaints of the support vector machine did precision of 75.9% and recall of 76.4%. And the dictionary-based result did precision of 76.4% and recall of 65%.요 약 4 ABSTRACT 5 그림 목차 8 표 목차 9 1. 서론 10

텍스트 코퍼스를 기반으로한 질의응답 시스템을 구현하기 위해서는 주어진 문장의 문법적 구조를 분석하고 의미적으로 중요한 단어를 추출하는 것이 중요하다. 이는 대용량의 텍스트 코퍼스에서 정답을 추출하기 위해 필수적인 정보이다.
본 연구에서는 문법적으로 중요한 단어를 키워드로서 정확하게 추출하기 위해 트리 패턴 표현식(TPE, Tree Pattern Expression)을 활용 하는 TPE 기반 Tuple 추출 기법과 Tuple을 질의응답에 활용하기 위한 정답 매칭 알고리즘에 관한 내용을 제안한다. TPE를 활용하면 문장의 문법 구조를 기존의 기법을 활용했을 때보다 빠르고 정확하게 비교하여 문법적으로 중요한 요소를 추출하는 것이 가능하다. 이 때, Tuple이란 <주어, 서술어, 목적어, 전치사, 전치사의 목적어>의 형태로 구성된 문장에서 문법적으로 중요한 요소의 집합으로, 주어진 문장에서 Tuple을 추출할 때 각 요소에 해당하는 Named Entity Tag가 존재할 경우 함께 추출한다. 이 Named Entity Tag는 질의문 타입과의 상응관계를 이용하여 정답을 찾기 위한 과정에 활용되었다.
힌편, 본 논문에서는 추가적으로 텍스트 코퍼스에 의존적이라는 TPE 기반 Tuple 추출 기법을 활용한 질의응답 시스템의 한계를 극복하기 위한 하이브리드(Hybrid) 질의응답 시스템을 제안한다. 하이브리드 질의응답 시스템은 텍스트 코퍼스를 활용한 TPE 기반 Tuple 추출 기법과 웹 자원을 기반으로 한 키워드 추출 기법의 결합으로 구성되어 있다.
마지막으로 성능 평가를 통해서는 본 연구에서 제시한 기법이 기존의 기법들에 비해 상대적으로 우수한 성능을 보임을 확인하였다. 이는 TPE 기반 Tuple 추출 기법을 사용하면 주어진 문장이 복잡한 문법 구조를 포함하고 있다고 하더라도 문장의 문법구조를 정확하게 분석하여 간결한 형태의 Tuple을 추출하는 것이 가능하기 때문이다. 또한 하이브리드 질의응답 시스템을 통하여 질의응답의 성능을 획기적으로 향상시킬 수 있음을 확인하였다.국문 요약 ······································································ 1 제 1장 서 론 ·································································· 3 제 1절 연구 배경 ································································ 3 제 2절 연구 내용 ································································ 5

지식베이스는 자연어 처리 기반의 다양한 응용 시스템 성능에 영향을 미치는 중요한 요소이다. 영어권에서는 YAGO, Cyc, BabelNet, DBpedia와 같은 지식베이스들이 널리 사용되고 있다. 그러나, 지식베이스 구축을 위한 자원의 부족으로 다른 언어를 지원하는 지식베이스는 영어에 비해 많이 부족한 실정이다. 본 논문에서는 DBpedia와 YAGO로부터 YAGO 형식의 한국어 지식베이스(이하 K-YAGO)를 자동 구축하는 방법을 제안한다. 제안 시스템은 YAGO와 인포박스 트리플 간의 간단한 매칭을 통해 초기 K-YAGO를 구축한 뒤, 기계학습을 이용하여 초기 K-YAGO를 확장한다. 실험 결과, 제안 시스템은 초기 K-YAGO 구축 실험에서 0.9642의 신뢰도를 보였고, K-YAGO 확장 실험에서 0.9468의 정밀도와 0.7596의 매크로 F1 척도를 보였다.The performance of many natural language processing applications depends on the knowledge base as a major resource. YAGO, Cyc, BabelNet and DBpedia have been extensively used as knowledge bases in English. However, we can not find large volumes of Korean knowledge bases due to lack of online resources for constructing knowledge bases. In this paper, we propose a method to construct a YAGO-style knowledge base automatically for Korean (hereafter, K-YAGO) from DBpedia and YAGO. The proposed system constructs an initial K-YAGO simply by matching YAGO to info-boxes in Wikipedia. Then, the initial K-YAGO is expanded through the use of a machine learning technique. Experiments with the initial K-YAGO shows that the proposed system has a precision of 0.9642. In the experiments with the expanded part of K-YAGO, an accuracy of 0.9468 was achieved with an average macro F1-measure of 0.7596.Ⅰ. 서 론 ------------------------------------ 1 Ⅱ. 관련연구 ------------------------------------ 3 1. YAGO ----------------------------------- 3 2. 지식베이스 구축 --------------------------- 3 3. 원거리 감독법 ----------------------------- 4

연어는 자연어 처리 분야에서 중요한 정보로, 각각의 단어로 나누지 않고 통합해 처리하면 효율적인 단어열을 의미한다.
이러한 연어를 추출하기 위해 본 논문에서는 연어를 연속된 연어와 분리된 연어로 나누었다. 연속된 연어는 단어와 단어 사이에 다른 단어열의 삽입이 불가능한 반면, 분리된 연어는 삽입이 가능하다. 다시 연속된 연어를 연속된 고빈도 연어와 연속된 통사적 연어로 나누었다. 여기서 연속된 고빈도 연어는 말뭉치에서 자주 나타나는 연어이고, 연속된 통사적 연어는 연어내의 특정 단어가 앞 또는 뒤에올 단어열을 선택 제약하는 특징을 갖는 연어이다.
이렇게 연어의 종류를 나누고 각각에 맞는 추출 방법을 제시했다. 첫 번째로, 연속된 고빈도 연어는 인접 단어의 확률값을 이용해 단어열의 엔트로피를 구했을때 이것이 임의의 값 이상이면 연어로 추출했다. 두 번째로, 연속된 통사적 연어는 특정 단어의 앞 또는 뒤에 한 가지 단어열이 집중적으로 분포하는지를 알아내기 위해, 특정 단어의 엔트로피를 구해 이것이 임의의 값보다 작으면 연어로 추출했다. 마지막으로, 분리된 연어는 특정 단어열과 일정 빈도이상 공기하는 단어열이 있을 때 추출했는데, 연속된 고빈도 연어를 추출하기 위해서 구한 단어열의 엔트로피를 이용했다.
실험은 품사 부착된 HANTEC 말뭉치를 가지고 수행했고, 각각의 방법을 평가한 결과 평균 85.6%의 정확률을 얻었다.This paper presents two methods using entropy to extract collocations in Korean. This is based on the idea that if a random variable X has many cases and each case has a uniform distribution, then entropy has higher value. On the other hand, entropy has lower value when it has few cases and each case has a uniformless distribution. The first method extracts collocations which frequently co-occur in the corpus. For experiments, the string is accepted as collocation only if the entropy of the string has higher value than a random one. The second method extracts collocations which have a selectional restriction on the choice of individual lexical units in construction with other lexical units. For experiments, the string is accepted as collocation only if the entropy of a restricting word has a lower value than a random one. In particular, we construct a interrupted collocation by the first method and co-occurrence frequency in the corpus. The collocations retrieved by the methods are applicable and useful in various fields.차례 = i 그림차례 = ii 표차례 = ii 국문요약 = iii 제1장 서론 = 1

Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation modeling. However, they suffer from the notorious degeneration problem, where the RNN decoders learn to ignore latent variables and reduce to vanilla RNNs. We empirically show that this degeneracy occurs mostly due to two reasons. First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the role of latent variables to capture variability of data. Second, the context-conditional VAE structure whose utterance generation process is conditioned on the current context of conversation, deprives training targets of variability; that is, target utterances in the training corpus can be deterministically deduced from the context, making the RNN decoders prone to overfitting given their expressive power. To solve the degeneration problem, we propose a novel hierarchical model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop for regularization of hierarchical RNNs. With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for
conversation generation. Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.계층적 회귀신경망과 (hierarchical RNNS) 결합된 Variational autoencoders (VAE) 는 대화 모델링을 위한 강력한 프레임워크를 제공한다. 그러나 이러한 모델은 잠재변수 (latent variable)을 무시하도록 학습되는 degeneration 문제를 겪는다. 우리는 실험적으로 이 문제에 크게 2가지 원인이 있는 것을 밝힌다. 첫째, 계층적 회귀신경망의 자기회귀적 (autoregressive) 분포 추정 능력이 매우 강력하기 때문에 잠재변수에 의존하지 않고도 데이터를 모델링 할 수 있다. 둘째, 문맥에 의존하는 conditional VAE 구조는 대화 문맥이 완전하게 주어지기 때문에 다음 발화를 거의 결정론적으로 추론할 수 있으며, 따라서 계층적 회귀신경망은 쉽게 학습 데이터에 과적합 (overfit) 할 수 있다. 이 문제를 해결하기 위하며 우리는 Variational Hierarchical Conversation RNNs (VHCR) 이라는 계층적 모델을 제시한다. 이 모델은 1) 잠재변수의 계층적 구조를 사용하는 것, 2) utterance drop regularization 을 사용하는 것의 2가지 중요한 아이디어를 활용한다. Cornel Move Dialog 와 Ubuntu Dialog Corpus 의 2가지 데이터셋에서 우리는 실험적으로 이 모델이 기존의 state-of-the-art 성능을 갱신하는 것을 보인다. 또한, 계층적인 잠재변수 구조는 대화 내의 발화 내용의 제어를 새로운 측면에서 가능케 한다.Abstract i Contents iii List of Figures v List of Tables vii



대화 시스템은 자연어를 사용하여 사람과 대화를 통해 상호작용하는 시스템을 말하며 자연어 대화는 사용자에게 매우 직관적이며 효율적으로 시스템을 다룰 수 있는 방법을 제공한다.
대화는 다양한 형태로 존재하며 대화 내의 발화에 대한 처리도 복잡하므로 사용자가 만족할 만한 자연스러운 대화를 이끌어 가는 시스템을 설계하는 것은 어려운 일이다. 이에 대한 해결책으로 미리 정한 대화 전략(Dialog Strategy)에 따라 대화를 진행하는 시스템이 고안되었다. 기존 연구들에서 대화 전략은 기계 학습 방법 중 하나인 강화학습(Reinforcement Learning)을 통해 주로 작성되었다.
강화학습은 최적의 대화 전략을 학습하는데 적합하지만 비교적 많은 학습 데이터를 필요로 하는 단점이 있다. 다른 종류의 학습 데이터 보다 대화 학습 데이터는 작성하는데 더 많은 노력과 시간이 소모되므로 학습에 충분한 대화 데이터를 확보하는데 어려움이 많다. 또한 학습에 사용되는 데이터가 많으므로 학습 속도가 느린 편이다.
본 논문에서는 사용자 모델(User Model)을 사용하여 적은 학습 데이터에서도 빠르게 학습이 가능한 대화 전략 강화학습 방법을 제안한다. 이 방법은 시스템이 할 수 있는 다양한 행동들 중, 현재 상황에 보다 적합한 행동들을 구별하고 이런 행동들만을 학습에 사용한다. 그러면 학습 과정에서 시스템이 불필요하게 탐색해야 할 공간을 줄여주어서 학습되는 대화 전략의 질(Quality)을 유지하면서도 학습 속도를 빠르게 할 수 있다.
다른 비교 시스템과 동일한 사용자 모델을 사용하여 학습 실험을 한 결과, 비교 시스템에 46%의 학습 데이터 만으로도 비슷한 성능을 내는 것을 볼 수 있었고, 학습 시간도 비교 시스템의 35%로 감소하는 것을 볼 수 있었다.A dialogue system interacts with human using the natural language. Although natural language conversation is a instinctive and efficient way for human users, designing a dialog system is very difficult because of the diversity of the dialog.
Most dialog system use their own dialog strategy when the systems interact with humans. Some of them have used a reinforcement learning method to learn the dialog strategy. But the reinforcement learning generally requires a lot of training data. Making enough dialog training data is time-consuming and hard work. Therefore, developing a fast way to learn a dialog strategy using reinforcement learning is very important.
In this paper, we proposed a fast reinforcement learning model to learn a dialog strategy using a user model reflects the characteristics of a user. With the user model, the proposed system can choose better actions for a current state in learning process. It makes the learning process fast while sustaining the quality of the learned dialog strategy.
We compare the proposed system with a baseline system. In the experiment the proposed system shows the similar performance with only 50% of the training data, and reduced the learning time by 35%.제 1 장 서론 = 1 제 2 장 관련 연구 = 3 2.1 사용자 시뮬레이션 = 4 2.2 빠른 강화학습 방법들 = 5 제 3 장 사용자 모델을 이용한 강화 학습 = 8

전자사전은 초기에 자연어 처리를 위한 지원 도구로 개발되었다. 이러한 이유로 단어와 의미가 단순 대응되는 수준에 그치고 있으며, 제품에 따라 각각 단어 내용을 표현하는 방법이 표준화되어 있지 않다.
인터넷 활용과 더불어 전자사전은 그 이용 빈도가 늘어나고 있다. 그러나 단순 대응되는 사전 내용은 사용자가 원하는 내용을 보여주지 못하며 표준화되지 않은 데이터 표현은 데이터 교환 및 재사용을 불가능하게 하고 있다.
본 논문에서는 발음, 문법과 의미 등 다양한 내용 제공을 위해 사전 내부 데이터 표현 형식을 정의하였으며, 재사용성 및 호환성을 향상시키기 위하여 사전 구조를 부품화 했다. 뿐만 아니라 사용자와 시스템 사전으로 구분된 사전 인덱스 구조를 통하여 검색속도를 높였다.At the early electronic age, electronic dictionaries were not widely used, because they were developed for natural language processors. They only provided simple mapping between words and their meanings and their data were not standardized.
As internet population grows, electronic dictionaries become necessary tools for computer users. However, they have same problems that the early electronic dictionaries have. They still provides only mapping between words and their meanings. It is not possible to reuse data or functions from other dictionaries.
In this thesis, flexible and reusable data representation scheme is proposed to support pronunciations, syntax as well meanings of words for electronic dictionaries. And we componentize electronic dictionary for reuse. As a result, we can get better electronic dictionary architecture which can be reused and performed better in searching words by splitting index structure into system and user dictionary.목차 제1장 서론 = 1 제2장 관련연구 = 4 2.1 전자사전 표준 = 4 2.1.1 SDML = 5



본 논문에서는 컨볼루션 신경망 네트워크(CNN:Convolution Neural Network)를 기반으로 단어의 의미와 순서를 고려하는 문서 색인 방법을 이용하여 한글 문서 분류 방법을 제안한다. 먼저 문서를 어절 단위로 분리하여 명사와 형용사만 추출 한 후 Tdm(Term Document matrix)에서 DF(Document Frequence)값이 높은 단어 들을 추출 해 불용어로 처리 하였습니다. 제목은 본문과 연결하였고. 각 문서마다 LDA를 이용하여 토픽을 생성 한 후 문서 내의 단어 중 토픽에 포함된 단어의 경우 기존 단어에 추가로 토픽 내 단어를 추가 하였습니다. 문서의 단어의 의미와 순서를 고려하여 문서를 벡터화 시키고 그 값을 CNN의 입력으로 사용하였습니다. 실험결과 CNN 분류기를 기반으로 본 논문에서 제안하는 문서 색인 방법은 기존의 방법보다 한국일보 데이터에서는 5.4%, 해외과학기술문헌속보의 데이터에서는 4.7%의 성능 상승을 이루었다. 이러한 결과를 통해 본 논문에서 제안하는 방법이 문서 범주화 데이터 셋에서 문서 분류 성능향상에 영향을 미친다는 것을 확인하였다.Ⅰ. 서론 1 1. 문제 제기 1 2. 연구 목적 및 방법 2 3. 논문의 구성 3 Ⅱ. 배경 지식 및 관련 연구 4

디지털 기기가 광범위하게 보급되면서 Information Stream 이 정보 인지의 보편적인 수단으로 활용되고 있다. 웹 2.0 기술의 발달과 모바일 기기의 광범위한 보급으로 수 많은 컨텐츠들이 온라인 상에 쏟아져 나오고 있다. 따라서 Information Stream을 이용하여 정보를 소비하는 사용자들은 구독하는 정보원이 늘어남에 따라 구독하는 정보의 숫자가 기하급수적으로 늘어나게 될 것이다. 따라서 수 많은 데이터를 사람이 처리 할 수 있게 정리, 요약하는 기술이 필요하게 된다. 트위터는 2011년 3월 현재 1억 7천 5백만명의 사용자를 보유하고 있는 가장 유명한 마이크로블로깅 서비스이다. 다른 Information Stream과 마찬가지로 트위터 역시 다른 사용자의 정보를 구독 할 수 있으며, 정보의 과다 현상이 일어나고 있다. 본 논문에서는 이런 트위터의 정보 과다 현상을 해결하기 위해 TextRank 알고리즘이라는 자연어 처리 기법을 응용하는 기법에 대한 연구를 진행하였다. 타임라인을 이루고 있는 트윗들을 그래프로 모델링 하고, 모델링한 타임라인 그래프에 그래프 기반의 랭킹 알고리즘을 적용하여 스코어를 얻었다. 주제를 대표하는 키워드를 뽑기 위해 본 논문에서는 제안한 산봉우리 개념이 적용된 알고리즘을 제안하였다. 그리고 이런 알고리즘이 단순히 빈도수 기반으로 키워드를 뽑았을 때보다 정확도와 커버리지가 높고, 일관도 역시 크게 떨어지지 않는 것을 보여 성능이 더 좋음을 확인하였다.digital device has come into wide use, Information Streams have recently emerged as a popular means of information awareness. Twitter is one of the most popular micro-blogging service and social media with a limit of 140 characters. As time has gone on Twitter user follows more accounts, eventually the user subscribes information more than the user can process. In this paper, we apply TextRank algorithm to resolving information overload in Twitter user timeline. After modeling user timeline as a graph, we apply graph-based rank algorithm to the timeline graph. Based on the score of each vertex, we apply concept of summit to summarizing user timeline. The experimental results show that proposed method summarizes user timeline more effectively than existing method that rely mainly on frequency based method.

In this paper, we propose a new method to understand video scenes and generate captions using past contextual information in order to solve the limitation that existing video captioning model does not understand video with long context. To this end, a sub-scene is formed by subdividing long and complicated video to reduce the dependence on the input sequence length and introduces an external memory for long context information management. To handle complex and abstract information such as context, we used the Differentiable Neural Computer (DNC). DNC can utilize similar information that is not present in the actual input based on the similarity between the given data and the information stored in memory.
In this paper, we construct long context information by connecting abstracted information of input data accumulated in DNC memory and parameters involved in memory input/output operation along the time axis. Then, language models for generating captions for each sub-scene are sequentially trained. Through this method, we noticed that it can generate more natural sentences using past context information unlike existing video captioning models, and noticed that it can also help to improve quantitative performance indicators (BLEU, METEOR).I. 서론. 1 II. 비디오 캡셔닝 분야에서의 맥락 이해에 관한 문헌조사 4 III. 제안 방법 : Differentiable Neural Computer(DNC) 메모리가 연결된 형태의 비디오 캡셔닝 모델 9 3.1 Differentiable Neural Computer(DNC) 9

컴퓨터와 모바일 산업의 발달로 인터넷 사용이 증가하면서 사람들이 자신의 의견을 온라인에 피력할 수 있는 기회가 많아지고 있다. 특히 상품에 대한 리뷰와 평점들은 기업에서도 신뢰도 분석이나 상품의 피드백 자료로 활용하고 있다. 온라인 유저들도 리뷰와 평점을 참고하여 상품 구매를 하고 있다.
온라인 영화 사이트는 영화에 대한 네티즌들의 리뷰와 평점 등을 반영하여 영화들의 순위를 알려주고 높은 평점의 영화를 추천해준다. 대부분의 온라인 영화 사이트의 평점 시스템은 리뷰와 평점을 동시에 개별적으로 작성하는 시스템으로 이루어져 있다. 리뷰와 평점은 네티즌들의 주관적 판단으로 이루어진다. 이로 인해 리뷰와 평점에서 감성 극성의 방향과 크기가 종종 일치하지 않는 경우가 발생하게 된다. 따라서 네티즌들의 의견을 완전히 반영하지 못한 영화 평점을 추론하게 되고 완벽한 영화 순위를 매길 수 없게 된다.
이러한 문제를 해결하기 위해 그 동안 감성 어휘 사전을 구축하거나 여러 기계학습 방법 등으로 리뷰에 대응하는 평점을 추론하는 연구가 진행되었다. 하지만 기존의 연구들의 범위는 단순히 단어 수준에 그쳐 리뷰 문장 내 단어들의 연관 관계에 따른 극성 변화를 고려하지 않고 있다. 이럴 경우 문장에 따라 극성이 달라지는 서술어들과 서술어를 강조하는 수식어를 사용한 문장들에서는 부정확한 평점이 부여될 수 있다. 또한 감성 어휘 사전의 단어들이 대부분 형용사에 국한되어 있어 평점을 추론할 수 있는 리뷰의 개수가 제한되는 문제점이 있다.
이에 본 논문에서는 문장 성분 조합을 통한 영화 평점 추론 및 텍스트 감성 분석 방법을 제시하고자 한다. 문장의 주성분인 주어와 서술어의 조합을 통해 기존의 감성 서술어뿐만 아니라, 문장에 따라 극성이 변하는 서술어에 대한 극성의 크기와 방향을 측정할 수 있다. 독립적으로는 중립 극성을 띠지만 다른 단어와의 결합을 통해 극성이 변하는 서술어에 대해서도 극성의 크기와 방향을 추론할 수 있게 된다. 더불어, 문장 내에서의 수식어 여부에 따른 감성 점수 변화를 측정하여 수식어의 강조에 따른 감성 점수 차이를 반영한다.
본 연구를 통해 문장 성분 조합을 통해 영화 리뷰에 대응하는 평점을 추론하는 것이 기존의 단일 감성 서술어를 통한 평점 추론보다 더 영화 리뷰에 가깝게 대응하는 평점 추론을 할 수 있음을 확인하였다. 또한 서술어와 서술어를 수식하는 수식어 간의 극성 크기 차이에 따른 문장 점수 변화에 대한 규칙도 추론할 수 있게 되었다.As increasing of using internet by development of IT industries, people are getting more opportunities to express their opinions online. In particular, enterprises use the reviews and ratings of their products to analyze their reliabilities and feedback. Online users also purchase products with referencing reviews and ratings.
Online movie sites show movie rankings which reflect netizens' reviews and ratings of movies with recommending high rating score movies. Most of online movie sites have rating systems which score ratings and reply comments individually. Reviews and ratings are determined by netizens’ subjective thoughts. For this reason, the systems may cause discordance of sentiment direction and rates of sentiment polarities between reviews and ratings. Thus, the systems cannot infer perfect movie ratings and movie ranking which reflect netizens' opinion.
To solve these problems, in the recent studies sentiment word dictionary or researched several machine learning methods are constructed to infer ratings to correspond to reviews. However, the studies only research simple word level without considering changes of polarities by correlation between words. In this case, predicates which polarities depends on sentences and adverbs which emphasize meanings of predicates might be scored inaccurately. In addition, vocabularies of sentiment word dictionaries are adjectives so that the number of reviews for inferring become lower.
In this paper, we propose methods of movie rating inference and sentiment analysis by considering combination of Sentence constituents. This paper can measure not only direction and rates of fixed-polarity predicates, but also unfixed polarity predicates by combination of subjects and predicate. We can infer direction and rates of predicate polarities which have neutral polarity in single usage, but changes according to combination with other words. Also, the system can measures changes of sentiment score using adverbs to reflect difference of sentiment scores by adverbs.
Through this study, infer ratings by Constituent of Sentence Combination can make more correspond to sentiment of review than single words. In addition, we can generalize the changes of sentence score by polarity difference between predicates and adverbs.제 1 장 서 론 제 2 장 관 련 연 구 제 3 장 감성 문장 성분 조합 집합 제 4 장 문장 점수 및 영화 평점 추론 제 5 장 성능 및 평점 평가

본 논문에서는 한국어 텍스트를 기반으로 향상된 오피니언을 도출하기 위한 방법과 이를 빅데이터 환경에서 안정적으로 적용 및 관리하고, 예측할 수 있는 시스템 설계 방법론에 대해서 연구하였다. 한국어 기반의 오피니언 추출을 위한 향상된 감성사전을 구축하기 위해서 반의어와 접속어 기반의 역접관계를 분석하고, 어휘에 연관된 단어의 패턴을 탐색하여, 두 가지 부류의 감성사전을 제시하였으며, 이를 주식 온라인 뉴스 콘텐츠를 테마로 선정하여 빅데이터에서 관리 및 예측 분석이 가능한 한국어 문법 기반의 오피니언 반의어 감성 규칙 기법 모델을 구축하였다.
오피니언의 정확도는 감성사전의 구축을 통하여 크게 차이를 보일 수 있으며, 해외의 다양한 감성분석과 데이터마이닝은 영어 문법 기반의 텍스트마이닝을 위주로 분석을 수행하고 있다. 이에 따라 한국어 문법에서는 영어문법과 비교하여 다양한 접속어 어휘들이 등장하고, 이로 인하여 문장 전체가 긍정적인 구조일지라도 부정 어휘의 출현율에 의해서 상반된 결과를 도출하기도 한다.
본 논문에서는 오피니언마인닝을 빅데이터에 적용할 경우 안정성 있는 환경과 예측 및 분석의 정확도를 높이기 위해서 문장 전체의 연관된 단어의 출현 빈도와 패턴 분석, 반의법을 고려한 형태소 분석, 접속어 중심의 역접관계를 고려한 OASR기법을 제안하여 주식관련 온라인 뉴스를 분석하였으며, 기존의 감성사전에 비해서 오피니언의 도출 및 정확성이 향상된 결과를 얻었다.
본 논문을 통해서 한국어 문법의 향상된 감성분석과 텍스트 처리 및 관리하는 빅데이터의 활용에 있어서 보다 정확한 예측 및 분석이 가능할 것으로 기대된다.국문 초록 ⅰ 목 차 ⅱ 표 목차 ⅵ 그림 목차 ⅸ

본 연구에서는 이미지 인식과 자연어 처리 등 다양한 분야의 데이터 분석에서 뛰어난 성능을 보여주는 심층신경망(Deep Neural Network) 모형과 P2P대출 데이터를 이용하여 AS(Application Score) 모형을 개발하고 판별이 어려운 그레이 존(Gray Zone)에 속하는 차주들의 판별 성능을 높이기 위하여 확률에 기반한 부스팅 기법을 제안한다.
최근 국내 P2P(Peer to Peer) 대출 시장의 누적 대출액은 2016년 9월 2,916억 원에서 2017년 9월 1조 4,730억 원 규모로 1년 사이 약 5배 가량 급격하게 증가하고 있다. 자금이 필요한 차주에게 다수의 개인이 온라인과 오프라인 P2P대출 플랫폼을 통해 대출을 실행하고, 추후 차주로부터 일정기간 원리금을 상환받는 방식으로 운영된다. P2P 대출은 제도권 금융기관에서 대출을 받기 어려운, 즉 기존 신용평가체계에서 그레이 존(Gray Zone)에 속하는 중신용자 및 씬 파일러(Thin Filer)에게 적절한 대안금융이 되고 있다.
신용평가시스템(Credit Scoring System)에서 그레이 존(Gray Zone)이란 우량과 불량 어느 곳에도 속하지 않으며, 구분하기 모호한 특징을 가진 신용등급 구간을 의미한다. 이 신용등급 구간에 속하는 차주들은 모형으로부터 우량, 불량을 판단하기 어려워 제도권 금융기관에서는 일반적으로 대출이 거절된다.
하지만 최근 상승하는 누적 대출액과 더불어 P2P 대출의 연체율이 2017년 7월 0.54%에서 2017년 10월 6.01%, 부실율은 2017년 7월 0.32%에서 2017년 10월 1.12%로 상승하는 경향을 보이고 있다. 이는 상당수의 P2P 이용자들이 판별이 어려운 중신용자 혹은 저신용자이기 때문으로 볼 수 있으며, 리스크관리에 부담이 되고 있다. 금융상품으로 분류되는 P2P 대출은 부도 발생 시 그 피해가 고스란히 투자자의 몫으로 이어지므로 투자자의 보호와 P2P 대출의 효과적인 리스크관리를 위해선 중신용 구간에 특화된 신용평가모형이 필요하지만 아직 관련 연구는 미비한 실정이다.
본 연구에서 제안하는 확률기반 부스팅은 모형에서 계산되는 판별확률의 높고 낮음에 따라 부스팅에 사용될 데이터를 선정, 학습하여 모형의 성능을 강화에 목적이 있다. 이는 그레이 존에 속하는 차주들의 분류율을 높이고 리스크 부담을 줄이기 위해 활용될 수 있으며, 실제로 전체 분류성능의 향상이 있음을 확인하였다.
데이터는 미국의 P2P 대출 업체인 렌딩클럽(Lending Club)에서 제공하는 2014년도와 2015년도의 고객정보 데이터를 이용하였다. 실증분석 결과 전통적인 이진분류 모형인 로지스틱회귀분석과 기본적인 심층 신경망 모형과 비교하여 분류성능이 향상됨을 확인할 수 있었고, 또한 모형에서 그레이존에 속하는 집단의 차주들의 특성을 확인하였다.

최근 패턴 인식, 번역, 자연어 처리, 영상 및 음성 인식 등 다양한 분야에 인공지능이 적용되면서 인공지능을 가능하게 한 딥러닝 기술에 모든 관심이 집중되고 있다. 특히, 딥러닝 중에서도 가장 대표적인 알고리즘으로 이미지 인식 및 분류에 강점을 가지고 있고 각 분야에 가장 많이 쓰이고 있는 CNN(Convolution Neural Network)에 대한 많은 연구들이 진행되고 있다.
과거에 간단한 요구만을 처리하는 기능을 한 CNN 모델은 비교적 단순한 구조를 가졌다. 하지만 현재는 과거보다 더 정교하고 난해한 요구를 처리하기 위하여 훨씬 더 복잡한 CNN 모델로 변화해오고 추세이다. 현재 CNN 모델이 복잡해짐에 따라 연산 시간이 많이 소요되는 문제가 제기되고 있다.
이와 같은 문제를 보완하기 위하여 소프트웨어 측면으로 많은 고속화 알고리즘 방안들이 연구되고 있는데, CNN 모델에서 가장 많은 소요시간을 차지하는 Convolution을 고속화하는 방안으로 FFT 기반 Convolution, 슈트라센 알고리즘, Weight Sharing 등의 다양한 고속화 알고리즘 연구들이 있다. 특히, 중복되는 계산을 미리 해두어 사용될 때마다 계산하지 않고 저장된 값을 가져다 쓰며 계산식을 줄여 고속화하는 행렬 곱 알고리즘인 Winograd 고속화 알고리즘에 대한 연구가 진행되어왔다.
모든 딥러닝은 기능적인 측면에서 크게 학습(Learning)+인퍼런스(Inference)를 같이 하는 것과 인퍼런스만을 하는 것으로 나눌 수 있다. 본 논문에서는 주로 인퍼런스만을 하는 반도체 검사 및 계측 장비, surveillance등의 하드웨어를 고속화 하기 위한 사전 작업으로 소프트웨어 기반으로 고속화를 검증하고자 했다. 특정 CNN 신경망에 대해서 Winograd 고속화 알고리즘을 이용하여 Convolution Layers부분을 고속화하고, 프로그래밍 고속화 기법인 SIMD 및 OpenMP를 사용하여 Pooling Layers과 Fully Connected Layers를 고속화 했다. 또한, 고속화에 대한 검증으로 가장 대표적인 딥러닝 프레임워크인 Caffe와 Feed Forward 소요시간을 비교 분석했다.
실험은 대표적인 딥러닝 모델로 알려진 VGG16과 ALEXNET, 그리고 반도체 계측에 특화된 비교적 간단한 모델인 TSOM 모델로 실험했다. 실험결과는 VGG16의 경우 약 26%, ALEXNET의 경우 약 19% 속도 향상이 있었고, TSOM 모델의 경우 약 6%의 속도 향상이 있는 것을 확인했다.
본 논문은 인퍼런스 기능만을 하는 반도체 검사 및 계측 장비, surveillance 등의 하드웨어에 직접 적용하기 위하여 CPU 환경에서 C언어로 고속화를 했다. 이에 따라 본 논문의 결과는 추 후 하드웨어 고속화 측면에서 기여하는 바가 클 것이라고 기대된다.In recent years, artificial intelligence has been applied to various fields such as pattern recognition, translation, natural language processing, video and speech recognition, and attention has been focused on deep learning technology that enables artificial intelligence. Especially, many researches on Convolution Neural Network (CNN), which is one of the most representative algorithms among deep running, has strong advantages in image recognition and classification and is widely used in various fields.
In the past, the CNN model, which functions only to handle simple requests, had a relatively simple structure. Today, however, it is becoming a more complex CNN model to handle more sophisticated and more demanding requests than in the past. As the CNN model becomes complicated, it takes a lot of computation time.
In order to overcome this problem, many high-speed algorithms have been studied in terms of software. In order to speed up the convergence which takes the most time in the CNN model, various high-speed algorithms such as FFT-based convolution, . In particular, research has been conducted on the Winograd acceleration algorithm, which is a matrix multiplication algorithm that reduces the number of calculations and speeds up the calculation by storing the values instead of calculating the duplicate calculations beforehand.
All deep learning can be divided into learning and referencing functions in terms of functionality and only referencing. In this paper, we tried to verify the speedup by using software as a preliminary task to speed up hardware such as semiconductor inspection, measurement equipment, surveillance, etc., which mainly only referenced. We accelerated Convolution Layers part by using Winograd acceleration algorithm for specific CNN neural network, and accelerated Pooling Layers and Fully Connected Layers by using SIMD and OpenMP programming speeding techniques. In addition, we compared the time required for Caffe and Feed Forward, which are the most representative deep-running frameworks for verification of high-speed operation.
Experiments were conducted on VGG16 and ALEXNET, which are known as typical deep-run models, and TSOM model, which is a relatively simple model specially designed for semiconductor measurement. Experimental results show that the speed of VGG16 is about 26%, that of ALEXNET is about 19%, and that of TSOM model is about 6%.
In this paper, we have implemented C language acceleration in CPU environment to apply directly to hardware such as semiconductor inspection, measurement equipment, surveillance, etc. Therefore, it is expected that the result of this paper will contribute greatly in terms of hardware acceleration.제 1 장 서 론 1.1. 연구배경 1 1.2. 관련 연구동향 2 1.3. 연구목적 2 1.4. 논문의 구성 4

구문구조 부착 말뭉치는 자연어처리나 언어학의 여러 분야에서 유용하게 쓰이는 언어자원이다. 그러나 말뭉치 구축은 많은 시간과 인력을 필요로 하는 작업이기 때문에 정확한 정보를 갖는 대량의 말뭉치를 구축하는 것은 어려운 작업이다. 그러므로 구문구조 부착 말뭉치 구축을 편리하게 할 수 있는 도구가 필요하다.
본 논문에서는 ‘자동 구문분석기를 단계적으로 이용한 구문구조 부착 말뭉치 구축도구’를 제안한다. 제안하는 도구는 구문구조 부착 말뭉치 구축 시 효율적인 작업이 가능하게 하고 사용자의 수작업을 감소시키는 것을 목적으로 하였다. 자동 구문분석기를 단계적으로 이용한다는 것은 구문구조 생성 시 입력 문장을 부분으로 분할 한 후 각 부분에 대한 구문구조를 먼저 생성하고 부분 구문구조의 정보를 통합하여 완전한 구조를 완성하는 것을 말한다. 입력 문장을 부분으로 분할하여 각 부분에 대한 구문구조를 생성 하는 것이 첫 번째 단계, 각 부분을 통합하여 완전한 구문구조를 완성하는 것이 두 번째 단계이다.
제안하는 도구의 입력은 품사 부착 문장이고 출력은 이진 구 구조 형식으로 구문구조가 부착된 문장이다. 문장이 입력되면 ‘문장 분할 단계’에서 분할 기준에 따라 문장을 부분으로 분할한다. 분할 기준들은 학습 말뭉치에서 자동으로 추출하였다. 부분의 경계가 확정되면 다음 단계인 부분 구조 생성 단계로 진행한다. ‘부분 구조 생성 단계’에서는 각 부분의 구문구조를 확정한다. 마지막 단계인 ‘부분 통합 단계’에서는 각 부분 구문구조를 통합하여 완전한 구조를 생성한다. 본 논문에서는 제안하는 도구를 이용하여 자동 구문분석기의 구문분석 정확률을 약 2% 향상시켰으며 구문구조 부착에 필요한 작업자의 수작업을 약 35% 감소 시켰다.작업자의 수작업을 약 35% 감소 시켰다. 제목 차례 제 1장 서론 1.1연구의 필요성 1.2연구의 목적

형태소 분석과 의존 파싱은 자연어 처리 분야에서 핵심적인 역할을 수행하고 있기 때문에 형태소를 분류하고 문장의 구조를 올바르게 분석하는 것은 매우 중요하다.
형태소 분석은 입력된 문장 내의 어절을 뜻을 지니는 최소의 단위인 형태소들로 분리하고 품사 태그를 부착하는 작업을 의미한다. 기존의 형태소 분석은 [B, I]등의 태그를 포함된 품사를 음절 단위로 결정하는 방식으로 주로 연구되었으며 음절 기반 방법은 원형 복원이라는 후처리 단계가 필요한데 후처리 방법으로는 학습데이터에 나타난 기분석 사전을 활용하는 방법 등이 주로 사용되었다.
한국어 의존 파싱에 대한 연구는 형태소 분석 모델로부터 형태소 분석 결과를 얻고 형태소 분석 결과를 입력 자질로 사용하여 구문 분석 모델에 적용하여 구문 분석 결과를 얻는 파이프 라인 방식으로 진행하는데 이러한 파이프 라인 방식은 형태소 분석 단계에서 발생한 에러가 의존 파싱 성능에 영향을 좌우하는 Error Propagation(에러 전파) 문제가 발생한다.
본 연구에서는 전이 기반 방식으로 형태소 분석을 수행하는 전이 기반 형태소 분석 모델을 제안하고 Sequence-to-Sequence 모델과 결합하여 기분석 사전을 이용한 후처리 과정을 필요로 하지 않는 복합 형태소에서 단위 형태소로의 원형 복원을 일괄적으로 학습하는 End-to-End 모델을 제안한다. 이에 나아가 본 논문에서는 의존 파싱에서 형태소 분석에 대한 전이 액션을 포함하도록 전이 액션을 확장하여 한국어 형태소 분석 & 의존파싱에 대한 통합모델을 제안한다.Since morphological analysis and dependency parsing play a key role in natural language processing, it is very important to classify morphemes and correctly analyze the structure of sentences.
Morphological analysis is the task of separating the word into morphemes, which is the smallest unit of meaning in the input sentence, and attaching the POS tag.
The existing morphological analysis was mainly studied in the way of determining the parts of speech including the tags [B, I] of syllable unit. the syllable-based method requires a post-processing step called Morpheme Recovery. As a post-processing method, a method using pre-analyzed dictionary is mainly used.
The existing Korean dependency parsing was performed a pipeline method that obtains the morphological analysis result from the morpheme analysis model and applies the morpheme analysis result to the dependency parsing model to obtain the result of the syntax analysis.
In this study, we propose a transition-based morphological analysis model that performs morphological analysis with a transition-based method and propose an end-to-end model that simultaneously learns the transformation from a compound morpheme to a atomic morpheme in combination with a Sequence-to-Sequence model.
In addition, this paper proposes an joint model for Korean morphological analysis and dependency parsing by extending the transition action to include the transition action for morphological analysis in dependency parsing.제1장 서 론 1 제1절 연구배경 및 목적 1 1.1.1 연구배경 1 1.1.2 연구목적 2

딥러닝 기법을 이용한 자연어 처리에서는 일반적으로 단어를 연속적인 벡터로 표현하고 단어 벡터를 딥러닝 모델의 입력으로 사용한다. 하지만 교착어인 한국어의 특성상 명사, 신조어, 외래어 등의 모든 단어를 사전에 등록하고 관리하는 것에 어려움이 발생한다. 따라서 등록되지 않은 단어를 연속적인 벡터 공간에 표현해야하는 문제가 발생한다. 또한, 한국어는 음절의 조합으로 단어를 표현한다. 그리고 한국어 시퀀스 태깅 문제에는 단어 기반으로 하는 것 외에도 음절 단위로 태그를 부착하는 자동 띄어쓰기, 형태소 분석 등의 문제가 존재한다. 본 논문에서는 명사, 신조어, 외래어 등과 같은 사전의 미등록 단어 처리에 유연한 음절을 연속적인 벡터 공간에 표현하고, 단방향 LSTM과 양방향 LSTM을 이용하여 다양한 문맥 음절 n-gram 임베딩 방법론을 제안한다. 다양한 임베딩 방법론의 성능 평가를 위해 한국어 자연어 처리의 기본적인 음절 시퀀스 태깅 문제인 자동 띄어쓰기에 적용한다. 그리고 입력 문장의 최적의 띄어쓰기 태그 열 예측을 위해 전방향 신경망과 선형 체인 CRF를 단방향 LSTM과 양방향 LSTM을 이용한 다양한 문맥 음절 n-gram 임베딩과 종단간 신경망 모델로 구성하여 성능을 평가하였다.In the natural language processing using the deep learning technique, words are generally represented as continuous vectors and word vectors are used as inputs to the deep learning model. However, due to the nature of the Korean language such as an agglutinative language, it is difficult to manage all vocabularies such as nouns, coined words, and foreign words in a dictionary. Therefore, there is a problem dealing with words not included in a dictionary. Also, the Korean word is represented by a combination of syllables. In addition to word-based sequence tagging problems in Korean, there are syllable-based sequence tagging problems such as automatic word spacing and morphology analysis. In this paper, a variety of context syllable n-gram embedding methodologies are proposed, which is flexible to deal with unregistered words such as nouns, coined words, and foreign words, by representing syllables in a continuous vector space and using unidirectional and bidirectional LSTM. To evaluate the performance of various embedding methodologies, it is applied to automatic word spacing task in Korean, which is the basic syllable sequence tagging problem of Korean natural language processing. In order to predict the optimal tag sequence of the input sentence, a feedforward neural network and the linear chain CRF are utilized. Both of them and the various context syllable n-gram embedding is constructed as end-to-end neural network model and the performance is evaluated.그림 차례 iii 표 차례 iv 국문요약 v 제 1장 서론 1 제 2장 관련 연구 2

1. 서론 1 1.1. 연구 배경 1 1.2. 선행 연구 및 한계점 5 1.3. 문제 해결 방안 6 1.4. 논문 구성 8

대역어 중의성 해소는 자연어 처리 분야의 어려운 문제 중 하나이다. 이 논문은 대역어 중의성 해소를 위한 정보검색 기반의 접근법을 제시하고 이를 교차언어 링크 결정 문제에 적용하였다. 교차언어 링크 결정(Cross-lingual Link Discovery)은 한 언어로 작성된 원시 문서를 입력으로 받아 해당 문서에 출현하는 링크 대상 키워드 각각에 대해 영어 위키피디아 등의 다른 언어로 작성된 외부 지식 자원으로의 자동 링킹을 수행하는 작업이다. 이 연구에서는 필리핀어인 Tagalog로 작성된 원시 문서 내의 링크 대상 용어들을 영어 위키피디아 페이지로 교차 링킹하는 작업의 핵심 모듈로 Tagalog-to-English 대역어 중의성 해소 기법을 제안한다. 원시 문서 내 Tagalog 용어의 올바른 영어 대역어를 결정하기 위해, 정보검색을 통해 얻어진 원시 Tagalog 문서와 유사한 토픽의 영어 위키피디아 문서 집합 내의 문맥 정보를 활용한다. 원시 Tagalog 용어의 영어 대역어들 간의 문맥 정보를 대표하는 자질로서, 임의의 Tagalog 대역어쌍에 대한 Topically Related Co-occurrences, Topical Distance, General Co-occurrences와, 임의의 Tagalog 대역어에 대한 Topical Salience, Term Popularity를 사용하였다. 제안한 방법은 93.7%의 대역어 중의성 해소 성능을 보였으며, Tagalog-to-English 교차언어 링크결정에서 36%의 F1 성능을 보였다.Translation disambiguation has been abundantly tackled in the world of natural language processing. Correctly identifying the exact sense between ambiguous translations had been a delicate task for many researchers. This thesis introduces an IR-based approach which can help improve the efficiency of any translation disambiguation related task. For this research, such new approach was incorporated in a cross-lingual link discovery system for which given a source document, it automatically picks out significant words or phrases within the context and links them to a target document in a different language. We had used Tagalog as our source language and English as the target language utilizing the Wikipedia corpus as the document collection for both languages. Our approach presumes to the notion that a collection of keywords perceives a common topic and such topic can be a key in determining the correct translation of a word. Using an IR system, we are able to gather related document of our source topic and this set documents can help us identify relationships between translations. Using these relationships in disambiguating ambigous translations basically means that for a term that has a two or more translation, a translation that are more related to all the other translations within our topic is most likely be the right translation. Our approach tries to calculate these translation relationships using five different features in order to identify the correct translation of a term with multiple translations.I. Introduction 1 II. Related Works 4 III. Objectives 8 IV. CLLD Architecture 10 1. N-gram generation and Translation 10

최근, 신경망 기반 자연어처리 기술들은 고정된 길이의 단어 임베딩(word embedding)을 전제로 한다. 단어 임베딩은 특정 위치의 단어를 그 주변 단어의 분포의 유사성을 보존하면서, 고정길이의 저차원 밀집 벡터(dense vector)로 수치화 하는 방법이다.
현재의 한국어 단어 임베딩 방법은 해외에서 연구한 방법들을 사용하고 있다. 그러나, 기존의 단어 임베딩 방법들은 원래 영어를 분석하는 목적으로 개발되었기 때문에 한국어의 어순과 구조를 반영하지 못하는 문제가 있다. 또한 한국어는 영어와 다르게 문장을 띄어쓰기로 분리하여 단어를 추출할 수 없다. 따라서, 단어 임베딩 방법을 사용하기 위해서는 어절을 다시 형태소로 분리하는 추가적인 전처리 기술이 요구된다.
본 논문에서는 한국어 단어 임베딩 과정에서 한국어의 어순과 구조를 반영하는 두 가지 방법을 제안한다. 첫 번째는, 기존의 단어 임베딩 방법인 Skip-gram을 한국어를 위해 개선한 Skip-gram-KR 방법이고, 두 번째는, Skip-gram-KR을 위한 새로운 단어 추출 방법인 한국어 접사 토크나이저(KR-affix tokenizer)이다.
Skip-gram-KR은 한국어 어순과 구조를 반영하기 위해서 Skip-gram의 학습 데이터 생성 알고리즘을 개선한 방법이다. 기존의 Skip-gram은 유사 단어의 학습 데이터를 생성할 때, 현재 위치의 단어와 현재 위치 보다 전, 후에 위치한 단어들을 유사한 단어로 매핑하고 있다. 이와 다르게, Skip-gram-KR은 3장의 정의 1에서 소개하는 역방향 매핑 방법과 정의 2에서 소개하는 두 단어 건너뛰기 방법을 사용하여 유사 단어를 매핑한다. 역방향 매핑 방법은 현재 위치의 단어와 현재 위치 보다 이전에 위치한 단어만을 매핑하는 방법이고, 두 단어 건너뛰기 방법은 어근과 접사를 분리하여 유사한 단어로 매핑하는 방법이다. 이 방법은 문맥은 유사하지만 의미적인 연관성이 없는 접사와 어근이 유사한 단어로 매핑되는 현상을 방지한다.
한국어 접사 토크나이저(KR-affix tokenizer)는 Skip-gram-KR의 두 단어 건너뛰기 방법을 위한 단어 추출 방법이다. 먼저, 사전과 전체 문서에서 단어의 통계 정보를 활용하여 접사를 판별한다. 그 후, 판별한 접사를 기준으로 어근과 접사로 분리한다. 접사가 없는 독립적인 단어일 경우에는 접사의 위치에 패딩코드 P를 출력하여 어근-접사 구조를 유지한다. 제안하는 방법은 기존의 형태소 기반 토크나이징과 비교하여, 단어의 원래의 의미를 최대한 보존하는 장점이 있다. 특히, 신조어, 미등록어에서 발생할 수 있는 토크나이징 오류를 개선하고, 딥러닝 모델에 정확한 단어를 전달한다.
한국어 접사 토크나이저는 형태소 분석기와 비교하는 문장 토크나이징 실험에서 가장 우수한 성능을 보였다. 랜덤 추출한 100개의 문장 중 67개의 문장을 오류없이 토크나이징 하였다. 단어 임베딩 정확도 실험에서는, 한국어 접사 토크나이저와 Skip-gram-KR을 사용한 모델이 가장 낮은 평균 오차율을 보였다. 기존 모델 대비 14.3% 오차율의 감소함을 보였고, 한국어 접사 토크나이저의 사용만으로는 3.1% 오차율이 감소한다는 것을 실험으로 보였다. 또한 임베딩 된 단어 벡터와 코사인 유사도(cosine similarity)가 높은 단어를 10개씩 추출하여, 의미적으로 관련성 있는 단어들이 군집화 되었음을 보였다. 이를 정량적 수치로 표현하기 위해 유사도 순위에 가중치를 부여하는 한국어 의미 유사도 평가지표(Korean semantic top-k score, KST)을 4장의 정의 8에서 처음 제안하였다. KST를 사용하여 임베딩 된 단어의 의미 군집화를 평가한 결과, 제안하는 모델이 10개 단어 중 평균 6.7개, KST 5.5점 중 4.05점으로 가장 정확한 의미 군집화를 보였다.Ⅰ. 서론 = 1 Ⅱ. 관련연구 = 6 2.1. 단어 임베딩 방법 = 6 2.2. 단어의 경계를 찾는 방법 = 11 Ⅲ. 제안하는 방법 = 14

차례 = i 그림 차례 = ii 표 차례 = ii 국문요약 = iii 제1장 서론 = 1

The increases in the numbers of smart devices and mobile users have made it easier to access the Internet anytime and anywhere. Nowadays, Internet users not only read websites but also respond to web content by leaving a comment. Often, user comments become another type of web content and enrich the content as a whole. For instance, there are many comments on SNSs, web community sites, or news services. However, not all comments are always beneﬁcial to Internet users. We can categorize such comments as follows: 1) benevolent comments, 2) neutral comments, and 3) malicious comments. We call malicious comments abusive texts. Abusive texts (indiscriminate slang, abusive language, and profanity) on the Internet are not just a message but rather a tool for very serious and brutal cyber violence. Therefore, it has become an important problem to devise a method for detecting and preventing abusive texts online. However, the intentional obfuscation of words and phrases makes this task very diﬃcult and challenging.
We design a system that successfully detects (obfuscated) abusive texts. Our system is based on an unsupervised learning of abusive texts based on word embedding using word2vec’s skip-gram and the cosine similarity. The system also deploys several eﬃcient gadgets for ﬁltering abusive texts such as blacklists, n-grams, edit-distance metrics, mixed languages, abbreviations, punctuation, and words with special characters as a way to detect the intentional obfuscation of words. We integrate both an unsupervised learning method and eﬃcient gadgets into a single system that enhances abusive and non-abusive word lists. The integrated detection system based on the enhanced word lists shows a precision of 86.24%, recall of 85.29%, and an f-score of 85.76% in malicious word detection for news article comments, and a precision of 86.97%, recall of 82.47%, and an f-score of 84.66% for online community comments. We expect that our approach can help to improve the current abusive word detection system, which is crucial for several web-based services including social networking services and online games.비윤리와 정상단어 리스트 확장 및 정제 기반의 비윤리 문서
탐지 시스템
현재 인터넷은 IoT(internet of things), CoT(Cloud of Things), 유비쿼터스 등
모든 전자제품이나 사물들이 보이지 않는 거미줄처럼 우리들 곁에 모두 연결되어
존재하고 있다. 또한 스마트폰과 채팅, 소셜미디어의 발달로 사람과 사람 사이의
소통이 시간, 공간을 초월하여 지금 이 순간에도 언제 어디서든지 이루어지고
있을 것이다. 이런 편리함의 음영으로 인터넷은 어떤 환경보다도 빠르게 비속어 및
욕설이 생성되고, 쓰이고, 전파된다. 이로 인해 과거에 청소년을 대상으로 발생하여
주목 받았던 사이버 폭력이 연령에 상관없이 발생하고 있으며 이에 대해 “게임산
업진흥원”의 권고로 게임 업체는 1000개 안팎의 금칙어를 설정하고 욕설의 사전
차단을 하거나 “한국콘텐츠진흥원”에서 2009년에 “게임언어 건전화 지침서 연구”
를 통해 금칙어 리스트를 배포하는 등의 대처방안이 마련되고 있다. 하지만 영어나
기타 다른 언어와는 다른 한국어의 특성을 이용하여 많은 사용자들이 자음과 모음
변이를 통해 같은 뜻을 지닌 단어의 변형 및 특수문자 사용과 띄어쓰기, 구두점 등을
사용하여 교묘히 피하는 방법으로 비속어 및 욕설을 사용하고 있어 이를 비윤리적
단어로 탐지하기에 기존의 탐지 방법인 정규표현식 방법과 블랙리스트 기반 탐지
만으로는 무리가 있다. 따라서 본 연구는 대표적인 소셜미디어 중 하나인 트위터와
네이버 뉴스의 정치, 경제 기사의 댓글을 크롤링하여 대량의 말뭉치를 제작한다.
비속어 및 욕설 사전으로는 국립국어원 비속어, 게임 금칙어, 온라인 사이트 금지어,
비속어 관련 논문 발췌 비속어, Google Bad Word 외국어 비속어로 제작되었다.
탐지 방법으로는 word2vec의 skip-gram모델을 사용한 word embedding과 cosine
similarity를 결합한 비지도 학습 탐지, Doc2vec. 그리고 blacklists, n-grams, editdistance
metrics, mixed languages, abbreviations, punctuation, and words with
special characters 의 여러 가젯들을 이용했다.

As Korea becomes more of a global powerhouse economically and technologically, gaining insight from information written in the Korean language is becoming more and more relevant. Korean users’ frequent use of the web both at home and on the go has resulted in a plethora of parseable Korean text, which could be further analyzed using sentiment analysis or other natural language processing (NLP) techniques for various
applications. However, the Korean language's wide range of characters, ambiguity, sensitivity to sentence-level context, and agglutinative nature pose several challenges for morphological analysis. Previous approaches have used a dictionary of known morphemes and tags ("actions"), or have split characters into sub-characters called graphemes. These models have been created with the assumption that character-level, dictionary-less
morphological analysis was intractable due to the number of actions required. In this thesis, we present a multi-stage sequence tagging model that can perform morphological transformation and part-of-speech tagging using arbitrary units of input and apply it to the case of character-level Korean morphological analysis. Among models that do not employ prior linguistic knowledge, we achieve state-of-the-art word, sentence-level, and out-of-vocabulary (OOV) tagging accuracy with the Sejong Korean corpus using our
proposed data-driven Bi-LSTM model.1. Introduction 1 1.1 Motivation 1 1.2 Contributions 2 1.3 Korean Language 2 1.4 Thesis Outline 4

새로운 컴퓨팅 환경과 이를 기반으로 다양한 분야에서 연구 활동이 진행됨에 따라 연구자들의 연구 역량을 분석하고 이를 확인할 수 있는 시스템들이 출현하였다. Google사에서 서비스 중인 Google Scholar는 연구자들의 개인 연구 역량 지표 및 개인의 결과물을 한번에 확인할 수 있도록 개인 프로필을 자동으로 수집 저장한다. Elsevier사에서 서비스 중인 Scopus는 해당 기관에서 출판하는 논문, 도서, 저널등을 수집하여 정보를 저장하고, 이를 바탕으로 다양한 정보를 제공한다. 이처럼 연구자들에게 제공하고자 하는 연구 역량 정보 제공 및 분석 서비스는 날로 발전을 거듭하고 있으나, 사용자들이 필요한 정보를 쉽게 찾는 방법을 효율적으로 제공하지 못하고 있다. 특히, 기존의 시스템들은 먼저 연구분야를 기반으로 분류하고 있으며, 하나의 연구분야 카테고리 내 한 개에서부터 많게는 300가지가 넘는 학술지를 포함하고 있다. 이로 인해 각각의 학술지는 한 개의 카테고리에 분류되지 않고 다수의 카테고리에 포함되어 있다는 부분까지 확인 할 수 있었다. 이와 같이, 수집 및 분류된 학술지들은 각각의 정보(인용 수, 피인용 수 등)를 바탕으로 다양한 지표로 환산하여 사용자에게 정보를 제공하지만, 이러한 연구 역량 정보 제공은 대부분 사용되는 범위가 사용자 개인이 아닌 학술지에 대한 평가를 위해 사용되기 때문에 사용자가 사용하기에는 많은 어려움이 발생하고, 이를 사용할 때 전반적인 시스템을 이해하지 않고 사용한다면 충분한 정보를 얻기가 어렵다. 또한 사용자의 연구분야를 확인하고, 연구 역량을 발전시키기 위한 정보를 확인하는 것도 현재까지 서비스 되고 있는 연구 역량 분석 서비스에서는 찾아보기 힘들다.
이러한 연구자들의 연구 분야를 재확인하고 이를 기반으로 한 연구 역량을 발전시키기 위해 학술지에 대한 정보를 수집하여 분석하였으며, 다양하게 분류되어 있는 각각의 카테고리 정보도 수집을 할 수 있었다. 이 과정에서 학술지 평가 지표 중 일반적으로 사용되는 Impact Factor와 Eigenvector Score를 통해 학술지 간 관계를 표현할 수 있었으나, 카테고리 간의 관계를 찾는데 기준 및 관계 선정이 모호함에 따라 문제를 해결하기 위한 다양한 연구가 진행되었던 사실을 확인하고 본 연구를 통해 방법을 제안하고자 한다.
따라서, 본 논문에서는 자신의 연구 결과물을 바탕으로 타 학술지와의 연관정보를 통해 자신의 연구 분야를 쉽게 확인하고, 이와 관련한 적절한 학술지를 추천해 줌으로써 연구자들의 연구 능력을 고취시키고 연구자의 연구 분야를 명확히 판단할 수 있는 카테고리 정보를 제공하고자 한다. 이를 위해 기존 학술 정보 서비스에서 제공하는 카테고리 정보를 바탕으로 카테고리 간의 유사도 측정 방법을 제안하며, 두 개 이상의 카테고리에 포함되어 있는 학술지는 각각의 카테고리 내 포함된 정보를 바탕으로 학술지 간의 거리를 측정하고 본 논문에서 제안하는 카테고리 간 유사도 측정 방법을 적용한 후 총 5개의 최적합 학술지를 추천하였다.
그 결과 한 카테고리 내에서 학술지를 추천했을 때보다 더 넓은 범위의 학술지 추천이 가능하였다. 또한, 카테고리 간 유사도 측정 방법에 대한 명확한 근거를 제시함으로써 유사도 측정을 위한 과정과 관련한 모호성을 해소할 수 있었다. 이를 바탕으로 학술지 추천뿐만 아니라 사용자 맞춤형 서비스 추천 분야에도 확장 적용할 수 있는 방법을 제안했다는 점에서 본 연구의 가치를 확인할 수 있었다.With research activities based on the new computing environment in diverse areas, the systems for analyzing and verifying the researchers’ research competencies have appeared. Google Scholar provided by Google automatically collects/stores personal profiles to verify each researcher’s research competency index and individual outcomes at a time. Scopus provided by Elsevier provides diverse information by collecting theses, books, and journals published by relevant institutes, and then storing information. Even though the services for the provision/analysis of research competency information for researchers have been repetitively and continuously advanced, the methods for users to easily find necessary information are not efficiently provided.
Especially, the existing systems perform the classification based on the preceding research area, and a single research area category includes one or as many as 300 journals. Thus, each journal was not classified into a single category, but included in multiple categories. Like this, the collected/classified journals provide information to users through diversely-converted indexes based on each information(the number of quotation, the number of being quoted, and etc). As this provision of research competency information is mostly used for the evaluation of journals, instead of individuals, however, there are lots of difficulties for users to use it. If the overall system is not fully understood, it is hard to get sufficient information from it. In the currently-provided research competency analysis service, it is also hard to verify the research area of users and the information for the development of research competencies. In order to re-verify the research area of researchers and also to develop the research competencies based on it, the information about journals was collected and analyzed, and the information of each category diversely classified was also collected.
In this process, the relationship between journals could be expressed through the impact factors and eigenvector scores generally used out of the journal evaluation indexes. However, there were lots of difficulties to find relationships between categories, so that this thesis aims to suggest a method to solve this. Therefore, this thesis aims to provide the information of category that could improve the researchers’ research abilities and clearly show their research areas, by easily showing their research areas through information related to other journals based on their own research outcomes, and also recommending the relevant proper journals.
For this, the measurement of similarity between categories based on the category information provided by the existing academic information service was suggested. In case of the journals included in two categories or more, the distance between journals was measured based on the information included in each category, and then the measurement of similarity between categories suggested by this thesis was applied. And, total five optimum journals were recommended.
In the results, it became possible to recommend journals in wider scope than the recommendation of journals within a category. Also, by suggesting the clear grounds for the method of measuring the similarity between categories, the ambiguity related to the process for the measurement of similarity could be resolved. It is meaningful in the aspect of suggesting the method that could be expansively applied to the user customized-service recommendation area, on top of recommendation of journals.Ⅰ. 서론 1 A. 연구 배경 및 목적 1 B. 연구 내용 2 C. 논문 구성 4

디지털 시대에 문화 콘텐츠 산업이 가져오는 경제적 이익에 주목하여 국내외적으로 애니메이션 산업 발전에 대한 관심도 다각적인 측면에서 급속히 높아지고 있다. 특히 관련 기술의 발달과 더불어 컴퓨터 그래픽 애니메이션의 중요성이 날로 부각되고 있다. 기타 문화기술 산업들에 비해 애니메이션 산업의 부가가치가 훨씬 높다는 점도 그 이유가 될 수 있겠지만 무엇보다는 동 산업 자체가 갖고 있는 기술적 난이도 및 복잡성 때문이다. 즉 높은 제작비용과 긴 시간의 제작시한이 필요할 뿐만 아니라 제작의 난이도 및 복잡성 때문에 장기간 전문 트레이닝을 받아온 전문가들만의 영역이 되고 있다는 문제점이 존재한다는 것이다. 이러한 점을 고려할 때 제작의 간편성과 접근성이 고려된 새로운 제작방식에 관한 연구가 필요하다.
따라서 본 연구는 기존의 시스템 중심적이고 기하학적 작업을 필요로 하는 애니메이션 툴과는 달리, 언어라는 사람의 기본 능력을 이용한 사용자 중심적 인터페이스 가능성에 관해 연구하고자 했다. 이 연구는 시각적 정보와 사용언어의 상호 변화를 고려하여 새로운 조작 방식을 디자인하는데 적용할 수 있도록 인터페이스 프레임워크를 제안하고, 또한 이를 실제 적용하여 하여 어플리케이션 구현 및 사용자 테스트를 통해 그 가능성을 검증하는 것에 목적을 두었다.
이러한 목적을 달성하기 위해 본 연구에서는 선행연구 및 문헌고찰을 통해 텍스트 기반 인터페이스 제안의 정당성을 살펴보고, 애니메이션 제작 방식에 대한 파악을 통하여 본 연구 제안이 애니메이션 제작에 있어서 적용 가능한 범위를 파악하였다. 또한 소프트웨어 평가를 위한 관련 연구를 분석을 통하여 본 연구의 사용자 테스트를 위한 평가 측정도구를 도출하였다. 다음으로 상술한 이론적 배경에 근거하여 두 단계의 실험을 거쳐 인터페이스 프레임워크 제안에 필요한 요소를 도출하고 도출한 요소에 근거하여 프레임워크 제안을 하였다. 제안된 프레임워크를 적용하여 소프트웨어 구현을 하였고 소프트웨어 사용자 테스트를 통하여 사용성, 제작시간, 전문가 평가 등 면으로부터 분석을 하여 제안된 텍스트 기반 인터페이스의 가능성에 대해 검증하였다.
이 연구의 결론은 다음과 같다. 본 연구에서 개발된 툴의 사용성 검증을 통하여 텍스트 기반 인터페이스가 애니메이션 제작 툴에 있어서 사용과, 제작 시간의 측면, 그리고 결과물의 측면으로 볼 때 높은 가능성을 가지고 있음을 알 수 있다. 따라서 이러한 식의 인터페이스는 난이도가 낮고 접근성이 높아지므로 기존의 인터페이스보다 더 사용자 중심으로 되어 있음을 알 수 있다. 본 연구는 텍스트 기반이라는 새로운 방식의 인터페이스를 제안했을 뿐만 아니라 실제 프로그램 구현을 통하여 그 가능성에 대해 검증을 하려고 하였는데 디자인 아이디어, 텍스트, 디자인 시각물 사이의 관계에 대한 실험에서 얻은 결론, 프로그램 구현을 위한 프레임워크 제안, 사용성에 대한 검증 등 자료들은 이후 사용자 중심적인 인터페이스 디자인 연구의 기초 자료로 될 것으로 기대된다.

The Ontology is the concept statement which is used to help the program and people to share the knowledge. And it presents the method to standardize the knowledge, so that provides the foundation for reasoning. The existing studies on building ontology have focused on the modeling of skima system. However, as the data capacity gets large, the technology of automatic instance extraction have become essential for ontology construction. Therefore, this study automatically extracts the individual information from the general text.
First, extract the information by noun-phase unit which the patterns of 53 phases were applied, and using the result; extract the information in the paragraph unit. And as for individual information extraction by paragraph unit, recover the original name of the individual before anything else, through individual cross-reference in the same document. Also, for information extraction by paragraph unit, construct the case frame which limits the object predicate, and expand information extraction with this caseframe.
The predicate conformity rate in the caseframe construction in 96% and the F-score of individual information extraction were shown as 85.8% in electronic newspaper, with 84.1%, 84.8% and 82.7% in Yahoo, Daum and Naver respectively.I. 서 론 II. 관련 연구 2.1. 자연어 처리 기반의 정보 추출 연구 2.2. IT-피플 이벤트 온톨로지 III. 온톨로지 인스턴스 생성을 위한 인물 정보 추출

최근 각광받고 있는 심층 신경망 기술은 자연어 처리, 컴퓨터 비전, 게임 등 다양한 분야에서 폭넓게 활용되고 있다. 특히 심층 신경망 기술을 적용한 모바일 애플리케이션 시장이 가파르게 성장함에 따라 말단 디바이스에서의 효율적인 심층 신경망 구현은 중대한 이슈이다. 기존 클라우드 기반의 심층 신경망 처리는 유지보수 비용, 네트워크 연결성, 사생활 침해 등의 문제점이 존재하며, 최근 하드웨어 성능과 심층 신경망 기술의 발전으로 말단 디바이스에서 일부 심층 신경망 작업을 처리코자 하는 분산처리 연구가 활발히 진행되고 있다. 분산처리에 관한 기존 연구들은 단일 디바이스 내 멀티 프로세서 활용, 작업량 분할할당, 알고리즘 간소화 등을 통한 추론 작업이 주요 관심사항이거나, 멀티 디바이스 간 네트워크 영향을 줄이기 위한 로컬 처리가 주요 관심사항이다. 하지만 이들은 말단 디바이스의 다양한 성능 및 이용 가능한 자원, 네트워크 환경 등을 반영하지 않거나 훈련 결과인 소요시간, 전력 및 에너지 소모, 에너지 효율 등을 고려하지 않는다.
따라서, 본 논문에서는 분산 클러스터에서 디바이스 이질성을 고려한 새로운 심층 신경망 훈련 프레임워크를 제안한다. 프레임워크는 훈련에 참가할 수 있는 각 디바이스로 사전훈련을 지시하여 이용 가능한 자원, 네트워크 대역폭 등을 수집한 후 성능, 전력 및 에너지 소모, 에너지 효율 측면에서 사용자 또는 서비스 공급자의 선호에 따라 최적의 디바이스 구성을 예측한다. 실험결과, 프레임워크를 통해 예측된 최적의 디바이스 구성은 76.67%의 확률로 성능, 전력 및 에너지 소모, 에너지효율 측면에서 실제 최적의 구성이었으며, 차선책을 포함할 경우 95.83%의 확률을 보였다. 이와 같이 사용자 또는 서비스 공급자는 프레임워크를 통해 높은 확률로 최적의 디바이스 클러스터를 구성하여 심층 신경망 훈련을 진행할 수 있다.1. Introduction 1 2. Related work 4 3. Background 6 3.1. Model parallelism and Data parallelism 6 3.2. Full mesh topology and Star topology 7

When a system is developed, requirements document is generated by requirement analysts and then translated to formal specifications by specifiers. If a formal specification can be generated automatically from a natural language requirements document, system development cost and system fault from experts' misunderstanding will be decreased. A pronoun can be classified in personal and demonstrative pronoun. In the characteristics of requirements document, the personal pronouns are almost not occurred, so we focused on the decision of antecedent for a demonstrative pronoun. For the higher accuracy in analysis of requirements document automatically, finding antecedent of demonstrative pronoun is very important for elicitation of formal requirements automatically from natural language requirements document via natural language processing. The final goal of this research is to automatically generate formal specifications from natural language requirements document. For this, this paper, based on previous research [Park07], proposes an anaphora resolution system to decide antecedent of pronoun using natural language processing from natural language requirements document in Korean. This paper proposes heuristic rules for the system implementation. By experiments, we got 91.34%, 70.31% as recall and precision respectively with ten requirements documents.그림 목차 표 목차 ABSTRACT 제1장 서론 1 1.1 연구 배경 및 필요성 1



구문분석이란 문장 성분 간의 관계를 구조적으로 분석하는 자연어처리 기술이다. 본 연구는 구문분석의 형태적 중의성, 구조적 중의성을 해소하기 위해 품사 태거를 사용하지 않는 광범위 구문분석에 초점을 둔다. 본 연구에서는 의존 문법기반 광범위 한국어 구문분석 시스템의 성능을 향상하고자 규칙기반의 통사 중의성 해소 방안을 제안한다. 형태소분석 후보를 줄여 구문분석의 중의성을 낮추는 방법으로는 단일 어절 형태소 전처리 규칙, 문맥 의존 형태소 전처리 규칙을 적용했다. 지배-의존 관계 생성을 제약하여 구문분석 후보의 과생성을 방지하는 방법으로는 체언 제약규칙, 용언 제약규칙을 사용했다. 기존의 구 묶음으로 발생한 구문분석 모호성의 문제는 지배-의존 관계의 패턴화 기법을 이용하며 통사 중의성(또는 구문 중의성)을 해소한다. 마지막으로, 생성된 구문분석 후보에 대해 문형정보 가중치를 부여해 순위화를 시도한다. 본 연구에서 제시한 방법을 이용하면 선행 연구에 비해 UAS(unlabeled attachment score)를 1.93% 향상시킬 수 있었다.1. 서론 1 1.1. 연구의 배경과 필요성 1 1.2. 연구의 범위 2 1.3. 논문의 구성 4

최근, 인간과 기계간의 정보 교환을 위한, 자연어 처리 및 음성 정보 처리에 대한 연구가 활발히 진행되고 있다. 특히, 한국어 음성 처리의 경우에는 한국어의 여러 특징으로 인해 단어 정보와 음가 정보 사이의 관계 정립을 규명하는 방식으로 연구되고 있으며, 이를 지원할 수 있는 체제에 대한 연구가 요구되고 있다. 본 논문에서는 한국어 음성 처리를 지원하기 위하여 다음과 같은 내용에 대해 연구하였다.
첫째, 한국어 음운 변동 현상을 27개의 종성별 규칙으로 설계하고, 구현하였다.
둘째, 음성 처리를 지원할 수 있는 한국어 사전의 구성에 대해서 연구하였으며, 단어 및 음가 사전을 설계, 구현하였다.
셋째, 한글 음절을 구성하고 있는 낱자의 음절내 위치와 면적 정보를 이용하여 한글의 낱자 설계를 하였으며, 이를 이용한 한글 음절 출력 알고리즘에 대해서 연구하였다.
본 논문에서 구현된 종성별 규칙은 표기 언어(한글)와 음성 언어(한국어) 사이의 문제 해결이 가능하며, 음가 사전의 생성 과정을 통하여, 음운 변동처리가 용이함을 확인하였다. 그리고, 음운 변동 처리 시스템의 구현을 통하여, 변동 규칙이 더욱 간략화 될 수 있음을 보였으며, 음절 단위의 한국어 음성 처리를 위한 음가 정보의 생성에 효율적으로 이용될 수 있음을 확인하였다.
한글 낱자로 설계된 유형의 개수는 모두 473자이며, 구현된 시스템의 사전 출력부에 이를 도입함으로써, 생성된 낱자의 정보표와 한글 출력 알고리즘의 타당성을 보였다.
본 논문에서는 한국어 음성 처리를 지원할 수 있는 한국어 사전의 생성과 이를 처리할 수 있는 한국어 음운 변동 규칙, 사전 검색 알고리즘, 한글 출력 알고리즘을 구현함으로써, 본 논문에서 구현한 시스템이 한국어 음성처리를 지원할 수 있음을 연구하였다.Research on natural language and speech processing in terms of the man-machine interface is now in progress.
On account of Korean speech idiosyncrasies, speech processing has been subject to a searching examination for the relation of word to phonetic value. This relation can be defined through the creation of an algorithmic supporting system.
Thus, contents of this paper are as follows:
First, a rule-based method for the phenomenon of Korean vocalization is proposed and the algorithms are implemented. The proposed rule on the phenomenon of vocal sound could also be used to solve problems in the conversion of symbolic language(Hangeul) into phonetic language(Korean) to illustrate vocal sound changes in a symbolic manner.
Similarly, the reverse process can also be implemented. Thus, it is useful to create data bases, both phonetic and symbolic, with phonetic data-base values for Korean voice processing in syllable units in correct correspondence with the symbolic data.
Second, the construction of a phonetic Korean dictionary for the supporting system of Korean speech processing is studied, and the symbolic and phonetic dictionaries are implemented with the proper support system between them established.
Third, it is proposed that a design method for coordinating phoneme fonts selected from the grouping tables based on the position and area of phonemes composing a Hangeul syllable.
Also, this design would include the output algorithm which is able to construct the output of all Korean syllables using 473 phoneme fonts. Through experimentation, the analyzed information and output algorithm can be applied to the design of Hangeul fonts effectively.
In this paper, methods are proposed to implement a system which can support Korean speech processing, a rule-based system for the illustration of Korean vocal sound which includes both the searching algorithm for a Korean electronic dictionary and the output algorithm of Hangeul syllables.
Through the experiment results, it is proved that the proposed system can be used as a supporting tool of Korean speech processing.목차 = ⅳ 국문 요약 = ⅰ 감사의 글 = ⅲ 표목차 = ⅵ 그림목차 = ⅶ

지식 베이스를 생성하는 관계 추출은 다양한 자연어처리 연구 및 서비스에 사용되는 중요한 정보이다. 관계 추출을 위한 대부분의 방법론은 관계 추출 말뭉치를 이용하고 있다. 그러나 관계 추출 말뭉치의 구축은 매우 시간 소모적이고, 인력 소모적인 작업으로 많은 양의 말뭉치를 구축할 수 없다. 이러한 문제점을 최소화하기 위해 자동으로 말뭉치를 생성하기 위한 원거리 감독법이 연구되었다. 본 논문에서는 기존의 말뭉치와 원거리 감독법으로 생성된 말뭉치에 대해 각각의 관계 추출 모델을 제안한다. 기존의 말뭉치를 대상으로 한 한국어 관계 추출을 위해 한국어 구문 구조에서 중요한 의존 트라이그램을 추출하였다. 추출된 의존 트라이그램으로부터 구문 구조의 유사도를 계산하는 의존 트라이그램 커널을 이용하여 관계 추출을 하였다. 실험 결과 관계 포함 문장 선택에서 0.846의 F1-measure, 관계 표현 어휘 추출에서 0.734의 F1-measure를 보였으며, 영어 평가 집합에 대해서도 0.626의 F1-measure로 높은 성능을 보였다. 다음으로 원거리 감독법으로 생성된 말뭉치에서 발생하는 오류를 해결하기 위해 어휘와 구문패턴을 이용하여 관계 포함 문장을 선택하는 one-class 모델을 제안하였으며, 관계 포함 문장에서 관계를 분류하기 위해 규칙 기반의 TBL 모델을 이용하였다. 실험을 통해 F1-measure 0.638의 결과를 얻을 수 있었다. 제안한 방법론은 한국어에 적합한 방법론으로 한국어의 구문 구조를 효과적으로 비교하였고, 자동으로 생성된 많은 양의 말뭉치의 오류를 줄이며, 규칙 기반의 방법으로 기존의 기계학습 모델들과 동등한 성능을 보임으로써 관계 추출이 가능함을 보였다. 따라서 제안한 2가지의 관계 추출 모델을 통하여 학습 데이터의 형태에 따라 학습 방법이 달라져야 함을 알 수 있었으며, 각각의 학습 방법이 관계 추출에 적합함을 확인하였다.Generating relation extraction on knowledge base is important information for various natural language processing researches and services. Most existing relation extraction methods using relation extraction corpus. However, due to matter of time consuming and labor intensive job a corpus of relation could not be generated a large number of corpora. To overcome these disadvantages, distant supervision methods has been studied to generate corpus automatically. We proposes each of the relation extraction models that generated by existing corpus and distant supervision. For relation extraction of Korean (language?) targeting on existing corpus, dependency trigram, important on Korean dependency structure was extracted. And for relation extraction, dependency trigram kernel calculates similarity on dependency structure using this extracted dependency diagram. The result shows high performance levels on each tests - F1-meature of 0.846 on sentence selection including relations, F1-meature of 0.734 on relation name extraction, and F1-measure of 0.626 on English evaluation corpus as ACE. Furthermore, we proposes one-class model that selecting sentences including relations, uses lexical dependency pattern to figure out errors occurs on corpus that generated from distance supervision, as well as rule based model, TBL to classified relationship on sentences including relations. Experiments shows F1-measure of 0.638. A proposed method that shows through the experiments is appropriate for Korean(language) and it compares dependency structure of Korea (language) effectively. Moreover, this experiment as a rule of base methods, exhibited similar performance as compared with previous supervised machine learning method as reduces a number of corpus errors generated automatically. Therefore, through the two relation extradition models we proposed, this study discovered that study methods should be changed based on types of study data and identified each of study methods is appropriate for relation extraction.1. 서 론 1 2. 연구 배경 3 2.1. 관계 추출 시스템의 필요성 3 2.2. 관계 추출 7 2.3. 지식 베이스 9

최근 챗봇 시장의 확대로 다양한 분야에 많은 챗봇들이 서비스 중에 있다. 하지만 이러한 환경은 사용자에게 어느 챗봇이 가장 적합한 정보를 제공해 줄 수 있는지 혼란을 야기할 수 있다. 그에 따라 사용자의 질의 의도를 파악하여 가장 적합한 답변을 반환할 수 있는 챗봇을 추천해주는 시스템을 필요로 한다. 하지만 챗봇 관련 기술 연구의 고도화로 인하여 현존하는 챗봇들이 서로 다른 기술 및 데이터 포맷을 사용하여 다양성을 보여주고 있다. 이러한 다양성 문제를 해결하기 위해 모든 챗봇의 공통점을 추출하였다. 이후 모든 챗봇은 질의응답 데이터를 보유하고 있다는 점을 착안하여 문장 분류기법을 이용한 다중 이종 챗봇 추천 시스템을 제안한다.
본 논문에서는 각 챗봇의 질의 데이터와 해당 챗봇 라벨 데이터를 학습 데이터 포맷으로 정하고 현재 문장 분류기법에 많이 사용되고 있는 Naive Bayes 분류기, SGD 분류기, Bi-LSTM 분류기, CNN 분류기를 이용하여 실험을 진행하였다. 실험에는 KorQuAD 데이터 세트를 이용했으며 악조건 상황에서도 가장 높은 정확도를 내는 분류기가 적합한 분류기라고 판단하여 기본 테스트 케이스와 명사만 남긴 케이스 및 각 형태소를 랜덤으로 섞은 케이스에 대한 정확도 측정 실험을 진행하였다.
실험결과 Naive Bayes 분류기가 모든 경우에 대해 평균 86%라는 정확도로 가장 높게 측정되어 챗봇 추천 시스템에 가장 적합한 분류기로 보여 진다. 하지만 KorQuAD 데이터 세트는 한 문맥에 관한 질의응답 데이터가 20개 내외로 작다. 향후 특정 내용에 집중된 질의응답 데이터가 공개된다면 딥러닝 계열의 Bi-LSTM과 CNN 기법 또한 높은 성능을 보여줄 것으로 보인다.Recently, there are many chabot services in a wide variety of fields due to market expansion. But, in this situation, it can cause confusion as to which chatbots can provide the most appropriate information to users. Thus, a chabot recommendation system that figures out user intent and return the most appropriate answers is required. However, due to the advancement of chatbot research, existing chabots show diversity using different technology and data formats. As a result, with an understanding that all chatbots have a question-and-answer data, this study proposes a multiple and heterogeneous chabot recommendation system.
In this paper, the question data of each chatbot and the label data of that chatbot are set in the study data format. The experiment was conducted using Naive Bayes classifiers, SGD classifiers, Bi-LSTM classifiers and CNN classifiers, which are currently widely used in sentence classification techniques. This experiment used KorQuAD data sets and reasoned that the highest accuracy classifier is suitable even under adverse conditions. An accuracy measurement experiment was conducted on three cases; basic, left only nouns, randomly mixed each morpheme.
The results of the experiment showed that in all the cases, the Naive Bayes classifier was the most suitable classifier for the chatbot recommendation system with an average accuracy of 86%. However, KorQuAD data sets have only about 20 questions and answers per context. If the question-and-answer data focused on specific contents are disclosed in the future, Bi-LSTM and CNN technologies from deep learning will also show high performance.제1장 서론 1 제1절 연구 동기와 목적 1 제2절 기본적인 챗봇의 동작 원리 3 제2장 관련 연구 4 제1절 기존 연구 4

기록 관리는 기록을 선별하고 수집하여 보존하는 것에서 이를 이용하는 방향으로 발전하고 있지만 여전히 기록관의 존재를 인지하지 못하는 잠재적 이용자가 많다. 따라서 본 연구는 기록관과 이용자의 상호작용을 지원하는 기록정보서비스 챗봇을 개발하였으며, 구체적인 개발 절차와 방법을 살펴보고자 하였다.
이에 본 연구는 적용 대상으로 명지대학교 대학사료실을 선정하고 이용자 요구 및 질의 분석을 진행하였고 챗봇 개발을 위한 논리적 구조 설계를 제안하였다. 이어 IBM Watson Conversation과 카카오톡 메신저를 통해 챗봇을 구축한 뒤, 시험 실행(Pilot run) 과정을 통해 기록정보서비스 챗봇이 이용자와 상호작용하는 모습을 확인할 수 있었다.
또한 기록정보서비스 챗봇 개발 과정의 경험을 바탕으로, 챗봇 도입, 챗봇의 수준 결정, 이용자 요구 분석, 챗봇 구축을 위한 도구 선정, 대화식 상호작용을 위한 구문 설정을 위한 시사점을 제시했다. 본 연구에서 제시된 기록정보서비스 챗봇 개발을 위한 논리적 구조 설계와 기록관 챗봇 구축의 시사점에 대한 논의는 기록연구사가 기록정보서비스 챗봇을 개발하여 기록관에 적용하는 것에 대한 실제적이고 구체적인 이해를 도울 것으로 기대된다.Records management is evolving from collecting and preserving records and archives to utilizing them, but there are still many potential users who are not aware of the existence of records centers and archives. Therefore, this study has developed a record information service Chatbot that supports the interaction between the user and the records centers and archives, and it examined the specific development procedures and methods.
In this study, we chose the Archives of Myongji University for the application of the Chatbot, analyzed user requests and questions, and proposed a logical structure design for Chatbot development. After building the Chatbot based on IBM Watson Conversation and KakaoTalk Messenger, we were able to find how the record information service Chatbot interacted with the user through a pilot run process.
Also, based on the experience of the development process of the developing information service Chatbot, the implication on the introduction of the Chatbot, the determination of the level of the Chatbot, the analysis of the user requests, the selection of the tool for the Chatbot, and the syntax setting for the interactive interaction were suggested. It is expected that the discussions about the logical structure design for the development of the information service Chatbot presented in this study and the implications of the construction of the Chatbot will help the archivists to understand the practical and detailed understanding of applying the information service Chatbot to the archives.그림 목차 ⅲ 표 목차 ⅴ 국문 초록 ⅵ 제 1 장 서론 1

4차 산업혁명의 등장과 빅 데이터의 발전으로 많은 기업들이 비정형 데이터인 자연어 분석을 시도하고 있다. 또한 국내 채용분야에서도 블라인드 채용이 확대됨에 따라 많은 공공기관과 기업들이 자기소개서의 내용을 기반으로 신입사원을 뽑는 비율이 높아지고 있으며, 이를 빅 데이터에 접목하는 시도도 늘어나도 있다.
채용과정에 빅 데이터를 사용하는 범위가 자기소개서뿐만 아닌 취업준비생들의 이력서 검토를 최소화하는 것 등 A.I 면접관을 통해 면접을 보는 분야까지 넓혀지고 있다.본 연구에서는 자기소개서는 구직 활동에 있어 서류전형부터 면접전형까지 매우 중요하게 판단되는 자료이고, 구직 활동에 성공하기 위해 지원하는 기업의 인재상에 맞추어 자기소개서를 작성한다는 것을 전제로 텍스트 마이닝을 활용하여 자기소개서와 기업의 인재상과의 유사도를 분석해보았다. 이를 위해, 대기업, 중견기업, 공기업에서 공개한 공채 합격 자기소개서를 취합하고, 형태소 분석 및 개체명 추출을 통해 데이터를 가공하였다. 그리고 대한 상공회의소에서 발간한 100대 기업이 원하는 인재상 보고서에 나타난 인재상 키워드와 각 기업들의 홈페이지에 게재된 인재상을 정리하여 인재상 키워드를 선별하였다. 이 후, 인재상 키워드와 자기소개서간의 유사도를 카운트 기반의 단어 표현 방법인 TF-IDF모델과 인공신경망 기반의 단어 표현 방법인 Word2Vec 모델을 활용해서 유사도를 분석해보았다.
인재상 키워드와 자기소개서의 비정형 데이터의 유사도를 실험 결과를 통해 확인 해 볼 수 있었으며 이를 통해 인사분야의 텍스트 마이닝 활용 방법에 대해 생각 해 볼 수 있었다.With the advent of the fourth industrial revolution and the development of big data, many companies are trying to analyze natural language, which is unstructured data. In addition, as blind recruitment has been expanded in the domestic recruitment field, many public organizations and companies are hiring new employees based on the contents of their introduction documents, and there are more attempts to incorporate them into big data.
The scope of using big data in the recruitment process is expanding to include not only self-introduction but also interviewing A.I. interviewers, such as minimizing the review of resumes of job seekers.
In this study, we used text mining to analyze the similarities between the self-introduction letter and the talent of the company, assuming that the self-introduction document is considered very important in the job search activities, from document form to interview type, and that the self-introduction document is prepared in accordance with the persona of the company that supports the job search activity. To that end, data were processed through form analysis and extraction of object names by collecting letters of self-introduction for acceptance of public bonds disclosed by large businesses, medium businesses and public corporations. In addition, the Korea Chamber of Commerce and Industry selected the keywords for the awards by compiling the keywords for the awards presented in the report for the top 100 companies' desired human resources
awards and those posted on the website of each company. Afterwards, similarities between keywords and magnetic introduction documents were analyzed using the TF-IDF model, a count-based method of word expression, and Word2Vec model, an artificial neural network-based method of word expression.
The results of the experiment showed the similarities between the keywords and the unstructured data in the self-introduction document, which enabled us to think about how to use text mining in the human resources field.국문초록 ⅵ 영문초록 ⅷ 제 1 장 서 론 1 1.1 연구 배경 및 목적 1

특정한 사회·문화공동체가 의사소통의 수단으로 이루어 낸 역사적 산물로서의`자연어'는 언어 기호및 일정한 규칙들로 구성된 하나의 총체적 `체계(System)'이다. 이같은 `자연어'의 체계 또는분석, 기술함으로서 얻어지는 것이 문법이라한다면,분석과 기술의 이론적 배경과 방법론에 따라 의 문법은 그 구성과 내용이 상당히달라질 수 있다. 외국어 교육과 관련하여 오래 전부터 학교문델이 되어 온 `전통문법'은품사분류를 근간으로 하여 형태및 통사적 현상을 문법서술의 출발점으다. 언어에의한 `컴뮤니케이션 메카니즘'에 비추어 볼 때, 이같은 전통문법은 겉으로 드러난형태istic form)에서 의미(Signification)를 추출해 가는 "청자"의 입장을 대변한다. 이와는 반대로,문법'은 일정한 발화상황에서 자신이 전달하고자 하는 의미 또는의도를 출발점으로 하여 이를 표에 적합한 형태를 찾아 가는 "발화자"의 입장에서기술되는 한 언어의 문법을 지칭한다. 즉, 발화즘을 토대를 하는 문법이다. 본학위논문에서는, 1) 언어에 의한 의사소통 메카니즘에서 화자의 윤毒灼構� 2)일반언어학의 관점에서 `화자문법'의 이론적 기반을 검토한 다음, 3) 화자문법 서술로 선택한 B. POTTIER의 발화행위이론을 분석·비판하고 6개의 보편문법적의미범주(conceptual caes)를 정의한 후, 4) 이 중 한 카테고리인 `태(態)'를택하여 화자의 입장에서 의미내지 개념적 쩌궈쳄岵막關��`태'의 종류를 구분한후, 불어에는 이를 언어적으로 표현 가능케 하는 형태및 통치들이 어떤 것들이있는지 살펴 보았다.

오늘날, 모바일 및 웨어러블 기기의 발달과 무선 인터넷 보급률의 상승 등으로 인해 약 90%의 국민이 인터넷 서비스를 사용하는 것으로 추정되며 상당수의 웹 사이트에서 제공하는 서비스 및 컨텐츠에 원활히 접근하기 위해 회원가입이 필수적이다. 그러나, 최근 5년간 발생한 116건의 회원정보 유출 사고는 웹 사이트 회원가입이 잠재적인 개인정보 유출의 가능성을 포함하고 있음을 보여준다. 과학기술정보통신부의 정보보호 실태 조사에 따르면 약 10%의 웹 사이트 이용자만이 회원가입 약관을 제대로 확인하는 것으로 조사되었으며, 이는 대부분의 사용자들이 회원 가입을 통해 웹 사이트에 제공되는 개인 정보 항목을 인지하지 못하고 있음을 말한다. 또한, 웹 사이트에 제공되는 개인정보 항목을 인지하더라도 정보보호에 대한 지식부족으로 인해 해당 정보가 얼마나 중요한지 제대로 판단하지 못한 체 개인정보를 제공하는 경우도 존재한다. 이를 해결하기 위해, 본 논문은 형태소 분석을 통한 회원가입약관 개인정보정보보호 영향도 평가 시스템을 제안한다. 본 시스템은 회원가입약관의 형태소를 분석해 사용자로부터 웹 사이트에 제공되는 개인정보 항목을 자동으로 인식하며 각 항목별 중요도에 기반 하여 회원가입약관의 개인정보보호 영향도를 평가한다. 37개의 웹 사이트를 대상으로 페이지에 존재하는 항목과 시스템이 수집한 항목을 비교하는 실험을 진행한 결과, 본 시스템의 정확도는 89.66%로 측정되었다.Today, since the development of mobile and wearable devices and the increasement of wireless internet penetration rate, about 90% of the people use internet services. In order to access to various services and contents smoothly, users are required to sign in the membership. However, considering 116 personal information leakage accidents occurred during last 5 years, the membership enrollment has potential threat of information leakage. According to Survey on the Information Security conducted by the Ministry of Science, only about 10% of website users are checking the terms of membership. In other word, most of users are not aware that what personal information is provided. In some cases, due to lack of knowledge, they do not understand how critical it is though users are aware the provided information. To handle these problems, this paper proposes a system which evaluates privacy level of membership terms. Through morpheme analysis on the terms of membership, the proposed system automatically recognizes what personal information is provided by users. Then, according to the importance of each information, the system evaluates privacy level of terms. The comparison result of the personal information collected by manual with the information collected by proposed system on 37 websites showed an accuracy of 89.66%.차 례 I. 서론 1 II. 연구 배경과 기존 연구 3 1. 자연어처리 3 2. 크롬 확장 애플리케이션 7

인공지능 기술은 일상생활의 많은 영역에서 활용되고 있다. 영상처리, 자연어처리, 디지털 헬스 케어, 스마트 그리드 등 다양한 영역에 인공지능 기술을 적용하고자 하는 연구가 진행되고 있다. 인공지능 작가와 인공지능 화가 등의 예술의 영역에서는 다양한 딥러닝 방법론들이 사용되어 ‘사람이 창작한 것 같은’작품들을 만들어낸다. 본 연구에서는 이러한 흐름에 발맞추어 작곡을 위한 인공지능 기술에 대해 고찰하였다.
본 연구는 인공지능 작곡기계 관련 연구의 조사와, 이를 통해 얻은 지식을 바탕으로 한 간단한 인공지능 작곡기계 개발로 이루어진다. 관련 연구 조사를 통해 인공지능 작곡기계에 대한 선행연구들을 조사하고 그 한계점을 도출하여 음악 생성에 맞는 방법 및 도구들을 조사한다. 이러한 방법 및 도구들을 활용하여 개인용 컴퓨터 수준의 환경에서 개발할 수 있는 간단한 인공지능 작곡기계를 제안하였으며, 개발을 통해 얻은 지식들에 대하여 정리하였다.Ⅰ. 서론 1 Ⅱ. 인공지능 작곡기계를 위한 방법론 2 1. 딥러닝 2 1.1 합성곱 신경망 3 1.2 오토인코더 7

구문분석은 문장 안에서 문장성분들의 관계를 찾는 과정이다. 품사태깅과 더불어 구문분석은 자연어처리 분야에서 필수 단계 중 하나이다. 응용 프로그램에서 언어 분석에 대한 요구가 증가하면 할수록 구문분석에 대한 요구는 증가한다.
예를 들어 기계번역에서 처음에는 단순히 단어들의 정렬을 통해서 번역을 시도하였으나 그 성능이 만족스럽지 못하여 구문분석 정보를 이용하여 문장에서 각 문장 성분들 간의 관계 정보를 이용하여 번역을 시도하고 있다. 또한 인터넷 문서에서 유용한 정보를 추출할 경우에도 단순히 문자열 패턴을 이용하는 것이 아니라 구문분석 결과를 이용하여 좀 더 정확한 추출을 할 수 있다.
이 경우에 반드시 필요한 것이 처리 속도, 안정성, 그리고 성능이다. 품사태깅과는 달리 구문분석은 각 문장 성분들 간의 관계를 조사하기 위해서 일반적으로 O(n3)의 처리 시간이 소요되기 때문에 문장이 길수록 속도는 더욱 느려지게 된다.
또한, 구문분석은 문장 전체에 대한 분석결과를 출력하므로 문장이 복잡해질수록 완전한 분석결과를 출력하는 것이 힘들어진다. 더욱이 인터넷 문서와 같이 비문이 많은 문장들에서는 분석을 성공하지 못하고 시스템이 죽어버리는 경우도 많이 발생한다. 성능은 모든 시스템에서 중요하지만 특히 구문분석에서는 구문분석의 오류가 응용 프로그램에 직접 영향을 미치기 때문에 더욱 중요하다.
영어권에서는 구문분석 연구가 성숙하여 구문분석 프로그램을 사용하여 다양한 연구 결과를 보이고 있다. 그러나 한국어의 경우는 다양한 연구에서 사용할 수 있는 고속, 고성능 구문분석기가 존재하지 않는다. 따라서 영어권에 비해서 사용할 수 있는 자원이 줄어들어 연구의 깊이와 결과가 좋지 못한 상황이다.
본 연구에서는 정문뿐만 아니라 인터넷 문서와 같이 비문이 많은 환경에서도 고속, 고성능 결과를 보이는 구문분석기를 제안한다. 본 연구에서는 일반적인 구문분석에서 사용하는 구성 성분들 간의 결합을 통한 구문분석을 하지 않아 사용하는 메모리가 작고 속도가 매우 빠른 장점을 가지고 있다. 또한 어절 구문태그와 CRFs(Conditional Random Fields)를 사용하여 고성능의 한국어 구문분석기를 구현하였다.I. 서론 1 II. 관련연구 4 III. 한국어 특징 6 IV. 다단계 구단위화를 이용한 한국어 의존구조 분석 10 1. 구문태그 12

오늘날 컨볼루션 신경망 (Convolutional Neural Network)은 컴퓨터 비전, 음성 인식, 자연어 처리 등 다양한 분야에서 사용되고 있다. 컨볼루션 신경망의 정확도를 높이기 위해 망을 깊게 모델을 설계하는 연구가 진행되고 있으며 이에 따라 연산량 또한 급격하게 증가하고 있다. 따라서 컨볼루션 신경망은 연산을 빠르게 처리하기 위해 Graphic Processing Unit (GPU)와 같은 가속기를 통해 구현되고 있는 추세이다. 하지만 Internet of Things (IoT) 기기와 같은 경량의 임베디드 시스템은 한정된 자원으로 인해 GPU와 같은 고성능의 가속기를 수용하는데 한계가 있다. 따라서 이러한 경량의 임베디드 시스템은 Central Processing Unit (CPU)를 효율적으로 활용하여 컨볼루션 신경망을 가속하는 것이 중요하다. 현대의 CPU에는 일반적으로 벡터 연산을 통해 데이터 병렬화를 수행할 수 있는 Single Instruction Multiple Data (SIMD) 연산기가 내장되어 있다.
본 논문은 경량의 임베디드 시스템을 위해 컨볼루션 신경망을 가속하기 위한 효율적인 SIMD 구현 기법을 제안하고, 성능 향상을 실험을 통해서 검증하였다. 본 논문에서는 컨볼루션 신경망의 기본 모델 중 하나인 LeNet-5을 ARM 코어에 내장된 SIMD 연산기인 ARM NEON으로 가속했다. 또한 멀티코어 환경에서 병렬프로그래밍 API인 OpenMP를 사용하여 각 코어의 SIMD 연산기를 모두 활용하여 가속하였다. 성능향상을 위해 제안한 방법은 SIMD 장치 내의 벡터 레지스터의 활용도를 높이기 위해 컨볼루션 커널의 형태를 수정하는 데이터 재구성 (Data Reshaping) 기법이다. 그 결과 특징 추출 과정의 실행 속도는 통상적 방법 대비 최대 1.9배까지 가속되었고, 에너지 소모는 최대 58%까지 감소하였다. 실험 결과, 제안하는 기법이 SIMD 연산을 이용해서 CNN을 구현한 프로그램의 성능향상에 기여한 점을 알 수 있다.Today, Convolutional Neural Network (CNN) is widely used in a variety of fields such as computer vision, speech recognition, and natural language processing. To increase the accuracy of CNN, researches have been carried out to design deeper CNN. For this reason, the amount of computation has also been increasing. Therefore, CNN has lately been accelerated through accelerators such as Graphic Processing Unit (GPU). However, resource-constrained embedded platforms such as Internet of Things (IoT) devices cannot afford to have such accelerators. Therefore, it is important to accelerate CNN by the Central Processing Unit (CPU) efficiently. The Single Instruction Multiple Data (SIMD) unit which can perform data parallelization through vector operation is integrated in many CPUs.
In this thesis, we propose an efficient SIMD implementation method for accelerating CNN for the resource-constrained embedded system and verify the performance improvement through experiments. LeNet-5, one of the basic model of CNN, is accelerated by ARM NEON which is the SIMD architecture integrated in ARM core. In addition, all the NEON units integrated in quad-core are employed by using OpenMP which is an API for shared-memory parallel programming. The proposed method is the Data Reshaping technique that modifies the shape of the convolution kernel to improve the utilization of the vector register in NEON units. In the experiment, the execution speed of the feature extraction is accelerated up to 1.9 times compared with the conventional method and the energy dissipation is also reduced by up to 58% than the conventional method. The experimental results show that the proposed method contributes to the performance improvement of CNN implementation with SIMD operation.제 1장 서 론 1 제 1절 연구 동기 및 배경 1 제 2절 연구 목표 3 제 2장 관련 연구 4

본 논문에서는 문맥에 맞는 문장 생성을 위한 언어 학습 모델을 제안한다. 자연어 처리 연구는 다방면에서 진행되었고, 기계 번역, 문서 요약, 감성 분석 등 많은 분야에 진전을 이뤄왔다. 뿐만 아니라 주어진 텍스트를 학습한 뒤 이를 이용하여 새로운 문장을 생성하는 연구로 발전하고 있다. 하지만 개별적인 단어의 일반적인 패턴을 학습하기 때문에 주제나 의도를 갖은 문장을 생성하기 어렵다는 한계가 있다. 이러한 문제를 해결하기 위하여 주어진 텍스트의 문맥을 분석하고 이에 기반하여 문장을 생성하는 개선된 문장 생성 방안을 제시한다. 본 논문에서는 문맥에 근거한 문장을 생성하기 위해 크게 세 단계 방법을 제시한다. 첫 번째로 문장의 의도를 분석하기 위해 주어진 문장을 표현할 수 있는 문장 벡터 생성 방법을 제시한다. 두 번째로 생성된 문장 벡터들 사이의 관계성을 분석하기 위하여 문장 벡터를 의미 별로 분류하고 이를 기반으로 주어진 문장간의 관계를 분석 함으로써 문맥을 이해하는 방법을 제시한다. 두 번째 방법을 통해서 주어진 텍스트의 문맥을 학습할 수 있고 이에 근거하여 문맥을 생성할 수 있다. 세 번째로 주어진 문맥과 이전 문장간의 관계를 분석하여 문맥에 맞는 문장을 생성할 수 있는 방법을 제시하였다목차 목 차 ⅰ 표 목차 ⅲ 그림 목차 ⅳ

최근 인공신경망을 기반으로 한 딥러닝 시스템은 영상인식, 음성인식, 자연어 처리등과 같은 분야에서 많은 성과를 거두고 있다.
이는 기존 인 공지능 알고리즘이 사람에 의해 정해진 전처리 (Preprocess) 방법을 통해 가공되어진 정형 데이터(Formal Data)를 이용하여 처리한 것과는 달리, 전 처리 이전의 비정형 데이터(Informal Data)를 그대로 이용하여 데이터가 가지고 있는 고유의 특징들(Features)을 딥러닝이 스스로 추출하고 학습함 으로써 최적의 성능을 얻을 수 있었다.
하지만, 뛰어난 성능에도 불구하고 아직 의료분야 및 자율주행 등과 같이 사람의 안전과 직접적으로 관련된 분야에서는 적용된 성과를 찾기 어렵다.
그러한 분야에서 딥러닝의 적용 을 어렵게 하는 원인들 중에는 딥러닝의 단점 중 하나인 결과에 대한 판 단 근거를 제시할 수 없다는 부분이 대표적이다.
이러한 딥러닝의 단점은 딥러닝이 판단에 필요한 특징들(Features)을 데이터로부터 추출할 때, 사람 이 이해하기 어려운 형태로 추출하기 때문이다.
이 단점을 보완하기 위해 머신러닝이 출력한 결과와 원인을 사람이 이해할 수 있도록 도와주는 설 명 가능한 인공지능(Explainable AI)이 연구되고 있으며, 해석 가능성 (Interpretability)은 그 연구의 핵심키워드 중 하나이다.
이러한 해석 가능 성을 나타내기 위해 CNN(Convolution Neural Network)기반의 모델들은 판 단한 결과에 영향을 주는 영역을 입력 이미지에서 표시해주는 방법들이 소개되었고, 그 중에서 Back-propagation 을 기반으로 한 Grad-CAM 이 처 리 속도나 수학적 당위성 측면에서 우수하다.
그러나 Grad-CAM은 미세영 역에 대한 설명이 부족하고, 위치정보가 일부 손실된다는 단점이 있다.
본 논문에서는 Grad-CAM의 한계인 미세 영역에 대한 설명 부족현상과 위치 정보 손실 현상을 개선한 Fine-Grad-CAM 기법을 제안하고자 한다 Fine-Grad-CAM은 미세 영역에 대한 설명부족 현상을 개선하기 위해 낮 은 레벨의 컨볼루션 레이어(Convolution Layer) 까지 기울기 값을 계산하여 이용하였다.
또한 Grad-CAM에서는 손실되었던 위치 정보를 유지시키기 위해 새로운 계산식을 도입하였다.
이렇게 제안된 Fine-Grad-CAM 기법을 CNN기반의 PCB(Printed Circuit Board) 비전 불량 검사 시스템에 적용하여 시스템이 출력한 결과에 대한 Interpretability를 기존 방식들과 비교하여 Fine-Grad-CAM 의 장점을 보여 주고, 나아가 사람이 딥러닝의 판단 근거를 이해하는데 도움을 주고자 한 다.1. 서론 1 1.1 연구의 필요성 1 1.2 연구의 내용 5 2. 관련 연구 7 3. 제안 모델 11

Various kinds of evaluation systems are used because an evaluation is an important factor in the educational field. The most commonly used system is the one with paper and pens, which has an advantage in evaluating one's intelligence effectively. The subjective evaluation induces students to state their thoughts and opinions and can measure the higher intelligence ability, but is actually often avoided because of the problems such as objectivity, reliability, time and expense. An automatic grading system is needed to complement these disadvantages.
This study suggests a processing model and an appropriate grading system for every type of questions and applies the grading system to an answer sheet after acquiring the background knowledge of processing a natural language, the types of subjective questions, and the limitation of the previous studies. Finally, it has done a comparative analysis of the grading results by a teacher and by the automatic grading system.
The types of questions are divided into six as follows: an independent task, a common task, a statement, an ordering, a presentation of strengths and weaknesses, and a partial presentation. The ways in which a correct answer and a student's answer are classified and graded are demanded differently according to these types. The analysis of the correlation between the grading system by a teacher and the automatic grading system shows that the automatic grading system is superior to the existing system in the common task, statement and partial presentation, but loses the efficiency in the ordering and presentation of strengths and weaknesses because of the limitation of a morpheme analysis and a detailed classification. A better processing method will be devised by analyzing question types thoroughly.
Furthermore, the increase of the number of questions, examining similar questions many times, and developing a better system in validity, reliability and systematization will ensure the accuracy of the grading system of subjective questions.
The classification of the grading system of subjective questions through an analysis of question types suggested in this thesis will make a big difference in the field of subjective evaluation.목차 Ⅰ. 서론 1 1. 연구의 필요성 1 2. 연구문제 2 3. 연구의 제한점 2

Drawing information from a commonsense point of view is called nonmonotonic be reasoning in that conclusions based on partial and incomplete information can be cancelled later when complete information is given.
This logic is very important in imitating the belief in behavioral process in which, confronting incomplete information, one builds up hypotheses based on new observations, and then revise then later.
This thesis defines the semantics. made up of intuitive bases. in which one restructures nonmnotonic logic as ideal agent's reasoning about his belife, and can prove autoepistemic logic to be sufficient and compleate for the resulting system called autoepistemic logic.목차 도목차 ABSTRACT Ⅰ. 서론 = 1 Ⅱ. Logic의 일반적 형태 = 2

본 논문에서는 기존의 협력추천기법을 적용한 영화 협력 추천 시스템의 문제점을 다각도 적으로 극복하고자, 영화를 설명하는 짧은 자연어 메타데이터를 추천에 효과적으로 활용하는 방안을 제시한다. 본 논문에서 완화하고자 하는 기존 협력추천의 문제점은 총 세 가지로, 논문의 3~4장을 통해 각 문제와 메타데이터를 이용한 극복방안을 자세히 설명한다.
제 3장에서는 기존 협력 추천 시스템 환경에서, 비교적 사용자의 숫자가 충분하지 못한 서비스 초기 단계에, 추천의 성능이 급격히 저하되는 현상을 극복하기 위한 아이템 메타데이터 활용 방안을 제시한다. 해당 절에서는 영화를 설명하는 짧은 자연어 형태의 아이템 메타데이터에 부가적으로 아이템간의 연관성을 나타내는 링크 정보와 사용자간의 Trust Network 정보를 적용하는 기법을 사용한다. 실험 결과, 아이템 메타데이터와 사용자가 Trust Network 정보를를 사용함으로써 추천 성능을 효과적으로 향상시킬 수 있었다.
제 4장에서는 기존의 전통적인 협력 추천 시스템의 성능 척도로는 평가할 수 없었던 문제점을 정의하고 그 해결 방안을 제안함으로써, 추천 시스템의 새로운 옵션을 제시한다.
제 4장 1절에서는 기존의 영화 추천시스템의 문제점으로 평가되는 사용자 의견 고립 현상, 일명 filter bubble 문제[1]를 완화시키기 위한 아이템 메타데이터 활용 방안을 제시한다. 기존의 사용자(혹은 아이템)기반 협력추천의 경우, 사용자와 유사한 의견을 갖는 사용자의 평점 패턴을 분석하여 추천을 제공하기 때문에, 그 결과에 있어 자신과 유사한 사용자들의 평점 패턴만 고려되어 사용자가 의견 측면에서 고립되는 문제가 존재한다. 이러한 문제를 해결하기 위해, 제 4장 1절에서는 아이템 메타데이터 간 유사도를 긍정적 유사도로, 아이템 간 평점 패턴 유사도를 부정적 유사도로 사용하여, 내용적으로는 유사하지만, 선호하는 사용자간 의견 유사도는 낮은 아이템 쌍을 이용하여 추천을 제공하는 기법을 제시한다. 실험 결과, 기존 기법과 비교하여 추천의 정확도를 기존 기법에 준하는 수준으로 유지하면서, 나와 상반되는 사용자의 의견이 고려된 추천을 제공할 수 있음을 확인하였다.
제 4장 2절에서는 기존의 영화 협력 추천시스템의 문제점으로 평가되는 추천된 아이템 집합의 내용적 다양성 저하 문제를 개선하기 위한 방안으로, 영화를 설명하는 아이템 메타데이터간의 유사도를 협력 추천에 새롭게 활용하는 방안을 제시한다. 기존의 협력추천에 아이템 메타데이터간의 유사도를 결합하여 추천의 성능을 높인 제 3장에서의 연구와는 차별적으로, 제 4장 2절의 연구에서는 아이템 메타데이터간의 유사도를 내용적으로 유사한 아이템이 반복적으로 추천되는 것을 제한하는 용도로 사용한다. 실험 결과, 기존 기법과 비교하여 추천의 정확도를 기존 기법에 준하는 수준으로 유지하면서 추천된 아이템의 내용적 다양성을 효과적으로 증진 시킬 수 있음을 확인하였다.국문 요약 ·························································· 1 제 1장 서 론 ······················································ 3 제 1절 연구 배경 ························································· 3 제 2절 연구 내용 ························································· 5

컴퓨터 네트워크 기술의 발전으로 인터넷 사용률 증가와 함께 웹 문서 또한 기하급수적으로 증가하고 있다. 이로 인해 폭발적으로 증가하는 웹 문서를 빠르고 정확하게 분류하는 문제 또한 큰 이슈가 되고 있다. 하지만 텍스트 마이닝 기법들은 주로 영어로 작성된 문서들을 중심으로 연구가 진행되고 있어 한국어로 작성된 문서들을 위한 텍스트 마이닝 기법에 대한 연구는 아직 활발하게 진행되지 않은 상태이다. 이는 한국어 특유의 중의적 의미를 가진 단어의 빈번한 사용과 용언의 불규칙 활용, 자유로운 어순, 띄어쓰기 문제 등 한국어 분석에 많은 어려움이 존재하기 때문으로 판단된다.
본 연구에서는 단어의 중의적 사용을 제거하는 방법으로 선택적 Bigram 모델을 제안한다. 빈번하게 함께 사용되는 단어 간의 결합과 형태소 분석을 통해 분리된 품사 간의 결합을 통해 문장 구성 요소의 중의적 해석을 제한한다. 또한, 웹을 통하여 수집한 한국어 웹 문서, 전자메일, 인터넷 신문기사를 형태소 분석, N-gram 모델, 품사결합을 통해 전처리하고, 이를 범주화하기 위한 텍스트 마이닝 기법을 제안하고 이를 통해 선택적 Bigram 모델의 성능을 평가한다.
텍스트마이닝을 위한 기법은 현재까지 많은 방법이 소개되었고, 현재도 개발되고 있다. 모든 문서에 가장 적합한 텍스트 마이닝 기법은 존재하지 않는다. 분석에 사용될 데이터의 특성을 잘 파악하고 그에 맞는 기법을 활용하는 것이 분석 결과의 품질을 높이는 방법이다. 본 연구에서는 웹 문서를 분석하여 사람의 스트레스 원인을 분석하고, 전자메일을 주제별로 군집화하는 문제, 낚시성 인터넷 신문기사를 분류하는 문제에 적합한 텍스트 마이닝 기법을 제안한다.1. 서 론 1 1.1. 연구 배경 1 1.2. 연구 내용 2 1.3. 논문 구성 2 2. 관련 연구 3

인터넷과 소셜 미디어의 발달로 인해 엄청난 양의 짧은 길이의 문서들이 쌓이고 있다. 하지만 이러한 정보의 방대함이 곧바로 사용자들이 원하는 정보를 편하게 찾을 수 있는 것을 의미하는 것은 아니며, 오히려 정보 과잉 문제를 발생시킨다. 따라서 본 논문에서는 온라인 상의 Q&A데이터를 분석하여, 문서에서 의도를 나타내는 핵심 구절을 찾아내고, 이를 이용해 짧은 문서간의 유사도를 구하는 새로운 방법을 제안한다. 제안된 방법은 설문조사를 통해, 기존 방법과 비교해서 사용자에게 더 나은 방법이 무엇인지 평가 받았다. 그 결과 제안된 방법이 기존 방법의 문제점을 해결하고, 문서의 의도를 파악할 뿐만 아니라, 결과물로 또한 더 좋음을 확인하였다. 본 논문에서 제안한 방법을 활용한다면, Q&A나 고객센터와 같은 사용자 질의를 중심으로 정형화된 해답을 제공하는 시스템의 자동화에 활용이 가능하다. 또한 인터넷상에 다수 분포되어있는 여러 비슷한 내용의 문서의 수를 줄일 수 있으며, 결과적으로 사용자가 원하는 의도에 맞는 정보만을 효율적으로 제공할 수 있다.Due to the development of the Internet and social media, there is an increasing amount of short documents. However the immensity of information does not necessarily mean that users can easily find the information they want, but rather they generate information overload. Therefore, in this thesis, we analyze the Q&A data provided online and propose a new measure deriving the intent of the document by finding the core passages in the document. Compared to the traditional methods, the proposed method has been evaluated by the users through a survey for better assess. As a result, the proposed method has not only solved the problem of the traditional methods, but also verified the intent of the document. Furthermore it also enhanced the performance. Utilizing the method proposed in this thesis, it is feasible to use the answers in the system which provide answers to user query questions such as Q&A or customer center. It also reduces multiple similar documents distributed over the Internet, allowing effective delivery of the information that the users want.1. 서론 1 2. 관련 연구 5 2.1 짧은 글의 정의 5 2.2 자연어 처리 8 2.3 텍스트 마이닝 12

원래 컴퓨터 비전을 위해 고안된 딥러닝 알고리즘인 CNN(Convolutional Neural Network)이 최근에 자연어 처리에 효과적이라고 알려지면서, word2vec과 CNN을 이용한 문장 분류 모델(이하 기반 모델로 표기)이 제안되었고 실제로 우수한 결과를 보여주었다. 그리고 기반 모델은 구조가 단순하고 빠른 훈련․예측 시간 및 성능 상의 장점을 가진다. 이런 장점을 이유로 기반 모델을 문서의 분류에 적용하여 성능을 검증해 보기로 하였다. 문서 분류에 적용하기 전에 기반 모델의 구조를 분석한 결과, 기반 모델에서 사용하는 CNN은 입력으로 고정 길이를 요구하여 CNN에 입력되는 단어의 개수를 데이터셋 내 최장 길이 문서가 포함한 단어의 개수로 설정해야 하기 때문에 CNN의 입력 길이를 맞추기 위해 상대적으로 길이가 짧은 대부분의 문서들은 많은 zero-padding을 추가함으로써 문서 전체를 훈련할 때 효율성이 저하되는 문제점이 발생할 수 있다는 것을 발견하였다. 따라서 기반 모델을 문서 분류에 적용 시 발생 가능한 문제점을 해결하고 성능을 향상시키기 위하여, 성능이 저하되지 않는 범위로 입력 문서의 길이를 제한하고 문서 자체를 하나의 고정 크기 벡터로 표현하는 알고리즘인 doc2vec을 활용한 추가적인 접근 방식을 구상하게 되었다. 이에 본 논문에서는 문장의 분류에 있어 성능이 입증된 word2vec을 활용한 CNN 모델을 기반으로 하여 문서 분류에 적용 시 성능을 향상시키기 위해 doc2vec을 함께 CNN에 적용하고 기반 모델의 구조를 개선한 방안을 제안한다. 먼저 단어 및 문서의 벡터 표현 생성에 앞서 문서의 토큰화가 선행되어야 하기 때문에, 문서 분류에 유용한 토큰화 방법을 선정하기 위하여 초보적인 실험을 수행하였다. 실험을 통하여, WPM(Word Piece Model)을 적용한 토큰화 방법이 분류율 79.5%를 산출하여 어절 단위와 형태소 분석을 이용한 토큰화 방법에 비해 문서의 분류에 유용함을 실증적으로 확인하였다. 다음으로 WPM을 활용하여 생성한 단어 및 문서의 벡터 표현을 기반 모델과 제안 모델에 입력하여 범주 10개의 한국어 신문 기사 분류에 적용한 실험을 수행하였다. 실험 결과, 제안 모델이 분류율 89.88%를 산출하여 기반 모델의 분류율 86.89%보다 2.99% 향상되고 22.80%의 개선 효과를 보였다. 본 연구를 통하여, doc2vec이 범주별로 문서들을 군집화 시켜주기 때문에 문서의 분류에 doc2vec을 함께 활용하는 것이 효과적임을 검증하였다.

There have been many trials to parse sentences in text to search complete and exact parses, but it is very hard because of unavoidable incompleteness of lexicon and grammar. Recently, to alleviate these difficulties, partial parsing appears as an alternative in the field. Partial parsing aims to recover syntactic information efficiently and reliably from unrestricted text, by sacrificing completeness and depth of analysis.
As a part of partial parsing, the identification of Korean numerical expressions in text is described in this paper. Numerical expressions are required in several systems such as information extraction systems and question-answering systems. One of desired characteristics of these systems is the fastness. To achieve this goal, we use a finite-state automaton, for which we could use a tool like lex. So that we could rapidly implement the system. We observed that the system is fast and correct through several experiments. To evaluate our system, we used newspaper as test collection. We achieved the recall of 90.8%, and the precision of 86.9%. Experiments show that our system is comparatively correct.목차 제1장 서론 = 1 제2장 관련 연구 = 3 2.1 정규표현과 유한상태 오토마타 = 3 2.2 유한상태 오토마타를 이용한 자연어 처리 방법 = 6

챗봇(Chatbot)이란 인공지능을 활용하여 사용자와 자연스러운 대화를 할 수 있는 대화형 시스템을 의미한다. 본 연구에서는 모바일 메신저 상에서 동작하는 챗봇을 이용한 일정 등록을 위한 대화 시스템 개발에 대한 개발 연구를 수행하였다. 기계는 사용자가 요구하는 일정 등록, 일정 수정 및 일정 삭제 등 다양한 목적에 따라 이에 맞는 API를 호출 하게 된다. DSCT6에서 제안 하였던 방법을 활용하여 호출되는 API의 종류에 따라 사람과 기계와의 대화를 task라 불리는 여러 종류의 소규모 목적 대화로 분류하였다. 또한 대화는 한국어로 이루어져 있기 때문에 사용자에 따라 띄어쓰기가 다르게 사용되고 있다. 사용자마다 다른 띄어쓰기 기준을 사용하였지만 기계는 이를 동일한 문장 및 의미로 파악 할 수 있도록 WPM(Word Piece Model) 또는 띄어쓰기를 활용하여 대화의 내용을 분석하고 한국어 기본 단위인 형태소로 나누었다. 생성된 형태소를 BOW (Bag-Of-Word)기법을 이용하여 벡터(Vector)의 형태로 변환하였다. 그 후 분류된 목적 task에서 적절한 API를 호출 할 수 있도록 동적 메모리 네트워크와 종단 메모리 네트워크를 활용하여 연구를 수행하였다. 그 결과 첫 api를 호출하는 첫 번째 task에서의 정확도는 75%, 메모리에 있는 내용을 삭제하거나 수정하는 등 저장되어 있는 정보를 바꾸는 두 번째 task의 정확도는 88%, 메모리에 저장되어 있는 정보를 조회하는 세 번째 task의 정확도 89%, 모든 task를 합쳤을 때 정확도는 90%의 성능을 확인 할 수 있었다.제 1장 서론 1 1.1 연구배경 1 1.2 연구의 대상과 목적 2 제 2장 이론적 배경 3 2.1 자연어 처리 3

본 연구에서는 스마트 디바이스(Smart Device) 기반 메신저 어플리케이션(Messenger Application)의 비정형화 데이터인 채팅 내용을 자연어 처리(Natural Language Processing)를 통해 단어를 추출하여 검색엔진을 통하지 않고 채팅 환경에서 사용자가 원하는 맛있기로 유명한 음식집(이하 ‘맛집’이라 한다)을 추천하는 시스템을 제안한다.
데이터 마이닝(Data Mining) 기반으로 채팅 환경에서 자동 맛집 추천 시스템을 구현하기 위해서는 채팅 메시지의 문법적 구조를 분석하고 중요한 키워드(Keyword)를 추출하는 것과 추출된 키워드를 통해 맛집을 추천하여 정확도가 높은 맛집을 추천하는 것이 중요하다. 이는 자동 맛집 추천이 가능한 시스템을 구현하기 위한 핵심적인 요소이다.
문법적으로 중요한 키워드로서 정확하게 추출하기 위해 텍스트 마이닝(Text Mining) 기반에 Bigram과 시그니처(Signature)에 관한 내용을 제안한다. 또한 단어 유사도에 최적의 임계치를 구하고, 시스템 성능 분석을 한다. 채팅 내용에서 추출된 단어들은 기존에 주소 데이터베이스와 음식타입 데이터베이스에 있는 단어와 유사도 비교를 통해 Bigram과 시그니처를 활용하면 채팅 내용에서 그대로 추출된 단어로 추천하는 것 보다 정확한 결과 값을 도출할 수 있다. 이 때, 추출된 키워드는 주소 키워드와 음식타입 키워드이다.
맛집을 추천하기 위해 주소 키워드와 음식타입 키워드를 통해 맛집 데이터베이스에서 정보를 획득한다. 이때, 맛집 데이터베이스는 웹 마이닝(Web mining)을 기반으로 웹 크롤링(Web Crawling)을 통해 대용량의 데이터를 수집하였다. 또한 제안하는 시스템과 네이버 블로그 맛집 검색을 통해 성능 분석을 한다.
이러한 기반을 통해 향후 검색엔진을 통하지 않고 구조화된 정보를 추출하여 자동으로 맛집 추천이 가능한 시스템에 설계 방향을 제시한다.국문 요약 ·························································· 1 제 1장 서 론 ······················································ 3 제 1절 연구 배경 ····························································· 3

본 논문에서는 협력 추천 기법의 성능 향상에 목적을 두고 기존 기법들의 특징 및 문제점을 알아보고 자연어 처리 분야에서 단어의 의미를 다차원의 벡터 공간에 벡터화하여 분류하는 Word2vec를 적용한 협력추천 기법인 User2vec 모델을 제안한다.
본 논문에서 제시하고자 하는 기법은 기존 협력 추천 기법에서 사용자의 유사도를 구하는 방식을 변경하는 기법이다. 기존 사용자기반 협력 추천에서는 사용자-아이템 간 평점정보의 코사인 유사도(Cosine Similarity) 나 피어슨 상관계수(Pearson Correlation Coefficient)를 이용하여 사용자간 유사도를 계산하였다면 단순히 평점정보만 이용하는 것이 아닌 사용자의 평점 내린 날짜를 반영하여 같은 영화를 보고 평점을 내린 사용자끼리 더 연결이 되는 방식을 제안한다.
제 3장에서는 User2vec의 실제 학습하는 데이터를 소개하고 학습 데이터를 생성하는 방법에 대해 제시하고, 학습을 진행하여 생성된 벡터 공간을 이용하여 계산된 사용자 유사도를 활용하여 평점 예측하는 방법을 설명한다. 해당 절에서 데이터 생성 기법은 U2V, U2V+E, U2V+R, U2V+M와 같은 명칭을 이용하여 생성하는 기법의 특징들에 대해 설명하고, 각 생성 기법을 결합한 U2V+ER, U2V+ERM기법을 소개한다.
제 4장에서는 기존의 전통 협력 추천 시스템과의 성능 비교를 통하여서 해당 논문에서 제안하는 기법의 성능을 평가하고, 추천 시스템의 새로운 방안을 제시한다.
실험 결과, 본 논문에서 제안하는 기법 중 User2vec를 활용한 기법과 기본 기법에서 여러 생성 기법을 결합한 U2V+ERM 기법의 성능이 가장 좋았음을 확인 할 수 있었다. 데이터 생성을 어떻게 하느냐에 따라서 사용자 간의 유사도를 구하는 성능이 달라질 수 있음을 보였다.
마지막으로, 기존 기법들과 비교하여 본 논문에서 제안한 기법이 추천의 정확도를 높일 뿐만 아니라 아이템의 다양성을 높여 추천 성능을 효과적으로 향상 시킬 수 있음을 확인하였다.국문 요약 1 제1장 서 론 3 제 1절 연구의 필요성 3 제 2절 연구 내용 5

As the cyber education is essential in information society, many instructors use it in education. Although it is frequently used, there are few tools to evaluate the study result. Expecially, it is difficult to find the suvjective type evaluation technique.
To solve the problem on the subjective-type evaluation, this dissertation proposes a grading model. It is based on the marking questions method and a grading method of subjectivity. So that the design and implementation, it can help to teachers especially subjective question and grading. Therefore, they can extract the index having some information from the document. It can give automatic grades not the simple keyword matching but boolean search technique, matching functional search technique and the thesaurus to use a search technique. Actually this method provides the results similar to teacher's own grading.
Making subjective questions which the standard has ambiguity and beging objective grading method, it gives students reliable and consistent marks and teachers the standard which make questions and grade for spending less time making subjective questions and marking hour. This study is based on estimating subjective questions which develope algorithm for making various subjective question and grading automatically in order to mark applying the point height and a grading similarity for being able to grade variously. For grading which is more accurate the automation of thesaurus construction and the system which helps the right answer drawing up control of the teacher must be introduced and natural language processing is continued.목차 Ⅰ.서론 = 1 1.논문의 목적 및 필요성 = 1 2.연구의 내용 및 방법 = 2 Ⅱ.연구 배경 = 4

로봇과 인간이 언어적 상호작용을 할 때 사회적인 요소가 어떻게 적용되는지 알아내기 위한 연구가 수행되어야 이를 분석 및 모델링하여 소셜 로봇 개발에 적용할 수 있다. 본 연구에서는 사회적 대화 분석 및 대화 모델 개발에서 분석 도구 및 대화 생성에 필요한 정보 제공 모듈로서 사용할 수 있는 텍스트 기반의 사회적 대화 요소 인식기를 개발하고자 한다. 인식기를 통해 분류하고자 하는 사회적 대화 요소는 사람들이 일상적인 대화에서 상대와의 관계를 개선하거나 변화시키기 위해 사용하는 사회적인 요소(Social Strategy)을 참고 문헌 조사를 통해 정리 및 재정의 하였다.
본 연구에서 사회적 대화 요소 인식기의 개발은 텍스트 기반의 대화 데이터 확보 및 가공, 취득 대화 데이터의 발화 문장 데이터 학습을 통한 분류 모델 개발을 통해 수행하였다. 분류 모델은 나이브 베이즈(Naive Bayes), 로지스틱 회귀(Logistic Regression), SVM(Support Vector Machine)의 세 가지 분류 기법들을 이용해서 문장 데이터를 학습하고 성능 비교를 통해 나이브 베이즈 기법의 학습 모델을 선정하여 인식기 개발에 사용하였다. 본 연구의 수행을 위해 자동 대화 취득 프로그램 개발 및 대화 취득, 사회적 요소 태깅 실험 등을 함께 수행하였으며 약 600개 대화 데이터와 약 4200 발화 문장이 포함된 데이터 셋을 확보하였다. 이 데이터 셋으로 분류 알고리즘을 학습해서 F1-Score 0.78의 인식 성능과 20ms 이하 시간에서 동작할 수 있는 사회적 대화 요소 인식기를 개발하였다. 이 연구를 통해 개발한 사회적 대화 요소 인식기와 연구 과정에서 얻어진 사회적 대화 데이터 셋은 사회적 대화 분석 및 모델링 및 사회적 대화 생성 기술 개발을 위한 향후 연구에서 유용하게 사용될 수 있다.1. 서 론 ································································· 1

This paper proposes a specific modification method that is made to useful query transformation of original user input query into the information retrieval engine using Korean natural language. The question-answering training set which, obtain a proper set of question-answering sentences on world wide web, employs to find a type of question query phrases and to make a candidates for the transformation. The type of question query phrases used to analysis of question query, selection of the candidate for transformation and extraction of the focus phrase. The candidates for the transformation consist of pre-defined words or phrases used to increase the efficiency of extraction and used for transformation of original input query. Therefore, transformation method is changed from original query into substituted query for candidate words [or phrases]. In this paper, The result shows that experimental tests are more higher precision than the information retrieval engine using the original user input queries.질의응답시스템(Question Answering System)은 사용자의 질문을 분석하여 의도를 파악한 후, 사용자가 원하는 정확한 정보를 다양한 문서로부터 추출하여 답을 제시한다. 따라서 검색결과를 다시 검토하는 수고를 피할 수 있다. 질의응답시스템은 정밀한 언어처리기술, 문서검색 기술, 정보추출 기술 및 추론 기술 등이 사용된다. 입력된 질의를 분석하고 정답추출을 통한, 답변을 제시하기까지 질의응답시스템은 많은 시간이 소요된다. 또한 질의응답시스템이 제시한 답변은 사용자가 원하는 정보가 없거나 정답을 제시하지 못하는 경우가 많다. 따라서 다양한 기술을 사용하여 정답을 제시하는 질의응답시스템은 사용자를 만족시키지 못하고 있다. 최근 TRITUS 시스템이나 QASM 시스템과 같은 상용검색엔진을 활용할 수 있는 질의응답 시스템의 연구가 진행되고 있다. 이러한 시스템들은 정확히 말하면 정답추출과정이 없기 때문에 질의응답시스템이라고 할 수 없다. 즉 자연언어질문을 질의 변환하여 검색엔진으로 효율적인 검색을 하는 시스템에 불과하다. 반면에, 구글처럼 거대한 상용검색엔진은 입력된 질의를 폭넓은 자료에 대해 효율적으로 검색하면서도 정확률이 높다. TRITUS 시스템이나 QASM 시스템은 상용검색엔진을 이용하여 정답추출과정을 제외하고, 질의변환을 통해서 정답을 제시하는 질의응답시스템으로 개선할 수 있다.
본 논문에서는 기수집 된 한국어 질의응답 집합으로부터 한국어 특성을 고려하여 시간, 장소, 대상, 사람의 질의 유형별 질문구(Question Phrases)를 생성하고, 질의 유형에 따른 질문구의 변환후보(Candidate Transform)를 생성한다. 생성된 질문구와 변환후보를 이용하여 자연언어 질의에 대해 사용자 질의 재생성 알고리즘을 통한 질의변환을 제안한다. 자연언어질문과 본 논문에서 제안한 방법으로 변환된 질의를 상용검색엔진에 각각 질의로 입력한 결과, 본 논문에서 제안한 방법이 더 높은 정확률을 보였다.제 1장 서론 = 1 제 2장 관련 연구 = 4 2.1 질의응답시스템의 개요 = 4 2.2 질의응답시스템의 분류 = 5 2.2.1 지식검색 = 5

This study used morphological analysis to develop a prototype which supports
automatic mapping of nursing terminology. For that purpose, the author
developed the prototype using Arirang Analyzer (Korean morpheme analyzer),
Korean keyword extraction system, and the Basic Data of Korea Standard
Health and Medical Terminology.
The whole process of the prototype development is, in accordance with the
system development life cycle, as follows: requirements definition, design,
implementation, test, and evaluation. As for requirements definition, the current
status of health organizations were analyzed, literature was reviewed and related
data was collected from opinion’s interviews, so the purpose, configuration,
operational conditions, and main features of the prototype were proposed. The
prototype which consists of pre-processing, morphological analysis and nursing
terminology mapping was designed using Spring framework, Java 1.6, and
Apache Tomcat. It implemented to generate outcome such as mapped nursing
term’s code and its name in Korean and English. While doing the system test,
those errors were debugged and properly improved according to language
conversion rule. The system performance was evaluated with respect to
expressiveness, use rate, use frequency, and mapping rate by using selected 673
nursing statements as for evaluation sample related to one specific nursing
diagnosis. The findings of the mapped nursing terms demonstrate that 64.3% are
conceptually expressed and 30.8% are partially expressed. In addition, the using- 75 -
rate of nursing terms is 9.4% and the mapping rate between the key-words
used in the nursing statements and the suggested nursing terminology is 45.5%.
As for the further investigation, it is necessary to apply morphological
analysis, syntax analysis, and machine learning to this kind of study to enhance
the accuracy of mapping as automatic. In particular, it is also desirable to
expand nursing axis and more specific terms for the adoption to the nursing
environment in reality. These results show that this study contributes to the
nursing terminology standardization and the knowledge accumulation through
nursing records.I. 서론 1 1. 연구배경 1 2. 연구의 목적 3 3. 용어의 정의 4

정서분석은 개인의 견해, 판단, 의견, 감정, 감성 등과 같이 사실이 아닌 주관적인 글을 분석하는 연구분야이다. 인터넷 참여자의 손수제작물 (UCC) 등 주관적인 내용을 담은 대량의 데이터가 많아지고 자연어처리 기반기술, 기계학습 등 기술의 발전으로 특히 최근 다양한 정서분석 연구가 수행되어지고 있다.

이 박사논문의 목적은 정서분석의 다국어 확장이다. 다국어 정서분석의 필수적인 기능 및 역할을 위하여, 본 연구는 a) 언어간 분석결과의 비교 및 호환이 가능하고 b) 다양한 입력 언어에 옳은 분석결과를 도출할 수 있는 언어중립적인, 그리고 c) 분석 언어의 추가시 되도록 적은 자원을 요구하는 다국어 정서분석 시스템의 제안을 목표로 한다.

먼저, 본 연구는 다국어 시스템의 판별 기준(decision criteria)이 이 언어 간 유지되는 정도를 분별하는 다국어 비교호환성(multilanguage-comparability)의 정의 및 평가방법을 정의한다. 실험을 통해 제안하는 평가방법이 다국어 정서분석 시스템의 다국어비교호환성을 효과적으로 판별하는 것을 보였다.

또한, 이 논문은 다국어비교호환성, 추가언어 적용의 용이성, 그리고 전반적인 정서분석 성능의 향상을 꾀할 수 있는 언어중립적(language-neutral) 접근 방법을 연구한다. 제안하는 용어 가중치(term weighting) 기법은 정서분석에 요긴한 용어의 특징을 연구하고, 이를 용어의 식별력(discriminativeness), 중요성(prominence), 그리고 주제와의 연관성(topic-relevance)과 관계지어 통계적(statistical) 및 근접적(proximity) 가중치를 부여한다. 더불어, 언어중의적 방법론의 추가 방편으로 의미중의성해소(word sense disambiguation)를 사용하여 글의 의미를 파악하는 다국어 정서분석 시스템을 연구한다.

마지막으로, 본 연구는 자원이 풍부한(resource-rich) 언어, 특히 영어에서, 자원이 드문(resource-poor) 언어로의 전환(knowledge transfer) 방법을 연구한다. 구체적으로, 영어 정서단어 목록(sentiment lexicon)을 그래프 기반(graph-based) 알고리즘의 한 종류인 링크 분석(link analysis) 방법과 영한사전만을 사용하여 한국어 자원으로 전환하는 방법을 연구하였다.

자연어처리 연구에 이 논문이 기여하는 바는 다음과 같다. 먼저, 본 연구는 기존연구가 고려치 못한 다국어 비교호환성의 개념 및 판별의 중요성을 가장 처음으로 주장하였고 그 분별력 및 효용성을 실험적으로 보였다. 두 번째로, 본 연구는 언어중립적 연구의 방편으로 다양한 용어가중치 기법 및 중의성해소 기법을 정서분석 연구에 적용하여 다국어 정서분석 시스템의 성능향상을 이루었다. 이 논문의 마지막 주요 기여는 대상언어의 최소한의 자원만을 가지고 원천언어의 정서분석 자원을 효과적으로 전환하는 방법론을 제안하고, 더불어 다양한 영어자원으로부터 한국어 정서분석자원을 구축하였다는 것이다.Sentiment analysis is the analysis of non-factual statements, i.e. private views such as attitudes, judgments, moods, and opinions. It is a relatively recent topic that has received much attention from different areas of study such as Natural Language Processing and Information Retrieval due to increasing demand and the availability of technology and resources.

The objective of this dissertation is a multilingual extension of sentiment analysis. To address the essential capabilities of multilingual sentiment analysis, this dissertation proposes a multilanguage-comparable sentiment analysis system capable of a) producing comparable outcomes across languages, b) yielding excellent performances across all input languages, and c) adapting the system to new languages with low resources.

This dissertation defines and measures the multilanguage-comparability of multilingual sentiment analysis systems, which is an analysis system's ability to retain its decision criteria across different languages. The proposed evaluation approach successfully distinguished the multilanguage-comparability of multilingual sentiment analysis systems.

Our work also develops language-neutral sentiment analysis approaches that can aid multilingual sentiment analysis in multilanguage-comparability, easy adaptation to new languages, and overall performance. The proposed term-weighting approach explores the characteristics of good sentiment terms and various statistical term weighting schemes that measure discriminativeness, prominence, and topic-relevance of sentiment words. We develop multilingual sentiment analysis with word sense disambiguation to analyze deeper semantics of texts.

This dissertation develops approaches that transfer knowledge from resource-rich languages to resource-poor languages. One of our approaches is to transfer the sentiment lexicon available in English to other languages using only a bilingual dictionary, and the other is to utilize the English WSD to disambiguate languages other than English. Our WSD-based sentiment analysis utilizes already available resources in English to disambiguate texts in languages other than English.

The contributions of this dissertation to the academic research community are as follows. First, this work is the first to introduce the notion of multilanguage-comparability in multilingual text analysis. While no previous work in text analysis had considered the merits of multilingual text analysis with no regard to the language differences, we pay attention to the virtues of preserving the judgment principles across languages. Secondly, our proposed language-neutral approaches successfully utilized various term weighting methods and word sense disambiguation to sentiment analysis modeling and improved the overall performances of sentiment retrieval and classification tasks. The Third key contribution of our work is our knowledge transfer approaches for sentiment resources, which produce fairly good quality sentiment resources in target languages with minimal linguistic resources.I Introduction 1 1.1 Research Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.2 General Approaches and Main Results . . . . . . . . . . . . . 4 1.2 Contribution of this Work . . . . . . . . . . . . . . . . . . . . . . . . 9

The international organization ICANN that is responsible for the regulation of internet domain names has launched a new domain service called "new gTLD" in which the TLD (Top Level Domain) strings are open to include unicode and terms of any nature. The TLD string can be company names, brand names, regional names, and other linguistic words. As a result the domain names can carry more precise meaning and the important change is that the referents of names are expanded to the units of services, contents, and products.
Minor scripts such as Hangul is about to fall behind the new IT service because of the cost required for the registration into ICANN's system. Considering the effects the new naming service will bring to the society there has arisen a need for a mechanism to deal with the new move by ICANN toward the future of information architecture.
The dissertation proposes an abstract model for a regional version of ICANN's new gTLD program from which various implementations can be derived. We give an implementation scheme to demonstrate the effectiveness of the model. An expanded name service model is also proposed that processes the binding between names and resources dynamically. The dynamic binding modifies the basic concept of naming service by allowing names to be bound to different resources depending on the conditions present in the name expressions.
Regional new gTLD model will require one to expand naming space to smaller units of services than sites. As the units of referents can be objects in the world, the resources to bind with names may need to be expanded. In this dissertation a new resource type is suggested in light of the new naming service. Dial addresses can return phone numbers as a result of binding. We demonstrated the utility needed to implement the dial addresses in the context of the new expanded address model.
The dissertation gives a set of algorithms to define the functions of regional implementation of the new gTLD program. The work is meaningful in understanding the regional new gTLD program. The working proof is given by an implementation instance that involves DNS, Complementary Root, and Complementary Name Server.
Dynamic binding and Dial addresses are developed to strengthen the new address model by expanding the functionality of binding and resource space respectively. An algorithmic explanation for dynamic addresses is made from which an implementation can be readily made. The notion of dynamic binding can be significant in the future development of address services as names become a link to a rich set of intelligent services. For example, names can lead to services customized for users, time, locations or any complex conditions underlying the need of users.
We have presented a procedure of implementation scheme and message protocol based on DNS messages as a working proof. The works can be a foundation of future naming service design and implementations.국문요약 1. 서론 1 2. 관련 연구 5

최근, 스마트 폰(smart phone)과 같은 다양한 스마트 디바이스(smart device)를 기반으로 한 서비스가 증가하고 있다. 다양한 스마트 디바이스를 기반 한 서비스의 효과적인 사용을 위해 자연어 기반의 인터페이스의 필요성이 증가하고 있다. 대화 시스템은 자연어(natural language)를 이용하여, 컴퓨터와 사용자 사이에 정보를 교환하거나 특정한 목적을 수행할 수 있는 시스템이다. 대화 시스템의 필수적인 요소는 사용자가 말한 발화(utterance)를 이해할 수 있어야 하며, 이를 위해 사용자가 말한 발화의 화행을 확인해야 한다. 화행(speech-act)이란 사용자의 발화 속에 포함되어 있는 화자에 의해 의도된 언어적 행위이다. 발화가 수행해야할 대화 내에서 목적을 결정하기 때문에 해당 발화의 정확한 화행을 효과적으로 확인하는 것이 매우 중요하다.

화행 분류는 전통적인 분류 문제를 다루고 있기 때문에, 기존의 화행 분류에 관한 연구는 분류기를 잘 구축하기 위해 지도 학습 방법을 사용한다. 많은 연구에서 지지벡터기계 기반의 분류기를 사용하고 있다. 하지만, 지지벡터기계 기반의 분류기는 2가지 문제점이 있다. 먼저, 분류할 범주의 발화 수가 많은 분류기로 분류 결과가 편향되는 문제가 존재하며, 다음으로 모든 분류기의 분류 점수가 음수인 경우 분류가 잘못될 확률이 높은 문제가 존재한다. 본 논문에서는 대표적인 기계학습 방법인 지지벡터기계(support vector machine, SVM)와 변환기반 학습(transformation-based learning, TBL)을 조합한 화행 분류 방법을 제안한다. 이를 위해, 화행별 발화 비율 기반의 분류 우선순위를 고려한 지지벡터기계를 이용하여 화행을 분류하고, 오답일 확률이 높은 분류 결과에 대해서 변환기반 학습을 통해 생성된 보정 규칙을 적용함으로써 화행분류 성능을 개선하는 방법을 제안한다. 본 논문에서 제안하는 방법을 실험을 통해 평가하였으며, 이는 우선순위를 고려하지 않은 기존의 화행 분류보다 성능이 향상되었다. 결과적으로 우리는 제안하는 방법이 지지벡터기계의 2가지 문제를 잘 해결한다고 판단한다.Ⅰ. 서 론 1 Ⅱ. 관련 연구 4 1. 대화 시스템 4 가. 언어 인식 모듈(Spoken Language Understanding) 4 2. 화행의 종류 5

As it has become an age of the fourth industrial revolution featuring hyper-connectivity and super-intelligence, the convergence of cutting-edge information and communication technologies such as AI, IoT, big data, mobile, and others all over the economy and society results into an innovative change. AI chatter robot that is a key field of the fourth industrial revolution is a service for chatting with others through the use of algorithms or machine running made by a human being and functions of AI and is positively developed in various areas because it is able to reduce expenses with no space-time restrictions. The online shopping market has recently continued to grow further, and especially, ‘Chatter Robot’ service through shopping has come to the fore because it makes an effort to provide shopping experiences and services that are familiar to customers and attract smart customers. Chatter Robot is a service that offers information about products, necessary customized product information, or notifications to customers. Furthermore, this service is highly anticipated because it can satisfy requirements of the customers by using big data and provide high-quality services as customer-related information is collected, and based on this, it is highly likely to be spread out in other areas.
This paper is about the effect of familiarity of Shopping Chatter Robot on preference and product purchasing intention as an empirical study that subdivided, tested, and analyzed how a difference in familiarity of intelligence services affected service preference and product purchasing intention depending on involvement of Chatter Robot users into the product. For this, we figured out contextual factors for information search by chatting based on a meaning formation theory of Durbin and personification and self-disclosure in order to control familiarity based on a social penetration theory. Furthermore, we investigated involvement, preference, and purchasing intention and academically considered them.
A test was carried out by a Google online questionnaire for four days from November 19th, 2017 to November 23th. There were 130 participants of men and women in their twenties and thirties who frequently used a smart phone a lot. We adjusted familiarity of intelligence services through self-exposure and personification depending on the product involvement and controlled and presented details of conversation. After then, we estimated the resulting service preference and product purchasing intention. Factors of self-exposure and personification exposed and suggested photos, personalities, name, interests, experiences, expressions, and others about intelligence services in a profile based on a social penetration theory. Then, we chose products depending on the product involvement based on precedent studies.
The result of examining preference and product purchasing intention depending on the degree of familiarity in terms of products of high and low involvement may be summed up as follows. First, it turned out that highly-familiar intelligence services had higher level of preference and product purchasing intention than lowly-familiar intelligence services had. Second, products of high involvement were more affected by familiarity of intelligence services than those of low involvement. Third, the preference depending on familiarity was related to the product purchasing intention. Also, in case that highly-familiar intelligence services offered products of high involvement, it was the most influential factor, whereas in case that lowly-familiar intelligence services provided products of low involvement, it was the least influential factor.
Through this research, we could provide a guide about the adjustment of familiarity of intelligence services when designing Chatter Robot for shopping. Also, it might be expected that it’s possible to systematically suggest familiarity depending on shopping recognition for certain products. In addition, it would be hoped that more satisfied Chatter Robot might be offered to users because the service was expected to be enlarged and applied into various areas in developing Chatter Robot in the future.초연결(Hyperconnectivity)과 초지능(Superintelligence)를 특징으로 하는 제4차 산업혁명의 시대가 되면서 인공지능, 사물 인터넷, 빅데이터, 모바일 등 첨단 정보통신 기술이 경제·사회 전반에 융합되어 혁신적인 변화가 나타나고 있다. 4차 산업혁명의 핵심 분야인 인공지능(AI) 챗봇은 사람이 만들어낸 알고리즘이나 머신러닝을 활용 및 인공지능 기능으로 대화할 수 있는 서비스를 말하며 시공간의 제약 없이 비용도 줄일 수 있어 다양한 분야에서 활발하게 개발되고 있다. 최근 온라인 쇼핑 시장은 계속 성장하고 있으며 특히 소비자들에게 밀접한 쇼핑 경험과 서비스를 제공하여 똑똑한 소비자를 잡으려는 노력들이 쇼핑을 통한 ‘챗봇’ 서비스가 주목을 받고 있다. 챗봇이란 소비자에게 상품에 대한 정보들을 필요한 맞춤 제품 정보 또는 알림 등을 제공하는 서비스이며 고객 관련 정보가 쌓일수록 빅데이터를 활용해 고객의 요구 사항을 충족시켜 양질의 서비스를 제공할 수 있기 때문에 챗봇에 있어서 기대가 높으며 이를 바탕으로 타 분야의 확산 가능성 또한 매우 높다.
이 연구는 ‘쇼핑 챗봇의 친밀도가 선호도와 제품의 구매의도에 미치는 영향’에 대한 것으로, 챗봇을 사용하는 사용자에게 제품의 관여도에 따라 정보원의 친밀도의 차이가 제품의 서비스 선호도와 제품의 구매의도에 어떤 영향을 주는지 세분화하여 실험·분석한 실증연구이다. 이를 위해 채팅을 통한 정보탐색을 위함 맥락적 요소를 더빈의 의미 형성 이론을 들어 알아보고 친밀도를 조절하기 위하여 의인화와 자기 노출을 사교적 침투 이론을 들어 알아보았다. 또한 관여도, 선호도, 구매의도에 대하여 알아보아 이론적으로 고찰하였다.
실험기간은 2017년 11월 19일부터 23일까지 총 4일간 진행되었으며 실험 자극물은 온라인 구글 설문지를 통하여 실시되었다. 참가자는 130명으로, 평소 스마트폰 사용을 많이 하는 20-30대를 대상으로 하였다. 실험은 제품의 관여도에 따라 정보원의 친밀도를 자기노출과 의인화로 조절하고 대화의 내용은 통제하여 제시한 후 그에 따른 서비스의 선호도와 제품의 구매의도에 대하여 측정하였다. 자기 노출과 의인화 요소는 사교적 침투이론을 바탕으로 정보원의 사진, 성격, 이름, 관심사, 경험, 감정등을 프로필에 노출하여 제시하였고 제품 관여도에 따른 제품의 선정은 선행연구를 바탕으로 선별하였다.
고관여도와 저관여도의 제품을 친밀 정도에 따라 선호도와 구매의도를 살펴본 결과는 다음과 같다. 첫째, 고친밀의 정보원의 경우 저친밀의 정보원보다 선호도와 구매의도가 높은 것으로 나타났다. 둘째, 저관여의 제품보다 고관여의 제품이 정보원의 친밀도의 영향을 더 받는 것으로 나타났다. 셋째, 친밀도에 따른 선호도는 제품의 구매의도와 연관이 있으며, 고관여의 제품을 고친밀의 정보원이 제공할 경우에 그 영향력이 가장 크고 저관여의 제품을 저친밀의 정보원이 제공할 경우에 그 영향력이 가장 적게 나타났다.
이 연구를 통해서 쇼핑을 위한 챗봇의 설계 시 정보원의 친밀도의 조절에 대한 가이드를 제공할 수 있었고, 어떤 제품을 위한 쇼핑인지에 따라 전략적으로 친밀도를 제안할 수 있을 것으로 기대된다. 또 추후 챗봇의 개발에 있어 다양한 범위에 확대 적용할 수 있을 것으로 기대되어 사용자에게 더욱 만족도 높은 챗봇을 제공하기를 바란다.Ⅰ. 서론 1 1. 연구의 필요성 1 2. 연구의 목적 3 3. 논문의 구성 4 Ⅱ. 챗봇과 쇼핑서비스6

최근에 웹이나 다른 플랫폼들에서 텍스트 정보의 양들이 급격하게 증가하고 있다. 이런 상황은 텍스트 마이닝의 필요성이 크게 대두되었으며, 이에 발맞춰서 문서를 처리하는 데 필요한 자연어 처리 기술은 항상 개선되어 왔다. 한 문서 안의 키워드들은 그 문서의 핵심적인 내용을 요약한 것으로 여겨 질 수 있다 그래서 이런 핵심문구들의 추출은 문서 요약, 클러스터링이나 정보 획득과 같은 많은 응용분야를 개선시키기 위해 활용되어 왔다.
최신 핵심문구 추출 방법들은 대개 핵심문구를 식별하기 위해 많은 수로 이뤄진 속성 집합을 이용했다. 그러나 그러한 속성 집합은 모든 핵심문구들을 다룰 수 가 없는데, 이는 핵심문구들은 유사한 패턴과 문체속성들을 공유하지 않기 때문이다. 그러나 이런 핵심문구들의 특징과는 달리 비핵심문구들의 후보들은 보통 많은 유사한 패턴과 문체 속성들을 공유하고 있다.
이러한 점들에 동기를 부여받아서 이 논문은 안티 패턴을 소개하고 안티 패턴을 이용한 의존도 그래프 기반의 키워드 추출 방법을 제안 한다. 안티 패턴은 원하지 않는 후보 어구들을 걸려내는 데 목적이 있다. 또한 그래프 기반의 핵심문구 추출에서 널리 사용되는 단어 동시 발생 그래프는 몇몇의 구문론적인 관계를 가진 단어들을 연결시킬 수 없었다. 이러한 문제점이 있어서 우리는 새로운 의존도 그래프를 제안했다.
실험을 위해 우리는 동시 발생 그래프와 의존도 그래프를 비교하는 실험들을 수행했고, 현존하는 다른 키워드 추출 방법들을 비교했다. 그리고 얼마나 안티 패턴들이 실험결과들에 영향을 미치는 지 발견했고, 그래프 기반의 자율학습적인 핵심문구 추출의 성능을 향상시켰다. 실험을 진행 후에, 우리가 제안한 방법이 핵심문구를 추출하는 일에서 다른 방법들보다 우수한 성능을 보여줬다는 걸 알 수 있다.Recently, a size of text information on the web and other platforms has been increasing dramatically. It requires text mining and natural language processing techniques are always better than before. Keywords can be considered as a summary of a document. Therefore, keyphrase extraction has been utilized to improve many applications such as document summarization and clustering, information retrieval.
The state-of-the-art keyphrase extraction methods are categorized into unsupervised or supervised methods. All of keyphrase extraction methods follow the two-step method: First step is generate all possible candidates, and second step is either rank(unsupervised) or classify(supervised) candidates. Supervised methods often used the rich feature set to identify keyphrases. However, those rich feature set could not cover all keypharses because keyphrases share no similar pattern and no stylistic features. Non-keyphrase candidates are often sharing many similar pattern and stylistic features. In this case, the patterns to identify non-keyphrases is very robust than the rich feature set. Most of researchers for unsupervised methods used only a word co-occurrence graph and paid very little attention for a dependency graph. A dependency graph has a number of advantages and better accuracy than a word co-occurrence graph.
By solving these two problems, we propose a dependency graph-based keyword extraction method using anti-patterns which is introduced in this thesis. A proposed approach includes the following three tasks: First, generate useful and strong anti-patterns which aim to filter out unwanted and suspicious candidate phrases. Second, eliminate candidates which match any anti-pattern. Third, calculate importance score of each survived candidate and rank them by using TextRank algorithm and the proposed modification for a basic dependency graph. This graph has a number of desirable advantages.
In experimental works we have conducted the experiments showing comparison between a co-occurrence graph and dependency graph, compare with other existing methods for keyphrase extraction, and find out the experimental results of how anti-patterns impact and improve the performances of a graph-based unsupervised keyphrase extraction. Finally, our proposed method outperformed other existing methods on keyphrase extraction task. This research has proved that the strong and useful anti-patterns can be effectively used in detecting non-keyphrases on any domains such as news, biomedical, clinical, and other domains. Proposed dependency graph is also proved to be significantly useful in term weighting approach which is currently important part of most well-known text classifications.Chapter 1 Introduction 1 1.1 Background and Motivation 1 1.2 Organization of the Thesis 4 Chapter 2 Related Work 5

오랜 학습시간과 과적합 등의 문제로 인해 한동안 사람들에게 외면당했던 딥러닝은 2000년대 이후 이러한 문제들이 해결되면서 다시 주목받기 시작했다. 특히 음성 인식, 이미지 인석, 자연어 처리 등의 분야에서 딥러닝을 활용한 연구가 활발히 진행되고 있으며 실제로도 매우 높은 성과를 보이고 있다.
기계학습의 일종인 딥러닝은 인공신경망을 기반으로 한다. 입력 층과 출력 층 사이에 여러 개의 은닉 층을 가지고 있는 구조로, 인공신경망 네트워크를 더 복잡하게 만들어 놓은 모델로 볼 수 있다. 복잡한 비선형 관계들에 대한 모델링이 가능하며, 예측력이 높다는 것이 특징이다.
본 연구에서는 딥러닝의 한 종류인 Convolutional Neural Network (CNN)를 중심으로, 딥러닝의 성능을 확인하고자 한다. CNN은 이미지 인식 분야에서 매우 높은 성능을 보이는 네트워크로, 대표적인 모델로는 LeNet(1998)과 AlexNet (2012)이 있다. CNN이 어떤 방식으로 이미지를 인식하고 분류하는지 보기 위해 실제 이미지 데이터를 활용하여 분석을 진행하였고, 그 성능을 확인할 수 있었다. 또한 CNN의 성능에 영향을 미치는 매개변수와 다른 요소들에 대해 살펴보며, 모델의 성능을 높이기 위한 방법에 대해 연구하였다.Deep learning, which has been ignored by people for a long time due to long learning time and overfitting, has been getting attention again since the 2000s when these problems were solved. Particularly, research using deep learning has been actively conducted in areas such as speech recognition, image recognition and natural language processing. And it is doing very well. Deep learning, a type of machine learning, is based on artificial neural networks. It has multiple hidden layers between input layer and output layer, which can be seen as a model that makes artificial neural network more complicated. It is possible to model complex nonlinear relations and it has high prediction ability.
In this study, we try to check the performance of deep learning by using Convolutional Neural Network (CNN), which is a kind of deep learning. CNN is very high-performance network in image recognition, with LeNet (1998) and AlexNet (2012) as typical models. In order to see how CNN recognizes and classifies images, the analysis was conducted using actual image data and its performance can be confirmed. In addition, we studied the method to improve the performance of the model by examining some parameters and other factors which influence the performance of CNN.I. Introduction 1 II. Description 3 A. Convolutional Neural Network 3 B. Algorithms 3 C. Option 5

LKB(Lexical Knowledge Base)는 단어의 사전적인 지식뿐만 아니라 의미적인 지식도 포함하고 있는 지식 베이스이며, 구문 분석, 의미 분석과 같은 자연어처리의 여러 분야에 도움을 줄 수 있다. 이러한 LKB는 많은 양의 어휘들로 구성되어 있기 때문에 수동으로 구축하는 것은 많은 어려움이 있다. 따라서, 근래에는 LKB를 자동으로 구축하는 연구들이 진행되고 있다. 이렇게 LKB가 포함하고 있는 정보가 큰 만큼, LKB에 포함된 단어들 사이에 존재하는 모든 의미 관계(semantic relation)를 자동으로 추출하는 것은 거의 불가능한 작업이다.
어떤 두 단어가 주어졌을 경우에 두 단어 사이를 직접적으로 연결해주는 의미 관계가 LKB에 포함되어 있지 않을 수 있다. 이런 경우에 LKB에 포함된 정보를 이용하여 간접적인 경로를 통한 두 단어 사이의 의미 관계를 결정해주는 방법이 필요하다.
두 단어를 연결하는 의미 관계를 결정할 때 LKB에 포함된 정보에 가중치를 이용한다면 검색을 보다 효율적으로 할 수 있다. 또한, 가중치를 이용함으로써 여러 후보가 등장하였을 때 가장 적합한 의미 관계를 선택할 수 있는 객관적인 기준이 될 수 있다. 본 논문은 두 단어 사이의 의미 관계를 결정하는 기존의 방법론들을 본 논문이 기반으로 하는 LKB에 알맞은 형태로 변형하여 적용한다.LKB(Lexical Knowledge Base) is the knowledge base containing not only dictionary information but also semantic information of a word and give help to other areas of natural language processing such as structural analysis or semantic analysis. Since LKB contains a lot of words, building LKB by hand is very difficult. Researches on automatic LKB building, therefore, are ongoing. It is nearly impossible to extract all the possible semantic relations between word contained in LKB.
When two words are given, LKB may contain no direct semantic relation between them. In this case, the method of determining semantic relation between the words by using the information contained in LKB is needed.
By using the weight for the information contained in LKB, search can be done efficiently. Moreover, when more than one candidate exist, the weight can be used as a standard for selecting the best choice. This thesis applies a modified version of the earlier methods to the LKB used in this thesis.

현재까지 기업에서는 정형화된 데이터를 주 대상으로 고객을 세분화하고 관리하고 있는 실정이나, 최근에 들어 텍스트에서 의사결정에 수렴할 만한 가치 있고 유용한 정보를 찾아내는 작업의 중요성이 부각되고 있다. 텍스트 데이터는 수치 데이터와 달리 자연어로 구성된 비구조적 데이터로 이루어져 있어, 텍스트 간의 잠재된 정보를 찾는 방법이 텍스트마이닝이다.
본 연구에서는 텍스트마이닝을 사용하여 패션회사의 VOC에 대해 텍스트 간의 연관성을 토대로 분류 및 군집화한 결과를 나타내었다. 이 연구 결과를 바탕으로 고객관계관리(CRM)를 넘어선 고객경험관리(CEM)를 하기 위한 하나의 방법을 제시하였다.표 목차 iii 그림 목차 iv Ⅰ. 서론 1

컴퓨팅 파워의 개선, 인터넷과 소셜미디어, 모바일 디바이스 등의 보급을 통한 수많은 데이터의 축적, 딥러닝을 비롯한 기계학습 알고리즘의 발전으로 인공지능 기술이 어느때보다 더욱 큰 성과를 보이고 있다. 음성 인식, 컴퓨터 비전, 자연어 처리 등의 분야에서 인공지능은 이미 인간에 필적하거나 혹은 인간을 뛰어넘는 성능을 보이고 있으며, 자율주행, 로봇, 의료서비스 등의 다양한 분야에 적용되어 우리의 삶에 많은 변화를 가져올 것으로 기대된다.
하지만 알고리즘 측면에서의 기술적인 발전에 비해 인공지능의 인간공학적 요소와 사용자 경험에 대한 관심과 논의가 상대적으로 부족한 편이다. 이에 이 연구는 인간컴퓨터상호작용의 관점에서 인공지능과 사용자가 상호작용 하는 방식에 대해 다층적이고 통합적으로 이해하는 것을 목표로 하고 이를 통해 인공지능 기반의 사용자 인터페이스 디자인을 위한 함의점을 도출하는 것을 목표로 한다. 특히 이 논문은 인공지능 기술을 이용한 알고리즘 기반의 시스템과 사용자의 상호작용에 주목하고, 이를 대상으로 인지, 해석 및 평가, 지속적인 인터랙션, 실용적인 어플리케이션을 주제로 한 네 단계의 연구를 기획하고 진행하였다.
첫번째 연구는 인공지능 알고리즘에 대한 사람들의 선험적 인식을 조사하였다. 연령과 성별, 직업의 다양성을 고려하여 인구통계학적 대표성을 갖는 참가자를 모집하였으며, 이들을 대상으로 인공지능 인식에 대한 정성적 방식의 조사를 진행하였다. 조사 결과 사람들이 인공지능 알고리즘에 대해 갖는 선입견과 고정관념을 확인할 수 있었으며, 사람들이 인공지능을 의인화 할 뿐만 아니라 타자화 하는 경향이 있음을 확인할 수 있었다. 또한 인공지능 알고리즘과 사용자의 관계에서 지속적이고 전체적인 경험이 중요함을 확인하였다.
두번째 연구는 인공지능 알고리즘에 대한 사용자의 해석과 평가에 관한 것이다. 이를 위해 이미지의 미적 점수를 계산해주는 신경망 기반의 알고리즘이 구현된 AI Mirror라는 연구 프로토타입을 제작하였으며, 인공지능/기계학습 분야의 전문가, 사진전문가, 일반인으로 구분된 세 집단의 사용자를 모집하여 실험을 진행하였다. 사용자는 저마다 다른 배경 지식을 반영해 인공지능 알고리즘을 해석하고 평가하는 경향을 보였다. 사진전문가 집단이 알고리즘을 가장 높은 정도로 해석하였으며 합리적이라고 여긴 반면, 인공지능/기계학습 전문가 집단은 가장 낮은 정도로 알고리즘을 해석하고 평가했다. 사용자는 다양한 전략을 통해 인공지능 알고리즘의 원리를 추론하고자 하였으며 이를 통해 인공지능 알고리즘과의 차이를 좁혀갈 수 있었다. 또한 사용자는 인공지능 알고리즘과 쌍방 소통을 통해 의견을 교환하고자 하는 니즈를 표출하였다.
세번째 연구는 인공지능 알고리즘과 사용자가 공동의 목표를 두고 지속적인 인터랙션을 이어가는 과정에 대한 이해를 목표로 하였다. 사용자가 일부 그린 물체를 완성하고 스케치에 색칠을 자동으로 완성해주는 신경망 기반의 알고리즘 API를 이용하여 DuetDraw라는 리서치 프로토타입을 제작하였고, 정량 및 정성적 방법으로 이에 대한 사용자 평가를 진행하였다. 사용자 평가 결과 사용자는 인공지능 알고리즘과의 협업 과정에서 인공지능으로부터 단순한 피드백 보다는 자세한 설명을 제공받기를 원했으며, 알고리즘과의 관계에서 항상 주도적인 위치에 있고자 하였다. 인공지능과의 인터랙션은 과업 수행에 대한 사용자의 예측가능성, 이해도, 통제력을 낮추는 경항이 있었지만, 사용자에게 상대적으로 높은 사용성을 제공하였을 뿐만 아니라 사용자가 전반적으로 만족스러운 경험을 할 수 있도록 하였다.
끝으로, 네번째 연구는 보다 실용적인 어플리케이션을 제작하여 이에 대한 사용자 인터랙션을 이해하고자 하였으며, 이에 최근 큰 각광을 받고 있는 로봇저널리즘 기술을 구현한 NewsRobot을 제작하였다. NewsRobot은 2018 평창동계올림픽의 주요 경기 결과를 자동으로 수집하고 요약하며, 내용과 형식을 각각 종합뉴스-선택뉴스, 텍스트-카드-동영상으로 달리하여 뉴스를 생성한다. 정량 및 정성적 방법의 사용자 평가 결과, 선택뉴스가 종합뉴스에 비해 낮은 신뢰도를 보였음에도 불구하고 선택뉴스에 대한 사용자의 높은 선호도를 확인할 수 있었다. 또한 멀티미디어 모달리티가 높아질수록 사용자의 뉴스에 대한 만족도가 높아지지만 사용자의 기대수준에 어긋난 경우 오히려 낮은 평가를 받는 것을 확인하였다. 사용자는 알고리즘이 자동으로 생성한 뉴스에 대해 정확하고 객관적이라고 평가하였으며, 빠른 뉴스 생성 속도와 다양한 정보 시각화 요소에 대해서도 만족감을 드러냈다.
본 연구는 이 네 가지 연구의 결과들을 바탕으로 인간-인공지능 상호작용에 대한 다양한 시사점들을 도출하였으며, 인공지능을 이용한 알고리즘 기반의 시스템의 사용자 인터페이스 디자인을 위한 함의점들을 제안한다.The recent development of artificial intelligence (AI) algorithms is affecting our daily lives in numerous areas. Moreover, AI is expected to evolve rapidly, bringing tremendous economic value. However, compared to the attention these technological improvements receive, there is relatively little discussion on human factors and user experience related to AI algorithms. Thus, this thesis aims to better understand how users interact with AI algorithms. Specifically, this work examined algorithm-based human–AI interaction in four stages, through various modes of human-computer interaction: The first study investigated how people perceive algorithm-based systems using AI, finding that people tend to anthropomorphize as well as alienate them, which is distinct from their perceptions of computers. The second study investigated how people interpret and evaluate the output from AI algorithms through a prototype, AI Mirror, which assigned aesthetic scores to images based on a neural network algorithm. The results revealed that people interpret AI algorithms differently based on their backgrounds, and that they want to understand and communicate with AI systems. The third study investigated how people build a sequence of actions with AI algorithms through a mixed method study using a research prototype called DuetDraw, a drawing tool in which users and AI can draw pictures together. The results showed that people want to lead collaborations while hoping to get appropriate instructions from the AI algorithm. Lastly, a case study on a practical application of AI was conducted with a research prototype called NewsRobot, which automatically generated news articles with different content and styles. Findings showed that users prefer selective news and multimedia news that have more functionality and modality, but at the same time they do not want AI to boast about its ability. With these distinct but intertwined studies, this thesis argues the importance of understanding human factors in the user interfaces of AI-based systems and suggests design principles to this end.1 INTRODUCTION 1 1.1 Background 1 1.2 Research Goal 10 1.3 Research Questions 11 1.4 How People Perceive Algorithm-based Systems Using Artificial Intelligence 12

Time is one of the important parts of any information space in a text document and in the same state very efficient at information retrieval systems such as identifying, document recognition and similarity searches. Text documents have temporal characteristics which are related to historical events. Such documents can be mapped to their corresponding time period.
This thesis designs and implements paragraph focus time which is explaining the time period to which documents content refers and which is considered to documents creation time.
Paragraph focus time describes the value of sentence resources and weighed as a supplementary measure to the time of the creation or duration.
We tested our algorithm in Web and WIKI dataset using word2vec and apply it in deep learning method and compare it with other algorithms. The reason that we adopt word2vec is finding a relationship and similarity between words in document and paragraph and we used F-measure to evaluate the performance of our algorithm.시대적/시간적 요소는 텍스트 문서가 가지고 있는 중요한 특징 중에 하나이며 다양한 정보 검색 분야에 활용되고 있다. 텍스트 문서는 역사적 사건과 관련된 시대적 요소를 가지고 있으며 이러한 요소는 해당 텍스트 문서의 대표적 시대를 유추할 수 있게 하는 중요한 요소이다.
본 연구에서는 텍스트 문서에서 출현되는 시대적 요소와 텍스트 문서 생성 시간을 활용하여 문단의 시대를 추정할 수 있는 방법을 제안한다. 이를 위해 문단의 문장들이 가지고 있는 시대적 요소와 이를 통한 시대적 가중치를 계산하였다.
실험을 위해 Web과 WIKI 데이터 셋을 사용하였으며 Word2Vec을 활용하여 텍스트 문서의 특징을 추출하였다. 또한, 딥 러닝 중 CNN(Convolutional Neural Network) 기법을 사용하였으며 F-measure를 통해 다른 알고리즘과 비교하였다.1. Introduction 1 2. Related work 5 2.1. Earlier research 5 2.2. Document pre-processing 8 2.3. Temporal information extraction 10

지능시스템이라 함은 지능적 기계 또는 지능적 컴퓨터 소프트웨어와 하드웨어에 의한 시스템을 통칭하는 말이다. 이는 컴퓨터를 사용해서 인간의 지능을 이해하고 구현하는 작업들과 관련되어있으며 생물학적인 방법에만 국한되지는 않는다. 또한 지능형 메커니즘이란 시스템 구현에 있어서 지능적 개념을 설명할 수 있는 알고리즘이나, 시스템의 운영에 있어서 지능적 개념의 기계동작을 통해 지능을 갖고 있다고 인식할 수 있는 판단근거의 체계를 말한다.
지능형 메커니즘은 신경망, 퍼지, 유전알고리즘, 면역 시스템, 진화연산 등의 기법들을 활용하여 다양한 분야에서 연구되고 있다.
본 논문에서는 산업화에 응용 가능한 지능형 메커니즘을 연구하고 이런 메커니즘을 실제 산업응용 시스템에 적용하여 최종적으로 지능형 메커니즘과 개발된 지능형 산업응용 시스템의 효용성을 입증한다. 이를 위해 다양한 분야의 산업응용 시스템에서 해결해야 할 문제점을 찾아 지능형 메커니즘을 적용하여 지능형 산업응용시스템을 구현하였다.
서론은 지능형 메커니즘을 정의하고, 연구배경 및 목적을 기술하였다.
본론은 지능형 산업응용 시스템의 적용분야와 적용된 지능형 메커니즘의 구현방법을 기술하였다. 그리고 실제 산업 현장에서 사용하고 있는 산업응용 시스템을 분석하여 새로운 지능형 산업응용 시스템을 개발하였다.
결론으로는 지능형 산업응용 시스템 개발을 통해 얻은 연구결과를 정리하였다.Intelligent system means intelligent machine or systems of the engineering field using intelligent computer software and hardware. It relates with the work of understanding Human-intelligence, but it is not limited by biological method only.
Intelligent mechanism means algorithm and systematical way for implementation of intelligent system. That is recognition of decision by intelligent ideas and mechanical operations.
Intelligent mechanism has been researched and applied in variety of fields including neural networks, fuzzy logic, genetic algorithms, immune system, evolution processing, and so on.
The purpose of this dissertation is to explore some new industrial application fields to apply intelligent mechanism and to verify it is used practically. For this reason, some problems those must be solved in industrial application system are found and applying intelligent mechanism, intelligent industrial application system is implemented.
The introduction part consists of the definition of intelligent mechanism, back-ground of research and purpose.
In the main subject part of this dissertation, the industrial field using intelligent mechanism and applied methods are described and new intelligent industrial application systems is developed by research in industrial application systems field.
The conclusion part contains comments about the result and future work plan of this research.Ⅰ. 서론 = 1 1. 연구의 배경 = 1 2. 연구의 목적 = 6 3. 논문의 구성 = 7 Ⅱ. 산업용 지능형 메커니즘 = 12

분자 생물학을 통한 연구가 심화되면서, 생물학 정보는 기하급수적으로 늘어나고 있다. 그에 따라 바이오메디컬(생물학, 의학) 관련 논문들의 출판 및 등록 건수도 해마다 증가하고 있다. 그러나 바이오메디컬 문서들에서 유용한 정보를 추출하는 기술은 이러한 분야의 전문가 큐레이터(curator)에 의존한 경우가 많아서, 그 작업의 속도와 양적인 면에서 한계를 가지고 있다. 이러한 이유 때문에 바이오메디컬 문서를 기계학습을 통하여 분석하는 기법이 도입되기 시작하였다. 아직까지는 기계학습을 이용하여 구축된 데이터베이스가 소수에 불과하지만, 점차 증가하는 추세에 있다. 이러한 현 추이를 분석하고 향후의 추세를 예측하고자 텍스트마이닝 기술이 생물학과 의학 분야에서 어떻게 사용되며, 그 정보들이 어떻게 관리되는지 연구, 조사 하게 되었다.
현재 바이오메디컬 관련 데이터베이스들이 여러 기관 및 단체에 의해 구축 및 관리되고 있으며, 국가적인 프로젝트로서 이러한 데이터베이스들을 통합하는 과정을 진행하고 있다. 이처럼 국가기관의 주도하에 데이터베이스를 통합하여 관리하고자 하는 노력들이 계속되고 있어, 앞으로는 바이오메디컬 자료들을 검색하기가 보다 용이해질 것으로 생각된다.
텍스트마이닝을 이용하여 바이오메디컬 정보들을 추출하는 기술은 초기에는 공동 발생(co-occurence)과 같이 단순한 통계적 방법을 이용하였지만, 최근에는 다른 문서에서 추출된 정보와 기존의 정보들을 연계하여 새로운 정보를 추출해 내는 기법이 확산되고 있음을 알 수 있었다.국문 초록 제 1 장 서 론 ········································································ 1 1.1 연구의 배경 ····································································· 1

Campaigns against transmission tower construction have been brought into relief as issues of social conflict to be resolved on a government level since they were reported in the press. Therefore, for effective policy establishment, investigations on the level of perception about electromagnetic waves from transmission towers, in other words, the electromagnetic field, should be performed together. Existing investigations in the form of surveys on interested parties have limitation in their investigation scope not sufficiently considering the population. In order to complement such problem, indirect investigation on ordinary people's level of perception about electromagnetic waves needs to be performed. Accordingly, this study collected social information (news, tweets, and comments related to the electromagnetic field), and based on the information conducted analysis of the level of perception and issues about the electromagnetic field, analysis of social networks, and positive and negative analysis.
For issue analysis, nouns were extracted using KoNLP, R's Korean text mining package. The nouns which appeared most are issues in which ordinary people had most interest. In addition, social network analysis using an apriori algorithm of association analysis was performed to identify the most central words and related words and visualize them. and positive and negative analysis was carried out based on a sensitivity classification dictionary.
This paper selected keywords related to the electromagnetic field in order to investigate people's level of perception about the electromagnetic field. In addition, unstructured data such as news, tweets, and comments related to the keywords
were collected and processed, and issue analysis, social network analysis, and positive and negative analysis of them were performed. The most frequent issues related to the electromagnetic field were transmission towers and residents and according to the social network analysis result, many terms related to transmission towers were those with negative meanings about the towers such as sit-in, opposition, and resistance. In the positive and negative analysis, the rate of negative thoughts (87%) was higher than that of positive thoughts (13%), although the rate of the former was slightly lower than the result of existing surveys.Ⅰ. 서론 1 1. 연구의 배경 및 목적 1 2. 연구 내용 2 Ⅱ. 이론적 배경 3

헬스케어 산업은 질병의 치료에서 예방을 위한 건강관리로 그 무게 중심이 변화하고 있다. 이 변화를 견인하고 있는 것은 인구의 고령화, 의료 기술과 정보통신 기술의 발전이다. 그중에서도 유헬스, 원격의료, 건강관리 서비스 등은 국내 시범서비스 사업을 통해 기술적인 유효성을 검증하고 있다. 하지만 서비스의 효과성 측면에서는 사회적 동의를 얻지 못하고 있다. 반면에 미국을 비롯한 해외에서는 정보통신기술과 융합한 서비스의 헬스케어 분야 이용이 보편적이다.
인간과 언어를 통해 자연스럽게 소통하는 인지컴퓨팅은 헬스케어 지식을 활용한 서비스 혁신의 가능성이 높다. 미국의 암센터를 중심으로 암 전문의사의 환자진단을 지원하는 분야에 이미 적용되고 있다. 특히 인지컴퓨팅으로 인한 사용자경험 혁신은 헬스케어 정보서비스와 융합하여 일반인의 평소 건강관리 효과를 높일 것으로 예상된다. 이는 치료에서 예방으로 전환되는 산업적 변화에 부합되는 중요한 기술이 될 가능성이 높다.
본 연구에서는 인지컴퓨팅과 헬스케어 정보서비스를 융합한 서비스 개념과 모델을 확립하고 일반인을 대상으로 이 융합서비스에 대한 수용요인을 연구하여 규명하였다. 이를 위해 기술수용모델과 선행연구에 대한 고찰을 통해 이론적 배경을 기술하고 연구 모델을 설계하였다. 건강관여, 기술혁신성, 서비스품질, 사회적영향, 개인화를 독립변수로, 매개변수는 인지된 유용성, 인지된 이용용이성을 그리고 종속변수는 이용의도를 제시하였다. 일반인을 대상으로 온라인 설문조사를 실시하고, 탐색적 요인분석과 신뢰도 분석으로 8개의 요인을 채택하였다. 구조방정식 모델을 이용해 확인적 요인분석과 타당성 검증을 완료하였다. 가설검증을 위해 경로분석을 시행하여 변수 간 인과관계를 규명하여 수정가설 1개를 포함한 총 10개의 가설이 모두 채택되었다. 또한 남녀 성별로 서비스 수용에 영향을 미치는 요인이 다름을 검증하였다.
본 연구 결과의 시사점은 다음 네 가지로 요약된다. 첫째, 응답자의 과반수가 건강관리 서비스를 선호하므로 이 분야에 초점을 둔 초기 서비스 개발이 필요하다. 둘째, 신뢰성이 매우 중요한 수용요인이므로 서비스에 대한 신뢰를 높이는 것이 가장 우선되어야 한다. 셋째, 세분 집단별 수용요인을 분석할 후속연구가 필요하다. 넷째, 융합서비스에 적합한 서비스품질 차원을 규명하는 연구가 필요하다.
본 연구는 일반인들을 위한 서비스 개발 및 제공을 위해 우선적으로 고려해야할 사용자의 수용요인을 규명하였다. 성공적인 융합 서비스의 디자인과 사용자 이용을 높이기 위해 향후 실제적인 서비스에 대한 다양한 수용요인을 검증하는 후속연구가 진행되어야 할 것이다.The healthcare industry shifts its focus from acute-care to health management for the prevention of diseases. The major drivers of this change are the aging population and the advances in medical technology and information and communication technologies. In Korea u-health, remote healthcare and healthcare management service are gaining confidence in terms of the effectiveness of the technology through the pilot projects, but still not getting the social agreement yet. However, globally the information and communication technologies converged with healthcare service are common to improve the effectiveness of healthcare.
The cognitive computing has high possibility to innovate healthcare service by leveraging natural language understanding to communicate with humans being. The oncologists adopt the healthcare information service converged with cognitive computing to improve their clinical research performance. The cognitive computing innovation in user experience and healthcare information services are expected to increase the health effects of consumers. And it will be the significant technology to support the shift of healthcare industry.
In this study the conceptual model of the healthcare information service converged with cognitive computing has been suggested. And the acceptance factors for the consumers have been investigated. For this purpose, research on Technology Acceptance Model and the theoretical background for the study was conducted and the research model was designed. The factors of health involvement, innovativeness of technology, service quality, social influence, personalization, perceived usefulness and perceived ease of use are suggested to effect intention to use. After the online surveys for the consumers, the eight factors were adopted through exploratory factor analysis and reliability analysis. Using structural equation modeling and confirmatory factor analysis was completed validation. The path analysis was performed to validate the hypotheses and to identify causal relationships among variables and all 10 hypotheses including one modified have been selected. In addition, the sexual differences in factors affecting the acceptance was verified. Accordingly, the implications of these findings are as follows.
First, the majority of respondents preferred healthcare management services, so it is necessary to develop an initial service focuses on it. Second, the reliability factor is very important to accept the service, so it is important to increase confidence on the service. Third, follow-up study is necessary to analyze the different factors per the group categorized on the base of statistics of population. Forth, the right level of service quality research is needed to investigate the quality dimension.
This study has identified the acceptance factors for the consumers on healthcare information service converged with cognitive computing. For the actual services in future accommodate a variety of factors, follow-up empirical studies should be conducted to validate for the appropriate design of service and user acceptance in the early stage.국문초록 ⅴ 영문초록 ⅶ 제 1 장 서 론 1 1.1 연구 배경 1

공공기록물은 그 규모가 방대하면서 행정적, 역사적 가치를 지니고 있기 때문에 다양한 주제를 동반한 광범위한 검색이 이루어진다. 검색에 입력되는 질의어를 적절히 처리하고 기술영역에 사용되는 용어의 통일성 확보를 위하여 적절한 색인어 입력과정이 필요하다. 기준이 모호한 자연어로 발생되는 불필요한 혼선을 막고 검색의 재현성을 높이기 위하여 색인어의 통제는 반드시 필요하다. 색인어 통제를 위한 통제어휘집인 시소러스는 이러한 역할을 수행하기 때문에 공공기록물에 적용되어야 한다. 국내에 구축된 공공기록물 시소러스인 국가기록원 시소러스의 경우 질의어 확장을 위한 역할에 한정되어있으며 기록물관리시스템과 분리되어 운용되고 있다. 이러한 문제점에 주목하여 새로운 국제 표준인 ISO 25964에서 제시하는 시소러스 구축을 위한 지시태그의 특성을 분석하고 차세대 웹 환경인 시맨틱 웹 환경에 대응하는 시소러스 구축 언어로서 SKOS의 특성과 적용요소를 기술하고자한다. 이에 앞서 시소러스에 관한 정의와 역할에 관하여 국내외 연구사 검토와 실제 운용사례를 분석하였다. 분석대상으로 FAO의 시소러스인 AGROVOC와 국가기록원의 일반주제어 시소러스와 기능시소러스를 분석하였다. 분석 결과 시소러스는 그 계층구조를 이용하여 공공기록물이 가진 계층적 특성에 대응할 수 있으며 주제어와 기능어 모두 시소러스를 통하여 통제 받아야함을 도출할 수 있었다. 결국 이러한 색인어 통제 시스템을 구축하기 위해서는 기록물 관리시스템과의 통합과 추가 색인어를 관리할 수 있는 시맨틱 웹 지향의 SKOS 구조로 설계를 검토할 필요가 있다.제 1 장 서론 제 1 절 연구목적 제 2 절 연구사 검토 제 2 장 시소러스의 정의

Ⅰ. 서론 = 1 Ⅱ. 색인어 추출 관련 연구 = 3 2.1 통계적 기법 = 4 2.2 구축된 데이터를 이용하는 방법 = 7 Ⅲ. 제안된 방법을 이용한 색인어 추출 = 10

단어의 의미 중의성이란, 단어가 문맥에 따라서 두개 이상의 의미를 가질 수 있는 특성을 말한다. 그리고 단어의 의미 중의성을 해소하는 작업이 단어 의미 분별(Word Sense Disambiguation)이다. 즉, 단어의 의미 중에서 문맥에 어울리는 의미를 선별해내는 작업인 것이다. 이러한 의미 중의성은 구문 분석, 의미 분석 등의 자연어처리 전 분야에 걸쳐서 고루 드러나는 문제이다.
본 논문에서는 의미 순서 휴리스틱, 평면 공기 정보 휴리스틱, 의존 구조 공기 정보 휴리스틱을 이용하여 단어 의미 분별을 실시하고, 이 휴리스틱을 결합한 단어 의미 분별 모델을 제안한다.
단어의 의미를 분별하기 위해서는 지식원(Knowledge Source)이 필요하다. 지식원은 보통 말뭉치(Corpus)나 사전(Dictionary)이 사용된다. 하지만 단어 의미 표지가 부착되어 있는 한국어 지식원을 찾기란 쉽지 않다. 본 논문에서는 의미 표지뿐 아니라, 품사 표지와 구문 표지가 전혀 부착되지 않은 온라인 국어 사전을 사용함으로써 이와 같은 문제를 극복한다. 이 때 품사 표지와 구문 표지는 형태소 분석기와 구문 분석기를 이용하여 자동으로 부착한다.
본 논문에서 제안한 단어 의미 분별 모델은 온라인 사전을 의미 분별의 대상으로 실험하였지만, 그 대상이 사전으로 제한될 필요는 없으며 어느 영역으로든 영역 전환이 용이한 장점이 있다.Sense ambiguity of word is that the word has 2 or more possible senses in context. Word Sense Disambiguation(WSD) is the process to resolve that ambiguity. The ambiguity is important problem in every natural language processing works (e.g. syntactic analysis, semantic analysis).
This thesis proposes 3 heuristics - sense order heuristic, plane co-occurrence heuristic, and dependency co-occurrence heuristic for word sense disambiguation. This thesis also suggests word sense disambiguation model which integrates the 3 heuristics.
For word sense disambiguation, we need knowledge source. Normally Machine Readable Dictionary(MRD) or corpus has been used for WSD. But it is difficult to find sense tagged Korean knowledge sources. The word sense disambiguation model suggested in this thesis does not need any(sense, POS, syntactic) tagged knowledge sources, just need raw MRD. And POS tag and syntactic tag will be automatically tagged by POS tagger and dependency syntactic parser. So the word sense disambiguation model proposed in this thesis has flexibility to any domain.표 및 그림차례 = ⅱ Abstract = ⅳ 요약 = ⅴ 제1장 서론 = 1 제2장 관련 연구 = 3

화행은 대화 시스템에서 발화를 통해 전달되는 화자의 의도이다. 화행 분류를 수행하기 위해서는 분류할 발화의 다양한 정보와 더불어 이전 발화들의 정보들을 학습해야 한다. 기존 연구들은 과도한 자질 엔지니어링 문제가 존재하였다. 또한, 최근 여러 자연어 처리 분야에서 인공신경망을 이용한 연구가 우수한 성능을 보이고 있다. 그러나 기존 인공신경망을 이용한 연구들은 이전 발화들의 정보를 제대로 학습하지 못하거나, 혹은 이전 발화와 현재 발화를 그대로 한번에 학습하여 네트워크의 길이가 길어지는 문제가 발생하였다. 본 연구에서는 이러한 문제를 해결하기 위해 문장 임베딩 벡터를 사용하는 2단계의 Recurrent Neural Network 모델을 제안하였다. 제안하는 모델에서는 기존 기계학습 방법론을 이용한 연구에서 발견한 주요 자질을 인공신경망에 효율적으로 학습시킨다. 실험 결과 제안하는 방법은 네트워크 길이를 늘리지 않으면서 이전 발화들에 대한 다양한 정보를 효율적으로 함께 학습할 수 있음을 보였으며, 기존 state-of-the-art 모델보다 높은 분류 정확도를 보였다.Contents 국문 요약 i Contents iiii Chapter 1. 서론 1

최근 인공지능 기술의 발전으로 챗봇에 대한 연구와 감정 모델링에 대한 연구가 활발히 진행되고 있다. 감정 모델링은 챗봇과의 대화에서 중요한 요소이지만, 챗봇과 감정 모델링은 주로 별개로 연구되었으며 그 수 또한 부족하다. 이에 본 연구는 기존의 감정 모델 기반 챗봇과 기반 기술을 분석하여 보다 개선된 감정모델기반 챗봇 모델을 설계 및 구현한다. 구체적으로, 대화의 감정을 분류하는 대화의 감정 분류기와 챗봇의 감정 상태를 관리하는 인공 감정 엔진을 통해 사용자의 감정과 챗봇 간의 상호작용할 수 있는 챗봇을 구현한다. 이 논문의 연구 내용을 통해 감정 모델 기반 챗봇 발전에 기여하고자 한다.Researches for emotion of human have been conducted lively at recent as AI technology has been developed. Though emotion modeling is a most important element in interaction with chatbots, But the study on chatbots and emotion modelling have been conducted separately and amount of academic researches on those are still not adequate. This paper covers design and implementation of improved chatbots model based on human emotions by analyzing current study of chatbots based on emotion model and relating technologies. The implementation contains how to implement chatbot which can do interactions with user’s emotion through artificial emotion engine that manages state of emotions and emotion classifier for chats. This paper could contribute improvement of chatbots technology.국문초록 ⅴ 영문초록 ⅵ 제 1 장 서론 1 1.1 연구 배경 및 목적 1

목차 국문요약 = ⅴ ABSTRACT = ⅵ 제1장 서론 = 1 1.1 연구동기 = 1

최근 스마트폰과 다양한 웨어러블 기기의 확산으로 개인정보의 축적 및 사용이 증가하여 프라이버시 보호가 이슈화되고 있다. 이에 따라 개인정보를 보호하기 위한 다양한 보안 기술이 연구 및 발전되고 관련 법률이 개정되고 있지만, 여전히 개인정보 유출 사고가 발생하고 있다. 이는 요구사항 명세 단계에서 프라이버시 요구사항이 명확히 정의되지 않은 채 보안 요구사항만 명세 되어 소프트웨어 개발 시 보안 기술 구현에만 초점을 두기 때문이다. 즉, 기존 연구들은 프라이버시와 보안의 관계성을 고려하지 않은 채 보안 요구사항을 도출하거나 프라이버시 보호를 위한 원칙, 법률 등을 보완하는 것에 집중되었다. 이로 인해 보안 기술의 발전과 개인정보 보호법 등의 개정에도 불구하고 개인정보 침해 사건이 계속해서 발생하고 있다. 따라서 법률을 기반으로 소프트웨어 개발에 적용 가능한 프라이버시 요구사항을 도출하는 방법과 프라이버시와 보안의 관계를 명확히 명시하는 방법이 요구된다.
본 논문에서는 개인정보 보호가 필수로 보장되어야 하는 헬스케어(Healthcare), 뱅킹(Banking) 등의 도메인에서 프라이버시 친화 시스템(Privacy Friendly System) 구축을 위해 필요한 프라이버시 요구사항을 검증 및 도출하고, 프라이버시와 보안의 관계성을 표현하기 위해 프라이버시 보증 사례를 작성하는 방법을 제안한다. 프라이버시 요구사항 도출은 Privacy by Design 7대 원칙과 GQM(Goal Question Metric) 접근법을 활용하고, 프라이버시 보증 사례는 기존의 GSN(Goal Structuring Notation)을 확장하여 작성할 수 있도록 하였다. 궁극적으로 프라이버시 친화 시스템 구축을 위한 프라이버시 요구사항을 도출하고 이를 기반으로 프라이버시 보증 사례를 작성하는 것을 목표로 한다.제１장 서론 제１절 연구 배경 1. 프라이버시의 정의 2. 개인정보의 이해 3. IT 발전에 따른 개인정보의 사용 증가

본고는 『대한민보』의 언어정리사업란 및 연재소설을 대상으로 하여 황제권이 붕괴되고 통감정치가 시행되는 상황 속에서 &#985172;대한민보&#985173;가 어떻게 자기생산체계를 구성해갔는가를 규명하는 것을 그 목적으로 한다.
1905년에서 1910년 사이의 시간은 가치의 거점이 다원화되고 사회가 체계적인 분화를 해 나가던 시기였고 이러한 변모는 통감부라는 외부세력에 의한 제도적, 물질적 차원의 교통체계의 정비에 기반하고 있었다. 『대한민보』는 자신이 놓인 주변세계를 인정하고 수용하였으며 주변세계를 전유하고 왜곡하면서 자신의 준거점을 확대해 나갔다. 『대한민보』의 자기생산은 『대한매일신보』 &#8231; 『황성신문』 계열 및 당시 공론장에서 담론을 생산하던 주체들의 자기생산과 차이를 발생시켰고 이 차이들로 인해 담론장에서의 교통은 활발할 수 있었다. 이들은 보호국체제, 동양주의, 아시아 연대론 등을 서로 다른 방식으로 독해하였으며 자기준거를 확장하고자 새로운 글쓰기, 수사학 등을 모색하였다.
본고가 첫 번째 분석대상으로 삼는 &#65378;신래성어문답&#65379;란은 일본에서 들여온 법률과 규칙이 현실세계에서 ‘실정성’을 띠고 안정화되어가는데 기여한 난이라고 할 수 있다. 실정화되어 가는 주변세계에 대한 주체의 관점을 표현할 수 있는 언어가 필요하였고 &#65378;이훈각비&#65379;란, &#65378;명사집요&#65379;란, &#65378;사전연구초&#65379;란은 그와 관련된 작업이었다고 볼 수 있다. 주어와 술어를 갖는 문장구조의 생산, 자연어 즉 말의 영역에 있던 고유어를 문자 및 문예어로 재탄생시키는 일, 다양한 곡용어의 개발 등이 이 난들을 통해 수행되었다.
&#65378;소설&#65379;란은 &#65378;신래성어문답&#65379;의 용어가 직조하고 있었던 세계를, &#65378;이훈각비&#65379; &#8231; &#65378;명사집요&#65379; &#8231; &#65378;사전연구초&#65379;가 발달시킨 문장체계를 그 표현수단으로 해서 구체적이고 풍부하게 형상화한 난이었다. 황제권의 붕괴로 중심 없는 분화된 시점들이 산재하게 되자 서사적 기획은 주체가 스스로를 만들어가는 기획과 분간할 수 없을 만큼 깊은 관련이 있게 되었다. &#985172;대한민보&#985173;의 &#65378;소설&#65379;은 현실세계와 역학관계를 유지하면서 현실세계를 주체적으로 독해하고 흡수하여 소화한 것을 표현하고자 하였다. 때문에 현실세계와의 접촉과 교통은 중요한 것이었는데 이러한 역학관계가 1910년에 연재되었던 소설에 접어들어 깨지게 된다. 이 시기는 한 &#8231; 일병합론이 구체화되고 식민화가 급속히 이루어지고 있던 때였기 때문에 소설을 통해 담론주체가 정치적 기획을 서사화하는 것이 점차 불가능해졌다. 1910년대 본격적으로 전개되는 자율적 문학관의 발달은 이와 같은 식민지적 특수성과 관련된 것이라고 할 수 있다.
본고는 『대한민보』가 발간되던 시기의 공론장을 민족과 국가라는 선험적 가치에 지배되거나 그러한 가치들로 수렴되는 것이 아닌 자기생산이란 목적 아래 오히려 민족과 국가라는 중립적으로 보이는 가치들이 놓이는 장으로 파악하고자 한다. 언어적 실천이 자기생산에 중요한 것이었던 만큼 &#985172;대한민보&#985173;는 어휘&#8231;통사적 차원에서 언어적 주도권을 잡기위한 사업도 활발히 벌였다. 그러한 노력을 『대한민보』의 언어정리사업을 통해 확인할 수 있다. 또한 본고는 1910년대 자율적 문학관이 1900년대의 애국계몽소설에 대한 반정립으로 구축되었다는 문학사의 논리로는 『대한민보』의 연재소설을 제대로 규명할 수 없다고 보고 1910년대 자율적 문학관의 발달은 사회의 체계분화 및 조직화와 식민화로 인한 정치적 공론장이 점차 폐색되어 가는 흐름 위에 놓인 것이었음을 밝히고자 한다.Ⅰ. 서론 1 1. 연구목적 및 연구사검토 1 2. 연구방법론 및 연구대상 11 Ⅱ. 교통체계 성립과 언어의 생산 21 1. 신래성어문답의 형식 21

인터넷상에 자신의 의견을 표현하는 일이 많아지면서 이를 분석하고자 하는 오피니언 마이닝 연구가 진행되고 있다. 특히 상품평, 영화평 분석에 대한 사람들의 수요가 많아지면서 이에 맞는 연구가 필요하지만 아직 영화평 분석에 특화된 오피니언 마이닝 연구는 많이 부족한 실정이다. 기존의 연구에서는 추천 시스템에 특화시킨 오피니언 마이닝 기법의 자연어 처리의 정교함을 높이거나 오피니언 마이닝을 사용하여 추천시스템의 결과를 정제하는 한쪽으로 의존적인 기법들이 많이 연구되었다. 따라서 본 논문에서는 오피니언 마이닝 기법이 영화평 분석에 적용될 수 있도록 영화 장르에 따른 감정폭 분류를 적용한 영화추천시스템을 오피니언 마이닝에 기반하여 제안한다. 공개용 영화평점 데이터 MovieLens 데이터 셋을 사용하여 실험을 진행하였고 성능평가는 정확률과 재현율, F-Measure 척도를 사용하였다. 실험결과 기존의 연구보다 정확률과 재현율 등 성능의 향상을 보였다.As people tend to express their own opinions more and more, there has been ongoing research on opinion mining. Especially, the demand for analysis of opinions on items and movies has increased, and it calls for appropriate research; however, opinion mining specially designed for the analysis of opinions on movies is not adequate. Previous research based on recommendation system coupled with opinion mining methods increase accuracy and make better use of the results. Therefore, in this research, I propose a movie recommendation system based on opinion mining using Sentimental-width categorization. Public data known as MovieLens was used for the experiment, and precision, recall, and F1-Value were measured for evaluation. As indicated in the result, there has been increase in precision and recall.제 1장 서론 1 제 2장 관련연구 5 2.1 오피니언 마이닝 연구 5 2.2 협업 필터링 추천 시스템 10 2.3 오피니언 마이닝을 활용한 추천 시스템 연구 13

대화 시스템은 사람과 기계 사이의 상호작용을 위한 대화 인터페이스를 제공한다. 예를 들어 사용자는 대화 시스템을 통해 일정을 관리하고, 날씨 정보를 얻고, 음악을 검색 할 수 있다. 이러한 대화 시스템은 최근 음성 인식 및 자연어 처리 기술의 비약적인 발전에 따라 차세대 인터페이스로 주목 받고 있다. 그러나 일반적인 대화 시스템은 특정한 영역에 대한 서비스만을 제공할 수 있으므로 사용자로부터 처리 할 수 없는 요청을 받는 경우 적합한 응답을 제공 할 수 없다. 그러므로 대화 시스템이 사용자로부터 영역 외 요청을 받은 경우 이를 감지해 요청을 거절하거나 대안을 제시 함으로써 사용자에게 향상된 대화 경험을 제공 할 수 있다.

본 박사 학위 논문은 대화 시스템을 위한 영역 외 문장 검출 방법을 제안한다. 제안하는 방법론은 데이터 기반 대화 시스템 개발 과정에서 이미 수집된 영역 내 문장만을 학습에 사용하며 영역 외 문장은 필요로 하지 않는다. 학습을 위해 다양한 영역에 대한 충분한 양의 영역 외 문장을 수집하는 것은 많은 노력과 시간을 필요로 하며, 서비스 과정에서 대화 시스템의 영역의 정의가 변경될 때 마다 기존에 수집한 모든 영역 외 문장들의 영역 외 여부를 재검토 할 필요가 있기 때문이다.

제안하는 영역 외 문장 검출 방법론은 두 단계로 구성된다. 첫 번째 단계에서는 사용자로부터 입력 받은 문장을 분산 표현으로 변환한다. 서로 다른 단어로 구성된 문장일지라도 영역 외 문장 검출 관점에서 그 의미가 유사하다면 그 분산 표현이 벡터 공간 상에서 가까운 거리를 갖도록 하기 위함이다. 이를 위해 입력 문장의 영역 범주를 분석하는 재귀 신경망(recurrent neural network)을 학습한다. 해당 재귀 신경망에 의해 추출된 특징은 영역에 대한 정보를 담고 있으므로 영역 외 문장 검출에 사용한다.

제안하는 방법론의 두 번째 단계에서는 추출된 분산 표현을 바탕으로 입력 문장의 영역 외 여부를 판별한다. 이를 위해 분산 표현을 저차원으로 압축하는 인코더(encoder)와 원래대로 복원하는 디코더(decoder)로 구성된 오토인코더(autoencoder)를 학습한다. 학습된 오토인코더는 영역 외 문장에 대해서는 높은 복원 오류를 가지므로 이를 영역 외 문장 검출에 사용한다. 두 번째 단계를 위한 다른 방법은 실제 분산 표현과 만들어진 분산 표현을 구별하는 구분자(discriminator)와 구분자를 속이기 위해 분산 표현을 만들어내는 생성자(generator)로 구성된 생성적 적대 신경망(generative adversarial network)을 학습하는 것이다. 학습된 구분자는 영역 외 문장에 대해서는 낮은 점수를 부여하므로 이를 영역 외 문장 검출에 사용한다. 실험을 통해 이 박사 학위 논문이 제안한 방법론이 영역 외 문장을 정확하게 검출 할 수 있음을 검증하였다.To ensure satisfactory user experience, dialog systems must be able to determine whether an input sentence is in-domain or out-of-domain (OOD). We assume that only in{domain sentences are available as training data because collecting enough OOD sentences in an unbiased way is a laborious and time-consuming job. This dissertation proposes a novel sentence-embedding method that represents sentences in a low-dimensional continuous vector space that emphasizes aspects that distinguish in-domain cases from OOD cases. We first used a large set of unlabeled text to pre-train word representations that are used to initialize sentence embedding. Then we used domain-category analysis as an auxiliary task to train sentence embedding for OOD sentence detection; the learned sentence representations are used to train one-class classifiers. We propose two one-class classifiers for OOD sentence detection: an autoencoder that generates high reconstruction errors for OOD sentences and a generative adversarial network in which the discriminator generates low scores for OOD sentences. We evaluated our OOD sentence detection method by experimentally comparing it to state-of-the-art methods in a 14-domain dialog system; our proposed method achieved the highest accuracy in all tests.I. Introduction 1.1 Task Definition 1.2 Motivation 1.3 Data set 1.4 Evaluation Metrics

This paper is implementation of responsive web application. Using LOD(Linked Open Data), a kind of semantic web, LOD objects around the user are searched based on the user's location information. LOD is a model that identifies data existing on the web by URI and links to each other by giving link information to each URI. With this linking It has more profitable to reusing data and interlinking. In this paper, use DBpedia for data. DBpedia is a kind of LOD, is based on Wikipedia, an online encyclopedia and it acts as a hub for various LOD datasets. DBpedia data has more than 300,000 pieces of geographic information. This data and the user location information are combined with each other to provide an opportunity for the user to conveniently utilize the semantic data around the user. In addition, it is implemented as a web application using the responsive web design. So that it provide more accessibility, can be used in a wide variety of environments or user devices.제 1 장 서 론 1 1.1 연구 배경 1 1.2 연구 목적 3 1.3 연구 방법과 구성 5 제 2 장 관련 연구 6

기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다.
이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다.Corporate bankruptcy causes serious damage to national economy, brings about economic losses to all the concerned of the company, and reduces social wealth. Therefore, forecasting corporate bankruptcy more accurately is very important in the social·economical aspect.
In this thesis, Deep Learning method, which shows excellent predictive power recently in many fields, such as image recognition, voice recognition and natural language process, is used to forecast corporate bankruptcy, and specifically it suggests DBN(Deep Belief Network) among many deep learning algorithms. In order to verify performance of the method comparing with existing analysis technique, this thesis tried to make a comparison with SVM(Support Vector Machine) which is studied for corporate bankruptcy forecast method until a recent date, with corporate data of non-financial business listed on domestic KOSDAQ·KOSPI from 1999 to 2015. The number of healthy companies was 1669 and the number of bankrupt companies was 495; analysis was performed using financial ratio variables introduced in `Business Management Analysis' of the Bank of Korea. As a results, DBN exhibited better performance in various rating scales than SVM. In particular, it showed more than 5% better performance in sensitivity of predicting a bankrupt company as a bankrupt company based on test data, and it was able to confirm applicability of deep learning in the field of corporate bankruptcy forecast.국문요지 표 목차 그림 목차 1. 서론 1 1.1 연구 배경 및 목적 1

지식산업과 정보기술의 발달과 함께 다양한 정보원에서 체계화된 정보들이 폭증하고 있다. 특히 온라인 문서량은 급속도로 증가하고 있으며 정보량의 증가와 함께 정보검색기술도 눈부시게 발전하여 다양한 검색 엔진을 통하여 필요한 정보를 탐색할 수 있게 되었다. 그러나 기존의 검색 엔진은 자연어 처리시스템을 기반으로 하기 때문에 같은 주제를 다루는 문서라도 문헌 생산자나 색인 작성자, 이용자간에 표현하는 용어가 달라 키워드 검색의 근본적 한계에서 크게 탈피하지 못한다. 이 근본적 제한점으로 인하여 데이터베이스 내에 이용자의 요구에 부합된 문서가 포함되어 있음에도 불구하고 해당 문서가 검색되지 못하는 반면 부적합한 문헌이 검색되는 현상이 발생한다.
따라서 본 연구는 개발자의 주관성에 종속되지 않고 문서에서 실제적으로 사용되는 표현에 근거한 객관적이고 과학적인 용어클러스터를 만들기 위하여 데이터마이닝 기법을 사용하여 전문 용어 클러스터링 알고리즘을 설계, 구현하고자 한다. 다양한 문서로부터 추출한 전문용어에 연관규칙발견 알고리즘을 적용하여 용어들 간의 연관 규칙을 찾고 발견된 연관 규칙집합을 이용하여 의미적으로 관련된 전문 용어들끼리 클러스터링하도록 하는 데 목적을 두었다.
본 연구의 클러스터링 결과를 전문 분야에 대한 검색엔진에 적용할 경우 키워드 중심의 검색엔진과는 달리 관련된 전문 용어에 대한 클러스터링 정보까지 활용되므로 지식 정보를 효과적으로 검색할 수 있는 지능적인 전문 검색엔진을 개발할 수 있을 것이다.According to the development of knowledge industry and information technique, the information is increasing explosively. Especially the quantity of on-line document is increasing rapidity. Information search technique also develops dazzlingly, and it will be able to search the information, which is necessary. But most of search engine does a natural language processing system in base. Because the word which the writer use, the term of indexing, and the terminology which it expresses user for are different with each other, the user can't search for the document which is included the information user want to read.
In this thesis, I proposed a method for clustering between relevant terminologies in order to extract useful information from large documents by using an algorithm searching techniques for the association rules.
Data mining, a burgeoning technology in information science, entails searching for patterns in data. By the same logic, text mining entails searching for patterns in text. It may be defined as the process of analyzing text to extract information useful for a particular purpose. Text is relatively unstructured, and difficult to deal with unlike most kinds of data stored in databases.
The number of terminologies were extracted from unstructured text document is irregular and the terminologies themselves are general in nature, making it difficult to retrieve useful information as efficiently as using algorithm searching for general association rules.
I extracted technical terminologies from research papers about Interior design and showed the efficiency of the proposed method through experiments involving cluster construction between relevant terminologies.
The proposed method can likewise be used in various areas including automatic document classification, automatic abstracting and intelligent search systems.요약 목차 표목차 그림목차 부록목차 = ⅳ

Recently, the perceived value of stories has risen, which has created a greater focus on storytelling. Storytelling is method used for immersing an audience and prompting interest in the development of plot. This method has been receiving a lot of attention recently as an important technique in the cultural industry.

More specifically, there have many more cases of stories being created using authoring tool systems through Human-Computer Interaction (HCI). Compared to other countries, there have not been many studies regarding using such systems for storytelling in Korea.

This thesis proposes a system which automatically extracts narrative information, including key storytelling elements, that comprises the main components of a written story. Patterns for extracting narrative information nouns within the text are defined by applying a Korean natural language process technique to story data collection and analysis. Story data is then extracted using term frequency(TF).

To assess this narrative information extraction system, experiments were done to analyze data from six stories. The results determined using the automated character extraction method. The results showed that approximately 71.83% of the average precision ratio and performance had been confirmed.ABSTRACT Ⅰ. 서론 1 A. 연구 배경 및 목적 1 B. 논문의 구성 2

본 연구는 소셜 미디어(social media) 플랫폼을 통해서 작성된 사용자 생성문에서 어문 규정을 준수하지 않은 형태의 유형을 분석하고 이를 처리하기 위한 언어 자원 구축을 목적으로 한다. 어문 규정을 준수하지 않은 표현은 대부분의 형태소 처리 모듈에서 미분석 혹은 오분석 되는 형태로서, 자연어 처리의 오류를 야기한다. 본 연구는 이러한 표현이 형태소 분석 단계에서 올바르게 자동 처리되도록 하고자, 코퍼스 내에 정형화되지 않은 다양한 표현에 대한 패턴을 검토하였다. 그리고 사전 활용을 기준으로 전후 단계를 나누어 정형화에 적합한 유형과 분석 규칙의 확장에 적합한 유형을 분류하고 각각을 형식화함으로써 실질적으로 코퍼스에 적용할 수 있는 언어 자원을 구축하였다.
이에 대한 일환으로 사전 정보를 적용할 수 없는 단계에서는 정규식(regular expression) 기반 전처리 문법 테이블(Preprocessing Grammar Table: PGT)을 통해서 원문을 정형화하고자 하였다. 그리고 사전 정보를 활용할 수 있는 후처리 단계에서는 LGG(Local-Grammar Graph) 기반 패턴문법 사전(pattern dictionaries)을 통해서 확장된 형태소 분석을 시도하였다. 특히 패턴문법 사전의 경우, DECO 한국어 전자사전(Dictionnaire Electronique du COreen: Korean Electronic Dictionary)의 품사, 의미, 활용형 정보 등을 토대로 언중의 언어 직관을 섬세하게 반영하고자 하였으며, 본 논문에서는 비정형 토큰(non-standard token)에 가장 고빈도로 나타나는 의존명사 내포 표현을 중심으로 구축한 자원을 소개한다.
이는 사회관계망 코퍼스에 나타나는 언어 직관을 표상한 자원으로서, 기존 형태소 분석기에서 미분석 혹은 오분석 되는 어절을 인식하기 위해 사용된다. 또한 기분류된 형태소 분석 결과에서 형태상 여러 단어로 분류될 수 있는 단어의 중의성 제거에 활용될 수 있다는 점에서 자연어 처리 분야에 기여하는 바가 크다고 판단된다.This paper aims to analyze non-standard tokens in a social media corpus and to construct linguistic resources that analyze theses tokens. The social media corpus is large-scale user generated texts in social media platforms. The corpus as unstructured texts contains various non-standard tokens. And these tokens are not properly analyzed by standard morphological analyzers of Korean. Generally, non-standard tokens occur due to spacing errors, repeating syllable, spelling mistakes, abbreviation and neologism.
The non-standard token can be handled by applying pre-processing or post-processing. In this case, for a token with morphologically regularity such as repeating syllable, it is appropriate to modify the original text in the pre-processing. If syntactic or semantic information is needed, like spacing error, it is appropriate to post-process using the dictionary information. Particularly, non-standard tokens, abundant in post-processing types, include bound nouns, which requires linguistically accurate pattern grammars.
In this study, the social media corpus was examined to classify the types of non-standard tokens and to analyze linguistic patterns. And the types to be applied to pre- or post-processing were separated according to the necessity of dictionary information. RegEx(Regular Expression) was utilized in the pre-processing, and DECO Korean dictionary & LGG formalism in the post-processing for representing pattern grammars. The performance of pattern resources showed 95.15% precision, 90.2% recall, and 92.61% f-measure in a test corpus.
In conclusion, this study presented various types of non-standard tokens appearing in the social media corpus, discussed processing methodologies, and constructed linguistic resources to handle them. These linguistic resources are useful for the processing of unstructured text such as the social media corpus. And they can also be utilized for morphological disambiguation. Therefore, I expect the methodology and resources of this study to be expanded for the development of natural language processing.제1장 서론 1 1.1. 연구의 목적과 필요성 1 1.2. 논문의 구성 6 제2장 문제 제기 및 선행 연구 9 2.1. 현행 형태소 분석기와 맞춤법 검사기의 비정형 토큰 처리 9

The development of streaming music services leads for users to listening to music easily. In addition, contents such as songs and music videos in the services are abundant. Due to the development, users obtain too many chances to make decisions to select which songs they will listen to. Therefore, music recommendation systems become much more important according to frequent selection.
Users usually create their playlists, collections of several songs, and play them. Some playlists are played depending on user’ preference or user’s situation. This listening behavior can be regarded as a sequence consisting of played songs.
In this research, we developed music recommendation system based on analytics of listening sequence and proved the usefulness of it. Compared to traditional recommendation systems based on filtering, our proposal has a different approach. 1) We adopted Recurrent Neural Network(RNN) which has made great achievements in Natural Language Process(NLP) due to commonality between music recommendation and NLP. 2) Numerous songs related to each other are represented by Song Embedding like word representation by Word Embedding in NLP. 3) Audio signals, the user’s listening situation, and user profile are added to the model to improve RNN performance. We demonstrated the development possibility of music recommendation system based on listening sequences with user listening history on LastFM.음악 스트리밍 서비스의 발달로 음악 청취는 용이해졌고 컨텐츠는 풍부해졌다. 이로 인해 수많은 곡들 중에서 선택을 하여야 하는 사용자들의 어려움은 가중되었고 음악 추천 시스템의 중요성은 더욱 커지고 있다.
사용자들은 음악을 들을 때 사전에 여러 곡들로 구성된 플레이리스트를 청취하는 경우가 많고 다양한 플레이리스트가 사용자의 취향에 맞게 혹은 상황에 맞게 재생된다. 이 때 사용자의 음악 청취 행위를 시간에 따른 일련의 곡들로 구성되는 하나의 시퀀스로 인식할 수 있다.
본 연구에서는 청취 시퀀스 기반의 예측 모형을 개발하여 음악 추천 시스템에 활용하는 방안과 유용성을 검토하였다. 이는 필터링 기반의 전통적 추천 기법들과는 다른 접근 방법을 시도한 것이다. 1) 자연어 처리(NLP) 분야와의 공통점을 이용하여, 사용자 집단의 청취 시퀀스 패턴을 최근 NLP 분야에서 큰 성과를 보이고 있는 Recurrent Neural Network(RNN)로 분석하였다. 2) 음악 스트리밍 서비스는 대단히 많은 곡들을 제공하고 그 곡들 간에는 관계가 존재하므로 Word Embedding을 응용하여 곡을 Song Embedding로 표현하였다. 3) RNN의 성능 개선을 위해 오디오 신호에 대한 정보, 사용자 음악 청취 상황, 청취하는 사용자의 정보를 입력 정보로 추가하였다. Last.Fm 자료를 이용한 실험 결과로 청취시퀀스 기반 음악 추천 시스템의 가능성이 높다는 사실을 확인하였다.제 1 장 서 론 1 1.1 연구 배경 1 1.2 연구 목적 및 방법 3 1.2.1 문제점 3 1.2.1 제안 방안 4

지속가능한 신성장에 동력을 찾기 위한 노력이 강조되고 있는 지금, 국가와 산업 전반에서 경제적, 사회적 그리고 산업 환경적 목표를 달성하기 위해 기술주도형 혁신의 중요성이 부각되고 있다. 과학기술은 현대 경제와 국방력의 주요 부분을 형성하여 온 만큼 기업과 국가 모두 기술경쟁력을 유지하기 위한 노력 중에 있다. 하나의 기관이나 국가가 단독으로 모든 과학기술영역에 연구투자를 한다는 것은 불가능하며 경쟁적인 시장환경 속에서 효과적이지도 못하다. 효과적인 과학기술 연구를 위해서는 다자간의 협력이 요구되고, 보다 효과적인 협력관계를 형성하기 위해서는 과학기술의 발전 동향을 모니터 하는 노력이 필요하다.
본 연구에서는 태양전지 분야를 중심으로 과학기술문헌을 분석하고 연구동향을 살펴보기 위해 정보검색과 정보추출에 효과적인 텍스트마이닝 기법을 이용하여 기존 연구 및 현재 진행 중에 있는 다양한 기초연구와 응용연구들을 분석하고, 태양전지 분야에서 주목받고 있는 연구주제와 그 추이를 살펴보고자 한다. 분석에 사용된 데이터는 태양전지와 관련해 발표된 SCI 논문들로, 태양전지와 관련된 다양한 주제를 살펴보기 위해 태양전지분야의 전문저널을 확인하고 선정된 저널에 수록된 모든 연구논물을 대상으로 연구주제를 분석하고 분류한다. 이 논문집합에 전처리를 통해 용어(Term) 및 주제어(Keyword)를 추출하고 문서를 특성벡터(Feature Vector)를 변환한다. 문서클러스터링(Document Clustering) 기법을 적용하여 논문집합에서 군집을 형성하는 연구주제를 확인하고, 텍스트범주화(Text Categorization)를 통해 각 연구주제로 문서를 분류한다. 논문들의 주제분류결과와 메타데이터를 통합함으로써 기술모니터의 관점에 부합하는 정보를 얻어낸다.Nowadays sustainable growth is being considered more important than ever before and then technology driven innovation is getting considered important for nations and companies in order to achieve their economical, social and industrial goals. They are working on developing and maintaining competitiveness in science and technology field as science and technology have formed essential part of the modern economy. No nation or company, however, can cover up the entire R&D field by themselves. Therefore, cooperation is needed for cost and time efficient researches, and monitoring science and technology is required to detect and keep abreast with trends and emerging topics of them.
This study is focused on researches of photovoltaics, which is one of renewable energy technologies. The purpose of this study is to examine the trend and detect emerging topics by analyzing science and technology literatures of photovoltaics. The data on this study are SCI journal papers, whose research topics are directly or indirectly related to solar cells. In order to make sure the data include diverse research topics in photovoltaics, two journals which are specialized in the field of photovoltaic research are selected, and all the papers that are included in those two journals during last 5 years from 2005 to 2009 are downloaded. Then, text-mining is applied on the data; first, the data are preprocessed to extract terms and keywords from documents and to transform the document into feature vectors; second, research topics are identified by extracting topics from clusters after document clustering; then papers are classified based on its topic by Text Categorization. Finally, as the result is consolidated with meta-data that include bibliographic data such as authors, nationalities and so on, technology monitoring information is figured out from the descriptive statistics.제1장 서론 1 1.1 연구의 배경 및 목적 1 1.1.1. 환경문제와 신재생에너지 1 1.1.2. 기술혁신과 기술경영 7 1.2 연구의 범위와 구성 8

Recently, foreign tourists visiting Korea trend seems to increase every year. Based on this, service quality of typical domestic hotel companies have seen the need to evaluate in a foreign perspective. This study is aimed at domestic and foreign-based representative hotels which based in Seoul and Busan, a lot of foreign tourists are visiting, and analyzed the sentiments of online reviews for these hotels.
To perform sentiment analysis on the review, this study used one of typical machine learning algorithms; word2vec, which provides calculation of similarities and affinities between vocabularies. Word2vec is made of CBOW(Continuous Bag-of-Words) and Continuous-Skip-gram-Model, based on this, it shows a very good accuracy to determine the similarities and affinities between the words.
Assessment of hotel service was classified by the LQI(Lodging Quality Index). LQI is as an index for evaluating the quality of the hotel services, based on this, evaluation was performed on the quality of service of the local hotels.
In this study, we separated reviews by every two years in order to examine changes in emotion.
This study is differentiated from prior studies by examining sentiment for hotel service quality, not only classify the word into positive group and negative group. Therefore, this study can explain detail of the service quality of domestic hotels and show how to be improved.
In conclusion, we hope the results of this study will provides competiveness to domestic hotels against foreign-based representative hotels.I. 서론 1 II. 이론적 배경 3 1. 서비스 품질 3

The study aims to make quick decisions as big data grows.
Error data derived from low-quality data is largely dependent on human resources to improve. In addition to the lack of automated features to support improvement activities, complex processes have increased the time and cost. However, due to advances in machine learning and artificial intelligence, processes are being automated.
We presented the need for data mining, carefully analyzed the usefulness and limitations of algorithms that determine the similarity among each data, and further studied the algorithms that recommended terminology when similar scores were obtained from morpheme analysis as a result.
Data quality improvement was collected from disease data using corresponding text clustering, and the Fuzzy matching algorithm was used to analyze morphetes to determine similarity.
In this study, users are able to make quick and accurate decisions without spending time and effort on pre-processing data by using text clustering to help improve data quality through automated processes without relying on manual tasks and experiences.

Key word(Guide word): text mining, text clustering, Fuzzy matching, editing distance, disease data이번 연구는 빅데이터가 증가하면서 의사결정을 신속하게 결정할 수 있는 것을 목적으로 하고 있다.
저품질의 데이터에서 도출되는 오류데이터는 개선하기 위해서 대부분 인적자원에 의존되고 있다. 개선활동을 지원할 수 있는 자동화된 기능이 부재될 뿐만 아니라 복잡한 프로세스로 인하여 많은 시간과 비용이 증가되었다. 하지만 현재 머신러닝, 인공지능 등의 발전으로 인해 프로세스가 자동화 되고 있다.
데이터 마이닝의 필요성에 대해 제시하였고, 각 데이터 간의 유사도를 판단하는 알고리즘에 대한 유용성과 한계점에 대해 정성적으로 분석하였으며, 더 나아가 형태소 분석하여 유사 점수를 결과 값으로 내어 값이 높으면 용어를 추천해주는 알고리즘을 연구하였다.
해당 텍스트 클러스터링을 활용하여 데이터 품질 개선을 질병 데이터로 수집하였고, Fuzzy matching 알고리즘을 통해 형태소를 분석하여 유사성을 판단하였다.
이에 이번 연구에서는 영문이 아닌 국문에도 수작업과 경험에 의존하지 않고, 자동화된 프로세스로 데이터 품질을 향상시키기 위해 텍스트 클러스터링을 활용하여 시간 절약에 도움을 줌으로써 사용자들이 데이터 전처리의 시간과 노력을 쏟지 않고, 빠르고 정확한 의사결정을 할 수 있을 것으로 본다.

주제어(키워드, 핵심어)
텍스트 마이닝, 텍스트 클러스터링, Fuzzy matching, 편집 거리, 질병데이터I. 서론 1 II. 관련 연구 7 1. 데이터 전처리 7 1) 데이터 전처리 정의 7

자연은 인간 삶의 터전으로 모든 예술의 근원이다. 자연이 인간에게 주는 감동은 고귀한 예술의 모체가 되며 따라서 자연은 모든 예술분야에 있어서 조형적 활동의 대상이 되어왔다. 특히, 창조적인 활동을 하는 예술가들에게는 자연을 대하는 관심이 더욱 크다고 하겠다. 그러나, 자연이 아무리 아름답다고 하여도 그 자체로는 예술이 될 수 없으며 예술가의 미적체험과 주관에 의하여 표현 될 때 비로소 예술이라 할 수 있다. 따라서 자연을 표현한다는 것은 자연 그대로 재현하는 것이 아니고 예술가의 내면적 반성에 의하여 느낀 감정을 표출하는 것이라 할 수 있다.
우리 고유의 미적 특성을 지닌 자수는 자연주의적 성향을 띤 작품이 많다. 특히, 현대에 와서는 작가가 자연을 자기 주관에 의해 다시 구성하고 해석하는 작품이 다양하게 선보여짐에 따라 여러 표현재료와 기법이 개발되고 있다. 그러므로, 이 논문은 자연에서 유출되는 끊임없는 감동의 경관 중 산을 주제로 삼아 본 연구자가 면을 나누어서 재 구성한 다음 한지와 Felt등의 다양한 재료와 현대적인 자수기법을 응용하여 보다 산을 독창적이게 표현하는데 그 목적을 두었다.
이를 위한 이론적 배경으로는 국내외의 문헌을 통하여 자수의 역사와 현대자수로서의 변천과정을 살펴 보았으며 자연에 대한 표현양식과 변형을 미술사적 견지에서 고찰하였다. 또한, 자연을 소재로한 현대자수 작품을 예로 제시하면서 분석해보았다.
작품제작에 있어서는 사실적인 산의 표현보다는 주관적으로 재 구성한 산을 소재로 삼아 독창적인 조형세계를 추구하려 노력하였다.
이러한 작품제작과 연구과정 결과, 자연을 형상화한 자수작품을 제작하고 분석하여 얻은 결론은 다음과 같다.
첫째, 산을 표현 대상으로 할때 산의 서정적 이미지에 알맞는 재료로서 한지와 Felt를 수 놓아 붙여 표현하였더니 재질감의 효과가 있었다.
둘째, 산의 형상은 여러가지 기법을 사용하여 입체감과 공간감이 확대되었다.
셋째, 실의 다양한 꼬임과 굵기의 활용은 산의 재질감 표현에 효과가 있었다.
넷째, 산을 사실적으로 표현하기보다는 기하학적인 형태로 단순화시킴으로써 보다 더 산의 형태에 가까운 결과를 가져올 수 있었다.
따라서, 본 연구자는 현대자수의 표현기법을 주관적인 표현의도와 방식에 따라 무한한 가능성을 표현하고자 하였으며 이를 통하여 보다 차원 높은 자수작품들을 제작하도록 노력하였다.The nature is the origin of all the arts and the site of human life. The impression that the nature gave human being became the matrix of noble art. Thus the nature has been the object of the plastic activity in the field of all art. Especially, it can be interpreted that the interest towards the nature will be great to every artist who does the creative activities. But, though the nature is beautiful, itself may not be the art. And, when it is expressed by the aesthetic experience and subjectivity of artist, it can be referred to as an art for the the first time. Thus, it can be said that expressing the nature is not to reproduce the nature as it is, but to express the feeling to be felt by the inner self-examination of an artist.
In the embroidery which shows our peculiar aesthetic feature, there are many works expressing the naturalistic inclination. Especially, since the beginning of the modern times, owing to the works that the artists compose and interpret the nature by their own subjectivity, it will be shown diversely and various materials and techniques of expression are being developed. So, the purpose of this study is an attempt to prepare a momentum that our embroidery will develop more with the positive expressional activity with an effort to be free from the traditional fixed notion.
As for the theoretical background, this study has examined the history of the embroidery the history of the embroidery and the changing process of the modern embroidery through the documented material evidences of local and foreign reference literatures, and also has studied the style of expression and the transformation of the nature from the viewpoint of the fire art history. In addition, this research has analyzed and introduced the modern embroidery works using the matters of the nature.
Through the work of art applying the serenity of mountains emit the mysterious and endless impression overwhelmed by the nature, this study therefore has expressed it with the subject of the mountain. This research has endeavored with great efforts to seek for an unique plastic world by making the mountain recreated subjectively rather than using a realistic expression.목차 논문개요 = ⅷ Ⅰ. 서론 = 1 A. 연구의 목적 = 1 B. 연구 범위와 방법 = 1

Thanks to rapid improvement of information technology, the emergence of various information channels such as mobile devices and social media have been producing tremendous amount of data. The evolution of smartphones and social network services(SNS) leads to the big data revolution.
Not only the amount of data have been growing up exponentially, but also more diverse types (structured, semi-structured, and unstructured) of data are emerging. In case the of Twitter and Facebook, there should be several analytical methods depending on the types of data.
In the case of online shopping, the log data can be used to analyze consumer's purchase pattern by measuring the time on how long they purchase items since they logged in the web. Collection and analysis of large and varied data presents a challenge, as compared to the standard and conventional data.
Even though the same data was used to extract the meaning, it can be interpreted in various ways depending on how it was pre-filtered and what kind of data mining methods was used. So the importance of pre-filtering and appropriate data mining techniques should be considered in mining the semantics of large and various data. The research for unstructured data, large and varied data, have been started for a more systematic and appropriate ways of collection and analysis.
In this study, Twitter data has been collected, stored and analyzed in a multi-dimensional fashion on top of Hadoop platform, widely used for distributed computing, in order to find out what kind of factors can affect the preference of smartphones. The data, which is around 600,000 tweets or 2.5 GB, has been collected for one month using smartphone-related keywords. The results affecting the preference of smartphones are processed in multi-dimensional analysis after pre-filtering and natural language processing. The most serious problem is the quality of the result that comes largely from the shortage of samples due to a short period of collection (one month). Another big problem comes from the synonyms including acronyms in Internet or smartphones. However, these problems can be moderated as the data collection time and the number of synonyms/acronyms in the dictionary increase.Ⅰ. 서 론 1 1. 연구의 배경 과 목적 1 2. 연구내용과 방법 2 Ⅱ. 이론적 배경 및 선행연구 검토 3

이러닝에 대한 관심이 높아지면서 LMS(LMS: Learning Management System)에 대한 시장 규모가 점차 늘고 있다. LMS란 온라인을 통하여 학생들의 성적과 진도는 물론 출석과 결석 등 학사 전반에 걸친 사항을 관리해 주는 시스템이다. 이러한 수요는 LMS와 관련된 사이트 증가로 나타나고 있으며 그와 관련된 학습 컨텐츠 개발 또한 주목 받고 있다. 한편 차세대 지능형 웹인 시맨틱 웹의 등장으로 정보들간의 관계를 정의함으로써 컴퓨터가 정보자원의 뜻을 이해하고 논리적 추론을 가능하게 하고 있다. 이는 사람이 이해할 수 있는 자연어 위주의 웹이 아닌 컴퓨터가 이해할 수 있도록 하는 지능형 웹을 말하며 정보시스템의 생산성과 효율성을 극대화 한다.
본 논문에서는 LMS를 통한 학습 후 생성되는 이력정보에 대해 시맨틱 웹 기반 기술을 바탕으로 추론을 할 수 있는 시스템을 제안하였다. 학습자들이 단순히 LMS를 사용하여 학습하고 컨텐츠를 이용하는 데에 나아가 사용자들의 행위를 이력으로 보관하고 그러한 이력 데이터를 통해 추론한 결과를 보여주는 것이다.
본 논문에서 제안하는 시스템의 특징은 다음과 같다. 첫째, 학습자의 학습 이력정보를 바탕으로 추론을 하여 학습자에게 유용한 정보를 제공할 수 있을 뿐아니라 학습 컨텐츠 제작에 피드백을 제공할 수 있다. 학습자의 이력정보에 성취도, 만족도등과 같은 정보를 포함하여 학습컨텐츠에 대한 학습자의 반응을 기록할 수 있다. 이러한 정보를 활용하게 된다면 학습컨텐츠 제작에 있어 피드백을 제공할 수 있다. 둘째, 학급 관리자 및 컨텐츠 관리자의 의사 결정에 도움이 될 수 있는 통계자료들을 제공할 수 있다. 이력정보를 통해 통계를 추출하는 것은 기초자료 작성에 정보를 제공할 것이다. 이는 통계자료뿐만 아니라 학급관리자 및 컨텐츠를 관리하는 입장에서 의사를 결정함에 의미 있는 정보가 될 것이다.제 1장 서론 1 제 2장 관련 연구 5 제 3장 시스템 설계 11 제 4장 시스템 구현 23 제 5장 결론 및 향후연구 35



목차 = ⅰ 그림 및 표의 차례 = ⅱ 〈Abstract〉= ⅲ 요약 = ⅳ Ⅰ. 서론. = 1

실제 웹이나 신문 또는 전문분야 서적 등에서 쉽게 전문용어들을 접하게 된다. 전문용어들은 사회가 발전함에 따라 쉽게 생성되고 또는 쉽게 소멸된다. 이러한 전문용어를 사람이 직점 찾아내고 기록하기란 여간 어려운 일이 아니다. 그리고 전문용어는 형태소분석 또는 구문분석 등에서 미등록어 또는 오류유형으로 분석되어 정확률을 떨어뜨리는 요인이 되기도 한다. 그래서 본 논문에서는 문서에서 정확하게 전문용어를 추출할 수 있는 전문용어 태깅 시스템을 개발하고자 한다.
지금까지 자연어처리에서의 품사태깅(parts-of-speech tagging)기술에 대한 연구는 활발히 진행된 반면,전문용어에 대한 처리기술은 미비한 점이 많았다.전문용어에 관련된 연구는 대부분 구축,표준화,추출 등에 대한 연구가 많았으나 전문용어 태그설정과 태깅 기술 연구는 부족한 상황이다. 본 논문에서는 전문용어 태그를 (분야정보:아이디) 순으로 설정하고 백과사전의 분류체계를 이용하여 어떤 특정 분야 문서의 전문용어를 자동으로 태깅하는 시스템을 구축하였다. 전문용어 태깅 시스템은 형태소분석기를 사용하지 않고 문맥의 규칙과 조사/어미사전을 이용해 자동으로 태깅을 하게 된다. 이 시스템의 정확률 측정을 위한 정답 말뭉치는 웹에 공개되어 있는 백과사전 html문서를 이용하였다. 우선 백과사전에 나와 있는 용어는 전문용어라고 가정한다. 하나의 문서에는 ‘용어’,‘요약’,‘본문’,‘이미지’,‘분류’,‘참조항목’ 등의 정보들이 있다. 이 중 ‘본문’에는 그 용어에 대한 자세한 설명이 있는데 특정 단어네는 <a>태그로 되어있는 것을 설정한 태크로 바꾸고 eksePquf로 확장 태깅을 해서 정답말뭉치를 만든다. 전문용어 태깅 시스템과 정답 말뭉치를 비교해 정확률을 계산해서 시스템의 성능을 측정하였다.1.서론 = 1 1.1 연구배경 = 1 1.2 연구방향 = 1 1.3 관련연구 = 2 1.4 논문의 구성 = 4

인공지능을 중심으로 한 차 산업혁명 시대가 도래함에 따라 인공지능(Artificial Intelligence) (Mobile 과 모바일 메신저 그리고 Messenger) 온라인 전자 상거래(E-Commerce)를 기반으로 한 대화형 커머스 챗봇(Conversational Commerce Chatbot)이 차세대 커머스 채널로서 주목받고 있다. 본 연구에서는 대화형 커머스 챗봇 서비스의 신뢰도 향상 요인으로써 대화형 커머스 챗봇의 복합적인 인터랙션 특성과 감정이입이 용이한 일상적인 자연어 기반의 대화체를 통해 실제 사람과 같은 인상을 주기 위한 시도, 즉 의인화적 요소에 초점을 맞추어 분석적인 연구를 진행하여 대화형 커머스 챗봇 디자인에 적용할 수 있는 실질적인 가이드라인을 제시하고자 하였다.
먼저 대화형 커머스 챗봇에서 나타나는 인터랙션 요소와 의인화 요소를 조사하고자 전 세계 최다 이용자를 보유한 모바일 메신저 플랫폼인 페이스북 메신저(Facebook Messenger), 킥 과 (Kik) 국내 최대 플랫폼인 카카오톡(Kakaotalk), 3 가지 플랫폼을 기반으로 한 대화형 커머스 챗봇을 4개 선정하여 7단계로 분류한 챗봇 주문 구매 프로세스 (챗봇검색, 채팅시작, 취향분석, 상품검색, 상품선택, 상품구매, 대화종료)에 따른 인터랙션 요소와 의인화 요
소를 분석하여 각각 38개 항목, 29개 항목을 추출하여 코드화 하였다.
도출한 인터랙션 요소 및 의인화 요소를 토대로 대화형 커머스 챗봇에서 사용자가 수행하는 태스크에 초점을 맞추어 사용자 조사를 진행한 뒤, 결과를 분석을 통해 서비스의 신뢰도와 사용성을 향상시키는 인터랙션 및 의인화 요소 디자인에 대한 인사이트를 도출하였다. 이를 토대로 보다 실용적으로 적용가능한 대화형 커머스 챗봇 설계 가이드라인과 디자인 프로토타입을 제시하였고, 제시된 프로토 타입은 챗봇 기획 개발 경험이 있는 전문가 3명을 대상으로 한 휴리스틱 평가를 통해 디자인 제안의 필요성, 개선 타당성, 서비스 사용 매력성, 기획 개발 시 적용 적절성을 평가하였다.
제시된 휴리스틱 평가 결과를 종합했을 때 본 연구에서 제시한 대화형 커머스 챗봇의 인터랙션 요소와 의인화 요소를 모두 고려한 디자인 개선안 제안이 실질적으로 필요하다는 점과 사용자 조사를 통해 도출한 불편사항과 사용자 니즈를 타당하게 개선하였다는 점에 모든 참여자가 만장일치로 동의하였다. 결론적으로 본 연구에서 제시된 대화형 커머스 챗봇 디자인 개선안이 현재 커머스 챗봇 설계에 활용된다면 활용되지 않는 것보다 사용자 경험이 더욱
개선될 것이라는 점을 휴리스틱 평가를 통해 검증하였다.
이 연구는 대화형 커머스 챗봇 서비스의 소비자 신뢰를 증가 시키기 위한 요소로 대화형 커머스 챗봇의 인터랙션 특징과 함께 의인화 요소에 초점을 맞추어 챗봇 디자인 연구를 진행했다는 점에서 기존의 대화형 인공지능 연구들과 차별점을 지닌다. 또한 이 연구는 사용자 측면에서 대화형 커머스 챗봇 디자인의 중요 인사이트를 도출하여 디자인 프로토타입을 제안하여 보다 실무적으로 적용할 수 있는 커머스 챗봇 디자인에 대한 심층적인 이해를 도모하였다.

프로그래밍 언어는 정확한 문법 체제와 키워드가 존재한다. 이러한 특징을 이용하여 본 논문은 키워드와 불리언 연산을 이용한 프로그래밍 언어 자동 채점 시스템을 설계하고 구현하여 실험하였다. 프로그래밍 언어를 정답 유형별로 7가지로 나누고 키워드 입력 시 접속사를 특수문자를 이용해 입력하게 하여 논리식을 바로 만들어 낼 수 있었으며 스택의 구현으로 쉽게 연산이 가능했다. 그리고 기존의 실기형 채점에 주관식 채점을 추가함으로써 프로그래밍 교과목 채점을 자동화 할 수 있다. 주관식 채점에서 활용되는 방대한 자연어 처리를 배제할 수 있도록 키워드와 불리언 연산을 이용한 방식을 제안하므로 구현의 효율성이 높아졌다. 또한 채점을 웹으로 이관함으로써 학생들에게 결과를 빠르게 피드백해주고 채점자의 주관적 개입을 배제하여 객관성, 공정성, 신속성을 높였다.
서답형은 출제자의 답안 입력 시 선택한 용어에 의해 채점 결과가 많이 좌우된다. 동의어 사전의 구축으로 출제자는 평소에 사용하는 한 단어만을 입력하더라도 사전에 의해 동의어 사용도 채점이 가능해 져야 한다.
추후 연구 과제로는 한글의 특성상 여러 가지 동의어가 활용될 수 있으므로 동의어 사전의 구축이 필요하다.The purpose of this paper is to design, implement and test the automatic marking system for programming languages using key-words and boolean operations to solve the processing problems of natural languages. There are accurate grammar systems and key-words in programming languages. Using these characteristics, We have designed, programmed, and tested automatic marking system for programming languages through key-words and boolean operations in this paper. We have categorized programming languages into 7 types as the type of answer and when a professor input any key-words, the system make him put conjunction with the special character. It can be logical expressions instantly so that the system easily operates through making stack.
We asked 10 students who are majoring in computer engineering to take a test on the paper and web to show how well automatic marking system that we have programmed works. Then we requested 3 professors if the subject problems marked objectively. As a result, automatic marking system proved to be appropriate.
We have proposed the way of using key-words and boolean operation for prohibiting huge natural language processing in marking of subjective question. It promotes efficiency rate of programming, objectivity and speed through the transferal to the web for marking since the system prohibits marker to include personal opinion on marking and gives feedback quickly.Ⅰ. 서론 = 1 Ⅱ. 연구 배경 = 4 1. 평가 방법 = 4 2. 컴퓨터 프로그래밍 언어의 평가 방법 = 7 3. 프로그래밍 자동 채점 시스템에 관한 선행 연구 분석 = 12

최근 하드웨어와 빅 데이터 기술의 발달로 인해 인공지능 분야가 빠른 속도로 발전하고 있다. 인공지능 분야 중에서도 전문가시스템과 딥러닝 기술이 가장 주목받고 있다. 전문가시스템은 인간의 특정 분야에 대해 전문지식을 정리하고 표현하여 컴퓨터에 기억시킴으로써, 전문가의 지식을 일반인도 쉽게 접근하고 이용할 수 있는 시스템이다. 그리고 딥러닝은 특징이나 패턴을 이용하여 분류하거나 인식하는 방법으로 기존의 기계학습 기법보다 뛰어난 성능을 보인다.
특히, 영상처리나 자연어처리 분야에서 가장 많이 사용되고 있으며, 기존에는 40~50%에서 머무르던 영상분할 기법, 물체탐지 등의 기술이 딥러닝을 통해 새로운 전환기를 맞이하고 있으며, 현재는 80~90%의 성능을 보이는데 이러한 수치는 매우 우수한 것이다.
영상분할이란 전경과 배경을 구분하는 방법을 말하는 것으로 인스턴스 영상분할과 의미론적 영상분할로 나뉠 수 있다. 인스턴스 영상분할은 픽셀 단위로 물체를 분류하는 방법을 말하며 의미론적 영상분할은 픽셀 단위로 나누어 물체의 클래스를 분류하는 방법을 말한다. 즉, 영상분할은 사물의 연관성과 경계선을 명확하게 알아보는 방법을 말한다. 기존에는 임계값 혹은 특정한 규칙을 이용하여 영상을 분할하였으나 이 임계값과 규칙을 정의하기에는 까다로우며 어렵다는 단점이 존재한다.
하지만 최근 딥러닝을 이용한 방법을 통해 그 까다로운 조건이 모델설계를 통해 해결되면서 성능도 향상되었다. 그 중에 의미론적 영상분할 모델 중 하나인 DeepLab은 Google Research에서 개발한 모델로서, 기존 모델들이 많이 사용하던 Unpooling, Convolution을 합친 Atrous Convolution을 사용한다. Atrous Convolution은 Unpooling과 같이 수용범위를 넓히면서 계산양을 최소화하는 Convolution 연산이다. 그리고 CRF라는 확률모델을 기반으로 한다. CRF는 추론할 때 최대사후확률을 예측하여, 기존의 픽셀과 이후 픽셀을 비교하여 픽셀을 섬세하게 만드는 방법이다. 이 방법은 후처리로 많이 사용된다.
기존의 학습 성능을 향상시키기 위한 방법으로 End-to-End 방법, 앙상블 방법을 사용했다. End-to-End 방법은 다양한 수용범위(Receptive Field)를 얻기 위해 복잡하고 깊은 모델을 설계했어야 했고, 앙상블의 경우 모델의 단일화의 연구가 많이 진행되었으나 물체의 세밀한 윤곽을 잡기는 힘들었다.
본 논문에서는 두 모델의 상호관계를 적용한 방법을 도입한다. 상호 관계적 방법은 두 모델이 서로 연결되어 있으며, 역전파과정에서 두 모델의 특징을 보완하면서 학습하는 방법을 말한다. 기존의 하나의 모델에서 학습할 때, 다른 하나의 모델을 더 설계하여 상관관계를 가지게 되어 내부에서 앙상블이 되는 방법을 제안한다. 이 방법은 두 개의 모델에서 입력하여 모델을 하나가 두 개가 되는 앙상블방식의 학습방법을 사용한다. 이 방법을 사용했을 때, 기존의 Deeplab 모델보다 1~3% 의 성능이 향상 되었다.Recently, artificial intelligence is rapidly developing due to the development of hardware and big data technology. Among the artificial intelligence fields, expert system and deep learning technology are attracting the most attention. The expert system is a system that can easily access and use the knowledge of the experts by summarizing and expressing expert knowledge about a specific field of human being and storing it on a computer. Deep learning is a method of classifying and recognizing, using features or patterns.
Especially, it is widely used in the field of image processing and natural language processing. The technology of the image segmentation technique and object detection, which had been in the 40~50% of the past, is coming to a new turning point through deep learning. Currently, 80~90% Performance is excellent, but these figures are excellent.
Image segmentation refers to a method of distinguishing between foreground and background, which can be divided into instance image segmentation and semantic image segmentation. Instance image segmentation is a method of classifying objects in pixel units. Semantic image segmentation is a method of classifying object classes by dividing them into pixel units. In other words, image segmentation refers to a method of clearly recognizing associations and boundaries of objects. Conventionally, images are segmented using a threshold value or a specific rule, but there is a disadvantage that it is difficult and difficult to define the threshold value and rule.
However, recently using the deep learning method, the difficult conditions have been solved through the model design and the performance has improved. Among them, DeepLab, one of the semantic image segmentation models, is developed by Google Research and uses Atrous Convolution, which is a combination of Unpooling and Convolution, which are commonly used in existing models. Atrous Convolution is a Convolution operation that minimizes the amount of computation while expanding the acceptance range like Unpooling. And it is based on a probability model called CRF. The CRF is a method of predicting the maximum posterior probability when inferring and delineating the pixel by comparing the existing pixel with the subsequent pixel. This method is often used as a post-treatment.
End-to-end method and ensemble method were used to improve existing learning performance. The end-to-end method had to design a complex and deep model to obtain various acceptable ranges, and in the case of an ensemble, the study of the unification of the model proceeded a lot, but it was difficult to precisely outline the object.
Is this paper, we propose a method in which CNN feeds back information on the back propagation process and correlates with each other, using the interaction of features extracted from the two models. In the case of existing ensembles, the results of learning one or more models are deduced together, but the proposed model suggests an ensemble method that enables mutual relations to be established within the model by changing the ensemble model. This method can be seen as a combination of existing end-to-end learning and ensemble methods. The proposed method has improved accuracy by 1~3% compared to the existing Deeplab model.국문 요약 제1장 서 론=1 1.1 연구배경 및 목적=1 1.2 논문의 구성=3

우리가 사용하고 있는 웹에는 뉴스, 일기예보, 법률, 평가 등 다양한 종류의 정보 형태가 존재하고 있다. 이와 같이, 웹에 존재하고 있는 정보형태 중 규칙을 추출하여 규칙기반시스템과 같은 지능적인 서비스를 제공하려는 연구들이 진행되고 있다. 하지만 여기에는 다음과 같은 세 가지 어려움이 존재한다. 첫째, 규칙을 추출하기 위해서는 규칙을 구성하는 변수와 변수에 연관된 값들에 대한 정보가 사전에 정의되어야 한다. 하지만, 링크와 같은 웹의 속성과 자연어로 이루어져 있는 웹 문서의 특성상, 규칙을 추출할 대상이 규칙 추출에 필요한 모든 규칙구성요소들을 포함하고 있지 않다. 둘째, 웹에서 규칙구성요소들을 추출하여, 한 도메인에서 사용될 규칙으로 생성하기 위해서는 해당 도메인에 대한 정확한 지식이 있어야 정확한 규칙 추출을 할 수 있게 된다. 하지만, 도매인 전문가가 해당 도메인에서 규칙 추출을 위한 정확한 지식이 없을 경우, 정확한 규칙 추출이 어렵게 되며, 규칙 추출을 위해 해당 도메인에 대한 추가적인 학습을 요구하게 된다. 마지막으로 기존의 방법들은 규칙을 생성하기 위해 규칙을 표현하는 언어를 알고 있어야 한다. 그러므로 이를 위해 해당 언어에 대한 추가적인 학습이 필요하며, 다른 규칙으로 변환 시, 또 다른 언어에 대한 학습이 요구된다. 또한, 규칙 표현 언어를 이용하여 코딩을 통해 규칙 생성 과정이 진행되기 때문에 도메인전문가에 의한 오류가 발생할 수 있다. 본 논문에서는 이들 문제를 해결하기 위해, 제한된 언어와 온톨로지를 사용한다. 첫 번째 문제와 두 번째 문제를 해결하기 위해, 우선 해당 도메인에 대한 정보를 가지고 있는, 즉 개념들과 개념들 간의 관계를 기술하고 있는 온톨로지를 미리 구축하여, 규칙 추출 대상에 존재하는 않는 규칙구성요소를 온톨로지를 통해 얻는다. 또한, 규칙 구성요소들 간의 변수와 값 관계가 온톨로지에 표현되어 있어 규칙 추출 시, 정확한 규칙을 생성할 수 있게 된다. 세 번째 문제를 위해서, 기계와 사람이 모두 이해할 수 있는 제한된 언어 형태를 이용하여, 규칙 표현 언어에 대한 추가적인 학습도 제거하였으며 기계에 의해 규칙 생성 과정을 유도하게 하여 규칙 생성 시, 도메인전문가에 의해 발생할 수 있는 규칙의 오류를 최소화하였다. 본 논문에서 제안하는 프레임워크를 설명하기 위해 임의의 인터넷 쇼핑몰에서 수행되는 배송관련 웹 페이지를 선정하였다. 본 논문에서 제시하는 프레임워크는 웹에서의 규칙추출 과정의 효율성을 높일 수 있을 것으로 기대되며, 또한 XRML에서의 지식 습득 과정의 효율성 제고에 기여할 수 있을 것으로 기대된다.제 1장 서론 1 1. 연구배경 1 2. 연구목적 2 3. 논문의 구성 3 제 2장 관련 연구 4

소셜미디어(social media)의 등장에 따라 사람들의 생각이나 의견을 담고 있는 문서들이 지속적으로 생성되고 있다. 이러한 문서에는 특정 사건에 대한 사람들의 견해, 상품에 대한 감상평 등이 기록되어 있다. 이러한 문서를 분석하면 다수의 사람들이 갖고 있는 생각과 트렌드를 이해할 수 있다. 감성 분석(Sentiment Analysis)은 이러한 문서를 분석 하는 대표적인 방법이다. 감성 분석은 자연어처리 태스크 중 하나로써 문서 내에 표현된 감성 극성들(sentiment polarities)을 알아내는 것을 목표로 한다.
최근 등장한 딥러닝 기반 모델은 감성 분석에서 높은 성능을 보여주었다. 대표적인 모델인 CNN은 간단하면서도 효과적인 모델 구조를 갖는다. 하지만 기존 CNN에는 다음과 같은 4가지 단점이 존재한다. 첫째, 모델의 성능이 word2vec에 의존적이다. 둘째, 문서의 구조적 정보 및 장기 의존성문제를 고려하지 못한다. 셋째, 중요도가 높은 단어나 문장을 동일하게 처리한다. 넷째, 딥러닝 모델의 특성상 black box문제를 갖고 있다.
본 논문에서 제안하는 어휘 강화 임베딩을 위한 멀티채널 CNN과 결합된 계층형 집중 네트워크에서는 위와 같은 4가지 문제를 다음과 같은 방법으로 해결하였다. 첫째, 멀티채널 기법 및 lexicon embedding을 적용하여 embedding의 표현을 개선했다. 둘째, 계층형 GRU encoder를 사용하여 문서의 구조적 정보를 고려하고 장기 의존성 문제를 해결했다. 셋째, Attention mechanism을 적용하여 중요도가 높은 단어와 문장을 고려했다. 넷째, Attention mechanism을 시각화하여 black box문제를 해결하기 위한 정보로 활용했다. 결과적으로, 제안하는 모델은 SemEval 2016 Testset에 대하여 64.11%의 F1 score를 기록하여 state-of-the-art의 성능을 얻을 수 있었다.요 약 문 1. 서론 1 1.1 연구의 필요성 1 1.2 연구의 내용 3

바야흐로 빅데이터(Big Data)를 넘어서 빅뱅 데이터(Big Bang Data) 시대
라고 해도 과언이 아닐 정도로 하루가 다르게 쏟아져 나오는 새롭고 풍부한
데이터와 이를 근간으로 사용자의 행동 패턴(Behavioral Pattern) 또한
급격한 변화가 이루어 지면서 기존의 산업구조 역시 급격한 변화와 재편이
이루어지고 있다.
대표적으로 인터넷을 통한 정보를 습득하는 채널이 PC 환경에서 모바일
디바이스 중심으로 전환이 되면서 데이터의 사용량이 급속하게 증가를
하였으며, 때와 장소의 제약에서 PC보다는 훨씬 자유롭게 되면서 이제는
모든 정보에 대한 피드백 또한 더욱 더 신속하고 정교화와 다양성을 이루지
못하면 빠르게 변화의 흐름 속에서 도태 될 수 밖에 없는 현실이다.
그 속에서 모바일 시대의 도래로 인하여 더욱 더 크게 팽창하는 오픈마켓,
소셜커머스, 종합몰 및 전문몰등 다양한 방식으로 존재하는 전자 상거래
(電子商去來, electronic commerce, e-commerce, eCommerce) 사이트 역시
나날이 증가하는 방대한 상품을 중심으로 한 제반 컨텐츠로 인하여 물품
구매를 희망하는 고객(Buyer) 또한 정확한 상품명이나 모델명을 인지하지
않은 상태에서는 구매를 원하는 상품을 검색하고 결제 하기는 여간 어렵지
않아서 때때로 과유불급(過猶不及)을 초래하는 경우가 왕왕 발생한다.
이에 본 논문에서는 전자상거래(e-commerce) 사이트에서 현재 거래되고
있는 실제 상품군(Item Group)을 분석하여 고객의 요구에 맞는 제품 검색을
한결 더 용이 하도록 상품 검색 키워드 추천 시스템에 관한 분석, 설계 및
사례 연구와 상품을 판매하는 판매자의 상품을 분석하여 검색에 용이한
일련의 키워드를 추천하는 시스템의 분석 및 개발을 통해 구매자와 판매자
상호간의 거래 행위를 위한 기반 데이터의 최적화를 연구하였다.
이러한 추천 시스템 개발을 위하여 다양한 머신러닝(Machine Learning, ML,
기계학습) 개발 프로세스 방법론을 분석하여 한국을 비롯하여 전자상거래
시장에서 중요한 상품(물품) 추천을 위하여 새롭게 확장 적립한 머신러닝
프로세스 방법론 모델로 Machine Learning Helical Amplifier in e-Commerce
(MLHA)을 또한 제시한다.요 약 1 1. 서 론 7 2. 본 론 9 2.1 머신러닝 개발 프로세스 방법론 및 MLHA 9 2.2 주요 머신러닝 개발 프로세스 방법론 9

최근 IT 산업 전반에서 빅 데이터라는 용어가 사용되기 시작하였다. 스마트 폰을 비롯한 모바일 디바이스의 발달과 페이스북, 트위터 등으로 대표되는 SNS는 하루에도 수백 테라바이트 이상의 데이터를 만들어 내고 있는데, 정부나 기업에서는 이렇게 만들어진 데이터를 이용하여 정책 개발, 미래 예측, 상품개발, 마케팅 등 다양한 분야에 활용할 수 있다.
이러한 빅 데이터를 분석하는 기술로는 기계 학습, 자연어 처리, 데이터 마이닝 등이 있는데 이러한 데이터 분석 기술에 관한 관심이 증가하고 있다. 이에 따라, Dr. Robert Lynch는 BDRA(Bayesian Classification and Feature Reduction Algorithm)라는 데이터 마이닝 알고리즘을 제안했다. 이 알고리즘은 다른 데이터 마이닝 알고리즘과 비교하여 상대적으로 적은 연산으로도 신뢰할만한 마이닝 결과를 보여준다.
그렇지만 BDRA는 탐욕(greedy) 알고리즘을 사용하기 때문에 대상 데이터에 따라 항상 최적의 결과를 제시하지 못한다는 단점이 존재한다. 항상 최선의 결과를 보장하기 위해서는 탐욕 알고리즘을 제거하고 조합 가능한 모든 경우의 수를 계산해야하는데, 이는 데이터가 커질수록 연산 시간이 크게 증가하게 된다.
이러한 문제점을 극복하고자 본 논문에서는 병렬시스템을 이용한 BDRA의 개선 기법에 대해 제안한다. GPU와 CPU를 사용하여 병렬화를 수행하고, 입력된 데이터에 따라 적절한 병렬화 기법을 사용함으로써 데이터의 연산 시간을 줄임으로써 조합 가능한 모든 경우의 수를 계산하고 기존의 알고리즘과 비교하여 수행시간에 있어 큰 차이가 없도록 개선함으로써 항상 최선의 결과를 보장할 수 있도록 알고리즘을 개선하였다.

최근 딥러닝 기술이 주목을 받고 있다. 대중들의 관심을 받았던 국제 이미지 인식 기술 대회(ILSVR)와 알파고(AlphaGo)에서 사용된 딥러닝 기술이 바로 합성곱 신경망(CNN; Convolution Neural Network)이다. 합성곱 신경망은 입력 이미지를 작은 구역으로 나누어 부분적인 특징을 인식하고 이것을 결합하여 전체를 인식하는 특징을 가진다. 이러한 딥러닝 기술이 우리의 생활에 있어 많은 변화를 야기할 것이라는 기대를 주고 있지만 현재까지는 이미지 인식과 자연어 처리 등에 그 성과가 국한되어 있다. 비즈니스 문제에 대한 딥러닝 활용은 아직까지 초기 연구 단계로 향후 마케팅 응답 예측이나 허위 거래 식별, 부도 예측과 같은 전통적 비즈니스 문제들에 대해 보다 깊게 활용되고 그 성능이 입증된다면 딥러닝 기술의 활용 가치가 보다 더 주목받게 될 것으로 기대된다. 이러한 때 비교적 고객 식별이 용이하고 활용 가치가 높은 빅데이터를 보유하고 있는 전자상거래 기업의 사례를 바탕으로 하여 딥러닝 기술의 비즈니스 문제 해결 가능성을 진단해보는 것은 학술적으로 매우 의미 있는 시도라 할 수 있겠다.
이에 본 논문에서는 전자상거래 기업의 고객 행태 예측력을 높이기 위한 방안으로 합성곱 신경망을 활용한 ‘이종 정보 결합(Heterogeneous Information Integration)의 CNN 모델’을 제시한다. 이는 정형과 비정형 정보를 결합하여 다층 퍼셉트론 구조의 합성곱 신경망에서 학습시키는 모델로서 최적의 성능을 발휘하도록 ‘비정형 정보의 벡터 전환’, ‘이종 정보 결합’, ‘다층 퍼셉트론’, 그리고 ‘합성곱 계층’으로 하는 4개의 설계 방식을 정의하고 각 설계 방식에 따른 성능을 평가하여 그 결과를 바탕으로 제안 모델을 확정하고자 한다. 본 연구에서 사용한 고객 행태 예측을 위한 목표 변수는 재구매 고객, 이탈 고객, 고빈도 구매 고객, 고빈도 반품 고객, 고단가 구매 고객, 고할인 구매 고객 등 모두 6개의 이진 분류 문제와 예상 구매 횟수, 예상 구매 금액, 예상 반품 횟수, 예상 반품 금액, 고객 예상 가치 등 5개의 수치 예측 문제로 각각 정의한다. 실험 구성은 훈련 데이터와 테스트 데이터를 분리하는 기준에 따라 동일 기간 실험과 분리 기간 실험 2가지로 구분한다. 먼저 동일 기간 실험은 훈련 데이터와 테스트 데이터를 랜덤 샘플링 방식으로 분리하는 방식이다. 다음으로 분리 기간 실험은 특정 시점을 기준으로 훈련 데이터와 테스트 데이터를 구분하는 방식으로 과거의 고객 행동 패턴을 기준으로 미래의 고객 행위를 예측하는 구조이다.
제안한 모델의 유용성을 검증하기 위해서 국내 특정 전자상거래 기업의 실제 데이터를 활용하여 실험을 수행하였다. 이진 분류 문제에 대한 실험 결과에서는 정형과 비정형 정보를 결합하여 CNN을 활용한 제안 모델이 NBC(Naïve bayes classification)과 SVM(Support vector machine), 그리고 ANN(Artificial neural network)에 비해서 예측 정확도가 높은 것으로 평가되었으며 NBC, SVM, ANN에서는 정형 정보만을 사용할 때 보다 정형과 비정형 정보를 결합하여 입력 변수로 함께 활용한 경우에 예측 정확도가 향상되는 것으로 나타났다. 수치 예측 문제의 실험 결과에서는 제안 모델이 5개의 목표 변수 중에서 2개의 목표 변수의 성능이 높게 평가되었다.
따라서 실험 결과로부터 비정형 정보의 활용이 고객 행태 예측의 정확도 향상에 기여한다는 점과 CNN 기법이 지금까지 소개된 이미지 인식이나 자연어 처리 분야 외에 비즈니스 문제 해결에도 활용 가치가 높다는 점을 확인하였다는데 이 연구의 의의가 있다 하겠다.

소프트웨어 시스템의 규모와 복잡성이 증가함에 따라 시각화(Visualization), 모델화(modeling)가 핵심적인 사항이 되었으며, UML은 이러한 필요성에 부응하기 위해 정의되었으며, 광범위하고 다양한 도메인을 수용하도록 설계되었다. 하지만, UML은 객체의 구조와 행위에 대한 기술을 위한 지원은 풍부하지만, 객체에 대한 제약을 기술하기에는 한계가 있어서 OMG에서 이를 보완한 정형언어가 바로 객체 제약 언어(Object Constraint Language)인 OCL이다.
그러나, OCL은 쉽게 적용할 수 있음에도 불구하고 목표로 하는 모델을 구축함에 있어 사용자의 요구와 의미를 반영하기에 한계를 가진다. 왜냐면, 요구 사항의 명세는 우선 자연어로 기술하며, 이를 토대로 OCL로 표현함에 있어 정확히 반영하기가 어렵고, 기술한 제약 사항이 제대로 기술되었는지에 대한 검증을 평가하기가 힘들다. 따라서, 본 연구에서는 UML기반 OCL 기술을 위한 불변조건 생성에 관한 가이드라인에 대한 연구를 하였으며, 생성된 불변조건을 토대로 USE 도구를 이용한 명세 모델 구축 및 검증에 대하여 기술하였다.
기대 효과로는 첫째, OCL 불변조건 생성에 관한 가이드라인은 기존의 비주얼한 UML표기에 대한 의미를 OCL로서 확장 할 수 있다. 둘째, 적용된 OCL에 대한 검증은 USE도구를 이용하여 평가함으로서 목표로 하는 명세모델의 완전성을 지원할 수 있다. 세째, 평가된 OCL은 클래스다이어 그램의 풍부한 어휘를 대상으로 적용함으로서 목표로 하는 모델과 모델사이의 일관성을 지원할 수 있다. 네째, 객체 관계에 대한 네비게이션의 흐름에 대한 제약은 사용자와 개발자가 간과하기 쉬운 제약으로서 제안하는 점검사항을 대상으로 확인하고 분석함으로서 초기 모델의 강건성에 대한 방향을 제시한다.Methods for object-oriented analysis and design have brought a greate number of languages and notations supporting many aspects of the software specification process. The Unified Modeling Language(UML) is the result of an effort in developing a single standardized language for object-oriented modeling. Part of the latest UML proposal is Object Constraint Language(OCL). OCL is formal language that remains easy to read and write. In context of information systems modeling, UML class diagrams can be utilized for describing the overall structure, whereas additional integrity constraints and queries are specified with OCL expressions. By using OCL, constraint and queries can be specified in a formal yet comprehensible way. However, the OCL itself is currently defined only in a semi-formal way. Thus the natural language of constraints is in general not precisely defined.
Consequently, in this paper approach the guideline regarding the OCL invariant expression creation of natural language specification. That is facilitates of invariant expression creation, validation and simulation of models and helps to improve the quality of models and software designs.목차 표차례=vi 그림차례=vii 감사의 글=viii 논문 개요=ix

최근 새로운 ICT 흐름으로 인공지능 및 머신러닝 기반의 자동화 서비스가 금융권과 유통업계를 중심으로 확대되고 있는 가운데 금융권에서 자동화된 채팅을 활용한 고객 응대 서비스인 ‘챗봇’ 개발에 주력하고 있다.
지금까지 많은 부분에서 인공지능 챗봇에 관한 연구가 이루어져 왔으나, 대부분이 단편적인 기술적 측면이나 공학적 측면에서 발전하였다. 이에 본 연구는 기존 연구결과와 챗봇 구현 기반의 인터렉션 특성에 따라 금융 챗봇의 전략적 연구를 수행한다. 금융 챗봇은 기본적으로 모바일에서 동작하는 대화형 시스템이지만 온전히 자연어 처리에만 의존할 수 없음을 고려할 필요가 있으며, 본디 속성으로는 금융업무라는 특정 서비스를 수행해야 하는 사용자의 목적 지향적인 태도를 인지하고 서비스를 제공해야 한다.
따라서 본 연구에서는 “금융 챗봇의 인터렉션 대화 유형은 사용자의 금융 서비스 목적에 따라 유용성, 사용성, 감성, 인지된 보안성에 차이가 있을 것이다.”라는 연구 문제를 중심으로 금융 서비스 챗봇의 인터렉션 유형이 사용자 경험에 영향을 미치는 요인에 대해 실증적인 연구를 진행하였다.
먼저, 현재 서비스하고 있는 금융 챗봇의 특성을 파악하고, 챗봇 구현 방식을 기반으로 인터렉션 대화 유형을 열린 대화, 닫힌 대화, 혼합 대화 3가지로 나누어 챗봇 프로토타입을 제작하였다. 사용자의 주요 금융 업무인 계좌조회, 계좌이체, Q&A 서비스를 중심으로 챗봇의 인터렉션 대화 유형이 사용자 경험에 미치는 영향을 사용자 관점에서 살펴보고자 하였다. 실험은 설문조사 형식으로 2018년 10월 15일부터 11월 15일까지 약 1달간 진행하였다. 실험자는 20-30대를 중심으로 온라인 설문조사 35명, 대면 설문조사 46명을 대상으로 실시하였다.
연구 문제에 대한 연구결과는 다음과 같다.
금융 서비스 챗봇의 인터렉션 유형은 유용성, 사용성 면에서 차이를 보였다.
닫힌 대화 인터렉션 방식은 계좌조회와 계좌이체 서비스 이용 시 유용성 면에서 가장 높은 선호도를 받았다. 닫힌 대화 인터렉션은 채팅 인터페이스 내에서 버튼 조작만으로 서비스가 이루어짐으로써 빠른 인터렉션이 가능했으며, 처음 서비스를 이용하는 사용자에게 버튼을 통해 시각적인 가이드 역할을 하였다. 사용자들은 다양한 업무를 병행하며 금융 서비스를 이용하는 현실 상황을 고려하여 오조작 없이 원하는 결과를 쉽게 처리할 수 있는 직관적인 인터페이스를 선호하는 것으로 나타났다.
혼합 대화 인터렉션 방식은 사용성 면에서 가장 높은 선호도를 받았다. 혼합 대화는 버튼 선택과 동시에 타이핑을 통해 자유로운 조작이 가능했다. 특히 실험자들은 금융 챗봇에게 가장 기대는 서비스인 계좌이체 업무 시, 금액 설정과 같은 필요한 설정이 가능하였기 때문에 사용자들이 가장 선호하는 금융 챗봇 인터렉션 유형이었다.
실험 직후 심층 인터뷰를 통해 금융 챗봇 서비스에 대한 사용자들의 기대와 요구 사항은 무엇인지 정리하여 금융 챗봇 기획 시 전략적인 인터렉션 방향을 탐색해보고자 하였다. 첫째, 사용자는 금융 챗봇과의 대화에서 감정적인 교류 보다는 금융 정보의 습득 면에서 긍정적인 의견을 보였다. 기존의 상담원과 비교하여 챗봇의 인공지능 서비스는 보다 객관적이고 합리적인 상품 추천이 가능할 것이라고 응답하였다. 또한 사소한 질문도 부담 없이 질문할 수 있는 채팅 시스템을 통해 사용자에게 친근하게 다가갈 수 있는 온 디맨드 서비스가 활발히 이루어질 수 있을 것이며, 이는 현재 단순한 업무와 질의응답과 같은 한정적인 서비스보다 사용자의 데이터를 활용한 개인화되고 다양한 서비스를 기대한다는 점을 의미한다. 둘째, 사용자들은 기존의 모바일 뱅킹 시스템의 인증 처리 방식보다 챗봇 시스템 내에서 더 많은 인증 절차를 요구하였다. 대화를 통해 이루어지는 금융 업무가 빠르게 이루진다는 느낌을 받았으며, 최종 업무가 완료되기 전에 사용자의 의사를 묻는 ‘확인’, 이전 단계로 회귀하는 ‘뒤로 가기‘와 같은 장치의 필요성을 대다수의 사용자들이 제시하였다. 또한 채팅의 특성상 금융 데이터 등 개인 이력이 남으므로 개인 정보 유출에 대한 불안감을 느꼈다. 매 업무마다 데이터 삭제 또는 대화 갱신 기능의 안전장치를 요구하였으며, 개인 자산이 안전하게 보관되고 있다는 것을 알림 받기 원하였다.
본 연구는 자연스러운 대화를 통해 서비스를 제공하는 인공지능의 감성적인 요소와 금융 업무를 수행하는 기능적인 요소를 모두 고려해야 하는 금융 챗봇 사용자들의 심층적인 이해를 필요로 하는 사용자 경험 향상을 위한 자료로 활용될 수 있다.Recently, with the new ICT flow, automation services based on artificial intelligence(AI) and machine learning have been expanding mainly in the financial industry and the distribution industry. In this situation, banking industry is focusing on the development of 'chatbot', a customer service using automated chat. Researches on artificial intelligence(AI) chatbots have been done so far, but most of them have developed in terms of fragmentary technical and engineering aspects.
Therefore, this study conducts strategic research of financial chatbots according to the results of studies and the characteristics of interaction based on chatbot implementation. Financial chatbot is basically an interactive system that operates on mobile phones, but it is necessary to consider that it can not depend entirely on natural language processing only, and the original attribute must recognize the purpose oriented attitude of the user who executes a specific service called financial business and provide the service. Therefore, in this research, “The type of interaction dialogue of Finance chatbot is different in efficiency, usability, sensibility and perceived security depending on the purpose of financial services of users.” Focused mainly on the factors that influence the interaction type of financial service chatbot to the user experience empirically.
First, I identify the characteristics of the financial chatbot currently in service, and created the chatbot prototype by dividing it into three types of interaction dialogue, open conversation, closed conversation and mixed conversation based on chatbot build method.
I investigated the effect of chatbot 's interaction dialogue type on the user experience, focusing on user' s main banking tasks such as account inquiry, account transfer and Q & A service.
The experiment was conducted in the form of a survey for about one month from October 15, 2018 to November 15, 2018. The experimenters conducted 35 online surveys and 46 face-to-face surveys mainly in the 20s and 30s. The results on the research problems are as follows.
Financial service chatbot's interaction type showed a difference in efficiency and usability.
Closed conversation interaction method received the highest evaluation in terms of efficiency, when using account inquiry and account transfer service. Closed conversation interaction is performed by button operation in the chat interface so that the service can be performed promptly and serving as a visual guide through the button to the user who used the service for the first time. Users have liked intuitive interfaces that can easily process desired results without erroneous operation, taking into consideration the real situation of using banking services in parallel with various tasks.
The mixed conversation interaction received the highest evaluation in terms of usability. Mixed conversation could be operated freely through input at the same time as button selection. Especially it was able to make the necessary settings, such as the amount setting, when using account transfer service, which is the most anticipated service for financial chatbots.
In-depth interviews were conducted immediately after the experiment to summarize the expectations and requirements of users of financial chatbot services, and to explore the direction of strategic interactions in financial chatbot planning.
First, users showed positive opinions in terms of acquisition of financial information rather than emotional exchange in conversation with financial chatbot. Compared with existing counselors, chatbot's artificial intelligence service was able to provide more objective and reasonable product recommendations. In addition, on-demand services will be able to actively reach out to users through a chat system that can ask questions freely without any questions. This means that it expect various personalized services utilizing user's data rather than limited services such as current simple tasks and question and answer.
Second, users require more authentication procedures in the chatbot system than the existing mobile banking system. Users feel that the financial work done through the dialogue is fast. Most users present the necessity of the device such as ‘confirmation’ asking the user's opinion before the final task is completed and ‘back’. Also, due to the characteristics of chat, personal history such as financial data remains, so it felt anxiety about the outflow of personal information. Each task required a safety device for deletion of data and conversation renewal function for each task and wanted to be notified that private property is safely kept.
This study will need a deep understanding of financial cheat users who need to consider both the sensibility elements of artificial intelligence that provides services via natural conversation and the functional elements that implement financial operations It can be utilized as a material for improving the user experience.Ⅰ. 서론 8 1.1 연구 배경 및 목적 8 1.2 연구 방법 9 1.3 연구현황 10 Ⅱ. 이론적 고찰 11

인터넷 보급의 확대는 인터넷 문화의 특성에 기반한 독창적인 인터넷 매체 언어를 발생시켰으며, 인터넷 매체 언어는 구어와는 국어 파괴 현상이 두드러지게 나타나고 있다.
본 연구는 인터넷 매체 언어상에서 나타나고 있는 국어 파괴 현상을 고찰하여, 이에 대한 기술적 개선 방법으로서 표준어 자동 번역 기술을 소개한다. 먼저 인터넷 매체 언어상에서 나타나고 있는 국어 파괴 현상과 관련된 기존 연구들을 분석하여, 국어 파괴 현상을 유형별로 특징지어, 각 특징에 따른 표준어 번역과 관련된 고려 사항을 제안한다.
또한, 실생활속에서 국어 파괴 현상과 관련된 경험 및 피해 사례들을 설문 조사 및 FGI(Focussed Group Interview)를 통하여 수집 및 분석한 결과를 제시한다. 이러한 관련 자료들을 기반으로, ‘국어 파괴도(degree of Korean destruction)'이라는 지표를 정의하였으며, 국어 파괴도는 본 연구에서 제안하는 표준어 자동 번역 기술의 대상이 되는 문장을 선별하기 위한 지표로 사용된다. 국어 파괴도는 표준어 자동 번역 기술이 갖고 있는 번역 오류가 가진 한계를 최소화하여, 표준어 자동 번역 기술의 효율성을 최대로 활용하기 위한 척도로서 활용될 수 있음을 인터넷 매체 문장 2,450 개의 분석을 통하여 제시하였다. 본 연구에서 제안된 국어 파괴도 측정 및 표준어 자동 번역 기술은 Java 언어를 사용하여 REST API 형태로 구현되어, 웹브라우저에서 동작을 확인하였으며, 향후 국어 파괴 현상과 관련된 기술적 개선 방법들의 연구와 실제적인 개선 프로그램을 마련하는데 도움이 될 것으로 판단된다.제 1 장 서론 1 제 1 절 연구 목적 2 제 2 절 연구 내용 4 제 2 장 이론적 배경 5 제 1 절 언론 보도 내용을 통한 사회적 인식 5

본 논문에서는 시간과 장소의 제약 없이 제어하기 위한 유비쿼터스 인터페이스를 기반으로 하여 언어와 연결된 원시동작의 연결에 의한 에피소드적 동작이 가능한 인간로봇 상호작용을 이용한 서비스로봇의 언어기반 행위제어 시스템을 제안한다.
먼저 서비스 로봇과 안드로이드 스마트폰으로 구성된 유비쿼터스 제어 환경을 구성하고 안드로이드 클라이언트에 버튼과 영상으로 이루어진 인터페이스를 구성한다. 로봇 서버에서 입력된 영상이나 상태를 나타내는 그래픽을 클라이언트에 전달하고, 버튼을 이용한 클라이언트의 제어 신호를 서버에 전달하여 사용자가 언제 어디서나 로봇의 제어가 가능하도록 한다. 또한 스마트폰에서 입력된 음성신호를 자연어 인식을 이용하여 텍스트로 변환하고 이를 서버에 전달하여 음성기반 제어가 가능하도록 한다.
또한 언어기반의 인지시스템을 기반으로 하여 로봇의 동작을 계층적으로 모델링하고 로봇의 행위에 대한 명령이 이루어지도록 하였다. 로봇제어 행위 계층구조 모델은 모든 동작은 에피소드, 원시동작 및 원자함수 순으로 계층적인 명령의 실행이 이루어지도록 한다. 또한 동일한 명령의 동사에 대해서 다양한 물체에 대해 적용할 수 있는 재사용이 가능하도록 하며, 이로 통해 일반 사용자들이 로봇에게 직접 프로그래밍 하는 과정 없이 명령어 언어로 로봇에게 요구되는 동작을 할 수 있도록 한다.
실험을 통해 제안한 다양한 모드의 인터페이스 환경이 사용자가 언제 어디서든지 로봇을 제어 할 수 있는 유비쿼터스 인터페이스 환경이 가능함을 보이고, 계층적 행위모델로 통해 다양한 명령어와 동사에 대해 적용을 확대할 수 있는 가능성을 보여준다. 주이로써 인간과 로봇이 언어로 통해 상호 작용할 수 있는 기본적인 기능이 가능함을 보인다.Ⅰ. 서 론 1 1. 연구의 목적 1 2. 기존의 연구 2 3. 연구의 내용과 구성 4 Ⅱ. 유비쿼터스 인터페이스 6

Online reviews, which would be a kind of short text documents, includes experienced story and opinions about product or services or features. The online reviews contains the evaluated results about the products or services in quantitative viewpoints as well as qualitative viewpoints. The quantitative assessment about the products would be represented as a form of star score while the qualitative assessment would be represented as a form of text sentences in online review. The text sentences in the online review includes sensitive assessments with positive or negative opinions about the products which would affect on product purchase behaviors of consumers. In this paper, we analyzed the linear regression relationship between the quantitative assessments and qualitative assessments in the online reviews. We derived implications for modeling ranking of online reviews being based on the result of linear regression relationship.표 차례 Ⅳ 그림 차례 Ⅴ ABSTRACT Ⅵ 제 1 장 서론 1 제 1 절 연구배경 및 목적 1

통신기술과 전자기기가 발전하면서 인터넷은 사람들이 정보를 얻기 위한 주요 수단으로 자리매김 하였다. 사람들은 영화의 평가나 구매 후기, 또는 정치적 사건에 대한 평가 등 다양한 정보를 보는 등 인터넷을 통해 특정 주제에 대한 의견을 손쉽게 접할 수 있으며 이에 익숙해져 있다. 이러한 시대적 흐름에 따라 수많은 리뷰로부터 사용자가 원하는 정보를 빠르게 분석해주고, 의미 있는 정보를 추출해내는 오피니언 마이닝이 중요해지고 있으며, 이에 따라 감성 분석, 자연어 처리와 같은 오피니언 마이닝을 효율적으로 하기 위한 분석들도 주목을 받고 있다.
본 논문에서는 길고 많은 평가글들을 사용자가 모두 읽을 필요 없이 점수화하여 한 눈에 파악할 수 있는 평가글 분석 방법을 제안한다. 연구 과정은 먼저 점수화되어 있는 영화 평가 데이터의 내용과 점수를 크롤링하였다. 이 점수가 높을수록 긍정적인 감성을 갖는다고 가정하며, 한 평가글 안의 단어들은 비슷한 감성을 갖는다고 가정한다. 평가 데이터의 내용을 트위터 형태소 분석기를 통해 명사와 형용사를 중심으로 각각 분석한 후, 벡터화 된 형용사와 명사를 이용하여 리뷰를 인공신경망에 학습시켜 평가글의 점수를 분석하는 평가글 분석기를 구하였다. 그 후 학습에 사용된 평가 데이터를 제외한 데이터를 사용하여 각각의 평가글 분석기의 성능을 비교하였다.With the development of communication technologies and electronic devices, the Internet has become a major means of getting information. People are familiar with and can easily get feedback on specific topics via the Internet, such as reviews of movies, purchases postscript, or opinions of political events. According to this trend, opinion mining that quickly analyzes information desired by users from a large number of reviews and intelligently deduces meaningful information is becoming important. Accordingly, emotions analysis and natural language processing are attracting attention which can make opinion mining more efficient.
In this paper, we propose a rating analyzer that can score long and many evaluation articles without having to read all, and grasp them at a glance. Firstly, I crawled the contents and scores of the movie review data. I assumed that if the review has higher score, it has the more positive sentiment, and also assumed that the words in one reviews have similar sentiment. I analyzed the contents of the review data by using the Twitter morpheme analyzer centering on nouns and adjectives, and then I made analyzer of reviews with artificial neural network learned with review contents using the vectorized nouns and vectorized adjectives. Then, I analyzed the performance of each analyzer with reviews data which are not used in training artificial neural networks.국문초록 ⅴ 영문초록 ⅶ 제 1 장 서론 1 1.1 연구배경 및 목적 1

감정인식 기술은 인간 중심의 휴먼 컴퓨터 인터페이스(HCI, Human computer interface) 또는 human-to-machine interface의 핵심 요소이다. 기존의 감정인식 기술은 주로 얼굴 영상, 음성, 생체신호를 이용하여 특징을 추출하고 이를 미리 학습시킨 인식 알고리즘 모델에 적용시켜 각 감정의 범주로 분류하는 방법을 사용한다. 하지만 정확한 감정인식을 위해서는 시간에 따른 감정의 변화나 사용자의 컨텍스트를 파악하는 것이 더 중요하다. 이를 위해서는 자연어 처리나 사용자의 의도 파악 등을 채용한 컨텍스트 인식(context-awareness) 기술에 대한 연구가 더불어 수행되어야 한다.

최근 컨텍스트 기반 감정인식 연구는 스마트폰 환경에서 활발히 수행되고 있다. 스마트폰은 멀티모달 센서들이 탑재되어 사용자의 컨텍스트 관련 정보들(text, life-log)을 수집하기에 용이하기 때문이다. 그러나 기존의 연구들은 감정 모델이 단순하고, 스마트폰에서 추론할 수 있는 컨텍스트 관련 정보들을 활용하지 못하고 사용자에게 수동으로 입력시키는 비효율성이 있다. 또한 개인화 된 감정 및 상황 모델을 사용하지 않아 인식의 정확도가 떨어지는 문제를 함께 가지고 있다.

본 논문에서는 스마트폰에서 사용자의 선호도 정보와 다양한 상황(context)에 대한 인식을 통해 사용자의 감정을 인식하고 이를 통해 사용자의 감정을 모델링하는 개인화된 감정 모델링 엔진을 제안한다. 제안하는 모델링 엔진은 퍼지 추론을 사용하여 인지된 상황정보에 대한 개인의 선호도 정보를 초기 설정된 감정모델에 맵핑(mapping)함으로써 기존의 스마트폰을 활용한 감정인식보다 다양한 감정을 인식하고 사용자의 검증과정을 거쳐 시간이 지날수록 감정인식의 정확도를 향상시킨다. 인식된 감정은 사용자의 검증과정을 거쳐 개인화된 감정인식이 가능하도록 하였다. 이를 검증하기 위해 스마트폰 환경에서 제안하는 감정인식 시스템을 구현하고 애플리케이션으로 실제 사용자의 사용 테스트를 통해 그 성능을 평가하였다.A study on emotion recognition is the key component of the human centric human-computer interface(HCI). Many studies on emotion recognition have classified emotions using machine learning technique that recognizes complex patterns and make intelligent decisions based on features from facial images, speech and bio-signals and classifies emotion. However, it is more important to infer changes of user's contexts according to user's preferences over time than to recognize only emotions. In order to achieve contexts with user's emotions, context-aware technique should be studied, which adopts natural-language processing and user's preferences understanding.
Recently context-aware researches have been performed in smartphone environments. Because we can collects user's various context-related information effectively using smartphone contained multi-modal sensors. But the existing studies just has classifying method for a few emotion, but also it is inefficient. Because it is not utilizing contexts can be infered from smartphone, which makes user input data manually. Moreover it can not reflect the difference depending on individual.
In this paper, we propose a method of emotion recognition on smartphone that extracts emotion from using user's preferences and context-awareness and provides result of personalized emotion recognition using by user's feedback. Therefore, it is possible to recognize diverse emotion applicatively and reliably. In experiment, we implements proposed emotion recognition system in smartphone. And we assess the performance of system from user's application test.제 1 장 서 론 3 제 2 장 관련 연구 6 제 3 장 감정 인식(Emotion Recognition) 8

격틀은 구문분석뿐만 아니라 의미분석 과정에도 중요한 정보원이 된다, 본 연구는 각 용언에 필요한 논항정보와 그 논항의 의미역을 파악하는 방법을 고려하여, 직접적으로 의미역을 파악할 수 있는 형태로 격틀을 정의하여 이의 구축 방법을 보인다.
본 연구에서는 문법 관계를 고려한 동사-명사구 어휘 관계 데이터를 추출하여 공기 관계 사전, 선택제약 사전, 하위 범주화 사전을 만들었고, 그를 통해 얻어진 어휘 정보를 이용하여 미지격의 격 결정을 통계적 방법으로 시도하였다. 그러나 자연어처리의 많은 예가 말뭉치에서 존재하지 않기 때문에 단어 자체에 대한 통계값에 의존하는 단어 기반 모델은 문제점이 있다.
따라서, 본 연구에서는 통계 기반 모델의 취약점인 데이터 희귀성을 극복하기 위해 단어 기반과 더불어 클래스 기반과 유사도 기반 모델을 시도해 관련도값을 구해낸다. 또한 새로운 단어, 즉 실험 말뭉치에서 나타나지 않은 명사의 결합은 가장 유사한 명사의 정보로 해결하는 방법과 그가 속한 클래스의 개념 정보로 해결하는 방법을 보인다. 이 때, 새로운 단어는 개념 클래스 정보가 시소러스에 존재한다고 가정하고 단어가 속한 모든 클래스의 개념 정보를 적용한 후 가장 연관도 값이 큰 클래스의 어휘 관계를 선택하게 된다.
고려한 문법 관계는 주격, 목적격, 부사격, 관형격, 미지격이다. 단어 기반의 통계 정보와 사전의 구조를 이용해 자동적으로 추출한 클래스 기반 통계 정보를 확대하여 학습 말뭉치에서 발견되지 않았던 단어의 관계까지 예측을 시도하였다. 문법관계 예측을 위해 미지격의 격 결정을 시도한 결과 91%의 성능을 보였다. 본 연구결과를 토대로 한국어 의미분석 작업에 선행되어져야 할 연구가 무엇이고, 어떤 분야가 상호 관련되어 학제간 연구로 수행되어져야 바람직한지를 파악하는데 활용될 수 있다.Case frames are important resources of information in the phase of semantic analysis as well as syntactic analysis. This research proposes a new form of Case frames as putting a focus on a level of semantic analysis. That is, we define Case frames in the form where thematic roles can directly be picked out, considering the argument information for each declinable words and how to pick out the thematic roles of the arguments. Finally, we show how to construct the Case frames.
We extract lexical information from corpus to build a lexical dictionary. The lexical relationships mean the triples consisted of predicate, noun phrase, and the condition of syntactic relations attached to noun. The generated lexical dictionaries are co-occurrence dictionary, selectional restriction dictionary, sub-categorization dictionary and noun-noun similarity dictionary to determine unknown case relation. However, word-based model depending upon statistical data of word has some problems because many examples can't exist in corpus.
In this study, to overcome data sparseness which is a drawback of statistical NLP, obtain class-based and similarity-based association score as well as word-based association score. The relation of unseen word in the corpus can be determined through information of the most similar noun and concept information of class which unseen word belongs to. We assume that class of unseen word is pre-determined through thesaurus and apply the concept information of all classes which the word belongs to. and select lexical relationships of class which association score is the largest.
The considered syntactic relations in this study are subjective, objective, adverbial. adjective, and unknown cases. After applying the information to unknown case relation, it shows good for resolution that we get the 91% performance rate. Based on the result of this research, we can know what studies should be preceded and what topics are desirable as interdisciplinary researches in the semantic analysis of Korean.차례 = iv 제1장 서론 = 1 1.1 연구 배경 및 목적 = 1 1.2 관련 연구 동향 = 2 1.3 연구 내용과 범위 = 4

국 문 요 약

최근 열처리 산업은 4차 산업혁명과 스마트 공장의 출연으로 지능정보 기술이 지속 개발되고 있다. 정부 또한 자동화·첨단화 지원사업, 스마트공장 보급·확산 사업 등을 통해 열처리 관련 뿌리기업이 스마트공장을 도입 할 수 있도록 적극 지원하고 있는 추세다. 또한 열처리 뿌리산업별 특성에 따른 자동화·스마트 화에 맞춰 MES, ERP 등의 전사관리 시스템 역시 기업의 요구에 따라 도입되고 있다. 열처리 산업 분야 중 침탄열처리는 금속 재료 특성 변화와 개량을 위해 가열, 템퍼링, 환기, 퀜칭 등의 공정기술을 활용되고 있다. 열처리 분야의 특성상 타 산업 대비 에너지 사용이 높은 편이다. 특히 기기를 일정 온도까지 가열할 때 가장 많은 에너지와 시간이 소요된다. 따라서 공정시간과 에너지 비용을 축소하기 위해 24시간 설비를 가동하는 현장이 대다수다. 이로 인해 근로자의 근무시간이 길고 고온의 근로환경으로 신규인력 유입 및 공정관리 효율화에 애로를 겪고 있다. 이러한 공정 환경을 해결하기 위해 각 장비에 센서를 부착하고 설비가동 사항을 자동으로 분석하며 원격제어하기 위한 스마트 공정설비가 도입되고 있지만 이 역시 관련 장비를 다룰 수 있는 특정 인력에 업무가 집중되는 상황이다. 이외에도 영세 업체의 경우, 독립적으로 공정 특성이 반영된 시스템 개발과 유지보수에 투자할 여력이 없어 관련 산업에 공통으로 활용 가능한 플랫폼 개발이 필요한 상황이다.
본 논문에서는 침탄열처리 현장의 공정 환경 개선과 효율화를 위해 전기제어, IOT 제어모듈, 미들웨어, 대화형 모듈 등을 활용한 플랫폼을 개발하였다. 우선 침탄열처리 환기처리를 하는 모터 제어를 위해 전기제어 회로를 구성하여 개발하였다. 이와 연동하여 사물인터넷 제어 유닛을 설계 및 구현하여 센서 데이터 통신, 디스플레이, 릴레이 가동 등과 연결하였다. 또한 데이터의 송수신을 통해 미들웨어와 연결하는 네트워크 구성을 진행하였고 이를 관리하는 중앙처리 프로그램과 데이터베이스를 구축하였다. 특히 클라우드 대화형 모듈인 구글 다이알로그 플로우를 활용해 다수의 사용자가 편리하게 공정사항을 모니터링 하고 제어 명령을 할 수 있는 유저인터페이스로 개발하였다. 이를 통해 공정 명령을 학습하고 사용자의 의도를 파악해 미들웨어 및 제어 유닛으로 전달할 수 있는 대화형 플랫폼을 구현하였다.
검증을 통해 기존 미들웨어 집중 시스템에 대비하여 대화형 모듈을 활용한 분산 처리 효과를 측정하였고, 프로세스 분산에 따른 안정적 응답속도의 장점을 얻게 되었다. 또한 별도의 소프트웨어 사용법이 없어도 단순 질의만으로 모니터링과 제어를 할 수 있는 대화형 모듈을 활용함으로써 사용자 편리성도 강화하였다. 사용자 의도에 대한 학습 기능을 활용해 인식 범위와 처리속도 개선을 위한 훈련 프로세스를 적용하였다. 이를 통해 훈련 회수 증가에 따라 응답속도가 개선되는 최적화 결과를 도출하였다.
본 논문에서는 시스템 검증을 위해 침탄 열처리 공정의 환기 부분만을 제어하고 있다. 향후 침탄열처리 공정의 전반적인 판단과 실행을 분석을 기반으로 QT 열처리, 고주파 열처리 등에도 적용가능한 대화형 공정 처리범위 확대와 열처리 공정의 효율적인 데이터 처리 및 정제, 효과적인 장비운용 계획 설계, 에너지 저감이 가능한 공정효율화 대화형 플랫폼으로 발전해야 될 것으로 생각한다.제 1 장 서론 1 제 1 절 연구의 배경 및 필요성 1 제 2 절 연구의 내용 및 논문의 구성 4 제 2 장 관련 연구 및 기술 5

소프트웨어 생산성이 사용자들의 서비스에 대한 요구를 만족시키지 못했고, 소프트웨어 품질이 향상되지 않았으며, 소프트웨어의 개발과 유지보수 비용의 증가로 인해 소프트웨어 위기 문제가 발생하게 되었다. 이러한 위기를 해결하기 위한 한가지 대응책으로 최근 컴포넌트기반 소프트웨어공학 혹은 컴포넌트웨어가 등장하였다. 컴포넌트기반 소프트웨어공학을 이루는 관련 기술들 중 하나로써 컴포넌트기반 소프트웨어 개발 프로세스들이 제안되고 있다.
특히, 재사용을 목적으로 하는 컴포넌트기반 소프트웨어 개발 프로세스에서 컴포넌트의 품질은 매우 중요하다. 컴포넌트 개발 프로세스의 초기활동, 즉 명세활동에서 명세의 정확성과 명세의 검증을 통하여 에러를 찾아낸다는 것은 컴포넌트의 전체 품질에 매우 중요한 의미를 갖는다.
그러나, 기존의 컴포넌트 기반 소프트웨어 개발 프로세스에서는 컴포넌트 명세의 구문적인 측면은 UML과 같은 언어를 이용하여 잘 정의하고 있지만, 컴포넌트 명세의 의미적인 측면은 자연어로 기술함으로써 규악이 복잡하거나 많아지게 되면, 자체적으로 모순과 모호성을 가질 수 있다. 더욱이,컴포넌트 사용에 대한 검증이 다른 컴포넌트들과의 통합작업 이후에 가능하다면 컴포넌트 개발 작업과 컴포넌트를 기반으로 하는 시스템 구성 작업이 병행하여 진행되기 힘들다. 또한, 컴포넌트에 대한 잘못된 이해에서 비롯된 오류가 통합 이후에 발견된다면, 전체시스템 구조에 영향을 줄 수 있다. 이는 컴포넌트 자체의 에러나 컴포넌트의 오용을 통해 시스템의 오동작을 유발할 수 있다. 컴포넌트의 의미적 측면이 수학적 모델링을 통해 정형적으로 명세되어 컴포넌트가 분석될 수 있다면, 컴포넌트 구현과 사용에 대한 작업을 독립적으로 진행하여 개발 작업의 생산성을 향상시킬 수 있다 .
본 논문에서는 컴포넌트기반 소프트웨어 개발 프로세스의 초기활동, 즉 명세활동에서 컴포넌트의 품질을 높일 수 있도록 분석력과 논리성이 검증된 정형 명세 언어 Z를 이용한 컴포넌트 정형 명세 활동을 제안하였다. 제안 활동은 명세의 정확성과 일관성 그리고 만족해야 하는 특성들을 보유하고 있는지를 검증하고, 명세가 사용자의 요구사항을 충족시키는지를 확인하기 위해서 구체적인 10개의 산출물과 12개의 서브태스크를 포함하여 5개의 태스크로 구성된다.
제안 활동은 개발초기에 명세의 정확성을 검증하여 품질 높은 컴포넌트 개발을 보증함으로써 생산된 컴포넌트의 신뢰성을 크게 증진시킬 수 있다. 또한, 비정형적인 사용자의 요구사항인 제약사항이 Z 스키마의 술부에 명시됨으로써 스키마 정확성 검사를 통하여 사용자의 요구사항 정의를 정확하게 명세할 수 있다. 그리고,정형 명세를 이용함으로써 명세이후의 설계, 구현, 시험 활동들의 비용을 크게 줄임으로써 전체 개발비용을 절감할 수 있다.Software reuse is generally considered as one of the most effective ways of increasing productivity and improving quality of software. Component-Based Software Engineering (CBSE) is an emerging software paradigm in which applications are developed by integrating existing components. Many Component Based Sofware Development (CBD) Processes as one of the important techniques in CBSE have been proposed. The main challenge that a component approach can meet is dealing with change, but the substitutibility of parts this requires can be achieved only if components are properly specifed. The quality of component of CBD processes for the purpose of reusing is very important aspects. In particular, component specification activity as an initial step of CBD process is essential to determine total qualily of component. During component specification activity, we should be able to match user requirements and detecting possble errors. The existing CBD processes, however, provide a well-defined diagram such as the UML or natural language such as english for syntax aspects of specification. However, if business rules are complex and numerous, semantics aspects of specification described by natural languages can have incompleteness and ambiguity itself. Moreover, if a verification of correctness of components can be accomplished after integrating with other components, component developments and system configuration have some difficulties in developing components concurrenty. Besides, if errors which is derived from misunderstanding of components is discovered after integrating components, it could affect system architects significantly. If the semantics of components is analyzed by mathematical modeling, which has been formally specified, component implementations and component usages can be progressed respectively and therefore the productivity for developing components can be improved.
In this paper, I summarized and reviewed the existing CBD processes such as Catalysis, CBD/e, Fusion and UML Components, and proposed that Formal activities, which specify component using formal specification language Z which is verified with the ability of analysis and logicality. I tried to improve quality of components by verifying correctness of component specification at specification activity as software component development initial step. The proposed activities are composed of 5 tasks and 12 subtasks. The specific 10 products is taken advantage of both following activities in CBD process and reusing.
The proposed activities can improve reliability of produced components by verifying correctness of specification in the early step of CBD process. By transforming informal user requirements to Z schem a predicates, we can also specify user requirements correctly. In addition, we can save development cost by reducing costs of design, implementation, test activities after specification with formal specification. However, the proposed specification activities with verification costs relatively higher than the specification activities of other CBD processes. So, it is necessary to development tools, which supports component formal specification activity. We need further research of development process which supports formal activities in other development activity: design, implementation, test, etc.그림차례 = iii 표차례 = iv 국문초록 = v 제1장 서론 = 1 1. 연구의 배경 = 1

오늘날의 추천 시스템은 다양한 도메인의 적용되고 사용하는 데이터 또한 단순한 평점 데이터가 아닌 고객의 구매 내역, 리뷰, 친구 및 공유 정보, 웹페이지의 클릭 스트림 등의 다양한 데이터가 사용되고 있습니다. 기존 협업 필터링은 평점을 기반으로 고객과 아이템의 관계를 통해 선호도를 분석하는 방법으로 구매 행동의 변화 혹은 최근 선호도를 고려하지 않기 때문에 다양한 도메인에 적용하기에는 한계점이 있습니다. 그렇기 때문에 시간에 따라 변화하는 고객의 성향 또는 선호도를 반영하고 다양한 데이터의 특성을 활용하기 위해 기존 시스템의 한계점을 보완하고 순차적인 데이터를 분석하기 위한 모델이 필요하기 때문에 그에 적합한 모형을 구축해 보고자 합니다. RNN은 규칙적인 패턴을 인식하고 추상화된 정보를 추출하는데 적합한 구조를 가지고 있으며 음성, 자연어 처리, 동영상 등의 도메인에 이미 많이 적용시켜 상용화 되었기 때문에 본 논문에서는 RNN을 적용하여 고객의 순차적인 구매 행동을 파악하고 다음 행동을 예측할 수 있는 추천시스템 모형을 구축하고자 합니다. 또한, RNN의 특성에 맞게 유사한 구매 패턴을 가진 고객을 군집하여 성능을 높이고자 합니다. 이를 위해 순차적인 패턴 추출 기법 중 하나인 SPADE를 사용하여 고객의 장바구니에서 순차적인 구매 패턴을 추출하고, 추출된 룰을 기반으로 고객을 더욱 정교하게 군집하여 RNN 모형의 성능을 높이고자 합니다.Recommendation systems have been an important research field due to wide range of application such domain as music, restaurant, book, document, image, movie, shopping, TV program etc. Over the last decade, most of researchers have studied new approach to overcome the limitation of collaborative filtering, content-based filtering and other recommendation models such as sparsity and scalability problem. Recently, few researchers have applied deep learning to recommendation system to predict future behaviors and capture user’s preference more precisely. In this research paper, we proposed a new approach on applying Recurrent Neural Networks (RNNs) on recommendation system. In particular, we study how the Long Short-Term Memory (LSTM) can be applied to predict next basket of items given user’s transaction history. Our approach is different in clustering matrix, which we use user’s sequence pattern in transaction instead of simple user and item matrix. The result proved that incorporating sequence pattern mining with clustering on RNN with LSTM cell outperform standard LSTM recommendation system. The contribution of our research paper is to supplement the research on application of deep learning on recommendation model and overcome the limitation of sparsity by using appropriate clustering mechanism.I. Introduction 1 A. Research background 1 B. Research objective. 2 C. Organization of the dissertation 3 II. Review on related study . 3

지시 표현 ‘것’은 한국어 구어 대화에서 자주 등장하는 표현이다. 그러나, 지시 해석(Reference Resolution)에 관한 기존 연구에서 ‘~것’이 제대로 다루어지지 못했다. 지시 표현 ‘것’은, 대명사나 한정 명사구와 같이 그 자체가 독립적인 지시 표현이 아니기 때문이며, 무엇보다 지시 표현 ‘것’은 지시 표현이 아닌 보문소(complementizer)로서의 ‘것’과 구별되지 못했기 때문이다.
지시적 쓰임과 보문소 쓰임을 구분하는 언어학적 규칙이 없기 때문에, 본 연구는 담화상에서 ‘것’이 쓰이고 있는 여러 가지 특성을 기반으로 기계 학습을 이용하여 식별하는 방법을 제안한다. 궁극적으로는 지시 해석의 대상을 넓히고, 담화 이해의 질을 높인다.
‘것’을 식별하기 위한 자질로, ‘것’이 가지고 있는 언어학적 속성과 담화 상의 속성을 기반으로 자질 집합을 제안하였고, 가장 기본적이고 믿을 만한 한국어 자연어처리 단계인 형태소 분석의 결과만으로 자질값을 추출하였다. 이를 바탕으로 결정 트리를 이용하여 여행 관련 대화를 대상으로 실험하였으며, 보문소에 대해서는 92.3%, 지시 표현에 대해서는 82.2%의 F-measure를 보였다.Referential expression ‘Geot’ is often occurred in Korean spoken dialogue. However, it has not been properly dealt with by the previous researchers of Reference Resolution, since it is not by itself the referential expression like pronoun and definite noun phrases. More than anything else, it has never been dealt with being discriminated from complementizer ‘geot’.
There is no rule to discriminate the referential usage of ‘geot’ from complementizer usage of it. Therefore, we propose a method to identify the referential ‘geot’ from the complementizer ‘geot’ using machine learning, based on the various characteristics that both ‘geot’ have in their discourse. Then, ultimately this research would like to broad the items of reference resolution and improve the level of discourse understanding.
We have proposed a feature set which is based on the linguistic property of ‘geot’ and the discourse property of its text, and extracted feature values from the result of morphological analysis which is the most practical and reliable in Korean NLP tasks. We made an experiment on this with a dialogue corpus which is related with travel. Our approach achieved the F-measures of 92.3% and 82.2% for complementizer and referential expression.

기업의 데이터의 80%이상은 비정형 데이터라고 한다. 데이터의 양과 복잡도가 계속 증가하면서 대량의 비정형 데이터 수집과 분석 및 활용에 대한 요구가 증대되고 있다. 그렇기에 방대한 양의 비정형 데이터를 어떻게 활용할 수 있을지에 대한 고찰이 필요하다. 비정형 데이터를 분석하기 위해서는 분석이 용이하도록 비정형데이터를 정형화시키고 새로이 정의하여야 한다. 이를 위해 온톨로지와 같은 분류 체계 구축이 필요하다. 온톨로지 구축으로 비정형 데이터의 정형화가 가능하고, 이는 데이터의 유지보수 관리 측면에서도 유용하다. 더불어 비정형데이터를 정형화할 때에 특정 키워드가 많이 출현한 경우, 키워드로 해당 문서를 분류한다. 하지만, 문맥상의 의미에 따라 키워드의 의미가 변할 수 있기 때문에 기존의 키워드 빈도수 기반의 정형화 방법론이 아닌 새로운 방법론이 필요하다.
본 연구는 N사의 프로젝트를 이용하여 키워드 빈도수 기반의 정형화가 아닌, 룰 기반으로 문장 단위의 분석을 통해 정형화를 진행하였다. 연구자는 비정형데이터를 정형화하기 위해 IBM의 Watson Explorer를 활용하였다. 툴을 활용한 비정형데이터 정형화 프로세스를 제시하고, 이를 사례연구를 통해 살펴보고 얻게 된 결론 및 시사점을 제시하고자 한다.More than 80% of enterprise data is unstructured data. As data volume and complexity continue to increase, there is a growing demand for large amounts of unstructured data collection, analysis, and utilization. Therefore, it is necessary to consider how to use a large amount of unstructured data. In order to analyze unstructured data, unstructured data should be formalized and newly defined to facilitate analysis. To do this, it is necessary to construct a classification system like ontology. It is possible to formulate unstructured data by constructing an ontology, which is also useful in maintenance of data. In addition, when a large number of specific keywords appear at the time of stereotyping unstructured data, the document is classified by keywords. However, since the meaning of a keyword can be changed according to the meaning of the context, a new methodology is required instead of the conventional keyword frequency based formatting methodology.
In this study, we used N project to formalize through rule - based sentence - level analysis rather than keyword frequency - based formalization. The researchers used IBM's Watson Explorer to format the unstructured data. This paper presents the process of unstructured data formatting using tools and presents conclusions and implications that have been obtained through case studies.국문 요약 v 제1장 서론 1 1.1 연구 배경 및 목적 1 1.2 연구 구성 3

인터페이스의 본질은 사용자로 하여금 컴퓨터 등의 정보처리 에이전트를 효과적으로 사용할 수 있게 도와주는데 있다. 금융 소비자는 각 조치를 자신의 상황에 맞게 적절하게 취함으로써 경제적 자유를 신장시킬 수 있다. 소비자가 특정 정보를 자신이 원하는 형태로 가공하고 접근할 수 있도록 인터페이스를 설계하는 것은 소비자의 만족스러운 금융 생활에 필요한 요소다. 챗봇 인터페이스는 뛰어난 직관성과 플랫폼으로서의 가능성으로 인해 주목받고 있다. 가상화페, 주식, 파생상품(선물, 옵션) 등의 거래가 활성화됨에 따라 트레이딩 매개체에 대한 관심도 많아졌다. 챗봇은 기본적으로 모바일에서 동작하는 대화형 시스템이지만 온전히 자연어 처리에만 의존할 수 없음을 고려할 필요가 있다. 본 연구는 우선 챗봇의 인터페이스 요소를 파악하기 위해 트레이딩 시스템별 사용자 조사와 채팅형 인터페이스에 대한 사용자 조사를 실시함으로써 금융권 챗봇 UI를 구성하기 위한 사전적 요소를 파악하고자 한다. 챗봇에서 필요한 인터페이스 요소들을 정리해보았다. 이러한 과정을 통해 금융서비스를 이용하기 위한 챗봇의 디자인 방향을 탐색해보고자 한다. 그리고 매수, 매도, 계좌확인 등의 행위와 사용자가 원하는 종목을 고르기 위해 복잡한 과정을 거치는 조건 검색 이라는 기능을 대화형 인터페이스를 통해 어떻게 구현하고 더 사용성이 좋게 만들 것인지를 다뤄볼 것이다.제1장 서론 제2장 문헌연구 1. 투자 트레이딩 매개체 2. 금융과 챗봇

현재의 소프트웨어는 복잡화, 분산화, 그리고 대형화되어있다. 기존의 소프트웨어 개발 방법론들은 고품질 소프트웨어 개발을 위해 정확한 요구 사항 분석에 적합하지 않다. 예를 들어, 개발 할 시스템의 기능들을 분석하는 방법인 구조적 방법론과 구성요소들을 식별하여 연관성을 통하여 객체를 분석하는 객체지향 방법론들은 식별이 너무 작은 단위부터로 상향식(Botton-up)방식이다. 앞으로 대형 시스템 개발은 더 큰 단위인 유스 케이스부터 하향식(Top-down)방식으로 상세한 단위까지 분석한다. 이에 따라 점점 중요한 이슈가 되고 있는 고품질의 소프트웨어를 더 큰 단위의 유스 케이스에서 하향식(Top-down)분석으로부터 정확한 요구사항의 이해를 통해 개발에 초석이 되고자 한다.
즉, 고객의 요구사항은 시스템 개발에서부터 테스트 과정까지 모든 과정에 영향을 미친다. 유스 케이스 기반의 개발과정에서는 정확한 요구사항 이해와 분석을 위해 고객의 요구사항에서 유스 케이스 단위로 부터 분석하고자 한다. 하지만, 자연어로 작성된 요구사항 기술서로부터 서술적인 묘사로 인해 유스 케이스 및 객체를 식별하는 기준이 모호하다는 단점을 가지고 있다. 본 논문은 유스 케이스 및 다이어그램을 식별하는 표준을 제시하고자, 언어학자인 Fillmore의 의미론적 Case Grammar를 요구공학에 접목하여 요구사항으로부터 유스 케이스 추출 및 유스 케이스 모델링 방법을 제안한다. 제안한 유스 케이스 모델 변환 식별 알고리즘은 자연어로 작성된 요구사항으로부터 보다 쉽고, 정확한 유스 케이스 식별 및 추출이 가능하다. 고객요구사항으로부터 명확한 분석으로 유스 케이스 추출을 한다. 또한 소프트웨어 개발에 최적화된 리스크(Risk~Criticality)식별 및 분석 프로세스 메커니즘을 제안한다. 제안한 메커니즘에서는 유스 케이스 단위로 소프트웨어 개발에 관련된 리스크 요구사항을 식별하고, 이를 리스크 결정 매트릭스에 적용하였다. 또한 유스 케이스의 복잡도를 측정하는 유스 케이스 점수(Use Case Point)를 이용하여 추출된 유스 케이스들의 복잡도를 계산 할 수 있다. 이를 통해 얻은 리스크 영향도 값과 유스 케이스 점수(Use Case Point)를 비교분석을 하여 두 값들의 유사 패턴으로 볼 때, 유스 케이스 점수(UCP)가 높을수록 (즉, 복잡도가 높을수록) 리스크 영향도도 높음을 보인다. 따라서 제안한 리스크(Criticality) 영향도 값을 측정함으로써 유스 케이스 단위의 리스크 우선순위화가 가능함을 확인하였다. 리스크 영향도 값을 측정하여 리스크 영향도가 높은 것은 반드시 테스트해야 하는 부분에 해당하기 때문에 우선적으로 주어진 시간과 자원을 할당하여 강력한 테스팅을 진행해야한다. 이렇게 리스크 영향도의 우선순위는 개발 복잡도로 고품질 소프트웨어 개발에 초점이 될 수 있다. 감소시켜야 하는 리스크에 대한 우선순위를 얻을 수 있으며, 리스크에 대한 불확실성을 감소시킬 수 도 있으리라 본다.softwares are complex, distributive and large recently. The conventional methodologies for software development are not sufficient to analysis the requirements accurately for developing high quality softwares. For example, the structural methodology which analysis the functions of developing system and the object-oriented methodology which analysis the objects through relationships among the identified components, are the Bottom-up methods from very small units.Future development of large system needs to use Top-down method that analysis from the use-cases which are more bigger unit.Therefore, we suggest the Top-down method to analysis the requirements accurately for the high quality software which is getting more important.
The requirements of customers influence all the processes from development of systems to test of systems. The use-case-based development analysis through the use-case units of the requirements of customers for accurate comprehension and analysis. However, the requirements statement written by natural language has a demerit that the standard for identifying the objects is ambiguous because of its descriptive portrait. In this paper, we propose the use-case extraction and modeling method from the requirements to suggest the standard for identifying the use-case and diagram by grafting the Fillmore’s Semantic Case Grammar onto the requirement engineering. The suggested algorithm of use-case model conversion and identification can extract and identify the use-case from natural-language-based requirements more easily and more accurately. We did clearly analyze Usecases extraction from Customer Requirements. We also propose the process mechanism for analysis and identification of Risk(Criticality) which is optimized to software development. The suggested mechanism identify the risk requirements related to development of software by use-case units and apply it for the risk decision matrix. By using this mechanism, the complexity of the extracted use-case can be calculated through the Use Case Point (UCP). By analysing the similarity patterns of risk Impact value and UCP, we shows that the higher UCP is, the higher the rick impact value is also. Therefore, the risk prioritization based on the use-case units can be possible by calculating the risk impact values. If the risk impact value is very high, it should be tested priorly and strictly through assigning the times and resources. The risk impact value prioritization can play an important role in developing the complex and high quality softwares. By using the suggested method, the priority of risk can be obtained and the uncertainty of the risks can be reduced considerably.제 1 장 서 론 1 1.1 연구배경 및 목적 1 1.2 연구내용 및 구성 2 제 2 장 관련연구 4 2.1 언어학적(Linguistics) 분석방법 4

DDoS(Distributed Denial of Service)는 대량의 좀비 PC를 이용하여 공격 대상 서버에 접근하여 자원을 고갈시켜 정상적인 사용자가 서버를 이용하지 못하게 하는 공격이다. DDoS 공격발생 사례가 꾸준히 증가하고 있고, 주요 공격대상은 IT 서비스, 금융권, 정부기관이기 때문에 DDoS를 탐지하는 것이 중요한 이슈로 떠오르고 있다. 본 논문에서는 DNS 서버를 이용하여 패킷을 증폭시키는 DNS DDoS 공격 즉, DNS Amplification 공격(이하 DNS 증폭 공격)을 Deep Learning(이하 딥 러닝)을 활용해 실시간으로 탐지하는 방법에 대해 소개한다. 기존 연구들의 한계점을 극복하기 위하여 실험망 환경의 데이터가 아닌 실 환경 데이터를 혼합하여 탐지 시스템을 학습하였다. 또한 이미지 인식에 주로 사용되는 Convolutional Neural Network(이하 CNN)와 자연어처리•음성인식 등에 주로 사용되는 Recurrent Neural Network(이하 RNN)을 이용하여 딥 러닝 모델을 구축하였다.1.서론 2.관련연구 2.1 DNS DDoS 공격 및 방어 2.1.1 DNS DDoS 공격 (DNS AMplification 공격) 2.1.2 DNS DDoS 탐지 연구

산업현장에서 작업부하의 증가에 따른 스트레스로 인하여 발생하는 산업재해는 현대 산업사회가 발전함에 따라 더욱 증가하고 있으며 이로 인해 막대한 인적·재정적 손실이 경영자를 위협하고 있는 실정이다.
본 논문은 이러한 작업부하를 산업현장에서 손쉽게 측정하여 작업장의 문제점을 발견하고 또한 이를 개선할 수 있는 방법을 제시할 수 있는 인간공학적 작업부하 스트레스 지수 모델을 개발하고 이 개념을 컴퓨터시스템화 하여 각종 작업장에 도입하게 함으로써 이로 인해서 파생되는 인적·재정적인 손실을 미연에 방지하고 이를 최소화하는데 기여한다.
정확하고 표준이 될 수 있는 인간공학적 작업부하 스테레스 지수 모델을 개발하기 위해 인간-기계 시스템 환경에서 작업자의 작업성취 및 생산성에 영향을 미치는 내적·외적 작업부하 스트레스 요인들을 발견하여 이를 모델개발에 고려하였다. 이들 작업 스트레스 요인들이 작업자에 감지되는 정도에 대한 반응을 퍼지이론을 도입하여 숫자가 아닌 자연어로 할 수 있게 하였으며 이렇게 작업자에 의해 주관적으로 판단된 작업스트레스 요인들은 각기 다른 가중치를 판단하기 위해 AHP(Analytic Hierarchy Process)를 사용하여 작업자가 직접 느끼는 작업부하 스트레스 정도를 비교판단 하게 하였다. 인간공학적 작업부하 스트레스 지수는 작업 스트레스 요인들이 작업자에 감지되는 정도에 대한 반응과 AHP의 계산결과를 곱하여 계산되어지며 이렇게 개념화된 모델을 실제 산업현장에서 사용할 수 있도록 데이터베이스와 전문가시스템을 사용하여 '컴퓨터를 통한 작업부하 분석 시스템'이라는 시험적인 컴퓨터 인터페이스 시스템을 개발하였다. 본 연구에 대한 검증을 위하여 실제 산업체에 종사하는 작업자들을 대상으로 인간공학적 작업부하 스트레스 지수 모델을 적용하여 측정하고 이를 지난 수년간의 재해통계와 작업자의 생리학적인 변화와의 관계를 비교 분석하여본 결과 본 논문에서 개발된 모델은 작업부하 스트레스를 비교적 정확하게 측정할 수 있어서 앞으로 산업현장에 적용될 수 있는 신뢰성 있는 연구로 판단되었다.

인터넷 사용이 보편화되면서 사용자들은 자신들이 이용해 본 제품이나 서비스에 대한 경험을 적극적으로 공개하거나, 다른 사용자들의 의견을 참고하여 의사결정을 하는 경향이 늘어나고 있다.
블로그, 카페, 포럼 등에 표현되어 있는 사용자들의 의견을 상품 개발에 적극적으로 활용하여 신상품을 기획함으로써 시장에서 좋은 호응을 얻는 경우도 있고, 사용자들의 비평을 수용하지 않고 방치하여 회사나 조직에 손해를 초래하는 경우도 있다.
인터넷 입소문의 영향력은 순간적인 유행에 그치지 않고 회사의 매출이나 국가의 정책까지도 확장되고 있어, 인터넷상의 사용자들의 의견을 분석하는 오피니언 마이닝시스템의 연구가 활발해지고 있다.
지금까지의 오피니언 마이닝시스템은 언어처리시스템을 이용하여 사용자들의 의견이 긍정적인지 부정적인지를 판별하는 기법에 의존하였다. 자연어 문장으로 표현되는 사용자 의견에 대한 긍부정을 정확하게 분석하기 위해서는 많은 언어 관련 지식 정보를 사전에 구축해야 한다. 긍부정 판별을 통한 접근 방법은 구축 시간이 길고 많은 비용이 들기 때문에 자동차와 같이 제품수명주기가 비교적 긴 경우에는 유용하나, 첨단제품과 같이 제품생명주기가 짧아 신속하게 소비자의 반응을 분석해야 하거나 긍부정 판별에 요구되는 세부 속성 정의가 모호한 사회적 트랜드나 이슈와 같은 경우에는 적합하지 않다.
본 연구에서는 첨단제품, 사회적 이슈, 정부정책 등 제품수명주기가 짧거나 긍부정 분석이 어려운 경우에 적용이 가능한 연관어 네트워크를 제안한다. 분석하려는 개념과 자주 같이 나타나는 개념들을 그래프 형식으로 표현한 연관어 네트워크를 기반으로 트랜드 분석에 적합한 오피니언 마이닝시스템인 트랜드맵 시스템을 구현한다.
구현된 트랜드맵 시스템의 유용성을 입증하기 위하여 여러 분야에 걸쳐 실험을 진행하였다. 제품생명주기가 짧은 첨단 가전제품과 핸드폰 분야를 설정하여 전세계 11개국의 인터넷 자료들을 수집하여 LG전자와 삼성전자를 비교했다. 실험 결과 제안된 연관어 네트워크를 통해 LG전자의 주력 제품들이 소비자들에게 어떻게 인식되는지를 쉽게 파악할 수 있었으며, 경쟁 브랜드인 삼성전자와 동시에 연관되어 있는 개념들을 쉽게 파악함으로써 비지니스 전략 수립이 용이해졌다.
긍부정을 판별의 기본 단위인 하부 속성을 정의하기 어려운 사회적 이슈에서 연관어 네트워크를 적용하기 위해 최근 인기를 끌고 있는 막걸리 문화를 1천 7백만 건의 블로그 데이터를 통해 분석하였으며, 막걸리와 연관되어 나타나는 안주류, 가공식품, 장소 등을 분석하여 막걸리와 어울리는 먹거리로서 새로운 신상품으로 기획할 수 있는 통찰력을 얻을 수 있었다. 음식문화에 대한 전반적인 연관어 네트워크 분석을 통하여 축구장과 야구장에서 취식하는 기호식품에 대한 사용자들의 의견을 연관어 네트워크의 자연스러운 확장 기능을 통해 쉽게 파악할 수 있었으며, 그 결과 제품 판매 전략에 대한 아이디어도 도출할 수 있었다.
연관어 네트워크를 기반으로 한 트랜드맵 시스템은 기존의 오피니언 마이닝 시스템에서 긍부정을 판별하기 위해 필수적으로 도입했던 복잡한 언어처리시스템의 도움 없이, 같이 나타나는 연관 개념을 그래프 형태로 표현함으로써 첨단제품 의견 분석, 시장 초기 반응 조사나 사회적 이슈 분석 등의 분야에 유용한 것을 알 수 있었다.

본 논문에서는 기계번역사 번역문장에서 발생하는 다의어의 애매성을 퍼지이론을 적용하여 해결하는 방법과 데이타베이스를 위한 확장된 퍼지언어처리 방법을 제안한다.
문장에서 다의어 발생시 다의어와 동일한 품사에 대한 문법 정보만으로 다의성을 처리할 수 없는 애매한 문장 상황이 발생하는 경우가 있다. 앞으로 문장에서 다의성이 발생한 어휘와 동일한 품사를 찾아 다의어가 갖는 각각의 의미와의 연관성을 비교한 후 연관된 소속정도를 0 과 1 사이의 임의의 값을 주어 다의어와 같은 품사와의 대응정도가 가장 높은 다의어를 선택하여 의미를 결정한다. 의미정보 사전에는 다의어와 관련된 단어에 관련 정도값을 넣어준다.
데이타베이스에서 사용되는 일반적인 데이타를 그대로 이용하면서 해석용 데이타베이스를 결부시켜 퍼지 데이타베이스를 구성한다. 자연언어 표현은 그 값으로 파악되는 퍼지 언어 연수에서의 변형자로 확장하여 한국어의 정도부사를 분류하고, 이를 변형자로 사용하여 데이터베이스에서 자연어 질의를 할 수 있도록 질의어 범위를 확장시킨다.This paper proposes a polysemy processing method to solve the ambiguity of polysemous words occurring in machine translation by applying fuzzy theory and an extended fuzzy linguistic processing method for databases.
When there is a polysemous word in a sentence, it can be translated to a correct one not only by its syntactic information but also by its semantic relation with the previous sentence and the next sentence. The membership grades which are set to values between 0 and 1 represent the semantic relation between the polysemous word and the words of the same part of speech in speech in the previous and next sentences. The meaning of the polysemous word is determined by selecting the word of the highest membership grade. The semantic information dictionary contains the membership grades among words.
Also, a fuzzy database is constructed with the connection to a analytic database, and it utilizes the general data in a database without modification. natural language description, extendes modifiers of the fuzzy linguistic variables presenting the values of that, classifies korean adverbs of degree, and it is used modifier, extendes the range of query that can be done a natural query목차 = 0 요지 = i 목차 = ii I. 서론 = 1 II. 기계번역 시스템의 개요 = 4

시스템이 대형화, 복잡화되어갈수록 소프트웨어 아키텍처의 중요성이 증대되어진다. 대형의 소프트웨어 시스템에서는 소프트웨어 아키텍처를 고려할 때, 시스템 아키텍처와 요구사항 명세를 모두 중요시한다. 아키텍처 설계 초기 한계부터 상위 수준의 요구사항을 제대로 적용시켜야만 올바른 아키텍처를 만들 수 있기 때문이다. 아키텍처 트레이드오프 분석 방법은 상위 수준의 요구사항을 시나리오의 형태로 받아들여서 아키텍처를 분석하는 방법이다. 아키텍처 트레이드오프 분석 방법은 시나리오를 주요 분석 자료로 사용함에도 불구하고, 이들 시나리오를 관리할 수 있는 아무런 방법도 제시하지 못하고 있다.
이에 본 논문에서는 아키텍처 트레이드오프 방법을 지원하는 점 기반의 시나리오 관리 도구를 제안한다. 이 도구는 시나리오를 입력, 수정, 삭제할 수 있게 할 뿐만 아니라, 변경관리, 아키택처 분석, 유틸리티 트리 작성 등의 기능도 제공한다. 특히, 이 도구는 웹을 통해 무조건적으로 입력되는 시나리오들을 체계적으로 분류하기 위해서 시나리오간의 유사도를 측정하여 시나리오를 자동으로 분류해주는 자연어 처리 엔진을 사용한다.As software becomes more complicated and large-scaled, software architecture is considered more important. Considering software architecture in large-scaled software system, people emphasize both architecture itself and requirement specification. Because it needs to apply high-level requirements to early analysis and design for more complete architecture. Architecture Tradeoff Analysis Method(ATAM) analyzes architectures with high-level requirements(scenarios). Although ATAM use scenarios for analyzing, it doesn't provide any method to manage scenarios.
This paper proposes the Web-based Scenario Management System(WSMS) supporting ATAM. WSMS enables not only scenario input, modification, deletion but also change management, architecture analysis, utility tree composition. Especially, WSMS can classify scenarios automatically through measuring similarities between scenarios.목차 = ⅰ Abstract = ⅴ 요약 = ⅵ 1. 서론 = 1 2. 관련연구 = 4

새로운 메신저 채널로 주목받고 있는 챗봇은 다양한 분야에서 활용되고 있다. 챗봇은 사용자가 자신이 사용하는 메신저를 통해 정보를 얻는다는 장점이 있어, 국내외 IT기업들이 클라우드 기반 오픈소스 챗봇 플랫폼을 구축하고 서비스를 제공하고 있다. 특히 고객 상담 챗봇 서비스가 긍정적 사례로 입증되면서 최근 공공기관 및 지자체에서도 민원행정 챗봇 서비스를 도입하고 있지만, 그 수가 한정적이다.
따라서 본 논문은 한 해 무수히 발생하는 분실물의 회수율을 높이고자 챗봇 시스템을 제안한다. 제안한 시스템은 사용자의 메신저에서 분실물 정보를 입력받아 대화의 맥락을 파악하고 분석하는 기계학습 기반의 자연어처리 기술을 사용하여 대화의 정확도를 높인다. 이후, 추출된 정보는 경찰청 유실물 통합포털과 연동해 분실물 정보를 검색하고 사용자에게 제공한다. 또한 본 논문에서는 제안한 챗봇 시스템을 카카오톡 플랫폼 API, 구글의 Dialoglflow API, 경찰청 공공 데이터를 사용하여 구현하고 실제 환경에 적용 가능함을 보인다.The Chatbot which is noticed as a new messenger channel, is being used in various fields. It has an advantage for getting information through user's own messenger, that makes domestic & international IT companies to provide and develop a service for open-source chatbot flat form based on cloud. In particularly, As customer consulting chatbot service was attested as a positive instance, that public institution and local government introduced civil affairs administration chatbot service recently but the number of used is limited.
Therefore this thesis suggests a chatbot system to increase the return of lost property which occurred myriad of missing. The proposed system understands a context of conversation which entered the lost property information from user's messenger, and it increase the accuracy of conversation by using Natural language processing technology that based on analysing machine learning. Henceforth, extracted information is retrieved lost property information from interlocked national police agency lost and found portal and provide a information to user. In addition, this proposed chatbot system implemented by using kakaotalk platform API, google's dialoglflow API, national police agency public data and can be applied to real environment.국문초록 ⅴ 영문초록 ⅵ 제 1 장 서론 1

There are many disaster that people can not predict, and the important to spread information about disaster on the site is increasing. therefore, in this thesis, I analyzed social network service for disaster information system and pointed the problem that SNS is used only information notification.
The end result is fastest disaster information gathering and spreading using short message on twitter. Especially I studied methods to find disaster tweets from a lot of messages on twitter. In addition, I implementation mash-up system that can get disaster information using Twitter API and display on the map using Google map API.
This mash-up system is difference with other application that people have to report disaster information using dedicated software. eventually, this study is not only propagation disaster conditions but information collection used crowd-sourcing on social network service.1. 서론 1 1.1 연구 배경 및 필요성 1 1.2 연구 목표 3 2. 관련연구 5

목차 I. 서론 = 1 II. 기계번역 시스템의 개요 = 4 II-1. 기계번역의 역사적 배경과 문법적 배경 = 4 II-2. 기계번역 시스템에서의 퍼지이론의 필요성 = 8

최근 구글 딥마인드에서 개발한 알파고와 이세돌의 바둑 경기를 계기로 인공지능 컴퓨터에 대한 관심이 매우 높아지고 있다. 인공지능컴퓨터와 인간과의 대결에는 1997년 IBM의 수퍼컴퓨터 ‘딥블루(Deep Blue)’와 인간과의 체스대결, 2011년 미국의 퀴즈쇼 ‘제퍼디(Jeopardy)’에서 IBM의 슈퍼컴퓨터 왓슨과 인간과의 퀴즈대결 등이 대표적이다. 이들 대결에서 모두 정교한 알고리즘과 계산력에 기초한 인공지능 컴퓨터가 인간에게 승리를 거두었다.

인간과의 퀴즈대결에서 승리한 왓슨은 현재 금융, 의료 등의 분야에 진출해 인간의 능력을 뛰어넘는 성과를 올리고 있다. 국내에서도 왓슨을 모델로 한 인공지능 컴퓨터인 WiseQA(가칭)가 한국전자통신연구원(ETRI)에서 개발 중에 있다. WiseQA는 국내의 장수 퀴즈 프로그램인 장학퀴즈를 대비한 질의응답시스템으로 개발되고 있으며, 왓슨 컴퓨터와 마찬가지로 방대한 양의 정보로부터 매우 빠른 속도로 정답을 찾아내는 자연어 처리 기술을 탑재하고 있다. 또한 인간과의 퀴즈대결에서 승리하기 위해서는 버즈-인(buzz-in) 전략과 배팅(betting) 전략이 필수적이다.

본 논문에서는 WiseQA가 각 라운드별로 합리적인 의사결정을 할 수 있도록 통계적 모형을 구축하였다. 장학퀴즈에서 1라운드는 사지선다형으로 맞힌 문제 수만큼 점수를 획득하게 되며(별도의 게임 전략은 불필요), 2, 3라운드는 주관식으로 먼저 버저를 누른 팀에게 응답기회가 주어지고 틀려도 감점은 없다(감점이 있다면 버즈-인 전략 필요). 3라운드 마지막 문제는 0~50점까지의 베팅을 할 수 있으며, 틀릴 경우 베팅한 점수만큼 점수를 잃게 되므로 승부를 결정짓는 매우 중요한 문제이다(베팅 및 버즈-인 전략 필요). 본 논문에서는, 감점이 있다는 전제하에, 2, 3라운드에서의 버즈-인 전략과 3라운드 마지막 문제에 사용될 베팅 전략을 위한 통계적 모형을 구축하였다. 버즈-인 전략을 위한 통계적 모형으로는 의사결정나무모형, 신경망모형, 로지스틱회귀모형이 사용되었으며, 베팅 전략을 위해서는 다범주 로짓모형이 적용되었다. 그 결과 버즈-인 전략에 사용된 세 모형은 대등한 성능을 보였으며, 통계적 모형 기반의 베팅 전략은 인간 참가자들 보다 우수함을 확인하였다.Due to the go match between Sedol Lee and Alphago, developed by Deepmind Google, interest in AI computer has increased significantly. The most famous matches between AI computer and human are 1997 IBM super computer Deep Blue vs. human chess match, and 2011 IBM super computer Watson vs. human quiz match in Jeopardy quiz show. In both games, the AI computers won on the basis of their elaborate algorithm and calculating skills.

Watson, after winning the quiz show match against human, is thriving in fields such as finance, medicine, and so on. Development of an AI computer that takes after Watson, named WiseQA by ETRI, is in process in Korea. WiseQA is being developed in order to prepare for long time quiz program named Janghak Quiz, and has a natural language processing technology that enables it to search for the correct answer among vast volume of information. Furthermore, in order to win in against humans in a quiz match, a buzz in strategy and a betting strategy should be taken into consideration.

This paper suggests a statistical model that enables WiseQA to go through a reasonable decision making process for each round. In Janghak Quiz, one acquires points for the number of correct answers in round 1 (alternate game strategy unnecessary), and the team that buzzes first gets a chance to answer the question and there is no penalty for a incorrect answer in round 2 and 3 (buzz in strategy necessary if penalties exist). One can make a betting between 0 to 50 points in the last question of round 3, and because one will lost points for the incorrect answer, this is a very important issue for winning (betting and buzz in strategy necessary). decision tree model, neural network model, logistic regression model were used for the buzz in strategy, and multi-category logit model was implemented for the betting strategy. As a result, the three models used in the buzz in strategy showed equal results, and the model used in the betting strategy showed to be more promising than other human competitors.Ⅰ. 서 론 1 Ⅱ. 이론 3 1. 로지스틱회귀 3 2. 의사결정나무 4

갑상선호르몬제인 레보티록신은 미국에서 가장 많이 처방된 의약품 1위를 기록하고 있음에도 불구하고 환자들의 치료만족도(treatment satisfaction)는 매우 낮은 실정이다. 본 연구에서는 환자가 의약품 치료에 대해 직접적으로 받은 느낌 및 만족도를 작성한 환자보고성과(Patient-reported outcome,PRO)의 일종인 의약품 리뷰 사이트의 환자 복약 경험 정보가 갖는 특성을 분석하여 레보티록신 약물에 대한 환자들의 인지적 특성을 알아보고 이를 고려한 맞춤형 복약 지도 방안을 도출하고자 하였다.
이를 위해서 연구는 세 단계를 통해 수행되었다. 첫 번째로, Synthroid와 Levothyroxine에 대한 1768개의 리뷰에 대해서 자연어 분석기법과 Latent Dirichlet Allocation(LDA) 알고리즘을 이용한 토픽모델링에 의해 분석함으로써, 레보티록신 약물에 대한 환자의 인지적 특성(토픽)들을 범주화하였다. 두 번째로, 첫 번째 작업의 인지적 특성(토픽) 범주화에 기반하여, Chi-square test를 통하여 연령별, 복용기간별, 성별에 따른 주요하게 고려하는 인지적 특성(토픽)의 차이를 분석하였다. 세 번째로, 첫 번째 작업의 요인 범주화에 기반하여, 회귀분석을 통하여 범주화된 각각의 요인들과 점수화된 만족도에 미치는 영향을 분석하였다.
연구결과를 요약하면, 약물에 대해서 환자들의 인지적 특성(토픽)은 총 6가지로 범주화 할 수 있다. 첫 번째 토픽은 레보티록신의 복용 방법에 관련된 정보에 대한 내용이다. 두 번째 토픽은 초기 복약 경험과 관련된 감정적 인식과 관련된 내용이다. 세 번째 토픽은 레보티록신의 용량 조절과 관련된 정보를 나타내고 있다. 네 번째 토픽은 통증 관련 약물 부작용 증상에 대한 내용을 담고 있다. 다섯 번째 토픽은 브랜드 약물과 제네릭 약물 사이의 효능 차이에 의한 약물 변경(제네릭 생동성)에 대한 내용이며 마지막으로는 약물 효능에 대한 불만족에 관한 내용을 나타내고 있다. 연령별, 복용 기간별, 성별에 따라서도 인지적 특성(토픽)의 차이가 있다는 것이 나타났으며 만족도에 영향을 미치는 요인으로는 복용기간과 성별, 리뷰의 작성 년도 및 약물복용 방법, 약물 용량 조절, 약물 부작용에 대한 내용임을 확인하였다.
이와 같은 결과는 레보티록신에 대한 실제 환자들의 미충족 수요(unmet needs)를 파악하는데 큰 기여를 할 수 있기에 환자 복약 경험에 대한 자료의 중요성을 재고할 필요가 있음을 생각하게 한다. 또한 실제 환자들의 복약 경험을 분석하였을 때 기존의 연구결과들과 일치한다는 점과 환자의 나이와 복용 기간, 성별 등의 항목에서 유의한 차이를 보이는 항목이 많았으며 이는 실제 환자들의 복약 경험의 폭넓은 활용 가능성을 시사하였다. 본 연구는 실제 환자들의 리뷰 속에서 토픽모델링을 통하여 레보티록신을 복용하고 있는 환자들의 인식을 분석하고 그들의 미충족 수요를 파악하고자 한 첫 번째 탐색적 연구임에 그 의의가 있다.제1장 서론 1 1.1 연구의 배경 1 1.2 연구 목적 4 1.3 연구의 구성 4 제2장 이론적 배경 및 문헌 고찰 5

웹2.0이라 불리는 웹 패러다임은 참여, 공유, 개방의 개념을 바탕으로 사회 전반에 많은 변화를 일으켰다. 웹2.0 패러다임은 누구나 정보를 쉽게 생산하고, 빠르고 폭넓게 확산 시킬 수 있게 하여 1인 미디어와 플랫폼으로서의 웹을 이루어 냈다. 이러한 웹 공간의 진화로 인터넷 사용자들은 자신이 가지고 있는 정보를 개방하고 공유함으로써 인간 개개인의 영향력을 증대 시켰다. 이에 따라 대형 검색 업체나 포털 사이트들은 자사 사이트에서만 서비스를 이용하도록 하는 폐쇄적인 구조를 버리고 자사 서비스를 공유, 개방하여 사용자들의 참여를 유도하는 형태의 Open-API를 제공하고 있다. 이뿐만 아니라 여러 웹사이트들의 Open-API를 제공받아 새로운 서비스를 창출하는 매쉬업(Mashup) 서비스들이 만들어 지고 있으며 그로 인해 사용자들의 웹 인프라는 더욱더 풍요로워지고 있다. 하지만 수많은 정보와 서비스가 생성 되는 상황에서 사용자들은 자신이 원하는 신뢰성 있는 정보를 찾거나 관리하기가 어려워졌다. 이로 인해 사용자들은 자신의 관심사와 개성을 공유하는 사람들과 연결된 SNS(Social Network Service)를 통하여 정보를 찾고 활용하는 경우가 많아졌지만 시간이 지나고 사용자들의 수가 많아짐에 따라 SNS 내에서도 정보 관리가 점점 어려워지고 있다. 이러한 문제를 해결하기 위해 본 연구에서는 모바일 SNS 사용자를 대상으로, 사용자들이 입력하는 글 정보와 검색 정보에서 색인어를 추출하여 관심사가 무엇인지 파악 하고자 하였다. 수많은 정보들 중에서 관심사와 연관성 있는 정보들만 분류하여 제공하거나 관련 서비스(타 사이트 연결, 타 사이트 검색)를 연결시켜 주는 시스템을 제시함으로써, 보다 쉽고 빠르게 사용자들이 SNS내에서 정보를 관리 및 활용할 수 있도록 하는 것을 목적으로 하였다. 본 연구를 통하여 SNS환경이 기존보다 발전될 수 있을 것으로 기대하는 바이다.Web paradigm called web 2.0 makes many changes in overall society on the basis of the concept of participation, sharing, and opening. As web 2.0 paradigm makes everybody able to produce information, spread rapidly and widely, it creates the one-person media and platform. Due to such a progression of web space, Internet users open and share their own information, so a man’s individual influence has been increased. Accordingly, large-scale search companies and portal sites open and share their own service instead of a closed structure of only using their site, and provide Open-API of leading users’ participation. In addition, as they receive many websites’ Open-API to make Mashup services creating a new service, users’ web infra is becoming abundant. However, users have a difficulty in searching reliable information they want to search or managing it due to creating numerous information and services. Therefore, users often search for and use information through SNS(Social Network Service) connected to people that users share with their interest and individuality. However, as time goes by and the number of users is growing, managing information within SNS is getting hard. To solve the problem this study abstracted the index from text information and search information that users input in order to grasp users’ interest by targeting mobile SNS users. This study is conducted to help users manage and use information in SNS more easily and quickly by suggesting the system that divides and provides information only related to a use’s interest among a lot of information as well as connects other relevant services(connecting other sites, searching other sites). Through this study, hopefully, SNS environment will be developed more than before.제 1장 서론 1 제 1절 연구의 배경 1 제 2절 연구의 필요성 및 목적 3 제 3절 연구 방법 7 제 2장 이론적 논의 8

현업 사용자들의 데이터베이스 설계에 대한 부담을 경감하고자 그동안 데이터베이스 자동화에 관한 연구가 다양한 측면에서 시도되었다. 하지만 대부분의 연구들은 논리적, 물리적 모델링의 자동화에 범위를 국한하여 수행되었으며, 데이터 모델링의 시작 단계인 요구 분석 및 개념적 모델링의 자동화에 대한 연구는 상대적으로 매우 부족한 실정이다. 또한 개념적 모델링의 자동화를 시도한 대부분의 선행 연구들은 개체와 속성의 식별을 사용자가 직접 수행할 것을 요구하였기 때문에, 데이터베이스에 대한 지식이 없는 현업 사용자가 이들 시스템으로부터 실질적인 도움을 받기가 매우 어려웠다. 따라서 본 논문에서는 사용자에게 데이터베이스 지식을 요구하지 않고, 사용자가 작성한 업무기술서에만 근거하여 데이터 모델링을 수행할 수 있는 설계 방법론을 제안한다.

제안 방법론은 ‘연관규칙 마이닝’이라는 통계적인 기법을 활용하여 자연어로 작성된 업무기술서에 포함되어 있는 객체들 간의 연관 관계를 추출하고, 발견된 관계에 근거하여 개념적 모델링을 자동으로 수행한다. 모델링 과정에서 신뢰도의 임계치를 다양하게 조절함으로써, 동일한 업무기술서로부터 여러 버전의 개체관계도를 사용자에게 제시할 수 있다. 본 개발 방법론이 갖는 편의성과 유연성은, 데이터베이스 구축 과정에서 설계 기간을 단축시킴과 동시에 현업 사용자의 부담을 경감시키는 효과가 있을 수 있을 것으로 기대된다. 제안하는 방법론의 현업 적용 가능성을 평가하기 위해, 단순화된 대학 업무에 대한 데이터 모델링 과정 및 그 결과를 본 논문의 실험 부분에 제시하였다.Ι. 서론 1 1.1 배경 1 1.2 동기 4 1.3 목적 6 Ⅱ. 관련연구 8

Korean and Chinese are considered that they belong to different Language-Family. But, new excavations of archaeological specimens and discovery of new historical documents cause we suppose that they belong to same Language-Family.
Therefore in this point of view I demonstrated that Korea and Chinese belong to same Language-Family in this paper.
In this paper I tried to compare two languages on the basis Basic-Vocabulary. The Basic-Vocabulary is classified into some items as below.
(1) Heavenly-Body Terms (2) Time Terms (3) Nature Terms (4) Animal Terms (5) Plant Terms (6) Human-Body Terms
The conclusion of comparison is at below.
1. "k-" First Consonant of Korean correspond with "k-, k'-, g-, g'-, r-, ng-, x-" First Consonant of Chinese.
(example) 골(kol) : 谷 (^(*)kuk)
개(kε) : 犬 (^(*)kiwen), 狗 (^(*)ku)
곰(kom) : 態 (^(*)g'□um)
가슴(kasi¨m) : 胸(^(*)x□ung)
2. "t-" First Consonant of Korean correspond with "t-, t'-, d-, d'-, ts-, ts'-, dz-, dz'- ts-, ts'-, dz-, dz'-, t-, t'-, d-, d'-, z-, ts-, ts'-, dz'-" First Consonant of Chinese.
(example) 두레(ture) : 洞(^(*)d'u＇ung), 村(^(*)ts'w□n)
돌(tol) : 石(^(*)d^□a^k)
닭(talk) : 鷄(^(*)tio^g)
때(t'ε) : 時(^(*)d^□□g)
3. "p-" First-Consonant of Korean correspond with "p-, p'-, b-, b'-" First-Consonant of Chinese.
(example) 바람(param) : 風(^(*)p□o^ng)
밥(pap) : 飯(^(*)b'iwa^n)
벌(p□l) : 峰(^(*)p'□ung)
발(pal) : 足(^(*)p'o)
4. "m-" First-Consonant of Korean correspond with "m-" First-Consonant of Chinese.
(example) 물(mul) : 沒(^(*)mw□t)
뫼(moi) : 山(^(*)mi□r)
말(mal) : 馬(^(*)maˇ)
몸(mom) : 貌(^(*)moˇg)
5. "s-" First-Consonant of Korean correspond with "s-, s-" First-Consonant of Chinese.
(example) 샘(sεm) : 水(^(*)s＇iwer)
손(son) : 手(^(*)s□o＾g)
6. "n-" First-Consonant of Korean correspond with "n-, n-" First-Consonant of Chinese.
(example) 날(nal) : 日(^(*)n□eˇt)
칼날(-nal) : 刃(^(*)n'□□n)
As seen above "k-, t-, p-, s-, n-" First-Consonant of Korean and Chinese correspond with each other. In addition to "Φ-" First-Consonant and "p- : m-" First-Consonant correspond with each other.차례 I. 序論 = 1 1. 硏究目的 = 1 2. 硏究方法 및 範圍 = 1 3. 語頭音 比較의 意義 = 2

스마트폰 제조사에서 고객 분석을 목적으로 분석하기 위해서는 기존의 고객 만족도 조사를 기반으로하는 자체적인 정량조사나, 신뢰도 있는 기관을 통한 통계에 의존 할 수 밖에 없다. 본 연구는, 객관적인 스마트폰의 사용자의 각 기능별 평가 만족도 영향 요소를 도출하기 위해, 머신러닝을 통해 도출해 보고자 하였다. 이를 위해, 현재 연구가 진행중인 NLP, CNN, Grad-CAM을 활용한 딥러닝 모델을 제안하여, 수집한 스마트폰 댓글을 통해 3개 대표 스마트폰 플레그쉽 모델을 중심으로 사용자 만족도 조사를 진행하였다. 연구 결과, 모델에서 도출된 제조사별 종합 만족도 평균은 Samsung, Apple, LG 순으로 나타났으며, 이는 99% 신뢰수준에서 통계적으로 유의미 한 것으로 나타났다. 세부적으로, Apple은 Design, Camera, CPU 에서, Samsung의 경우 Screen, Battery, Sound에서 상대적으로 높은 긍정적 평가를 받았고, Software 는 3개 제조사에서 모두 부정적인 평가를 받았다. 본 연구에서 제시한 모델을 통해, 온라인으로 실시간으로 수집한 데이터를 실무적으로 즉시 활용 할 수 있다는 점에서 대단히 많은 장점을 가지며, 사용자의 세부적인 긍/부정 요소를 파악 할 수 있기 때문에 어떤 의사결정이나 주목되지 않는 기능의 사용자 반응을 모아서 활용이 가능할 수 있다. 또한 현재 머신러닝에서의 CNN을 통한 자연어처리(NLP) 및 감성(Sentiment) 분석을 Grad-CAM을 통해 실무적 활용이 가능함을 보여 주었다. 이를 통해, 기존 사용자 조사 방법에서 나아가, 현재 빠르게 발전하는 머신러닝 기법을 사용자 조사 방법론으로 활용하는 방법을 제안할 수 있을 것 이다.

Question answering (QA) systems find answers to natural-language questions. QA systems are of two types: Knowledgebase-based QA (KBQA) systems that use structured knowledge bases (KBs), and Information Retrieval-based QA (IRQA) systems that use unstructured raw text data. KBQAs are divided into curated KBQAs and auto-extracted KBQAs. I propose a system that use several sources to find an answer; these sources include a curated KB, raw text, and an auto-extracted KB which is a large set of triples. To rank answer candidates from multi-source, I use (1) a semantic answer-type classifier to filter answer candidates, (2) a multi-source tagged text database, and (3) a hybrid answer ranking model trained with multi-level features extracted from multi-source tagged text database. The proposed hybrid answer ranking method for multi-source question answering system achieves higher accuracy and Mean Reciprocal Rank (MRR) than single-sources based QA.질의 응답 시스템은 사용자의 자연어 질문에 대해 정답을 출력하는 시스템이다. 질의 응답 시스템은 지식베이스 기반 질의 응답 시스템과 정보 검색 기반 질의 응답 시스템으로 분류된다. 지식 베이스 기반 질의 응답 시스템은 구조화된 지식베이스에서 답을 찾고, 정보 검색 기반 질의 응답 시스템은 구조화 되어 있지 않고 가공되지 않은 텍스트에서 답을 찾는다. 지식베이스 기반 질의 응답 시스템은 사람의 노동에 의해 정제된 지식베이스 또는 자동 추출된 지식베이스에 기반하는 질의 응답 시스템으로 나뉜다. 본 논문에서는 사람에 의해 정제된 지식베이스, 가공되지 않은 텍스트, 대량의 트리플로 이루어진 자동 추출된 지식베이스를 모두 활용하는 질의 응답 시스템을 제안한다. 다양한 자원에서 추출된 정답 후보를 순위화 하기 위하여 (1) 의미적 정답 유형 분류기를 사용하여 정답 후보들을 필터링하고, (2) 다중 소스 레이블 데이터베이스를 사용하고 (3) 다중 소스 레이블 데이터베이스에서 추출한 다계층적인 자질을 사용하여 하이브리드 정답 랭킹모델을 훈련시킨다. 본 논문에서 제안하는 다중 소스를 이용한 질의 응답 시스템에서의 하이브리드 정답 랭킹 방법은 단일 소스를 사용한 질의 응답 시스템 대비 정확도와 상호간의 순위 평균을 향상시켰다.I. Introduction 1 II. Terminology 6 III. Related Work 8 3.1. IRQA 8 3.2. Curated-KBQA 9

Information Extraction is a task of automatically extracting structured information from unstructured documents in NLP(natural language processing) and text mining. This includes recognizing named entities, resolving coreference, and extracting relations between the entities. In detail, a relation extraction task requires the detection and classification of semantic relationship from documents. It is useful task of constructing knowledge base and using Q&A(Question and Answer) Systems.
In this paper, This study adapts semi-supervised learning method for reducing cost of requiring trained person and times. A distant supervision is semi-supervised learning scheme using knowledge base or heuristic function. This study proposes method to improve distant supervision and proposes structure undefined relation in advance.
According to the experimental result, the proposed method improves the performance of relation extraction using distant supervision. This approach extracts relation triples than method using distant supervision baseline.정보 추출은 비정형적인 문장으로부터 유용한 정보를 추출하는 자연어 처리 및 텍스트 마이닝 분야의 주요 연구 과제중 하나이다. 이러한 정보 추출 기술에는 개체명 인식, 관계 추출, 대용어 참조 해소 등의 분야가 있다. 그 중 관계 추출은 문서 내에 존재하는 인명, 지명 등의 개체 간의 의미적인 관계를 추출하는 작업에 해당하며 지식 베이스(Knowledge Base) 구축 및 QA(Question & Answer) 시스템 등에 활용되는 중요한 연구 분야이다.
본 논문에서는 지도 학습(Supervised Learning)에 필요한 사람의 수작업 및 비용을 줄이기 위하여 최근 주목받고 있는 준지도 학습(Semi-Supervised Learning) 방법들 중 Distant Supervision 학습 기법을 이용하여 관계 추출을 수행하는 연구 방향으로 구성되어있다. 기존의 Distant Supervision을 이용한 관계 추출에서는 파악하기 힘든 의미 분석을 적용하고 학습 말뭉치 생성을 위해 클러스터링 방식을 적용하여 Distant Supervision 학습 기법을 개선하여 관계 추출 문제를 향상시킬 수 있는 방안을 제시한다.
다양한 의미 유사도 비교 방법의 비교 실험을 통해 Distant Supervision을 이용한 관계 추출에 유용한 유사도 계산 방법들을 찾아보았으며, 본 논문에서 제안하는 구조적인 이점과 관계의 의미 유사도 비교를 이용하여 보다 많고 정확한 관계 트리플을 추출할 수 있음을 확인하였다.

객체지향 개발 방법론을 활용하기 위해서는 실제적인 개념과 규격화된 패턴, 상황에 따른 올바른 객체의 선택과 적용이 필요하다. 이러한 객체지향 설계 개념의 이론적 제시와 프로그래밍에 적용 가능하도록 구체적으로 규격화시킨 것이 설계 패턴이다. 설계 패턴은 객체지향 방법론의 가장 큰 장점인 재사용성과 모듈성을 극대화시킨 실제 구현 과정에서의 해결책이다. 설계 패턴은 Gamma에 의해서 패턴의 역할에 따라 3부분으로 분류되는데 객체의 생성 방식을 결정하는 포괄적인 방법을 제공하는 생성 패턴과 구조 패턴, 그리고 행위 패턴으로 분류하였다. 그러나 수 백 가지의 패턴을 효율적으로 재사용하기 위해서는 패턴이 수행하는 역할에 따른 3가지 분류와 패턴명 만으로 검색해야하는 Gamma의 분류 방법으로는 사용자가 원하는 패턴을 찾아 적용하는데는 많은 어려움이 따른다.
본 연구의 목적은 설계 패턴을 효율적으로 검색하기 위하여 기존의 패턴들 사이의 관계성을 의미적으로 표현하고, 새로운 패턴들의 추가, 삭제할 때 기존의 관계성을 이용하여 자동적인 객체 사이의 관계가 형성되며, 사용자가 원하는 패턴을 검색할 수 있는 객체기반 시소러스로 구축에 그 목적을 두었다. 그리고 설계패턴의 검색을 위해서는 질의의 의미 파악을 위한 추론 방법을 사용하였고, 임의의 자연어 처리는 매우 복잡하기 때문에 Gamma의 패턴을 중심으로 패싯 기능을 이용한 메뉴 방식의 질의어를 통하여 검색이 이루어지도록 하였다. 패턴 검색은 질의가 갖는 의미를 제한된 범위 내에서 표현할 수 있도록 확장된 부울 형식을 사용하였다. 연산자는 존재연산자와 개념연산자의 혼합형식을 이용하여 검색할 수 있도록 하였다.
따라서 본 연구는 패턴 검색을 위하여 질의와 객체사이의 관계를 개별적으로 해야하는 단점을 극복하고, 자동적으로 관계를 연결시키는 객체 기반 시소러스를 구축하였다. 제안한 시소러스 구축 방법은 저장소에 패턴들의 관계성을 정의하여 구축한 후 새로운 패턴을 삽입할 때 사용자가 임의의 패턴과의 관계성을 지정해주면 시스템은 임의의 패턴에 연관되어 있는 패턴들을 사용자에게 자동적으로 보여주고, 사용자는 이들 패턴 중 가장 관계성이 있는 패턴과의 관계성을 지정해 준다. 따라서 패턴 삽입 시 기존의 패턴들을 일일이 조사할 필요가 없는 패턴 사이의 관계성을 보다 편리하게 정의할 수 있도록 시소러스를 구축하였다.In order to take advantage of object-based development method, practical concept and formed pattern, and the selection and application of reasonable object according to the circumstance are needed. To make it possible to apply to theoretical suggestion of object-based design definition like this and programming, design pattern is standardized concretely. Design pattern is a solution in the real embodiment process that recycling which is the biggest merit of object-based development and modularity are maximized. Design pattern is sorted out into three parts on the basis of pattern's task by Gamma, that is, creational pattern providing comprehensive means to determine producing method of objects ,structural pattern, and behavioral pattern. But, it is hard to find and apply patterns that users want by Gamma's classification system in order to reuse hundreds of patterns efficiently, gamma's system only uses three groupings by pattern's duty and has to make a search only by the name of pattern.
The purpose of this research is to express relevance between existing patterns semantically and also is to build up the automatic relation of the objects using the existing references when new patterns are inserted or deleted and lastly is to develop object-based thesaurus retrieving patterns that use to want.
Inference method is used to understand meanings of inquiries for browsing, and exploration is made by inquiries in menu form using facet analysis focusing on Gamma's pattern because arbitrary natural language processing is very complex. In pattern searching, meaning of inquires can be presented in a limited range and extended boolean operator can be used. The mixed form of existence operator and concept operator are used for searching.
In this paper, the object-based Thesaurus is implemented for pattern matching.
Queries and objects are automatically related with Thesaurus, which have been kept separate in the previous research.
The presented methodology of Thesaurus first defines the relations of patterns in the storage. Therefore, when new patterns arrive, the system automatically shows related patterns with those random patterns with which users designate the relations in advance.
Then, users just specify the relations with the most-significant patterns among those which the system presented.
In conclusion, the 'Thesaurus' is implemented in order to easily define the relations among patterns without enumerating the existing patterns, when new patterns arrive.■국문요약 = I ■그림목차 = V ■표 목차 = VI 제1장 서론 = 1 1.1 연구 배경 = 1

최근 다양한 분야에서 다량의 정형 및 비정형 자료, 즉 빅데이터를 분석하여 자료의 연관성을 규명하고 미래전략을 수립하는 방법이 활용되고 있다. 공시지가의 경우에도 지역적 및 시간적으로 다양하고 복잡한 민원이 제기되고 있어 빅데이터를 활용하여 이를 효과적으로 분석하고 관리하는 방법에 관한 연구가 필요하다. 이 연구의 목적은 빅데이터 마이닝에 의해서 공시지가 민원의 시공간적 특성 분석 방법을 정립하는 것이다. 특히 이 연구는 제도적인 측면보다는 공시지가 민원 발생의 시공간적 원인 분석에 주안점을 두고, 그 변화 추세를 모니터링 할 수 있는 표준화 모델을 정립하고자 하였다.
연구 수행을 위해서 빅데이터 마이닝에 의한 공시지가 민원의 시공간적 특성 분석 표준화 모델을 정립하였다. 그리고 시간과 공간적인 속성을 함께 포함하고 있는 2006년부터 2015년까지의 인천광역시 중구의 공시지가 민원자료 6,481건을 수집하고, 이것을 공간정보와 결합하여 공간 빅데이터 기반의 공시지가 민원정보 데이터베이스를 구축하였다. 텍스트 마이닝 방법을 이용하여 주요 키워드의 빈도를 분석하고, 소셜 네트워크 분석을 통해서 주요 키워드들의 상호 연관성을 파악하였다. 키워드 가중치를 산출하여 공시지가 민원발생 관심 키워드를 선정한 후, 국지적 자기상관의 지표인 G 통계량(Getis-Ord Gi*)을 적용한 핫스팟 분석을 통하여 시공간적 특성을 분석하였다.
연구 결과, 공시지가 민원의 특성은 시공간적으로 연계된 군집 형태를 형성하면서 변화하고 있음을 알 수 있었다. 텍스트 마이닝과 소셜 네트워크 분석 방법을 이용하여 자연어 기반의 공시지가 민원에 대한 발생 원인을 정량적으로 규명할 수 있음을 알 수 있었으며, 키워드 가중치인 단어 빈도(TF) 및 단어 빈도와 역문서 빈도의 조합값(TF-IDF)의 상대적인 차이가 있어 시공간적인 민원 특성을 분석하기 위한 주요 설명변수로 활용될 수 있음을 알 수 있었다.Recently, a method of analyzing structured as well as unstructured data, the so-called big data, has been widely applied for creating valuable information to use it for a future strategy in various fields. With respect to the officially assessed land price, there have been many civil complaints that are complex and diverse in terms of both region and time. Thus, more research should be done in order to effectively examine and manage such complaints by applying the big data. The purpose of this study is to establish a method of analyzing spatio-temporal characteristics of the civil complaints for the officially assessed land price based on big data mining. Specifically, this study is established the underlying reasons for the civil complaints from the spatio-temporal perspectives, rather than the institutional factors, and to suggest a model of monitoring a trend of the occurrence of such complaints.
The official documents of 6,481 civil complaints for the officially assessed land price in the district of Jung-gu of Incheon Metropolitan City over the period from 2006 to 2015 along with their temporal and spatial properties were collected and used for the analysis. Frequencies of major key words were examined by using a text mining method. Correlations among major key words were studied through the social network analysis. By calculating Term Frequency(TF) and Term Frequency-Inverse Document Frequency(TF-IDF), which correspond to the weighted value of key words, I identified the major key words of interest for the occurrence of the civil complaint for the officially assessed land price were selected. Then, the spatio-temporal characteristics of the civil complaints were examined by analyzing hot spot based on the statistics of Getis-Ord Gi*.
It was found that the civil complaints for the officially assessed land price vary in a cluster that changes spatio-temporally. This study shows the proposed standard model can identify the causes of the occurrence of the civil complaints which is written in natural language for the officially assessed land price quantitatively through text mining and social-network analysis method. In addition, TF and TF-IDF, the weighted averages of key words, can be used as main explanatory variables to analyze spatio-temporal characteristics of civil complaints for the officially assessed land price since these statistics are different over time across different regions.국 문 초 록 ⅰ 목 차 ⅲ 표 목 차 ⅶ 그 림 목 차 ⅸ 제 1 장 서 론 1

소프트웨어 시스템 개발을 위해서는 요구사항 단계에서 요구사항을 분석할 때 중복, 충돌이 없는 정확한 요구사항을 분석하는 것이 무엇보다 중요하다. 요구사항을 분석하는 방법으로는 유스케이스 분석(Use Case Modeling), 시나리오 기반의 분석(Scenario based requirement analysis), 목표 기반의 분석 방법(Goal based requirement analysis)등이 있으나 최근 목표와 시나리오 모델 분석의 방법(Goal and Scenario Modeling)이 많이 사용되고 있다. 그러나 현재 목표와 시나리오 모델의 분석 방법에서는 요구사항의 중복 및 충돌 식별 방법에 대한 연구가 미비하다. 또한 기존 연구에서는 정형화된 표현 언어를 사용하여 요구사항을 기술하고 충돌을 식별하므로 전문가가 아니면 사용하기 힘들다.
최근 기술의 발달로 개발 가능한 소프트웨어 시스템이 복잡화, 대형화되면서 기존의 소프트웨어 시스템의 개선, 유지 및 보수를 목적으로 새로운 기능을 추가하는 경우가 많아지게 되었다. 이에 따라 요구사항이 수가 증가하고 요구사항끼리 공유하는 자원 및 객체가 생성되어 요구사항의 중복 및 충돌이 나타나는 경우가 많아졌다. 그러므로 본 논문에서는 최근 요구사항 분석 방법으로 많이 사용되고 있는 목표와 시나리오 모델을 사용하여 요구사항을 자연어로 기술했을 때 요구사항의 중복 및 충돌을 식별하는 방법을 제안하고 도구를 개발하고자 한다.
본 논문에서는 요구사항의 중복 및 충돌 식별 방법의 유용성을 검증하기 위하여 ATM 시스템에 새로운 기능을 추가했을 경우와 지능형 로봇 시스템에 새로운 기능을 추가했을 경우에 요구사항의 중복 및 충돌 식별 방법을 적용하여 그 결과를 분석하였다.It is important to analyze correct requirements without redundancies and conflicts in requirements engineering phase to develop software system. We analyze requirements using use case modeling, scenario based requirement analysis, goal based requirement analysis, but we use goal and scenario modeling a lot these days to analyze requirements. However we don’t have many studies to detect requirements redundancies and conflicts using goal and scenario modeling. Also it is hard to use existing studies because they use formal language.
As software become more complicated and large scaled, we add new functions for managing and improving existing system. Thus the more requirements are added, the more chances to arise requirements redundancies and conflicts. Therefore, we propose a method to identify requirements redundancies and conflicts for goal and scenario modeling.
In conclusion, to verify the efficiency of suggested method, ATM and HROT is applied to the method and the result is analyzed.

소프트웨어에 내재되어있는 결함을 통해 발생하는 보안 사고가 늘어나고 있는 추세이다. 이에 소프트웨어 보안 사고는 발생 후에 대처를 하는 것보다 사전에 소프트웨어의 결함을 제거함으로써 예방하는 것이 그에 따른 피해를 줄이는 데 효과적이라는 인식이 전반적으로 공감대를 얻고 있으며, 이러한 인식의 일환으로 우리나라는 2012년 소프트웨어 개발 보안 제도를 만들어 시행 중이다. 소프트웨어 개발보안제도의 핵심은 프로그램의 구현단계에서의 보안 활동인 시큐어코딩이다. 시큐어코딩과 연계된 개발보안 활동은 보안 취약점이 내재되어 있는 소스코드의 특징과 패턴을 찾아내는 것이며, 소스코드의 정확한 분석을 위해 자동화된 정적 분석기의 활용을 권고하고 있다.
그러나 현실적으로는 코딩 방법과 개발자의 스타일에 따라 다양한 형태로 소스코드가 작성 될 수 있기 때문에 보안에 취약하다고 판단되는 패턴을 탐지하기 위한 룰셋을 만들어주고 정적 분석 도구에 적용 시키는 일은 여전히 전문가의 개입을 필요로 한다. 또한 소스코드 분석은 고려해야 될 변수가 많으며, 어떤 취약점을 탐지할 건지에 따라 소스코드를 분석하는 관점을 달리해야하기 때문에 정적분석 도구의 복잡성을 높이고 정확한 진단을 어렵게 만든다는 문제가 있다.
본 연구에서는 이러한 문제를 완화시키기 위한 방법으로 기계학습 알고리즘의 도입을 제안하는 바이다. 기계학습은 최근 음성신호, 자연어 처리, 사물개체 인식 등과 같이 사람의 인지 능력으로 판단할 수 있던 문제 영역을 해결하는 성과를 보이고 있다. 이와 같은 최근 연구 사례를 보았을 때, 전문가의 의해 정의되었던 안전하지 않은 소스코드의 룰셋을 기계학습을 이용해 보안 취약점을 식별하고 탐지할 수 있는지 실험 결과를 통해 알아보고자 한다. 또한 실험 결과를 기반으로 기계학습을 이용한 정적분석의 가능성을 평가하고 발전방향을 제시하고 연구를 마친다.논문개요 Ⅰ. 서론 1. 연구의 배경 및 목적 1 2. 논문의 구성 3

버스는 대표적인 우리나라의 대중교통수단으로 현재 우리나라의 대중교통은 수도권을 중심으로 지하철과 버스를 위주로 이용하고 있으며 지하철 망이 확보되지 않은 지역은 버스의 의존률이 매우 높은 상황이다. 또한 정부의 정책에 따라 대중교통의 이용 활성화와 지구 온난화의 대책으로서 버스의 이용을 적극 권장하여 버스의 이용율은 꾸준히 증가하고 있다. 이와 동시에 증가하는 버스의 이용율에 따라 버스의 사고 또한 증가하고 있는데 이 중 버스 차내 안전사고의 비율이 전체 사고 중 반 이상으로 매우 높은 비율을 차지하고 있다.
그러나 현재 우리나라에서는 버스 차내 안전사고와 관련된 통계자료를 수집하고 있지 않을뿐만 아니라 사고 유형과 원인의 구분이 통일되어있지 않다. 따라서 버스 차내 안전사고에 대한 무분별한 사고의 분석과 개선대책의 제시가 아닌 새로운 노력이 필요한 시점이다.
본 연구에서는 도로교통공단 교통사고분석시스템에서 제공하는 2014년 교통사고데이터를 기반으로 서울특별시에서 발생한 버스 사고 중 버스 차내 안전사고를 분류하여 분석하였다. 분류한 버스 차내 안전사고 데이터 중 텍스트 기반의 사고개요 데이터를 통계분석 패키지 R Project를 활용하여 텍스트 마이닝 분석을 수행하였다. 텍스트 마이닝을 위하여 사고개요 텍스트를 단어 단위로 분해하고 자연어 처리 등의 단어 정제 과정을 통하여 분석에 알맞은 형태로 텍스트들을 변환하였다. 이를 통하여 버스 차내 안전사고와 관련된 단어들의 빈도수를 파악한 후 각 단어들이 갖는 의미들을 분석하여 사고 유형과 원인을 구분하였다. 그 결과 사고 유형은 전도, 추락, 부딪힘, 끼임으로 크게 4가지로 분류되었으며 사고 원인은 급제동, 승객행동, 미끄러움, 이륜차 등으로 다양하게 집계되었다. 이를 통하여 버스 차내 안전사고의 유형과 원인을 재정립하였으며 마지막으로 연구에서 사고 유형별 원인에 따른 해결방안을 추가적으로 제시하였다.
본 연구는 버스 차내 안전사고에 대하여 1년간 서울 특별시로 범위를 한정하였으나 다년간 전국적으로 분석을 수행한다면 더욱 많은 사고 유형과 원인에 대한 패턴을 분석할 수 있을 것이고 더 나아가 지역특색 또한 파악이 가능할 것으로 판단된다.The bus, as a typical Korean public transportation means, the current South Korean public transport, especially in the metropolitan area and has been used in the center of the metro and bus, regional subway system has not been secured, dependent of bus rate is very high. In addition, as a measure of use activation and global warming of public transport on the basis of the policy of the government, bus utilization and actively recommend the use of the bus is steadily increasing. In response to this and bus utilization to increase at the same time, the accident of the bus is also increasing, this one, the proportion of bus-car accident occupies a very high percentage and more than half of the total accidents.
However, now, in Korea, not only do not collect statistical data associated with the bus vehicle accidents, classification types and causes of the accident are not unified. Therefore, rather than the presentation of measures to improve the analysis of senseless accident of bus-car accident, it is a time that requires new initiatives.
In this study, based on the traffic accident data 2014 that is provided in a Road Traffic Authority traffic accidents analysis system, and analyzed by classifying the bus vehicle accident bus accident at Seoul. Of the vehicle accident data classified bus, by utilizing the statistical analysis package R Project summary data of the text-based accidents were text mining analysis. It was converted to text in a form suitable for analysis through a word purification process such as natural language processing for text mining by decomposing a summary text of the accident to the word units. Thus, after understanding the frequency of words associated with the bus vehicle accident by analyzing the meaning each word has to distinguish the type and cause of the accident. And re-establish the type and source of the bus vehicle accidents, and finally were further presents a solution according to the cause of the accident by types in the study. In this study, one year for bus car accidents.제1장 서 론 1 1. 연구의 배경 1 2. 연구의 목적 1 3. 연구의 범위 및 방법 2

시맨틱 웹 관련연구가 증가함에 따라 하나의 관련분야로 규칙기반 시스템 등과 에이전트등을 기반으로 한 지능적인 웹 환경에 대한 기대 역시 커지고 있다. 하지만 규칙기반 시스템을 활용하기에는 아직도 규칙습득이 많은 제약이 되고 있다. 직접 규칙습득이 어려우므로 단계화된 방법이 필요하다. 그러나 이러한 규칙을 식별하는 작업은 대부분 지식관리자의 수작업에 의해 이루어지고 있다.본 연구의 목적은 웹으로부터 규칙구성요소 식별을 최대한 자동화하고 지식관리자의 수작업을 최소화함으로써 그 부담을 줄여 주는 데 있다. 규칙 식별을 자동화 하기 위하여 이러한 방법으로는 온톨로지를 근간으로 하여 웹 페이지와의 문자열 비교, 이러한 비교의 한계를 극복하기 위한 확장등의 방법이 있다.첫째 온톨로지 기반으로 규칙식별 할 웹페이지와 비교를 통해 지식관리자의 규칙식별과정을 최대한 자동화하고자 하였다. 여기서 만약 현재 규칙을 식별하고자 하는 웹 사이트와 유사한 시스템의 규칙들을 활용하여 일반화된 온톨로지가 구축되었다면, 이 온톨로지를 기반으로 규칙을 식별하고자 하는 웹 사이트와의 비교를 통해 규칙구성요소를 자동화하여 추출할 수 있다.이러한 온톨로지를 기반으로 규칙을 식별하기 위해서는 문자열 비교 기법을 사용하게 된다. 하지만 단순한 문자열 비교 기법으로 비교하여 규칙을 식별하는 데에는 자연어 처리에 대한 한계가 있다. 이를 극복하기 위해 다음의 두 번째 방법을 사용하고자 한다.두 번째 정형화되지 않은 정보들을 확장하여 사용하는 것이다. 우선 찾고자 하는 단어들의 원형을 찾기 위한 스테밍 알고리즘 기법, 워드넷을 이용하여 동의어‧유의어등으로 확장을 하는 워드넷 확장 기법, 의미유사도를 측정하기 위한 방법인 의미 유사도 측정등을 단계적으로 수행하여 자동화되고 정확한 규칙식별을 하고자 한다.이러한 방법들의 조합으로 인하여 규칙구성요소 추출이 되지 않을 후보 단어들의 수를 줄임으로써, 보다 더 정확하고 지능적인 규칙구성요소 추출 방법론을 제시 및 에이전트를 구현하고자 한다.As the research about Semantic Web is increasing, the expectation about intelligent web environment of a rule-based system is growing. However, there are many restrictions of learning rules for using a rule-based system. A learning ruleis the way that it can learn necessary rules from web. It needs to distinguish constructing factors of rules first to learn a rule, but these distinguishing works manually are from knowledge mangers in most of cases.Our research goal is to reduce a load by maximizing automatic works of distinguishing and minimizing manual works from web. There are two ways to achieve our goal. One is to compare web pages with string matching based on ontology and the other is an extension for overcomingthe limit of comparison.The first way is to maximize automatically the distinguishing works of knowledge mangers by comparison based on ontology. If there are the website to distinguish current rules and the generalized ontology by using rules of similar systems, we can gather a constructing factor of rules automatically by comparison with web sites which is for distinguishing rules based on the ontology. We can use the string matching method for distinguishing rules based on the ontology. However, there is the limitation for natural language processing to identify rules with comparison of simple string matching method. To solve this problem, we suggest the second way.The second is to extend not standardized information. We can distinguish rules automatically and accurately using three ways: One is the Stemming Algorithm that is to find the original form of words we want, another is the WordNet Expansion that is to expand a synonym using WordNet, and the other is the Semantic Similarity Measure that is to measure similarity of meaning.We can reduce the number of substitute words not to extract the factor by compounding these ways, and then suggest more accurate and intellectual way to extract the factors. Finally, we can reduce the load of learning rules of a knowledge manager.

디지털 기술의 발전으로 산업의 생태계가 변화되고 기존의 기업들을 위협하는 혁신기업이 성장하면서 기술을 활용한 전략적 방식에 대한 중요성이 강조되고 있다. 기술의 활용이 운영 측면의 효율성을 제고함과 동시에 고객의 편의성과 만족을 극대화하기 위한 수단으로 활용되고 있으나, 디지털 기술이 생산경제의 측면에서 제조업에 편중하여 다양화되지 못하는 측면이 있다. 현대 사회는 서비스의 경제화로 서비스산업이 중심이 되어 서비스산업에 대한 이해와 활용이 국가경제와 사회발전에 중요한 동인이 되고 있다. 국내의 서비스산업은 외식산업을 비롯해 노동집약적이고 생산성이 낮은 구조로 외부환경에 민감하게 대응해야하는 취약성이 있다.
본 연구는 디지털 트랜스포메이션의 기술요소 중 데이터 분석을 활용하여 외식산업에서 소비자들의 고객경험 행위를 능동적으로 파악하고, 고객의 경험에 영향을 미칠 수 있는 차별적인 접점을 밝히고자 하였다. 이를 위해 실제 거래 데이터를 활용하여 고객의 외식상품의 선호도를 탐색하고, 이를 토대로 판매자가 취할 수 있는 전략으로 추천 시스템을 제안하고자 하였다.
본 연구를 통해서 외식상품에 대한 고객과 판매자의 접점과 방향성이 상이하다는 것이 나타났다. 또한 시간대 별로 소비자의 상품에 대한 선택 패턴에 차이가 발생하는 것을 확인하였다. 위와 같은 연구결과에 따라 소비자의 선택 패턴을 검증하여, 텍스트 기반의 자연어 처리 학습방식을 활용하여 판매자가 데이터 분석을 토대로 마케팅과 운영 전략에 활용할 수 있는 추천 시스템에 대한 결과물을 제시하였다.
본 연구는 위와 같은 연구결과를 통해서 학술적으로 디지털 트랜스포메이션의 개념을 데이터 분석이라는 범주에서 세분화하여 실증적으로 분석하였다는 점에서 의의가 있으며, 디지털 트랜스포메이션 전략의 활용이 개인매장과 같은 소규모 환경에서도 활용 가능한 전략적인 자료로 시사점을 제공해 줄 것이다.

이 연구에서는 『표준국어대사전』에 나타나는 다의어의 기술 실태를 분석한다. 이 연구에서는 그 양상을 살펴봄으로써 문맥적 변이 의미의 문제점을 지적해 본다. 그리고 이에 대한 대안을 제시하는 데 목적이 있다. 이를 위해 어휘적인 측면과 통사적인 측면과 화용적인 측면에서 문제점을 집중적으로 다루어 보고자 한다.
‘문맥적 변이 의미’란 문맥에 맞게 변이된 의미이다. 특히 단어가 문장을 형성하고 그 문장이 어떤 상황을 서술할 때 형성되는 의미이다. 문맥에서는 단어의 의미가 한정적으로 쓰이거나 상황에 맞게 조정된다. 이러한 의미 변화의 결과를 본고에서는 ‘변이 의미’로 다루고 있다.
이러한 ‘변이 의미’들을 문장의 의미 해석에 관여한다. 인간의 직관적인 의미 해석은 다의성과 관련이 있다. 하나의 단어가 다양한 의미를 지닐 수 있기 때문에 인간은 유한한 언어로 무한한 현상과 사실을 재구성해 낼 수 있다. 반면 다의성 때문에 인간의 언어는 복잡하다. 이러한 복잡성이 인공지능 시대에 필요한 자연어의 기계 번역에 어려움을 주고 있다. 따라서 본 연구에서는 자연어 처리에 적합한 다의어 모델을 발굴하기 위해 이 연구를 진행할 예정이다.
먼저 1장에서는 연구의 목적과 필요성에 대해 살펴본다. 또한 연구 대상 및 방법에 대해 논의한 후 선행 연구에 대해 검토해 볼 예정이다. 그런 다음 2장에서는 연구 대상이 되는 『표준국어대사전』에 대해 전반적인 사항을 검토해 본다. 그런 후 문맥적 변이 의미에 대한 이론적인 검토를 실시한다. 다음으로 3장에서 5장까지는 ‘어휘적 측면’, ‘통사적 측면’, ‘화용적 측면’에서 변이 의미의 기술 양상에 대해 살펴볼 예정이다.
3장에서는 ‘실체성 명사’와 ‘비실체성 명사’로 나누어 각각의 변이 의미와 사전 기술 양상을 살펴본다. ‘실체성 명사’ 가운데서는 ‘책’과 같은 작품류와 ‘시청’과 같은 법인류를 중심으로 살펴볼 예정이다. ‘비실체성 명사’ 가운데서는 ‘말’과 같은 언어류와 ‘노래’와 같은 음악류를 중심으로 살펴볼 예정이다.
4장에서는 ‘문형 형성’ 및 ‘연어성구’와 관련된 변이 의미와 사전 기술 양상을 살펴본다. 문형 형성이란 문형이란 문장을 만들기 위한 통사구조를 의미한다. 그런데 일반적인 구조와는 달리 변이 의미는 문맥에 맞게 조정할 수 있는 특수한 통사구조를 지니고 있다. 어휘적인 측면의 변이 의미와는 달리 통사적인 측면의 변이 의미는 문형의 영향을 많이 받는다. 이 연구에서는 그러한 문형 형성의 측면에서 다의성 문제를 논의해 보고자 한다. 또한 연어성구란 하나의 숙어로 굳어진 구 단위 이상의 표현을 의미한다. 어떤 단어와 단어는 특수한 방식으로 결속되어 있는데, 이들은 하나로 굳어져서 분리되지 않는다. 그러한 특수한 결속 상태에서는 특수한 의미가 발생하여 다의성을 지니게 된다. 본고에서는 그러한 다의성 문제에 대해 논의해 보고자 한다.
5장에서는 ‘비유적 변이’에 대해 살펴보고자 한다. 비유적 변이란 은유나 환유 같은 인지적·수사적 특징으로 인해 발생하는 문맥적 변이이다. 이러한 비유에는 일상적인 비유와 문학적인 비유가 있다. 이러한 비유 방식에 의해 단어는 기본의미 이외의 새로운 의미를 생성하게 된다.
그런 다음 6장에서는 변이 의미의 기술 문제를 짚어보고, 그에 대한 대안과 방향성을 제시해 보고자 한다. 그런 후 7장에서는 이를 종합적으로 정리하여 결론을 짓는다.In this study, I analyze descriptive aspects of Korean polysemy as found in the Standard Korean Language Dictionary and propose an alternate approach for the issue of “contextual transferred meanings.” To do so, I investigate expressions on the lexical, syntactic, and pragmatic levels and discuss the implications of each of the conditions.
The meaning of “contextual transferred meaning” describes when a word or phrase has the capacity for multiple meanings contingent on the context in which it is found. Dependent on the content and circumstance within a sentence, a word’s meaning may be limited or adjusted accordingly. The resulting semantic change is recognized as “contextual transferred meaning.”

These “contextual transferred meanings” are innately involved in the human process of linguistic comprehension. With our intrinsic understanding that words have polysemous characteristics, humans construct infinite perceptions and phenomena from the finite framework of language. However, in our modern world of artificial intelligence, the sheer multiplicity of language brings forth difficulties in natural language translation processes via machine translation due to the layered semantic complexities. This study was carried out in order to find a model of Korean polysemy suitable for natural language processing.

Chapter 1 begins by examining the objectives and necessity of this study; after reviewing the research scope and methodologies, previous studies in the field are reviewed. In Chapter 2, the general characteristics of the Standard Korean Language Dictionary as the primary subject of the study are assessed, followed by a theoretical discussion of the meaning of “contextual transferred meaning.” Next, Chapters 3 through 5 analyze the descriptive aspects of “contextual transferred meaning” terms of the lexical, syntactic, and pragmatic aspects, respectively.

Chapter 3 examines the variations of contextual transferred meanings by dividing words into categories of substantive nouns and non-substantive nouns. In the substantive noun classification, words referring to specific works (such as “책,” meaning “book”) or legal terms (such as “시청,” meaning “city hall”) will be examined, while among the non-substantive nouns, linguistic terms (such as “말,” meaning “word” or “language”) or musical terms (such as “노래,” meaning “song”) will be analyzed.

Chapter 4 examines the descriptive and technical aspects of contextual transferred meanings as they apply to sentence formation and collocation phrases. Sentence formation refers to the syntactic structures for the creation of a sentence; however, unlike typical general structures, those with polysemous expressions allow for a special syntactic structure that adequately reflects the content. Unlike the contextual meanings viewed from a lexical perspective, from a syntactic standpoint, contextual meanings depend highly on sentence structure patterns. This chapter explores these varied polysemous examples through syntactic analysis. In addition, this section goes further to examine “contextual transferred meanings” in collocations or idiomatic phrases. Collocations are a sequence of words that co-occur with one another. These words or terms are bound together and are set in their preferred usages. This paper aims to analyze the varied contextual meanings that arise in such collocations.

In Chapter 5, I examine metaphorical variations in meaning. A metaphorical variation is a contextual transference of meaning arising from cognitive or rhetorical features such as metaphors and metonymy. Among metaphors, there are everyday metaphors and literary metaphors ; through such usage, words are given additional, new meanings in addition to its original definition.

Chapter 6 investigates the descriptive mechanisms of contextual transferred meanings and aim to provide an alternative solution. Finally, in Chapter 7, a conclusion is drawn from the analyses provided.1. 서론 1 1.1 연구 목적 및 필요성 1 1.2 연구 대상 및 방법 8 1.3 선행 연구 검토 12

대부분의 XML 질의 언어는 부울 검색에 의한 단순 매칭 검색만을 허용하기 때문에 그 기능이 매우 한정되어 있고 질의에 대한 재현도는 높지만 정확률은 매우 낮은 문제점을 안고 있다. HyREX는 독일 도르트문트 대학에서 개발된 연구용 프로토타입 XML 하이퍼미디어 문서 검색 시스템으로 다국어를 지원하고 있다. HyREX는 검색을 위한 효율적인 접근 경로들을 처리하는 물리적인 계층 HyPath와 질의어를 처리하는 논리적인 계층 XIRQL 그리고 사용자 인터페이스인 HyGate로 구성되어 있다.
이 연구에서는 HyREX에서 한글 XML 문서 검색이 가능하도록 하기 위해 한글 데이터타입, 한글 인덱싱 모듈 HAM 5.0을 라이브러리로 연결한 인덱싱 기능, 인덱싱된 키워드에 빈도수를 구하여 가중치를 할당하고 질의에 관련된 문서들이 유사도의 랭크 순서대로 검색할 수 있는 기능을 추가하였다.
현재 한글 데이터를 인코딩하는 처리과정에서 미흡한 부분이 있어 단일 키워드를 이용한 질의만이 가능하다. 그리고 문서 내에서 엘리먼트들의 중요도에 대해 고려하지 않고 키워드의 빈도수만 고려하여 가중치를 할당하고 있다. 향후에는 자연어 형태나 여러 키워드들을 이용하여 질의할 수 있도록 질의를 확장하고 문서 내에서 엘리먼트들의 중요도를 반영하여 가중치를 할당함으로써, 한글 질의에 대하여 검색되는 문서들의 정확도와 재현도가 향상될 수 있도록 개선하고자 한다.Most XML query languages permit retrievals by simple pattern matching and have limited functions. So recall of queries using such languages is high and precisions is low. HyREX is a prototype XML hypermedia retrieval system for research. It is developed by the University of Dortmund. It supports multilingual search service. HyREX has three levels for effective access paths, HyGate for user interface, XIRQL of logical level and HyPath of physical level.
In this research, a few functions were added to HyREX for Korean XML document retrieval, including data type and indexing module HAM 5.0 for Korean. Furthermore the weights for indexed keywords were assigned based on frequencies of occurrences of them. Similarities between queries and documents are calculated and the documents retrieved are ranked in the order of similarities.
Currently, only queries of single keyword is processed because of the limited encoding power for Korean. Without considering the significance of elements in the documents to which the keywords belong, the weights of them are assigned. For future research, queries can be extended to be represented in a form of natural languages or using several keywords. Weights of keywords can be assigned by reflecting the significance of the elements which they belong to, so that recalls and precisions of documents retrieved can be improved.목차 요약 Ⅰ. 서론 = 1 Ⅱ. 관련연구 = 4 1. XML 문서의 구조 = 4

본 연구에서는 소셜미디어 상에서 청소년 우울과 관련된 정보를 수집하고 분석하기 위한 분석틀(framework)로써 청소년 우울 온톨로지를 개발하였다,
Noy & McGuiness(2001)가 고안한 ontology development 101를 근거로, 4개국의 임상실무지침서 내 주요 개념을 추출한 뒤 분류체계를 작성하였고 클래스별 속성과 속성값, 클래스간 관계를 정의하는 청소년 우울 온톨로지를 개발하였다.
개발된 청소년 우울 온톨로지는 452개의 클래스를 포함하여 총 2,073개의 용어와 클래스의 유사어•동의어를 수록하고 있으며 청소년 우울의 위험요인, 증상 및 징후, 측정, 진단결과, 치료 및 관리 도메인으로 구성되어있다.
청소년 우울 온톨로지는 아래와 같은 세 가지 방법으로 평가되었고 모든 평가에서 우수성이 입증되었다.
1) 약 16만 건의 청소년 우울 관련 소셜미디어 데이터를 대상으로 온톨로지 개념의 출현여부를 조사한 결과 온톨로지 개념의 72.8%가 소셜미디어 데이터 내에서 출현하였다.
2) 청소년 우울 관련 상담기록 66건을 자연어처리과정(NLP)을 거쳐 핵심개념을 추출하고 핵심 개념과 온톨로지 개념을 매핑한 결과 73.1%가 완전히 매핑(completely mapped)되었으며 부분 매핑(partially mapped)의 경우에도 21%로 나타났다.
3) 온톨로지 개발 경험이 있는 간호정보학 전문가 4인과 청소년 정신건강 전문가 2인을 대상으로 Kehagias, Papadimitriou, Hois, Tzovaras & Bateman(2008)이 제안한 온톨로지 평가 기준에 따라 온톨로지의 구조 및 표현력을 평가한 결과, 모든 항목에서 평균 4.3이상의 점수를 받았다.
본 연구에서 개발한 청소년 우울 온톨로지는 송태민(2015)의 연구를 통해 소셜빅데이터 내 우리나라 청소년의 우울 현황과 우울 요인을 분석하는 틀로써 활용되었고 매우 의미 있는 결과를 도출하였다.
청소년 우울 온톨로지는 청소년 자살을 비롯해 청소년 비행과 같은 개인, 가족, 학교 및 지역사회 차원의 통합적 관점이 요구되는 사회 현상과 이슈를 정의하는데 일반화 되어 활용 가능할 것으로 예상된다.Ⅰ. 서론 1 1. 연구의 필요성 1 2. 연구 목적 3 3. 용어 정의 3

Although many researches on Korean Morphological Analysis and Korean POS (Part-of-Speech) tagging have been performed since 1980s, it is not easy to find a Korean POS tagger for mobile devices.
We will introduce a Korean POS tagger that performs well on mobile devices such as cellular phones and portable media players. Mobile devices have some critical restrictions that a computing power is low and a working memory capacity is small. Therefore, it may be impossible to perform POS tagging on mobile devices by using traditional methods based on statistical information.
Our system performs morphological analysis by using a rule-based method (a modified left-longest-match-preference method), and performs POS tagging by using a statistical method (a Reduced Hidden Markov Model). In the experiments, our system requested smaller memory use and showed faster tagging speed than the system based a traditional Hidden Markov Model.형태소란 문장에서 의미를 갖는 최소 단위이다. 형태소 분석 및 품사 부착은 자연어 처리의 가장 기본적인 단계로 문장을 형태소 단위로 분리하는 작업이다. 한국어 형태소 분석 및 품사 부착에 대한 연구는 80년대부터 많이 이루어져 왔다. 규칙 기반 방법, 통계 기반 방법 등이 형태소 분석 및 품사 부착에 이용 되었다.
본 논문에서는 최근 활용도가 높아지고 있는 모바일 기기에 적합한 한국어 형태소 분석 및 품사 부착 방법을 제안한다. 모바일 기기는 계산 능력이 떨어지고 저장 공간과 사용 가능한 메모리가 부족하다는 특징을 갖는다. 따라서 모바일 기기에서 전통적인 방법을 사용하여 형태소 분석 및 품사 부착을 수행하기에는 한계가 있다. 본 논문에서는 기존의 규칙 기반 형태소 분석 방법인 좌최장일치법을 변형하여 형태소 분석을 수행 한다. 여기에 통계적인 방법인 Hidden Markov Model(HMM)을 축소해서 적용하여 형태소 품사 부착을 수행한다. 제안하는 시스템은 기존의 HMM을 사용한 시스템과 비교하여 적은 저장 공간과 메모리만을 사용하고 월등히 빠른 속도로 형태소 분석 및 품사 부착을 수행한다.

Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntatic features obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET.
Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.관계 분류(Relation Classification)는 문장에서 나타나는 개체(Entity) 쌍 사이의 의미적 관계를 분류하는 문제로써 자연어처리(Natural Language Processing; NLP) 분야에서 중요한 과제 중 하나이다.
합성곱 기반의 Convolutional Neural Networks(CNNs)과 순환적 구조를 기반으로 한 Recurrent Neural Networks(RNNs) 계열의 딥러닝 모델들이 좋은 성능을 보이지만, 이들은 모두 고정적인 단어의 표현 벡터를 사용하기 때문에 문맥에 따라 의미와 관계가 변하는 단어의 의미를 학습하기 어렵다. 그리고 주의집중 매커니즘(Attention Mechanisms) 기반의 State-of-the-art 모델들은 관계 분류에 있어서 가장 중요한 특징일 수 있는 개체에 대한 정보를 완전히 활용하지 못하고 있다.
이러한 문제를 해결하기 위해, 본 논문에서는 새로운 개체-인식 주의집중(Entity-aware Attention) 매커니즘과 잠재 개체 타이핑(Latent Entity Typing) 기법을 제안하고 이를 기반으로 한 End-to-End 순환 신경 모델을 소개한다. 모델은 자가-주의집중(Self-Attention) 매커니즘을 통해 문장에서 함께 나타난 단어 간의 의미적 관계를 고려한 표현 벡터를 생성하고, 개체와 그의 잠재 개체 타입(Latent Entity Type)을 자질로써 효과적으로 활용한다.
관계 분류 분야에서의 가장 유명한 벤치마크 데이터 중 하나인 SemEval-2010 Task 8을 이용하여 모델의 성능을 평가했으며, 기존 제안되었던 State-of-the-art 모델들을 능가하는 85.2%의 F1-score를 기록하였다. 또한, 모델에 적용된 자가-주의집중과 개체-인식 주의집중 매커니즘 그리고 잠재 개체 타입의 시각화를 통해 모델의 해석가능성(Interpretability)을 크게 향상시켰다.차 례 국문 요약 제1장 서 론 1

최근 인터넷이 폭 넓게 보급되어 온라인 상에서 얻을 수 있는 텍스트 정보의 양이 급증함에 따라 효율적인 정보 관리 및 검색이 요구되고 있다. 자동 문서 범주화란 문서의 내용에 기반하여 미리 정의되어 있는 범주에 문서를 자동으로 할당하는 작업으로서 효율적인 정보관리 및 검색을 가능하게 하는 동시에 방대한 양의 수작업을 감소시키도록 하는데 그 목적이 있다.
문서 분류를 위해서는 문서들을 가장 잘 표현할 수 있는 자질들을 정하고, 이러한 자질들을 통해 분류할 문서를 표현해야 한다. 기존의 연구들은 문장간의 구분 없이, 문서 전체에 나타난 각 자질의 빈도를 이용하여 문서를 표현 한다. 그러나 하나의 문서 내에서도 중요한 문장과 그렇지 못한 문장의 구분이 있으며, 이러한 문장 중요도의 차이는 각각의 문장에 나타나는 자질의 중요도에도 영향을 미친다.
본 논문에서는 문서 요약에서 사용되는 중요 문장 추출 기법을 문서 분류에 적용하여, 문서 내에 나타나는 각 문장들의 문장 중요도를 계산하고 문서의 내용을 잘 나타내는 문장들과 그렇지 못한 문장들을 구분하여 각 문장에서 출현하는 자질들의 가중치를 다르게 부여하여 문서를 표현한다. 이렇게 문장들의 중요도를 고려하여 문서를 표현한 기법의 성능을 평가하기 위해서 한국어 뉴스 그룹 데이터를 구축하였으며, 다양한 문서 범주화 모델을 통하여 실험하였다. 제안한 방법은 기존 시스템에 비해 좋은 성능을 얻을 수 있었다.With the growth of the Internet, available online text information has increased exponentially. Therefore, the need for an efficient data management and retrieval technique has become greater. Automatic text categorization is problem of automatically assigning predefined categories to free text documents, thus allowing for less manual labor required by traditional categorization methods.
In order to classify text documents, we have to select good features from them and index them with the features. In previous reserches, a text document is commonly represented by the frequency of each feature in the document. But there is a difference between important and unimportant sentences in a text document. It have an effect on the importance of features in a text document.
In this paper, we measure the importance of sentences in a text document using text summarizing techniques. A text document is represented by features with different weights according to the importance of each sentence. To verify the new method, we construct Korean news group data set which is collected from a news server. We experiment our method with various text classification model to the data set. The experimental results show that our proposed method achieves a better performance than other systems.목차 = ⅰ 표 및 그림 차례 = ⅳ Abstract = ⅴ 요약 = ⅵ 제1장 서론 = 1

만성 폐쇄성 폐 질환(COPD, Chronic Obstructive Pulmonary Disease)은 사망률 세계 4위이며, 한국 7위의 질병으로 노년층에서 많이 발병하는 호흡기 질환이다. 만성 폐쇄성 폐 질환은 천식과 비슷하게 호흡 곤란, 기침, 가래 등의 기도 질환 증상을 나타낸다. 만성 폐쇄성 폐 질환은 주로 폐기능을 악화시켜 사망에 이르게 하는 것으로 알려져있다. 이러한 만성 폐쇄성 폐 질환 이외에도 여러 질병들에 대한 관심이 높아지면서 건강 상태를 확인하는데 지표가 되는 바이오 마커에 대한 연구도 다양하게 진행되고 있다. 바이오 마커란 질병을 예측하거나 예방 치료를 할 수 있는 신체 표지자를 뜻한다. 특정 질병과 관련 있는 바이오 마커 찾는 가장 정확한 방법은 임상 실험을 이용하는 것이다. 그러나 특정 질병과 관련이 있을 것으로 추정되는 바이오 마커에 대하여 섣불리 임상연구를 진행 할 수 없다는 점을 고려할 때, 문서의 사전 정보에서 관련성을 입증하는 것은 아주 큰 의미를 가진다. 이처럼, 문서의 사전 정보로부터 단어 간의 관계를 파악하는 연구가 자연어처리 분야에서도 다양하게 이뤄지고 있다 임상실험에서 특정 질병에 대한 새로운 바이오 마커를 찾는 것은 많은 비용과 시간이 소요된다는 문제점이 있다. 본 연구에서는 이러한 문제점을 보완하기 위해서 텍스트를 이용하여 특정 질병과 관련이 있는 바이오마커를 찾고자 한다. 본 논문에서는 호흡기 질병으로 널리 알려져있는 만성 폐쇄성 폐 질환 (COPD, Chronic Obstructive Pulmonary Disease)과 연관 있는 바이오마커를 찾기 위하여 강원대학교 병원에서 추천 받은 바이오마커를 이용하여 COPD와 바이오마커의 유사도를 측정한다. 워드임베딩 모델 CCA(Canonical Correlation Analysis), Word2Vec, GloVe(Global Vector)를 사용하여 COPD와 바이오마커를 워드임베딩한다. 워드 임베딩한 K차원의 COPD와 바이오마커의 고차원 데이터를 t-SNE를 사용하여 2차원으로 축소하여 매핑하고 시각화한다. 그리고 코사인 유사도를 계산하여 유사도를 측정한다. 계산된 유사도 값을 이용하여 구글 스칼라 (Google Scholar) 검색을 통해 임상 평가를 대체하기 위해 에서 검색한 논문의 제목과 요약을 사용하여 점수화 하여 실제 연구가 이루어지고 있는지를 확인한다. 그리고 임상 전문가들의 바이오마커 순위와 워드임베딩 모델의 순위를 비교하였다. 만성 폐쇄성 폐 질환과 바이오 마커의 상관관계를 파악하기 위해 워드 임베딩 모델 Word2vec의 결과와 임상 전문가의 순위를 비교하여 서로 상관 관계가 높지만 아직 연구가 이루어지고 있지 않는 새로운 바이오 마커를 찾는다. 실험 결과 , Word2Vec과 GloVe가 100차원 일 때 최고의 성능을 나타 낸 것을 알 수 있다. Word2Vec에서 권장하는 CC-16과 GloVe에서 권장하는 Adiponectin 및 Leptin 은 이미 COPD에 대한 많은 연구를 이뤄지고 있음을 보여준다. 반면 Word2Vec의 Eotaxin-1, Cyfra21-1 및 GloVe의 CEA 및 SAA는 연구가 많이 진행되고 있지 않다. 이것은 새로운 임상 연구의 방향을 제시한다. 즉, 유사도가 높은 바이오 마커 중에서 구글 학술검색에서 검색 빈도가 낮은 바이오 마커는 향후 다양한 임상 연구의 대상이 될 수 있다. 추가적으로 특정 질병에 대한 새로운 마커를 찾는 데 도움이 될 수 있다. 실제로, 지금까지 많은 연구가 이뤄지고 있지 않은 Cyfra21-1이 만성 폐쇄성 폐 질환과 높은 연관성이 있음이 밝혀졌다. 실험 결과를 바탕으로, 현재 강원 대학교 병원은 보다 신뢰성 있는 임상 결과를 얻기 위해 실험을 수행하고 있다. 유사도 결과 값을 기준으로 만성 폐쇄성 폐 질환과 바이오 마커의 상관 관계를 분석 해보았을 때, 관계에 대한 분석이 잘 되었고, 이들이 서로 관련이 깊다는 것을 알 수 있었다.Chronic Obstructive Pulmonary Disease (COPD) is the fourth most common cause of mortality in the world. It is the 7th most common disease in Korea. Chronic obstructive pulmonary disease, like asthma, is a symptom of airway disorders such as dyspnea, cough, and sputum. Chronic obstructive pulmonary disease (COPD) is known to worsen pulmonary function leading to death. In addition to chronic obstructive pulmonary disease, there is a growing interest in various diseases and researches on biomarkers that are indicators of health status. Biomarkers are physical markers that can predict disease or prevent disease. The most accurate method of finding biomarkers related to a particular disease is to use clinical trials. However, it is of great significance to prove relevance in the preliminary in-formation of the document, considering that it is not possible to carry out a prospective clin-ical study on a biomarker presumably related to a specific disease. In this way, research on understanding the relationship between words from dictionary information of documents is being carried out variously in the field of natural language processing. In clinical trials, find-ing a new biomarker for a specific disease is costly and time consuming. In this study, we try to find biomarkers related to specific diseases using texts to overcome these problems. In this paper, we use the biomarker recommended by Kangwon National University Hospital to find the biomarkers related to COPD (Chronic Obstructive Pulmonary Disease), widely known as respiratory disease, and measure the similarity of COPD and biomarker. Word embedding model Canonical Correlation Analysis (CCA), Word2Vec, Glove (Global Vector) are used to word-embed COPD and biomarkers. High-dimensional data of CODD and biomarker of word-embedded K-dimension are reduced to two-dimensional map and visualized using t-SNE. Then, the degree of similarity is measured by calculating the cosine similarity. In order to substitute the clinical evaluation by Google Scholar search using the calculated similarity value, we use scoring and title of the thesis searched in to check whether the actual research is done. We compared the ranking of biomarkers and word embedding models of clinical experts. To ascertain the correlation between chronic obstructive pulmo-nary disease and biomarkers, we compare the results of the word embedding model Word2vec with that of clinical experts, and find new biomarkers that are highly correlated but have not been studied yet. Experimental results show that Word2Vec and GloVe perform best when they are 100 dimensions. The recommendations recommended by Word2Vec for CC-16 and GloVe suggest that adiponectin and leptin are already undergoing extensive re-search on COPD. In contrast, Eotaxin-1, Cyfra21-1 and GloVe's cea and saa of Word2Vec have not been studied much. This provides a direction for new clinical studies. In other words, among biomarkers with a high degree of similarity, biomarkers with low search fre-quency in Google's academic search can be subject to various clinical studies in the future. In addition, it can help find new markers for specific diseases. Indeed, cyfra 21-1, which has not been studied so far, has been found to be highly associated with chronic obstructive pulmonary disease. Based on the experimental results, Kangwon National University Hospital is conducting experiments to obtain more reliable clinical results. When we analyzed the correlation between chronic obstructive pulmonary disease and biomarker on the basis of the similarity result, the relationship was well analyzed and they were related to each other.
Keywords: Word Embedding, Biomarker, COPD, CCA, Word2vec, GloVe, Cosine Similarity1. 서 론 1 1.1 개 요1 1.2 관련 연구 2 2. 벡터 표현 학습 3 2.1 ONE-HOT ENCODING 3

Recently, the growth of number of patent application is unprecedented globally. Meanwhile the patent examination is still strongly dependent on manual works by few patent experts, which slows the overall patent registration process. Therefore, an automatic patent classification algorithm is necessary. In this paper, we propose an effective multi-label patent classification algorithm based on the GRU encoder and attention mechanism. We use the USPTO-2M data set, which consists of about 2 million US patent documents, to train our patent classification model. Precision, recall, and F score are used to evaluate our model on multi-label patent classification task. By visualizing the attention scores, we could identify and analyze keywords from each patent document which determine the context and IPC codes for subclass level.전 세계적으로 지적 재산권에 관한 특허 출원은 계속해서 증가하는 추세이다. 하지만 특허 심사는 여전히 소수의 전문적인 지식을 갖춘 심사관들에 의존하고 있기 때문에 특허청의 등록 승인을 받는데까지 긴 시간이 걸리고 있다. 따라서 방대한 양의 특허 정보를 기술적 분야에 따라 자동적으로 분류하는 방법에 대한 연구가 활발히 이루어져 왔다. 본 연구에서는 최근 컴퓨터 비전에 이어 자연어 처리에서도 널리 사용되고 있는 딥러닝 알고리즘을 통해 특허 문서의 다중 레이블 분류 문제에 접근하고자 한다. 구체적으로 GRU 기반의 문서 인코더와 어텐션 메커니즘을 활용하여 특허 문서의 국제특허분류(IPC) 코드를 예측하는 모델을 제안한다. 제안하는 모델의 학습과 평가를 위해 앞선 연구에서 사용한 특허 문서 데이터셋 USPTO-2M을 사용한다. 정밀도(Precision), 재현율(Recall), F 점수를 통해 평가한다. 또한 어텐션 메커니즘을 통해 특허 문서의 분류 결과에 대한 단어별 영향력을 분석하여 키워드를 탐색한다. 특히 특허 문서의 단어별 어텐션 스코어의 시각화를 통해 분류 결과에 대한 기여도를 단어 단위로 비교하고 비중이 높은 단어를 키워드로 선별할 수 있다. 이를 통해 향후 특허 분석이나 키워드 검색에서 활용할 수 있는 의의를 갖는다.초록 i 목차 iii 표 목차 iv 그림 목차 v 제 1장 서론 1

As a design method that reflects advanced science and technology, building an artificial intelligence based architecture design studio will prove the efficiency of new design design. It will have a useful effect on the creation of aesthetic buildings. The development of computer performance and the information communication network have developed dramatically. Artificial intelligence helps human beings in various fields by imitating human brain activity, learning by oneself, using accumulated data. With the development of the computer design program, the method of architectural design has been developed and expressed more aesthetically. The building reflects human history and social culture and is newly constructed according to the development of new materials and new technology.1. 서 론 1 1.1. 연구의 배경 1 1.2. 연구의 목적 2 1.3. 건축설계와 인공지능기반 설계흐름도 5

Coreference resolution searches noun phrases referring to the same concep or subject in a document. Since coreference resolution is a semantic analysis stage in natural language precessing, it can be used in various services based on semantic analysis like text sumerization or QA systems. “Mention” and “Entity” are elements of coreference resolution. “Mention” refers to all possible noun phrases could be corefered. “Entity” is considered as a set of the mentions refering to the same subject. Mention-pair and entity-mention are two typical methodologies of coreference resolution.
The paper restricted the domain of each stage of entity-centric rule-based coreference resolution method to improve its performance. The research conducted the comparitive experiment by setting the system without context restriction as the comparison system. As the evaluation menthod, MUC-6 is used. The experiment result showed the proposing system improved the performance by 4.4%p. The result shows that context restriction influenced the system positively, especially the anaphore resolution.본 연구는 한국어 문장에서 나타나는 동일한 개체를 가리키는 명사구들을 찾아내기 위한 상호참조해결 연구이다. 상호참조해결은 의미 분석 단계로 의미 분석에 기반을 둔 각종 자연어처리 서비스들에 의해 응용될 수 있다. 예들 들면 문서 요약이나 질의응답 시스템과 같은 서비스들이다. 상호참조해결에서 멘션은 상호참조의 가능성이 있는 명사구를 지칭하는 용어이고 엔티티는 하나의 대상을 가리키는 명사구들을 하나로 묶어 둔 집합이다. 상호참조해결에는 멘션 페어 방식과 엔티티 멘션 방식이 존재한다.
본 논문은 한국어 상호참조해결의 성능을 높이기 위해 엔티티 멘션 방식의 다단계 시스템에서 단계별 상호참조해결 범위를 제한하였다. 논문은 문맥제한을 두지 않은 시스템, 즉 기존 이론을 바탕으로 구현한 시스템을 비교 시스템으로 하여 대조실험을 진행하였다. 평가척도는 상호참조해결이나 대명사 참조해결에서 많이 사용되는 MUC 방식을 사용하였다. 실험 결과 논문에서 제안한 시스템이 4.4%p의 성능제고를 가져왔다. 이는 문맥제한을 두는 방식이 시스템의 성능 향상에 도움이 되었다는 것을 보여준다.Ⅰ. 서론 1 1. 연구배경 1 2. 연구 목적과 방법 2 3. 논문의 구성 2

정보통신기술(ICT)의 비약적인 발전을 기반으로 모든 산업이 4차 산업혁명의 패러다임으로 변화하기 위한 준비를 하고 있다. 이 중 인공지능 기술이 차세대 핵심 성장 동력으로 주목받고 있다. 과거에 비해 제반환경이 뒷받침되면서 급진적인 발전이 이루어지고 있다. 또한 인공지능 기술은 전 산업에 적용될 수 있으며, 이를 통한 산업구조의 변화와 다양한 비즈니스 모델의 창출을 기대하고 있다. 그러나 국내의 인공지능 연구개발 실정은 선진국 대비 75%에 불과하는 수준으로 분석되었으며 이에 인공지능 기술에 대하여 모니터링하여 유망기술을 탐색하는 것이 필요하다. 또한, 인공지능과 관련하여 특허 등을 통한 학술적 연구가 부족한 상황이다.
이에 본 연구는 전 세계 기술관련 특허가 집약되어있는 미국의 특허청에서 인공지능 분야의 특허를 수집하여 연구하였다. 인공지능 기술로 분류되는 U.S Class 706에 등록된 모든 특허를 대상으로, 각 특허의 초록을 활용하여 텍스트마이닝으로 분석하였다. 토픽모델링을 통해 토픽을 추출하여 세부 기술로 정의하였으며, 추출한 토픽들을 회귀분석을 통해 유망/성숙기술을 판단하였다. 또한, 도출된 유망기술에 대하여 Bass 모형을 적용시켜 기술수명주기 측면에서 개별 기술에 대하여 연구하였다.
토픽모델링을 통해 분석한 결과, 15개의 토픽을 추출하였으며 14개의 주제는 인공지능의 기술로 나타났으며, 1개의 주제만 인공지능 적용 분야로 나타났다. 유망/성숙기술을 알아보기 위해 회귀분석을 실시한 결과, 6가지의 유망기술과 5가지의 성숙기술로 분석되었다. 유망 기술로는 자연어 처리, 지식 표현, 예측 모델, 진단 시스템, 모니터링 시스템, 의료 분야에서 인공지능 기술로 나타났으며, 성숙기술은 패턴 인식, 전문가 시스템, 제어 시스템, 최적화, VLSI 기술로 나타났다. 6가지 유망기술에 대해 Bass 모형을 적용한 결과, 앞으로 발전 가능성이 가장 클 것으로 주목되는 기술은 예측 모델이며, 2017년을 기준으로 곧 한계에 다다르는 기술은 의료 산업에 적용된 인공지능 기술로 나타났다.
본 연구는 유망기술을 도출하기 위해 특허 데이터를 활용한 정량적인 분석을 실시하여 객관성 있는 결과를 제시하였다. 또한, Bass 모형을 적용하여 기술성장주기 측면에서 개별 기술의 발전가능성에 대하여 제시하였다.All industry is undergoing shift of 4th industrial revolution through the rapid development of Information and Communication Technology(ICT). Among them, Artificial Intelligence(AI) receive a lot of attention as a next growth engine. The surrounding environment is backed up and the development is more advanced than the past. In addition, AI can be applied to all industry. which is expected to change the industrial structure. AI can create a variety of business models. However, domestic AI R&D situation is only 75% compared with advanced countries. Therefore, we need to monitor AI and explore promising technologies. There is also a lack of academic research using patents of AI.
This study collected and analyzed patents of AI in USPTO, that has a lot of technology patents all over the world. AI is classified as U.S. Class 706. We anlayzed text-mining using the abstracts of each patent. Topics were extracted through topic modeling and defined themes. And we determined the promising/mature technology by regression analysis of topics. In addition, we have studied technology life cycle using the Bass model for promising technology.
As a result of analyzing through topic modeling, we found 15 topics. 14 topics were AI technologies, and only one AI were application field. We have also analyzed the promising technologies through regression analysis. The results showed six promising technologies and five mature technologies. Next, the result of the Bass model, the most promising technology is the predictive model, and the most mature technology is the medical in AI.
In this study, we have studied quantitative analysis in studying promising technology and get the objective results. In addition, we have presented the possibility of technology development through the Bass Model.Ⅰ. 서 론 1 1.1. 연구의 배경 1 1.2. 연구의 필요성 및 목적 4 1.3. 연구의 구성 5

2000년대 이후 스마트 폰, 컴퓨터의 대중화와 모바일 인터넷, 소셜 미디어 이용 등의 확산으로 대중이 텍스트의 수용자가 아닌 생산자로 대두되면서 대중이 만들어 낸 대량의 텍스트가 빠른 속도로 증가하고, 분석 기술의 발달로 디지털화된 텍스트에 대한 분석이 가능해 지면서 감성 분석에 대한 연구와 이를 실무에 적용하려는 시도가 활발하다.
감성 분석(sentiment analysis) 또는 오피니언 마이닝(opinion mining)이란 텍스트에 나타난 주관적 요소(subjectivity)를 탐지하여, 감성을 표현하는 이의 평가(evaluation), 판단(judgement), 감정(emotion), 감성(sentiment), 태도(attitude), 입장(stance) 등을 처리하는 텍스트 분석 기법 중 하나이다. 감성 분석의 주 목적은 단순히 감정이나 태도를 긍정 또는 부정, 이분법적으로 구분한다기보다는 인간의 감성 및 사람들의 의사를 결정하는 요인을 파악하고, 이를 정량화된 수치나 도식, 등급 등으로 표현하는 데 있다. 감성 분석의 단위는 크게 문서, 문장, 속성으로 구분되는데, 속성 단위 감성 분석은 문서나 문장과 달리 ‘의견의 대상(opinion target)이 무엇인지’, 그 대상의 ‘어떤 측면(feature or aspect)을 좋고, 싫어하는지’에 대한 세부적인 정보를 추출할 수 있다. 반면, 분석 절차가 복잡하고 보다 정교한 분석 방법을 요구한다.
본 연구에서는 속성 단위 감성 분석을 수행했으며, 이를 위해 최근 자연어 처리(natural language processing)에서 우수한 성과를 보이고 있는 워드 임베딩(word embedding)에 기반한 딥 러닝 (deep learning neural networks) 기법을 적용했다. 감성 분석에도 워드 임베딩과 결합한 딥 러닝이 우수한 성과를 보인다는 연구 결과들이 축적되면서 최근 다양한 형태의 변형된 딥 러닝 모델들이 제시되고 있지만, 주로 영어 텍스트를 분석 대상으로 하고 있다. 신경망 모델의 강점이 비교적 데이터의 특징에 자유롭다는 점이지만 ‘언어’라는 텍스트 데이터의 특수성을 고려할 때 이러한 모델들이 한국어에도 유사한 성과를 보일지는 실증 분석을 통해서만 검증 가능하다.
선행 연구들과 본 연구의 가장 큰 차이점은 기존 워드 임베딩이 가진 한계점을 완화하는 방안으로, 한국어의 특성을 고려한 감성 어휘 임베딩(sentiment lexicon embedding) 기법을 제안했다는 것이다. 본 연구에서 제안한 감성 어휘 임베딩이 감성 분석에 효과적인지 확인하기 위해 감성 어휘 임베딩을 통해 추출된 감성 어휘 벡터(sentiment lexicon vector)를 Convolutional Neural Network(CNN)와 Long Short-Term Memory(LSTM) 모델의 입력 계층에 사용하여 모델을 학습시켰다. 그 결과, 기존 워드 임베딩에 비해 감성 어휘 임베딩을 사용할 경우 모델의 성과 지표가 모든 경우에서 개선된 것으로 나타나 본 연구에서 제안한 감성 어휘 임베딩이 감성 분석에 효과적인 단어 표현 (word representation) 방법이 될 수 있음을 확인할 수 있었다.
한국어는 다른 언어에 비해 조사와 어미가 많이 발달한 언어라는 점, 어순의 변화에 자유롭고, 동음이의어 비중이 높다는 등 영어와는 매우 상이한 특징을 갖는다. 따라서 영어 텍스트에 대해서는 성과가 입증된 분석 방법이라 하더라도 한국어 텍스트의 특징을 반영하는 것이 필요하다. 본 연구에서는 기존 워드 임베딩이 가진 한계점을 완화하고, 감성 분석의 성능을 높이기 위해 한국어 텍스트의 특징을 반영한 감성 어휘 임베딩 방법을 제안하였다. 감성 어휘 임베딩이 단어 특성을 효과적으로 표현하는 기법이 될 수 있는지 다양한 실증 분석을 통해 확인하였다. 본 연구는 감성 분석을 중점으로 감성 어휘 임베딩의 효과를 살펴보았지만, 다른 자연어 처리 분야에 대해 적용하는 것도 흥미로운 연구 주제가 될 수 있을 것으로 기대한다.As the user-generated content (UGC) has proliferated and been regarded as invaluable information sources for most organizations, there has been a great deal of interest in natural language processing (NLP) and text-mining techniques for accurately extracting information from text (Chen et al., 2017; Van de Kauter et al., 2015). In particular, researchers have made impressive progress in a sentiment analysis of subjective texts, including online product reviews or social media (Schumaker et al., 2017; Ghiassi et al., 2013; Ghiassi and Lee, 2018).
Sentiment analysis, also called opinion mining, uses computational methods to analyze sentiments, opinions, attitudes, and appraisals toward topics or aspects expressed in natural language texts (Pang et al., 2002; Pang and Lee, 2008; Wang et al., 2015). In the past decades, sentiment analysis takes traditional classification models such as Naive Bayes (NB) or support vector machines (SVMs) with bag of words features (Ouyang et al, 2015). These machine learning approaches targeting NLP problems have been based on shallow models using very high dimensional and sparse features. However, feature engineering is labor intensive and almost reaches its performance bottleneck. Therefore, it is necessary to find explanatory factors from the data and build a classifier less dependent on feature engineering (Bengio et al, 2003).
In this context, word vectors obtained from embedding learning techniques, such as Word2vec (Mikolov et al. 2013a; 2013b) or GloVe (Pennington et al., 2014), are used as inputs or extra word features to deep neural net models. Word vector representation has been proven powerful in various NLP tasks because it can capture the semantic and syntactic relationship between words (Tang et al., 2016b). Meanwhile, recent advances in various word representation models have often focused on widely-used languages, such as English. It raises the question of whether these methods will be equally effective when applied to other languages with different linguistic characteristics such as, in particular, morphologically rich languages (MRLs), such as Korean and Turkish (Amram et al., 2018; Berardi et al., 2015; Park et al., 2018b; Tsarfaty et al., 2010). Word vector representation that takes into account the distinct characteristics of individual languages still remain challenging in regards to generalized representations.
Although the effectiveness of word embedding has been verified in recent studies, traditional embedding learning models have some limitations. Existing unsupervised embedding learning approaches are based on the distributional hypothesis (Harris, 1954), which exposes that the words that occur in similar contexts tend to have similar meanings. For this reason, semantically opposite, but syntactically similar words (e.g., good and bad) have similar word vectors because these words commonly share a small subset of similar surrounding words. Sentiment analysis targets at identifying and classifying sentiment/opinion of text (Tang et al., 2016b); hence it is more problematic when the word embedding is used for sentiment analysis than other NLP applications (Tang et al., 2014).
In this regard, this paper proposes a method of sentiment lexicon embedding that better represents sentiment word’s semantic relationships than existing word embedding techniques. We obtained word vectors through Word2vec model, but input and output word formats are revised by jointly encoding morphemes and their corresponding part of speech (POS) tags. And then, only important POS’s morphemes are learned in Word2vec model.
To verify the effectiveness of the proposed sentiment lexicon embedding method, we conducted experiments comparing with baseline models, which only used general-word embedding or concatenated general-word and aspect embedding. Experiment results indicate that sentiment lexicon vectors obtained by the proposed sentiment lexicon embedding can strengthen attributional similarities compared to the current word embedding method, and these attributional similarities can be more qualitative features of words for sentiment analysis task. In addition, the revised embedding approach mitigated the problem of conventional context-based word embedding method and, in turn, improved the performance of aspect detection and sentiment classification. Furthermore, the sentiment polarity of reviews is highly included in the sentiment-bearing words with respect to specific aspects. Therefore, it is worthwhile to model the connection between aspect and sentiment words for aspect-based sentiment analysis and this effect was enhanced as the features of sentiment words are better represented.I. Introduction 1 A. Research background 1 B. Research objective 5 C. Organization of the dissertation 8 II. Review of Related Research 10

현대 컴퓨터의 기술 수준은 전통적인 텍스트 데이타나 알파뉴메릭 데이타 뿐만 아니라 이미지, 사운드 등의 멀티미디어 데이타를 포착하고, 저장하며 표현하는 데 까지 이르렀다. 이와 함께 사회 제 분야에서는 이러한 멀티미디어 데이타를 각 분야에서 이용하려는 현상이 자연스럽게 일고 있다. 이것은 당연한 현상으로서 텍스트 데이타 만을 처리하는 것 보다는 눈으로 보고 귀로 들으면서 업무 처리를 한다면 훨씬 더 능률적일 것이다. 그러나, 멀티미디어 데이타는 기존의 정형적인 텍스트 또는 알파뉴메릭 데이타와는 달리 매우 비정형적이고, 가변적이며 길이가 긴 특성을 가지고 있기 때문에 이들 데이타를 표현하고, 조작하며 검색하는 데 있어 많은 어려움이 따른다. 산업계와 학계에서는 이러한 멀티미디어 데이타를 표준적으로 다루기 위한 많은 연구를 수행하였으며, 현재도 진행 중에 있다.
본 논문에서는 멀티미디어 데이타를 표준적으로 다루기 위해 객체 지향 모델을 이용해 텍스트, 이미지, 그래픽 및 사운드 미디어 객체를 포함하는 멀티미디어 데이타 모델을 설계하였으며, 본 모델에서의 다중 키워드를 이용한 검색 기법을 제안하였다.
본 연구에서 수행한 다중 키워드에 의한 멀티미디어 데이타 검색 기법은 기존의 자연어 처리 검색 기법이 가지는 수행시의 오버로드(overload)를 줄이는 데 목적을 두었다. 이를 위해 적절한 기억 장치 구조를 제안하고, 미디어 객체가 저장될 때의 키워드 등록 알고리즘과 검색 알고리즘을 제시한 후, 이들 알고리즘의 검증을 위한 시뮬레이션 프로그램을 C++ 언어로 작성하였다.In recent computer technology, it is possible to capture, store, and present the multimedia data such as image, sound besides the conventional text or alphanumeric data. According to this, the current situation in various social areas trends to user the multimedia data. As that is the natural phenomenon, it will be better productive to user the multimedia data rather than to use the text data. However, it is hard to present, manipulate and retrieve such a data because the multimedia data is unformatted, variable, and long data field. Many industrial and academic researches to manage the multimedia data standard have been implemented and is implementing now.
In this thesis, multimedia data model using object-oriented data model is designed and presents the retrieval scheme using multiple keywords. The text, image, graphic, and sound media object are included in the designed model. The multimedia data retrieval scheme using multiple keywords has proposed in order to reduce the overload occured when the natural language approach is implemented. Therefore keywords registration and retieval algorithms are presented. And finally, in terms of the proper storage structure, the simulation program for these algorithm is described with C++ language.국문요약 = ⅱ ABSTRACT = ⅲ 차례 = ⅳ 제1장 서론 = 1 제2장 객체 지향 데이타 모델 = 16

현 시기에 대두되고 있는 제4차 산업혁명은 지난 2016년 세계 경제 포럼(WEF)에서 언급되었고 새로운 산업시대를 대표하게 되었다. 첨단 정보통신기술이 경제, 사회적으로 적용되어 혁신적인 변화가 나타나고 있고 기존 산업혁명에 비해 더욱 빠른 속도로 영향을 미치고 있다. 산업혁명은 새로운 기술의 등장으로 산업 생태계를 만들고 그것이 인간의 삶의 방식을 변화시킬 때 사용되는 단어이다. 정치, 사회 문화의 분야에서도 획기적인 변화를 나타낸다. 그런 의미에서 제4차 산업혁명은 우리의 삶을 지금보다 나은 삶으로 바꾸어 준다. 인공지능의 발전으로 딥러닝을 이용해 빅테이터를 정교하게 분석할 수 있는 기법도 함께 발전하였다. 인공지능, 빅데이터 기술의 활용 여부에 따라 미래에 더욱 발전할 수 있는 발판이 될 것으로 보인다. Google, Microsoft, IBM, AWS는 각자의 Cloud platform을 제공함으로 데이터 과학자들이 높은 수준에서 사용할 수 있는 딥러닝 framework가 점차 늘어나는 추세이다. 딥러닝은 컴퓨터가 사람과 같이 스스로 학습할 수 있는 인공지능 기술을 의미한다. 최근에 딥러닝은 음성인식, 자연어처리, 이미지 분석 등에서 획기적인 발전을 하여 급성장 하고 있고 빅데이터를 분석해 차이점을 가려내고 유사한 것들을 분류하는 데 탁월한 효과를 보이고 있다.
인공지능의 한 분야인 인공신경망은 생물학의 뇌 구조를 모방한 것인데 주어진 환경에 대한 학습 능력을 가지고 있다. 딥러닝을 이용하여 컴퓨터가 수 천만장의 이미지를 학습하여 객체를 인식하게 하는 데는 다수의 고성능 컴퓨터가 필요하다. 따라서 딥러닝에는 다수의 컴퓨터를 효율적으로 이용하기 위한 분산처리 기술이 필요하며 관련 연구가 활발히 진행되고 있다. 이미지를 데이터화 시켜서 컴퓨터에 인식 시킨 다음 특징을 추출하여 학습을 시키게 된다. 이미지 데이터에는 다수의 데이터 정보가 있다. 그 데이터 정보는 수치로 표현이 가능하다. 수치에 대해서 특징을 추출하는 다수의 계층을 갖춘 신경망을 통해 컴퓨터가 학습할 수 있는 Feature map으로 정보를 체계화하여 스스로 학습패턴을 찾아 낼 수 있게 한다.
본 논문은 H-CNN 알고리즘을 이용하여 이미지 데이터를 학습시키고 정확도 및 학습속도를 비교 분석한다. H-CNN 알고리즘은 이미지 분석으로 많이 알려진 CNN 알고리즘에 Hippocampal 알고리즘의 개념을 추가하였다. CNN 알고리즘에서 Convolution연산과 Max Pooling기법을 통해 추출된 최대값 Feature map에서 Hippocampal 알고리즘을 통해 장기기억과 단기기억의 형태로 나누어 Simple Feature map을 생성하고 학습시킨다. Simple Feature map은 특징이 단순화 되어있지만 필요한 특징들만의 집합이기 때문에 정확도에 영향을 주지 않으며 학습속도를 향상시킬 수 있다. 연구에는 Python언어를 사용하였고 Tensorflow라이브러리를 이용하여 해당 프로그램을 구현하였다. 연구에서 사용하는 데이터는 MNIST, Cifar10, 임상 영상 데이터이다. 다양한 이미지를 학습시켜 정확도를 측정하고 학습속도를 비교분석 하여 성능 향상을 확인 할 수 있다.Ⅰ. 서론 1 Ⅱ. 연구 배경 4 1. 제4차 산업혁명 4 1.1 산업혁명의 역사 4

본 연구의 목적은 혈액순환계의 구성요소들 사이의 원인관계(causal relationship) 와 일정한 혈압을 유지하기 위한 복잡한 순환계 메카니즘을 지능적으로 교육하는 컴 퓨터보조교육 시스템개발에 있다. 지능적 컴퓨터보조교육(ICAI : Intelligent comput er Aided Instruction)시스템은 효과적인 교육을 위해 다음에 무엇을 어떻게 교육할 것인가를 결정할 수 있는 기능이 있어야 한다. 본 시스템에서는 lesson planner와 di scourse planner에 있어서 전자에서는 무엇을 가르칠 것인가를 결정하고 후자에서는 결정된 교육단위 (Instructional unit)를 어떻게 교육할 것인가를 결정한다. 학생들에 게 전달하려는 모든 지식은 Semantic Network에 표현되어 있고 학생들의 학습습득지식 을 표현하는 학생모델은 Semantic Net 지식베이스 위에 표현된다. 학생모델은 학습과정 중 계속 수정보완되며 lesson planner와 discourse planner는 이 학생모델을 근거로 학습plan을 작성수행한다. 즉 학생수준에 관계없이 획일적으로 교육을 진행하는 것이 아니고 학생모델을 추론하여 Rule로 표현된 plan에 관한 지식을 Rule Interpreter에 의해 학생에 가장 적합한 교육내용과 방법을 선택수행케한다. 아울러 컴퓨터 지원교 육시스템의 필수적인 기능중의 하나는 자연어 처리이기 때문에 본 연구에서는 Lexical Functional Grammar에 의한 자연어 이해 모듈을 구현하여 학생의 간단한 질문을 이해 하고 답을 할 수 있는 기능을 부여하였다.

상품추천 시스템은 인터넷과 정보통신기술의 발전으로 새롭게 등장한 전자상거래 개인화 서비스 시스템이다. 상품추천 시스템은 개별 고객의 선호도와 취향에 적합한 상품이나 컨텐츠를 자동으로 추천하는 시스템이다. 상품추천 시스템은 상품이나 서비스에 관한 고객의 선택이 필요한 거의 모든 분야에 폭넓게 적용되고 있으며 그 결과도 매우 성공적이다.
상품추천 시스템의 핵심인 상품추천 알고리즘의 종류는 매우 다양하다. 가장 널리 사용되는 상품추천 알고리즘은 협업 필터링 알고리즘이다. 상품의 종류가 많아지고 고객 수가 커질수록 동일한 상품을 구매한 고객의 수가 상대적으로 줄어들게 된다. 과거 구매 이력에 전적으로 의존하는 이웃기반 협업 필터링 알고리즘은 선호도 추정 정확도가 떨어지게 되는데, 이를 데이터 희소성 문제라고 한다.
데이터 희소성 문제를 해결하기 위해 제시된 알고리즘이 모델기반 협업 필터링 알고리즘이다. 모델기반 협업 필터링 알고리즘은 고객-상품 간의 선호도 정보를 직접 추천에 사용하지 않고, 선호도 추정 모델을 트레이닝시키는 데 사용한다. 기본적인 아이디어는 고객-상품 간의 상호작용을 그들 간의 잠재적인 특성을 나타내는 요인으로 모델링하는 것이다. 모델링 기반 협업 필터링 알고리즘에는 행렬분해(MF) 모델, 특잇값 분해(SVD) 모델, Slope One 모델, Co-Clustering 모델 등이 있다.
벡터공간모델은 단어를 벡터화하여 다차원의 벡터공간에 표현하고 의미론적으로 비슷한 단어들이 서로 가까운 곳에 있도록 하는 방법이다. 벡터공간모델은 주로 자연어 처리 분야에 활용되었으나, 최근에는 상품추천영역에도 활용성이 높아지고 있다.
본 논문에서는 벡터공간모델을 활용한 상품추천 알고리즘을 제안하였고 실증분석을 통해 추정 정확성을 입증하였다. 특히 현존하는 가장 우수한 상품추천 알고리즘으로 알려진 SVD++와 비교하여 비슷하거나 약간 더 우수한 성능을 보였음을 확인한 것은 매우 의미가 크다고 하겠다.
SVD++는 백만불의 상금을 걸고 진행되었던 Netflix Prize라는 영화 평점 예측 경진대회를 통해 제안된 알고리즘이다. SVD++알고리즘은 모델링에 많은 시간이 소요되는데 이점은 가장 큰 단점으로 지적되고 있다. 반면에 벡터공간모델을 활용한 상품추천 알고리즘의 모델링 시간은 상대적으로 매우 적게 걸린다. 그러므로 고객의 구매 빈도가 잦고 상품의 추가 및 변경이 잦은 전자상거래 부문의 추천 모델개발에 벡터공간모델을 활용한 상품추천 알고리즘의 유용성이 더욱 커질 것으로 기대된다.The recommendation system, newly-emerging from the development of Internet and information communication technology, is an e-commerce personalization service system. It automatically recommends goods or contents suitable for individual customers' preferences. These choices and results have been very successful.
Product recommendation algorithms, which are the core of product recommendation systems, vary widely. The most commonly used product recommendation algorithm is a collaborative filtering algorithm. As the number of products and cutomers increases, the number of customers who purchase the same products becomes relatively small. Neighborhood-based collaborative filtering algorithms, which rely solely on past purchasing records, are vulnerable to data sparsity problems, thereby resulting in estimation accuracy problems.
The proposed algorithm to solve the data sparsity problem is a model based collaborative filtering algorithm. Model-based collaborative filtering algorithms are used to learn preference estimation models without using product preference information for direct recommendation. The basic idea is to model the potential characteristics that exist between the customer and the product. Modeling-based collaborative filtering algorithms include Matrix Factorization, Singular Value Decomposition, Slope One, and Co-Clustering model.
The vector space model is a method of vectorizing words and locating semantically similar words close to each other in a multidimensional vector space. It is mainly used in natural language processing area. It has recently increased usability in the recommendation area. This doctoral dissertation suggests a product recommendation algorithm using vector space model, and proves its superiority through empirical analysis based on experimental data. It is especially very meaningful to confirm that the proposed algorithm shows similar or slightly better performance than SVD++, which is the most recommended algorithm.
SVD++ is a proposed algorithm through a recommendation algorithm competition called Netflix Prize with a prize worth 1 million dollars. One disadvantage of the SVD++ algorithm is that modeling takes a long time. The modeling time of vector space model based recommendation algorithm is relatively faster than SVD++. It seems a big advantage that modeling time of vector space model based product recommendation algorithm takes elatively less time compared to modeling of the SVD++ algorithm. The vector space model based product recommendation algorithm is expected to be useful for developing a recommendation model for e-commerce, where customers frequently purchase, add, or change, products.국문초록 vii 영문초록 ix 제 1 장 서론 1 1.1 연구 배경 1

본 논문에서는 문자, 화상, 음성 등의 멀티미디어중에서 가장 중요한 컬러 정지 화상의 자유로운 처리를 위하여 JPEG 압축 표준안을 이용한 압축과 복원에 대해 기술하며, 다수의 화상을 신속하고 다양한 방법으로 검색하기 위하여 키워드 방식과 직접 조작 사용자 인터페이스에 관하여 기술한다.
화상 저장 시스템은 스캐너를 통해 화상 자료를 입력하는 도구와 화상의 압축, 복원을 처리하는 JPEG 도구 및 컬러와 그레이 출력을 위한 포스트 스크립트 프린팅 도구, 멀티미디어 처리를 위한 오디오 입력재생 도구, 화상의 키워드 검색과 관리도구들로 연구, 구현하였다.
또한, 입력되고 압축된 컬러 정지 화상의 자유롭고 신속한 검색을 위한 화상의 관리 구조와 직접 조작 형식을 제안하였다.
구현 환경으로는 워크스테이션 상에서 시분할 운영 체제인 UNIX를 운영체제로 X 윈도우 시스템과 그래피컬 사용자 인터페이스인 Motif를 사용하였다.This paper describes the design and implementation of the retrieval tool which allow to retrieve and represent color-still images. To retrieve color-still image database, keyword method have been proposed for effective retrieval easily.
As an user interface, a direct manupulation method such as selecting icons of images has been implemented.
To minimize dataspace, JPEG as international standard of color-still image compression method be accepted and implemented. Also this retrieval tool included variety of function which has Postscript output, Audio input function.
This study has been carried out in UNIX environment which include SUN SPARC II workstation, X window system and OSF/Motif as user interface.차례 = iii 국문 요약 영문 요약 제1장 서론 = 1 제2장 화상저장시스템의 개념 = 4

이 논문에서는 1997-2000년 사이의 INSPEC 데이터베이스의 논문 4,555개를 대상으로 동시출현 단어 분석(co-word analysis) 실험을 하여 인공지능(artificial intelligence) 분야의 지식 구조를 나타내었다.
본 연구의 목적은 통제된 색인어인 주제명표목(subject heading)을 이용한 지식 구조와 통제되지 않은 키워드(key phrase identifier)를 이용한 지식 구조를 비교하여 둘의 구조가 어떤 차이점을 보이는지를 살펴보고, 또한 색인효과(index effect)가 어떻게 나타나는지, 비통제어를 사용한 경우가 실제적으로 더 상세한 영역을 표현하는지를 살펴보는 것이다. 또한 이 논문에서는 결과 분석을 용이하게 하기 위해 클러스터링을 먼저 한 후 클러스터들을 다차원축척 지도에 표현하는 접근법을 사용하였는데, 이 방법이 직접적인 다차원축척 영역지도와 어느 정도 일치하는지 검증하는 실험을 하였고, 추가적으로 코사인계수 이외의 다른 유사계수를 사용하였을 경우, 영역지도가 어떻게 달라지는지를 살펴보기 위해 피어슨 상관계수(Pearson correlation coefficient)를 사용한 영역지도와 비교하는 실험을 하였다.
본 연구에서 수행한 실험의 결과들을 다음과 같다.
첫째, 인공지능 분야는 크게 학습, 제어, 에이전트 및 로봇, 퍼지모델, 지식공학, 응용분야 등으로 하위 영역을 분석할 수 있었다. 이들 중 학습과 관련된 분야가 40% 이상을 차지하였다. INSPEC 데이터베이스의 인공지능 분야와 관련된 분류코드에는
제어와 로봇 분야가 포함되어 있지 않지만, 이들 분야도 상당한 비중을 차지하는 것으로 나타났다.
둘째, 주제명표목을 사용한 경우에 색인효과가 나타나는 부분들이 있었다. 즉, 퍼지모델의 경우 키워드에서는 제어와 관련되어 나타나는 반면 주제명표목에서는 지식기반 시스템과 관련되어 나타남을 살펴볼 수 있었다.
셋째, 키워드를 사용한 영역지도가 더 상세한 영역을 형성하는 것을 학습, 에이전트, 제어 등의 분야를 통해 알 수 있었다. 하지만 주제명표목을 사용한 경우가 항상 일반적인 영역을 형성하는 것은 아니다. 자연어처리나 컴퓨터비전 같은 분야는 주제명표목에서 더 상세한 영역을 형성하였다.
넷째, 클러스터링을 이용한 영역지도와 단어들을 직접적으로 다차원축척 지도에 표현한 영역지도의 패턴은 비슷하게 나타난다.
다섯째, 코사인계수를 사용한 영역지도와 피어슨 상관계수를 사용한 영역지도는 둘 다 비슷한 인접 영역들을 보여주고 있지만, 피어슨 상관계수를 사용한 경우에 크기가 큰 클러스터가 분리되서 나타나는 경향이 있다.Co-word analysis is a method exploring and mapping intellectual structure of subject field through analyzing co-occurrence patterns of terms in document set. Many researchers used co-word analysis to reveal patterns, trends, and changes of intellectual structure in various subject fields.
This study applied co-word analysis to 4,555 articles from INSPEC database during the period of 1997-2000 to explore the intellectual structure of Artificial Intelligence(AI) field.
The purpose of this study is to explore similarities and differences between two types of intellectual structures, one of which uses subject heading as controlled terms and the other uses key phrase identifier as uncontrolled terms. Through this comparison, this study shows if there exist index effects. And then it also shows if intellectual structure is actually more specific using uncontrolled terms instead of controlled terms.
For this purpose, this study extracts each of subject heading and key phrase identifier from selected documents, counts co-occurrences of terms, calculates cosine coefficient values to measure similarity between term pairs, clusters terms by using the Ward’s method, and maps clusters to two dimensional map by using multidimensional scaling(MDS).
This study used co-word mapping method by combining clustering and MDS techniques(cluster-based MDS) which is supposed more comprehensible than co-word mapping method by displaying terms directly to MDS map(direct-MDS). However it is needed to examine coherence between the cluster-based MDS map and the direct-MDS map. So there is an experiment of comparison of the cluster-based MDS map with the direct-MDS map. Additionally the MDS map using cosine coefficient is compared with the MDS map using Pearson correlation coefficient in order to see similarity coefficients affects on intellectual structure.
The results of this study are as follows:
First, major areas of AI field include ‘learning’, ‘control’, ‘agent and robot’, ‘fuzzy model’, ‘knowledge engineering’, ‘applications’. Among these areas, ‘learning’ area holds more than 40 percents.
Second, there existed a little index effect in the result of experiment using subject heading. That is, although ‘fuzzy model’ displays close relationship to ‘control’ in experiment using key phrase identifier, it displays close relationship to ‘knowledge engineering’ in experiment using subject heading.
Third, this study has compared the intellectual structure using subject heading with the intellectual structure using key phrase identifier. As the result of it, both of the maps display similarity in major areas in AI field. Though, the intellectual structure is actually more specific and more topical using key phrase identifier instead of using subject heading in the case of ‘learning’ ‘agent’, ‘control’. However the intellectual structure using subject heading doesn’t always form general areas. In the case of ‘natural language processing’ and ‘computer vision’, the intellectual structure using subject heading forms more specific areas.
Fourth, the cluster-based MDS map displays very similar patterns to the direct-MDS map in major themes and subject fields. That means the cluster-based MDS has considerable coherence in intellectual structure analysis.
Finally, the MDS map using Pearson correlation coefficient and the MDS map using cosine coefficient displays very similar patterns in major themes and subject fields. However larger size cluster appeared apart from other clusters in the MDS maps using Pearson correlation coefficient.차례 = i 그림 차례 = iii 표 차례 = iv 국문 요약 = v 1 서론 = 1

앞으로의 4차 산업혁명은 산업 전분야로 확산되고, 특히 자율적 · 지능형 소프트웨어가 필수이게 될 것이다. 이들에 대한 소프트웨어 강건성/신뢰성/안전성에 대한 보장이 이슈로 예상된다. 즉, 4차 산업혁명 분야 소프트웨어는 기존의 품질 보장과 다르게 소프트웨어 생명주기 전반적으로 다른 품질보장과 기준이 필요할 수 있다. 현재 국내 중소/중견 소프트웨어 기업은 대기업에 비해 소프트웨어 공학 수준과 품질 역량이 미흡하며, 소프트웨어 개발 생명주기 동안 가시성 및 검증체계 미비로 인한 소프트웨어 품질 확보도 어려운 실정이다. 또한 모든 기업들은 요구사항의 구체성/명료성이 아직도 문제가 있다. 불명확한 요구사항은 올바른 시스템 개발에도 영향을 주며, 요구사항 규모 판단 및 산정도 어려워서, 인력·자원에 확보 등등의 문제로 고품질의 소프트웨어 개발에 악영향을 줄 수 있다. 국가기관 및 공공기관에서는 비용 규모 산정의 정확성에 대한 의문을 갖고 있다. 즉, 예산의 적합성 판단이 필요하다. 과거에 대기업은 공공기관의 대형 소프트웨어 위주로 개발을 해왔고, 요구사항 변경을 무조건적으로 수용하고, 추가 비용을 요청하지 않았다. 이런 생태계 속에서 중소/중견 업체들의 요구사항 변경 수용은 소프트웨어의 품질 저하 및 기업의 부담에 영향을 준다.
이 문제를 해결하기 위해 순공학 전에 비용 예측 방안을 제안한다. 이 방법은 소프트웨어 개발 프로세스가 중심이 되어야 하고, 구축되어야 한다. 순/역공학을 위해 소프트웨어 개발 프로세스를 정의하고, 자동 프로세스 구축을 통해 벤처/중소/중견 기업에게 프로세스 구축과 내재화를 제공한다. 초기 단계의 불명확한 요구사항은 소프트웨어 개발 생명주기 동안 계속적으로 악영향을 주며 지속적 요구사항 변경이 필요할 것이다. 즉, 요구사항의 명확성과 개발 비용의 적절성을 높이기 위해 요구사항 Spec 구조화를 제안하고, 시스템 측면의 요구사항 명확성을 높여 기능 요구사항을 도출한다. 기능 요구사항을 자연어 처리 방법으로 분석하고, 기능 점수(Function Point) 계산을 통해 요구사항 기반 비용 예측을 한다. 이를 통해 인력·자원 등등을 확보하여 효율적 개발로 소프트웨어 고품질화를 가능하게 한다.
더불어 비용 예측과 개발 후의 비용 검증을 통해 요구사항 기반으로 효율적이고 효과적으로 개발을 완료하고 요구사항 만족도를 통해 고품질화를 이루려 한다. 이를 위해, 역공학 기반 “비용 예측” 검증을 위한 자동 검증 및 요구사항 만족(Requirement Satisfaction) 메커니즘을 제안한다. 비용 검증을 위해 구현이 완료된 소스코드를 분석해야 한다. 자동 비용 검증을 위해 비용 검증 분석기를 개발하고, 비용 검증을 통해 추가 개발 비용 및 납기 지연을 줄임으로써 고품질 소프트웨어를 개발하고 성공적인 프로젝트를 수행 할 수 있다.
순/역공학 기반 비용 예측 및 자동 검증의 요구사항 프레임워크를 제안한다. 이는 기업의 소프트웨어 프로세스 내재화와 4차 산업혁명 분야 소프트웨어 품질 보장 및 향상에 도움을 줄 것으로 기대한다.The fourth industrial revolution in Korea will spread to all industrial fields, especially autonomous and intelligent software will be necessary. It will be a big issue to guarantee software robustness / reliability / safety for them. In other words, software for the fourth industrial revolution area may require different quality assurance and standards throughout the software life cycle, unlike the previous software development approach. Currently, domestic small and medium sized software companies have insufficient to keep software engineering level and quality competitiveness compared to large companies. It is difficult to obtain software quality due to lack of visibility and validation system during software development life cycle. In addition, all companies still have problems with the specificity / clarity of requirements. Unclear requirements affect the development of the correct system, and difficult judgement and estimation the size of the requirements, and the development of high-quality software due to problems such as keeping the manpower and resources. Specially korea’s national institutions and public institutions have questioned the accuracy of cost estimates. In other words, there is no certainty that the cost estimation is correct. In the past, large corporations have been in the process of software development by software development companies in their subsidiaries, accepting unconditionally changes in requirements, and not requesting additional costs even if the requirements change. In this environment, small and medium-sized companies continue to accept changes in requirements, thereby affecting software quality degradation and burdening enterprises.
To solve this problem, I propose cost estimation method before the forward engineering. This method that should be centered on the software development process, and should be built. I define software development processes for forward / reverse engineering, provide process build and internalization for venture / small / medium companies through automatic process construction. Unclear requirements at an early stage will continue to adversely affect the software development life cycle and require continual requirements change. In other words, for improving the clarity of the requirements and the appropriateness of the development cost, to propose the structuring of the requirements specification and derive the functional requirements by increasing the clarity of the requirements on the system side. Functional requirements are analyzed by a natural language processing method, requirements based cost estimation is done through function point calculation. Through this, it is possible to acquire manpower and resources, and to make the software high-quality by efficient development. In addition, through cost estimation and post-development cost validation, development is completed efficiently and effectively based on requirements, and high quality is achieved through requirement satisfaction.
To do this, I also propose an automatic validation and requirement satisfaction mechanism for "cost estimation" validation based on reverse engineering. We need to analyze the source code to validate the estimated cost. Through developing cost validation analyzer for automatic cost validation, I calculate function point with the data extracted from the cost validation analyzer. By cost validation, we can develop high-quality software, and perform successful projects by reducing additional development costs and delivery delays.
I propose a requirement framework for cost estimation and automatic validation based on forward / reverse engineering. With this approach, I expect to guarantee and improve software quality in industrial fields of the fourth industrial revolution with internalization of software process in korea companies.제 1 장 서론 1 1.1 연구배경 및 목적 1 1.2 연구내용 3 제 2 장 관련연구 5 2.1 소프트웨어 프로세스 5

Sentiment analysis refers to the use of natural language processing (NLP) techniques for analyzing subjective data such as attitudes, opinions, and propensity of people in text. Although recent, studies employing Korean sentiment analysis have been steadily increasing, its practical use is still difficult. First there is a lack of open emotional dictionaries. First, there is a lack of open Korean lexicon for sentiment analysis up to the present. Second, in the fields of specific domains, the linguistic characteristics of Korean, which is an agglutinative language and based on Chinese, makes it difficult to sort new words and technical terms as well as to learn words by distributed representation.
Thus, this research proposes the Korean sentiment analysis employing the character level Convolutional Neural Network. Through this, the model was able to generate syllable vectors based on the array structure of Korean character, and then minimize the problems caused by low frequency words and by out of vocabulary while avoiding morphological analysis. Finally, this proposed model records 88% of accuracy, and it is not much influenced by the Unstructured characteristic of input data as well as is able to be analyzed through of context of the text.감성 분석은 텍스트 내 사람들의 태도, 의견, 성향과 같은 주관적인 데이터를 분석하기 위한 자연어 처리 기술을 의미한다. 최근 한국어 감성 분석을 활용한 연구들은 꾸준히 증가하고 있지만, 그 사용은 여전히 어려운 실정이다. 첫 번째로, 공개된 감성사전이 부족하며, 두 번째로, 전문 도메인을 위한 감성 분석의 학습 시, 교착어이며 한자기반 언어라는 한국어의 특성이, 신조어 및 전문용어의 구별과 단어 임베딩을 통한 단어학습을 어렵게 만들기 때문이다.
따라서 본 연구는 자소 단위의 임베딩과 컨볼루션 신경망을 활용한 한국어 감성분석 알고리즘을 제안한다. 이를 통하여 해당모델은 한국어 감성 분석 시 형태소 분석을 배제할 수 있었으며 한국어의 언어적 특징인 초성, 중성, 종성을 기반으로 음절 벡터를 생성함으로써 낮은 빈도의 단어 학습 문제와 Out of Vocabulary의 문제점을 최소화 할 수 있었다. 본 연구의 모델은 88%의 준수한 정확도를 기록하였다. 또한, 입력 데이터의 비정형성에 적은 영향을 받으며 텍스트의 맥락에 따른 극성 분류가 가능함을 알 수 있었다.

모든 언어가 그렇듯이, 한국어 역시 구 혹은 문장을 이루는 표현 개별 요소의 의미 결합이 전체 의미를 나타내지 못하는 경우가 존재한다. 가령, ‘미역국을 먹다’라는 동사구는 음식을 의미하는 ‘미역국’과 음식을 섭취한다는 동사 ‘먹다’의 의미 그 자체가 결합하여 말 그대로 ‘미역국을 먹다’라는 의미가 나올 수도 있지만, 문맥에 따라 ‘응시한 시험에 떨어지다’라는 의미 역시 가질 수 있다. 전자의 구 전체 의미가 구를 이루는 개별 단어의 합으로 이루어졌다면 후자는 의미의 불투명성이 나타나는데, 이러한 경우를 흔히 ‘관용표현’이라고 한다.
학자마다 관용표현을 칭하고, 또 분류하는 방법은 모두 다르지만, 관용표현을 숙어와 연어로 구분하며, 연어를 다시 어휘 연어와 문법 연어로 구분하는 것이 일반적이다. 문법 연어(Grammatical collocation)는 연어를 이루는 구성 요소가 하나의 문법소와 하나의 어휘소를 포함하는 연어를 말한다. 가령, ‘-기 십상이다’라는 표현은 ‘십상이-’라는 하나의 고정된 술어가 선행 요소로 반드시 명사화소 어미인 ‘-기’를 취해야하며, 그 전체가 하나로 굳어져 사용된다. 문법 연어는 숙어, 혹은 어휘연어와 다르게 그 분포가 매우 제약적이고 문법소를 반드시 포함하고 있다는 뚜렷한 특징이 있는 범주이기 때문에 하나의 독립적인 연구 분야로 다뤄져야 한다. 그럼에도 불구하고 문법 연어에 대한 연구는 찾아보기 힘들며, 임근석(2010)과 여춘연(2010)을 제외하고서는 관용표현에 대한 포괄적인 연구 속에서 가볍게 언급되는 것이 전부다. 따라서 본 논문은 한국어 관용 표현에 속하는 문법 연어에 대한 심층적인 연구를 실시하고자 한다. 한국어에서의 문법 연어는 조사나 어미로 표현되는 문법소와 독자적인 어휘소가 결합하기 때문에 문법 연어의 분석에 형태·통사적 접근을 취하는 것이 본 논문의 주된 접근법이다. 본 연구는 순수언어학적으로는 당연히 연구되어야 할 문법 연어에 대한 심층적인 연구를 진행했다는 점에서, 응용언어학적으로는 크게 자연어처리와 외국인을 위한 한국어 교육을 위한 질적인 자료의 재고라는 점에서 의의가 있다.This paper analyzes syntactic structures of Korean idiomatic expressions, especially focusing on grammatical collocations. Idiomatic expressions mean any phrases or sentences which do not follow the principle of compositionality. Grammatical collocations, one of Korean idiomatic expressions, are the complex constructions with more than one grammatical morphs and lexical morphs. Grammatical collocations have to be dealt independently. This is because they contain at least one grammatical morph and are heterogeneously distributed. For example, in case of ‘-nun palam-ey', one of grammatical collocations, there must be ending '-nun' before ’palam’. Also, the postposition ‘-ey’ after ‘palam’ is required in order to reflect the meaning of ‘because'.
Thus, the present study suggests the definition and criteria to sort grammatical collocations. Secondly, by using "representational system" (H-R Chae, 2016), the study also analyzes syntactic structure of grammatical collocations individually. In conclusion, the study finally investigates 164 entries of Korean grammatical collocations and establishes a dictionary by using Excel program.1. 서론 1 1.1. 연구 목적과 필요성 2 1.2. 논의의 구성 7 2. 이론적 배경 9

Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Building an algorithms to teach machines to read and to comprehend text is one of the biggest challenge in the natural language processing research topics. A common way for proving these algorithms is by formulating them as RC tasks. This can be defined as selecting an answer corresponding to a question in the given document as an evidence. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document. We assume that a reason of failing in previous approaches is lack of ability to capture long-term dependency between the words. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In detail, our method has two novel aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory. Our proposed architecture is widely applicable to other models. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed method outperforms existing methods, especially for lengthy documents.기계독해 방법은 자연어로 쓰여진 인간의 지식을 기계가 학습하는 방법 중 하나이다. 자연언어처리 연구 분야에서 기계에게 언어를 읽고 이해하는 알고리즘을 설계하는것은 매우 도전적인 작업이다. 이 작업을 증명하는데 쓰이는 보편적인 방법은 RC 테스크인데, 이 테스크는 주어진 문서에 대한 질문에 대해 문서 안에서 해당 질문에 맞는 답을 골라내는 작업으로 정의될 수 있다. 이 테스크를 풀기위해 기존의 제안된 모델들의 성능은 어느정도 인간의 성능과 비슷하지만, 길이가 긴 문서에 대해서는 성능이 매우 제한적이다. 우리는 그 이유가 긴 문서에서 주로 발생 하는 멀리 떨어진 단어들에 대한 연관관계를 찾는 능력이 현재 모델에서는 고려되고 있지 않기 때문이라고 생각하였다. 이 논문에서는 멀리 떨어진 단어들의 연관성을 찾을 수 있는 새로운 딥뉴럴 네트워크 구조를 제안하였다. 우리는 다음과 같이 두 가지의 새로운 측면에서 기존의 모델을 향상 시켰다: (1) 진보된 메모리 증강 구조 개발, (2) 밀도있는 연결을 이용하여 GRU 를 확장한 새로운 인코더 개발. 우리는 많이 알려진 TriviaQA, QUASAR-T, SQuAD 데이터세트에 대해서 제안된 모델을 이용하여 실험을 하였다. 실험 결과, 길이가 긴 문서들에 대해서 제안된 모델이 기존의 모든 모델들 대비 더 높은 성능을 보임을 확인할 수 있었다.Abstract Contents i List of Figures iii List of Tables iv 1 Introduction 1

A parsing is a task of achieving syntactic informations in natural language processing. Accurate parsing is a important role at understanding natural language. In the case of Korean language parsing, dependency parsing that representing syntactic information is the head-modifier dependency is studied more active than phrase structure parsing. In this paper, we focus on the dependency parsing for Korean language.
Lately studies of dependency parsing was classified two categories as deterministic or non-deterministic. A main advantage of deterministic parsing is rapid speed and disadvantage is low accuracy than non-deterministic parsing.
In this paper, we propose a Korean dependency parsing using correction of projectivity. The proposed parser corrects errors of deterministic parsing model using projectivity for basic dependency parsing model using head-final. In result of deterministic parsing algorithm, the crossed dependencies must have errors. In this case, the proposed parser can improved performance by re-search. The proposed parser can use informations of parsing history, because have a form of deterministic algorithm. Therefore expect to improved performance than nondeterministic parsers. We evaluate our parser on the ETRI(2005) corpus that consist of complex sentences, and show higher accuracy than other parsers.구문분석은 자연어 문장의 문법적인 구조를 분석하는 것이다. 구문분석을 통해 문장내의 목적격, 주격 관계와 같은 수식관계들을 파악할 수 있으며 이 정보들을 이용하여 문장의 정확한 의미를 분석할 수 있다. 한국어의 구문분석은 주로 단어와 단어사이 의존관계를 분석하는 의존 구문분석을 대상으로 많이 연구되어 왔다.

최근 의존 구문분석은 결정적 의존 구문분석과 비결정적 의존구문분석 2 가지 방식으로 연구되고 있다. 결정적 의존 구문분석은 문장의 일부만을 고려하여 의존관계를 분석하기 때문에 분석 속도 측면에서는 유리하지만 정확률 측면에서 비결정적 의존 구문분석에 비해 낮은 성능을 보여준다.

본 논문에서는 결정적 의존 구문분석의 오류를 보정하여 성능이 개선된 결정적 한국어 의존 구문분석을 제안한다. 본 논문에서는 한국어의 특성인 지배소 후위의 원칙을 이용해 기본적인 결정적 의존 구문분석 알고리즘을 제안한다. 이때 투사성을 근거로 하여 기본적인 결정적 의존 구문분석 결과의 오류를 검사하였고 재 탐색 알고리즘을 제안하여 찾아낸 오류를 보정하였다. 또한 결정적 의존 구문분석에서 사용할 수 있는 분석 중간결과(parsing history)를 이용하여 효과적인 자질들을 제안하였다. 우리는 실험을 통하여 제안하는 모델이 기존의 비결정적 알고리즘과 동등한 성능을 제공하고 중간분석 결과를 자질로 사용할 경우 향상된 성능을 제공함을 증명하였다.

This paper proposes probabilistic models for Korean morphological analysis and part-of-speech tagging, which are the most fundamental problems in natural language processing.
Contrary to the previous morphological analyzers that depend on manually constructed linguistic knowledge, the proposed one can fully automatically acquire the linguistic knowledge from annotated corpora (e.g. part-of-speech tagged corpora).
Besides, without any modification of the system, it can be easily applied to other corpora having different tagsets and annotation guidelines.

This paper presents four probabilistic models for morphological analysis that are based on Korean linguistic units (such as Eojeol, morpheme, and syllable) and substring.
The Eojeol-unit model and the syllable-unit model compensate the previous methods that only depend on morpheme-unit analysis.
The Eojeol-unit model is very simple and highly efficient, and also contributes the improvement of precision.
The syllable-unit model is robust to the unknown-word problem and shows high precision.
This paper describes the models and presents the experimental results on three corpora with a wide range of conditions.
Through the experiments, the proposed system achieved comparable and improved performances with the previous systems.

The proposed part-of-speech models are based on the Markov model and the maximum entropy model, which are the most representative probabilistic models.
They consist of the morphological analysis model and the Eojeol transition model.
By the morphological analysis model, the system can consider lexical forms.
By the Eojeol transition model, the system can consider Eojeol-unit context.
This paper proposes various tagging models according to the contexts used in the Eojeol transition model.
Through the experiments, the second-order Markov model shows the best performance among the other models including the previous one.본 논문은 자연어 처리의 가장 기본적인 단계인 형태론의 두 가지 문제인 형태소 분석과 품사 부착을 위한 확률적 모형을 제안한다.
수작업으로 구축된 언어 지식에 의존하는 기존의 연구와는 달리 제안하는 형태소 분석 모형은 언어 지식을 품사 부착 말뭉치로부터 자동으로 획득할 수 있다.
또한 시스템을 수정하지 않고 다른 품사 집합과 분석 지침을 가진 말뭉치에 쉽게 적용할 수 있다.

형태소 분석을 위해 한국어의 언어 단위인 어절, 형태소, 음절과 부분 문자열에 기반한 네 가지 확률 모형을 제시한다.
어절과 음절 단위 모형은 형태소 단위 분석에만 의존했던 기존의 방법을 보완한다.
어절 단위 모형은 단순성과 높은 효율성을 보이며 정확도의 향상에도 기여한다.
음절 단위 모형은 미등록어 문제에 견고하고, 높은 정확성을 보인다.
본 논문은 제안하는 형태소 분석 모형에 대해 다양한 실험 조건에 따른 실험 결과를 제시하였고, 제시된 실험 결과는 기존의 형태소 분석기에 필적하거나 향상된 성능을 보였다.

제안하는 품사 부착 모형은 가장 대표적인 확률 모형인 마르코프 모형과 최대 엔트로피 모형에 기반하며
형태소 분석 모형과 어절 전이 모형으로 이루어져 있다.
제안하는 방법은 형태소 분석 모형에 의해 표층형을 고려할 수 있으며, 어절 전이 모형에 의해 어절 단위의 문맥을 고려할 수 있다.
본 논문은 어절 전이 모형에 사용된 문맥에 따라 다양한 품사 부착 모형을 제안하고 있다.
실험을 통하여, 2차 마르코프 모형에 기반한 품사 부착 모형이 기존 모형을 포함한 다른 모형 가운데 가장 높은 성능을 보였다.

맥락탐구란 사용자경험디자인 방법의 하나로 사용자 작업환경에서 사용자와 함께 긴밀한 관계를 맺고 관찰과 인터뷰를 수행하는 것이다. 이로부터 관찰되는 업무와 프로세스 등으로 많은 양의 비정형 텍스트 데이터가 생성이 된다. 비정형 텍스트 형태로 생성되는 맥락탐구 데이터는 다차원 시각으로의 효율적인 분석이나 데이터 활용, 공유, 보관 및 저장 관리의 한계가 있다. 이러한 부분을 개선하기 위해 맥락탐구를 통해 도출되는 비정형 텍스트 데이터의 속성을 분석하고 이를 정량화, 정형화하여 분류 및 저장, 검색이 가능한 형태로 변환시키는 방식에 대한 고찰과, 궁극적으로는 변환된 데이터의 시각화를 통해 분석의 효율화를 도출하는데 연구의 목적이 있다. 이를 위하여 맥락탐구 과정에서 조사와 관찰을 통해 수집된 데이터로부터 추출된 이슈항목과 흐름 모델, 순차 모델, 물리적 모델, 문화 모델, 산출물 모델 등에서 보이는 정보의 흐름을 연구하였다. 이렇게 연구된 정보의 흐름을 통해 분석 과정에서 집중적으로 찾아야 하는 것이 무엇이고 어떤 정보를 제공하는 것이 필요한지를 파악하였다. 이 자료를 기반으로 맥락탐구 프로세스 상에서 필요하다고 판단되는 속성과 정보의 추출시점, 가공 방법, 그리고 효과적인 시각화 방안에 대한 연구를 진행하였다.
맥락탐구 데이터는 비정형 텍스트 데이터로서 비정형 텍스트의 특징은 수십~수십만 개에 이르는 다양한 크기를 가지고 있으며, 셀 수 없을 정도의 조합이 가능하고 그 쓰임새도 매우 다양하다. 텍스트 데이터는 중복된 데이터의 집합으로 비교적 간단한 알고리즘만으로도 반복되는 낱말들을 관찰하여 의미가 있는 낱말을 찾아낼 수 있다. 또한 최근의 자연어 처리 기술과 온톨로지의 발전으로 보다 의미 있는 정보를 추출하는 과정이 가능해졌으며 이를 기반으로 비정형 텍스트 데이터인 맥락탐구 데이터로부터 정량적 요소들을 추출 할 수 있는 다양한 방법을 연구하였다.
비정형 텍스트 데이터를 시각화하는 방안으로는 크게 언어 구조에 의존하지 않고 문서의 수와 낱말의 빈도수에 대한 정보만으로 가능한 방법, 언어가 가지고 있는 특성 중에서 고유명사 혹은 지식 사전을 기반으로 한 분석을 통해 수집된 정보를 이용하는 방법, 최종적으로는 언어 자체의 문법과 구조를 이용한 방법 등이 연구되어 왔으며, 이러한 기존의 연구결과들을 바탕으로 디지털 미디어를 이용한 시각화 방안을 연구하였다.
디지털 미디어를 이용한 시각화를 통해서 기대하는 것은 텍스트 구성에 대한 이해를 돕고, 대상 텍스트에 대한 상위 수준의 이해 즉, 사용자의 맥락을 이해하고 그 맥락 속의 주제어와 각종 요소들 사이의 관계를 용이하게 파악하는데 있다. 시각화 사례들로써 StarTree, TreeMap, WebSOM, TextArc, ElasticTag, Text Visualizaion Tool, Contexter와 같은 선행 연구결과를 검토하였으며 이 사례들은 분석을 위해서 필요한 주요 기능들인 탐색, 추출, 정제, 연결, 정렬, 비교, 가설과 같은 기능들에 대해서 고려되어 있지 않거나 주로 탐색 기능 중심으로만 제공된다는 것을 알게 되었다. 본 논문에서는 맥락탐구 데이터의 분석 과정을 지원하기 위해 필요한 탐색, 추출, 정제, 연결, 비교 기능이 가능한 도구를 디자인 및 구현하였다.Contextual inquiry is one of the famous user research methods used in the field of user experience design. This method involves interviews and observations conducted in the user?s environment as well as a tight relationship between the researcher and user. The process produces large amounts of unstructured text data containing a lot of details of daily work tasks and processes. As a result, contextual inquiry has limitations concerning data storage, sharing, usage, and presentation and analysis method. To remedy this situation, the researcher must develop a method for analyzing the unstructured text data drawn from contextual inquiry and modifying it into a classifiable or storable form by quantifying and structuring it. The ultimate purpose of this research is to create efficiency of data analysis through visualization of modified data.
Researchers have studied the information flow from the lists of issues resulting from the field data to the flow model, sequence model, physical model, cultural model, and output model. When the information flow was studied in this matter, it was learned what should be from analysis and what information should be provided from the information flow. Based on this data, the research was conducted about features that are regarded to be necessary on the contextual inquiry and the point of data extraction and effective visualization of it.
Contextual inquiry data is a form unstructured text data, and its three salient characteristics are its various sizes (from dozens to hundreds of thousands of information bits), its numerous possible combinations, and its use in diverse situations. Text data has a lot of redundancy; however, it is possible to make meaningful information from its characteristics by a relatively simple algorithm. Recently, there has been advancement of natural language processing and ontology analysis, and they are also available to extract highly intelligent information from large quantities of text data. Based on this method, researchers have found various ways to perform quantitative analysis on the unstructured contextual inquiry data.
There are three major ways of visualizing text data. The first one is based on only the numbers appearing in the text. It does not rely on the characteristics of the natural language itself. The second method is done using the information through analysis based on knowledge banks and proper names, among other language features. The third way is syntax analysis, which depends on the characteristics and knowledge of the natural language. Syntax analysis is studied to find the most efficient and effective visualization of the contextual inquiry data.
What can be expected from the visualization using digital media is that it can help understand the text structure. It is also expected to produce a high level of understanding about the data ? that is, understanding the user?s context and comprehending the relationship among the key words and individual elements from that content. Previous research results, such as StarTree, TreeMap, WebSOM, TextArc, ElasticTag, Text Visualizaion Tool, and Contexter, are studied as examples of visualization. However, past research mainly focused on the explore function, and it did not include such major functions as extract, filter, link, arrange, compare, and hypothesize. For this thesis, tools that can explore, extract, filter, link and compare were designed and developed to support the data analysis process of contextual inquiry.제1장 서론 = 1 1.1 연구배경 = 1 1.2 연구주제 = 3 1.3 연구문제 및 연구방법 = 4 1.4 논문의 구성 = 5

4차 산업혁명 시대 로봇은 기계적 장치를 넘어 소프트웨어로서 반복 생성되는 방대한 데이터를 자동으로 정리하고 편집하는 봇의 개념으로 발전했다. 현재 의료, 법률은 물론 언론분야에서 봇을 이용한 다양한 서비스가 이뤄지고 있다.
미국의 경우 LA타임스, OPB(오린건주 공영방송), AP통신, 포브스, 더 가디언 등에서 로봇 저널리즘을 이용한 서비스를 하고 있다. 또 내러티브 사이언스(포브스), 오토메이티드 인사이트(AP), 윕비츠, 우칫, 데이터마이너 등 스타트기업은 로봇 저널리즘 프로그램 및 유사 프로그램을 언론사 및 다른 사기업에 제공하는 서비스를 하고 있다.
해외에 이어 2016년 국내 4개 언론사에서 로봇 저널리즘 서비스를 시작하면서 로봇 저널리즘이 언론 지형, 기자들의 일자리에 직접적인 영향을 줄 것인지 가 언론계 내외의 관심사가 됐다. 이에 국내 로봇 저널리즘 서비스가 시작된 지 1년 3개월을 돌아보고 향후 발전 방향을 모색하기 위한 연구를 진행했다. 이를 위해 로봇 저널리즘 3개 서비스 사 간부, 로봇 저널리즘 기사를 봤을 가능성이 높은 10개 언론사 증권, 금융 분야 담당 기자에 대한 심층 인터뷰를 진행했다.
연구 결과 ‘로봇 저널리즘이 기자들의 일자리를 위협할 것이냐에 동의하지 않고 로봇 저널리즘 기사 수준은 언론시장에 위협을 줄 정도가 아니다’ 는 의견이 제시됐다. 로봇 저널리즘 서비스 회사 간부에 대한 조사에서는 로봇 저널리즘 도입으로 인력 활용도가 높아졌고 단순한 기사는 로봇에게 넘겼다는 답변이 나왔다.
또 로봇 저널리즘을 통한 온-디맨드 수요는 있을 것이지만 유료화 가능성은 유보했다. 국내 로봇 저널리즘 도입 1년 3개월의 성과로 볼 때 현재 서비스를 하는 언론사들은 서비스를 보강하면서 다른 수익원을 찾는 노력을 계속할 가능성이 높다.
국내 로봇 저널리즘 비(非)서비스사는 기존 언론사 서비스가 큰 성과를 거두지 못한 만큼 당장 도입 필요성을 상대적으로 적겠지만 빠른 기사처리, 맞춤형 기사 생산, 인건비 절약 등은 여전히 매력적인 요인으로 남아 있다.
최근 인공지능 기술이 급속도로 발전하고 있어 빅 데이터, 자연어처리, 머신 러닝 등과 결합할 경우 로봇 저널리즘을 통해 심층적인 분석, 가치 판단 등 보다 고차원적인 기사를 작성할 수 있다. 2세대 로봇 저널리즘 등장 추이와 함께 유관 스타트업 기업들이 언론시장에 어떤 영향을 미칠지 관심 있게 지켜볼 것을 제안한다.In the 4th Industrial Revolution era, robots evolved into the concept of mechanism that automatically rearrange and edit vast amounts of data that are generated repeatedly as software, from concept of old mechanical devices. Currently, various services using robots are being performed in the fields of medicine, law and media.

In the United States, they are using robots journalism services such as the LA Times, OPB (Orin Dean Public Broadcasting), AP, Forbes, The Guardian, and others.
In addition, Start - up companies such as Narrative Science (Forbes), Automated Insight (AP), Hipwits, Wuchitsch, and Data Miner are providing robots journalism programs and similar programs to the media and other private companies.
After launching robotic journalism services in four domestic media in 2016, whether robot journalism directly affects the media landscape and the jobs of reporters has become a matter of interest to both journalists and press. Therefore, we have conducted a research to look back on the development direction of the robot journalism service in Korea for last 1 year and 3 months.
To this end, I conducted in-depth interviews with the executives of the three companies serving robots journalism and ten journalists likely to have seen articles in robot journalism. The results of the study were as follows. First, lots of attendee suggested that the article level of robot journalism is not enough to pose a threat to the media market. Secondly, therefore there were many opinions that robot journalism would not threaten journalists' jobs.

In interviews with executives show that the introduction of robotic journalism has increased the utilization of manpower, and a simple article has been passed to the robot. Also, there will also be demand for on-demand through robotic journalism, but the possibility of money-paid system has been reserved.
According to the results of 1 year and 3 months introduction of domestic robot journalism, it is highly likely that the media companies that provide robot services are continuing their efforts to find other sources of revenue while enhancing their robot services.

In the case of domestic media companies that do not serve robot journalism, robotic journalism has not shown great visible benefits, so it is not necessary to introduce them right away. However, fast article processing, customized article production, and labor cost savings are still attractive.
Recently, artificial intelligence technology is rapidly developing, so it can forecast that robot journalism can be combined with big data, natural language processing, and machine learning. In this case, it may be possible to make more high-level articles such as in-depth analysis and value judgment through robot journalism.
With the emergence of second-generation robot journalism, it is suggested that startup companies pay attention to how they will affect the media market.Ι. 문제 제기 및 연구 목적 1 1. 문제 제기 1 2. 연구 목적 4 Ⅱ. 로봇 저널리즘의 등장과 이론적 논의 5 1. 기술의 진보와 로봇 저널리즘의 등장 5

With the advancement of machine learning algorithms that deal with text, more and more companies are using chatbots for their business. Current chatbot technology relies on key word extraction through Natural Language Processing (NLP) and human-generated IF-THEN rules. However, as recent developments in deep learning shows great possibility to imitate human reasoning ability without external intervention, many deep learning models are being developed to find the right answer when a question is given. To solve the text-based question and answering task that requires relational reasoning, it is necessary to memorize a large amount of information and find out the question relevant information from the memory. Most approaches were based on external memory and four components proposed by Memory Network. The distinctive component among them was the way of finding the necessary information and it contributes to the performance. Recently, a simple but powerful neural network module for reasoning called Relation Network (RN) has been introduced. We analyzed RN from the view of Memory Network, and realized that its MLP component is able to reveal the complicate relation between question and object pair. Motivated from it, we introduce Relation Memory Network (RMN) which uses MLP to find out relevant information on Memory Network architecture. It shows new state-of-the-art results in jointly trained bAbI-10k story-based question answering tasks and bAbI dialog-based question answering tasks. By virtue of RMN's high reasoning ability even in memory-intensive situations, it is expected to be better applied to general situations compared to other models.머신러닝의 발전으로 텍스트를 다룰 수 있는 능력이 향상되면서 챗봇을 비즈니스에 활용하는 사례가 늘고 있다. 현재의 챗봇 기술은 자연어처리 (Natural Language Processing, NLP) 를 통한 중요 단어 추출과 사람이 생성한 IF-THEN 규칙에 의존한다. 그러나 최근 딥러닝의 발전으로 외부 개입없이 사람의 추론 능력을 모방함으로써, 질문이 주어졌을 때 정답을 찾는 모델이 개발되고 있다. 텍스트를 통한 질문과 답변이 원활하게 이루어지기 위해서는 많은 양의 정보를 기억하고 그 중에서 질문에 관련된 정보를 찾는 추론 능력이 필요하다. 그래서 대부분의 모델은 외부 메모리와 메모리 네트워크 (Memory Network, MemNN) 에서 제안한 네 가지 구성 요소를 기반으로 이루어져 디자인 되어왔고, 좋은 성능을 보였다. 각 모델은 필요한 정보를 찾아내는 구성 요소에서 두드러진 차이를 나타냈는데, 이 부분이 복잡하게 구성될수록 복잡한 관계를 잘 찾아냄으로써 성능 향상을 보였다. 최근에는 관계 네트워크 (Relation Network, RN) 라고 불리는 추론을 위한 간단하면서도 강력한 신경망 모듈이 소개되었다. 우리는 메모리 네트워크 관점에서 관계 네트워크를 분석하여 다층 퍼셉트론 (Multi-layer perceptron, MLP) 로 이루어진 구성 요소가 질문과 저장된 정보 간의 복잡한 관계를 나타낼 수 있음을 알게 되었다. 이를 바탕으로 다층 퍼셉트론을 사용하여 메모리 네트워크 아키텍처에서 관련 정보를 찾는 관계 메모리 네트워크 (Relation Memory Network, RMN) 를 소개한다. RMN은 모델의 단순함에도 불구하고 bAbI-10k 스토리 기반 (bAbI-10k story based) 및 bAbI 대화 기반 (bAbI dialog based) 질문 응답 작업에서 가장 높은 정확도를 보였다. 또한, 다른 모델에 비해 메모리가 많은 상황에서도 높은 정확도를 보이기 때문에 향후 데이터가 많은 일반적인 상황에 보다 잘 적용될 것이라고 기대한다.1 Introduction 1 2 Related Work 6 2.1 Memory-Augmented Neural Network 6 2.2 Relation Network 12 3 Relation Memory Network 14

의미 중의성 해결은 문맥 내에 출현하는 단어가 둘 이상의 의미를 지닐 때, 의미들 중 문맥상 옳은 하나의 의미를 분별하는 것으로, 자연 언어 처리의 가장 힘든 문제 중의 하나이다. 본 논문에서는 품사 부착된 영어 말뭉치를 이용하여 단어 의미 중의성을 해결한다.
한국어 말뭉치만을 이용할 경우 중의성을 가지는 명사와 공기(共起)하는 동사가 여러 개 있을 수 있으며, 각각의 동사는 말뭉치 전체로 보아도 공기 출현 빈도가 낮아 충분히 활용하기가 어렵다. 이러한 한국어 자원의 부족을 영어 자원들을 이용하여 해결하고자 한다. 또한 동사가 갖는 격 정보, 즉 어떤 동사가 취할 수 있는 명사들은 그 의미가 유사하다는 점을 이용하였다. 한국어 명사, 동사를 영어 대역어로 변환하고 대역어로 변환된 동사와 다른 단어사이의 공기 정보를 이용함으로써 중의성을 해결하고자 하였다.
말뭉치를 이용한 단어 의미 중의성 해결 시스템은 품사 추출기, 공기 빈도 추출기 및 분류기로 구성된다. 품사 추출기에서는 말뭉치에 존재하는 품사를 추출하는데, 본 논문에서는 명사(/N)와 동사(/V)로 태깅된 단어들을 모두 추출한다. 공기 빈도 추출기에서는 품사 추출기로부터 추출된 명사와 동사에 대하여 각각의 단어와 공기하는 횟수를 구한다. 분류기에서는 입력 문맥의 명사와 동사들을 추출하고, 영어 대역어로 변환한 후 공기 빈도 추출기로부터 구해진 정보를 이용하여 중의성의 의미를 결정한다.
비교적 중의성이 자주 발생하는 7개의 동형이의어 명사에 대하여 각각 30문장씩 실험하였다. 본 논문에서 제안한 방법으로 실험한 결과, 평균 79.9%의 비교적 높은 정확률을 보였다.
또한 본 논문에서 제시한 단어 의미 중의성 해결 방법은 정보검색에 이용되는 자연어 처리 기술의 한 분야로써 정보 검색 시스템에서 사용자의 요구에 부합하는 정확한 정보를 제공할 수 있다.Word Sense Disambiguation is choosing one specific meaning from the word exists in context which has more than two meanings, it is one of the most difficult problem in Natural Language Processing. In this paper, we will solve the sense ambiguity using English Corpora tagged a part of speech.
If we use only Korean Corpora, there might be too many ambiguity nouns and co-occurrence verbs and each verb can't be sufficiently utilized because of low co-occurrence appearance frequency in while Corpora. We will solve this rack of Korean resources with English resources. Also, We used information that one verb has similar meaning's nouns. We solved ambiguity by using co-occurrence information between English translated with the original verbs and the other nouns.
Word Sense Disambiguation System using Corpora is composed of a part of speech extractor, a co-occurrence frequency extractor and distributor. A part of speech extractor abstracts a part of speech exists in Corpora, but in this paper, we will abstracts all tagged nouns(/N) and verbs(/V). A co-occurrence frequency extractor examine how many times abstracted nouns and verbs from a part of speech extractor co-occur with each words. Distributor abstracts verbs and nouns from input context, and after translating with the original English and select the meaning of ambiguity by using the information gotten from co-occurrence frequency extractor.
We experimented 30 sentences from each seven homograph nouns which sense ambiguity are frequently happen. As a result of this experiment, average of precision rate was 79.9%
And Word Sense Disambiguation strategy shown in this paper is one of the field in Natural Language Processing technology used in information retrieval and provide certain information about users' demand in information retrieval system.목차 국문초록 = ⅰ Abstract = ⅲ Ⅰ. 서론 = 1 Ⅱ. 관련연구 = 3

소셜 미디어가 보편화되며 매시간 누적되는 데이터의 양이 기하급수적으로 증가하였다. 이는 기존의 데이터 처리 체계 및 인식에 변화를 몰고 왔다. 데이터가 곧 경제적 자산이 되는 빅데이터 시대가 도래한 것이다. 이에 따라 빅데이터 관련 연구가 활발히 진행되고 있지만 빅데이터에 대한 개념 및 기술 동향 등으로 연구 범위가 국한되어 있다. 본 연구에서는 헬스 케어 분야에서 빅데이터를 활용하여 사용자 인식 현황을 분석함으로써 빅데이터 관련 연구 범위를 확대하고자 하였다.
전문의약품으로 분류된 사후피임제를 일반의약품으로 전환하고자 하는 계획안이 2012년 6월 식품의약품안전청에 의해 추진되었으나 2015년 7월 현재까지 구체적 방향 모색을 이유로 보류되어 있는 상태이다. 본 연구는 이에 대한 정책적 방향성을 제시하고자 사후피임약에 대한 사용자의 인식 현황 실태 조사를 실시하였다. 한국은 전통적으로 엄격한 성 문화권을 가지고 있으며, 이러한 문화에서 설문 조사 등의 방법으로 ‘피임’이라는 다소 노골적인 주제에 대한 정확한 실태 조사 결과를 얻기란 쉽지 않을 것으로 예상하였다. 따라서 이를 보완하기 위한 방법으로 온라인 텍스트 데이터 기반 빅데이터 분석 방법을 도입하였다. 익명성이 보장된 사이버 문화에서는 보다 솔직한 개인의 의견이 반영되어 있으므로, 이를 실증적으로 분석하면 사후피임약에 대해 보다 실제적인 인식 및 이용 현황을 파악할 수 있으리라 예상하였다.
본 연구에 사용된 데이터의 범위는 피임 키워드가 포함된 텍스트 데이터로 2009년부터 2014년까지 총 7,921건의 데이터를 웹 크롤링 통해 수집하였다. 이를 자연어처리와 형태소분석을 통해 명사를 추출하였으며, 추출된 명사를 토대로 현황 분석, 시계열 분석, 소셜 네트워크 분석, 군집 분석을 수행하였다. 분석 결과 사용자들은 주로 사후피임약에 대한 전반적인 정보(효능, 복용방법)를 필요로 했으며, 특히 남성과 미성년을 대상으로 하는 교육이 부족하다는 것을 발견하였다.제 1장 서론 제 1절 연구배경 제 2절 연구 목적 및 논문 구성 제 2장 문헌연구 제 1절 빅데이터

도메인 분석은 새로운 소프트웨어 응용의 요구시 이전의 비슷한 시스템을 손쉽게 재사용할 수 있도록 비슷한 시스템들 사이에 공통점과 차이점을 분석하는 작업으로 소프트웨어 재사용성을 향상시키는 새로운 방식으로 기대를 모으고 있다. 그 동안의 도메인 분석에 관한 연구는 분석의 절차에 많은 발전이 있었으나, 분석의 결과는 공통적인 표준이 없었고 다양한 도식적으로 표현되어 재사용의 자동화가 없이 수작업에 의해 소극적으로 재사용되었다.
본 논문은 CASE에서 도메인 분석정보 재사용의 자동화를 통한 재사용성 향상을 위하여 1) 잘 정의된 구문을 갖는 재사용 도메인 분석명세 언어로 재사용 가능한 도메인 분석정보의 표현이 가능한가와 2) 재사용 분석정보 처리기를 통해 분석명세의 재사용 연산이 가능한가를 연구하였다.
이러한 연구 결과 도메인 분석명세 재사용 활동 모델은 도메인 분석명세 자산, 도메인 분석명세 제어, 도메인 분석명세 메카니즘, 도메인 분석명세 산물의 4가지 요소로 구성됨을 식별하였다. 재사용의 자산이 되는 도메인 분석명세는 표현의 전산 처리가 가능한 규칙 유무와 정도에 따라 비 정형적, 반 정형적 및 정형적 형태의 3 가지로 구분하였으며, 도메인 분석명세 재사용 연산의 자동화에는 정형적 도메인 분석명세가 용이함을 식별하였다. 잘 정의된 언어는 애매모호하지않고 전산 처리가 가능한 특성을 갖기 때문에 정형적 도메인 분석명세를 위한 도메인, 파트, 객체의 재사용 단위와 추가, 삭제, 추출의 재사용 연산을 지원하는 재사용 도메인 분석명세 언어를 설계할 수 있었으며 이러한 언어를 통해 도메인 분석명세 언어를 재사용 단위별로 표현할 수 있었다. 도메인 분석명세 언어의 처리를 위해 도메인 분석명세 분석기와 도메인 분석명세 재사용기로 구성되는 도메인 분석정보 처리기를 구현하였다. SUN 워크스테이션에서 문제를 대여 도메인으로 한정하고 도메인 분석명세 언어로 작성된 명세를 재사용 처리하여 새로운 응용에 맞는 도메인 분석명세 인스탄스를 생성할 수 있었다. 마지막으로 도메인 분석정보 처리기에 의한 재사용 방식과 데이타 흐름도 방식의 Teamwork/SA에 의한 재사용 처리 시간을 비교하였는데 추가 연산을 통한 재사용에는 도메인 분석정보 처리기의 방식이 평균 32% 빨랐으며, 삭제 연산을 통한 재사용은 Teamwork/SA의 도식적 방식이 평균 54.8% 빨랐다.
본 연구를 통해 대여 도메인에서 도메인 분석정보를 도메인 분석명세 언어로 표현할 수 있었으며, 도메인 분석정보 처리기를 통해 재사용할 수 있었다. 추가 연산을 통한 재사용의 경우 도메인 분석정보 처리기의 의한 재사용 처리 시간은 Teamwork/SA에 의한 시간보다 빨랐으며, 삭제를 통한 재사용의 경우 Teamwork/SA가 빨랐다. 그리고 도메인 분석명세 언어와 도메인 분석정보 처리기를 통한 재사용 방식은 재사용자의 경험과 능력에 크게 영향을 받지 않았다. 보다 높은 도메인 분석정보의 재사용성을 위해서는 도식적 방식과 분석명세에 의한 방식 두 가지를 함께 지원하는 복합적 방식의 필요성을 발견할 수 있었다. 아울러 도식적 표현을 도메인 분석명세 언어로 변환하고 도메인 분석명세 언어를 도식적으로 표현해 주는 소프트웨어 도구의 필요성이 요구되었다.Domain Analysis is an activity to identify commonalities and variabilities within similar application areas in order to reuse in new software construction and is regarded as a new method to increase software reusability. Although there are lots of progresses in the domain analysis process, most of domain analysis output is represented by various diagrams without standard and its manual reuses result in low reusability.
For the increase of software reusability through automating the reuse process of domain analysis information in CASE, we researched 1) the possibility of reusable representation of domain analysis information by using domain analysis specification language, 2) the possibility of reusability of domain analysis specification by use of a domain analysis information processor.
In this research, we identified four elements for the reuse activity of domain analysis specification: domain analysis specification resources, domain analysis specification controls, domain analysis specification mechanisms, and domain analysis specification products. We categorized the resources of domain analysis specificationas to whether it has uniformity in expression : informal domain analysis specification, semiformal domain analysis specification and formal domain analysis specification, and found the last one was well-suited to automate domain analysis information reuseprocess by feature comparisons of three types. Since a well-defined language has characteristics of unambiguity and machine processability, we could design domain analysis specification language supporting various reusable modular unit and domain analysis information reuse operation for formal domain analysis specification. We could model the rental domain in this language and implement the domain analysis information processor having an analyzer and reuse the processor for the domain analysis specification. The analyzer was designed to have lexical analysis, syntax analysis and semantic analysis to do syntax and semantic error checks. The domain information processor was designed as an interpreter to interpret reuse operations. In SUN workstation, we could get a domain analysis specificationinstance for new application by reusing domain analysis specification. Finally we compared the reuse processing time of the DFD method by Teamwork/SA with the reuse of domain information processor.
In our research on the reusability of domain analysis information, we found followings:
o By well defined domain analysis specification language it is possible to represent domain analysis information of the rental domain in various reusable domain specification units.
o By reusing domain analysis specification , it is possible to get a new domain analysis specificationin the use of a domain analysis information processor.
o In the case of insert operations of the reuse, reuse of a domain information processor method was faster and in case of delete reuse, Teamwork/SA was faster.
o Compared to the manual reuse of the DFD tool,the result of reuse approach using domain analysis specification language with domain analysis information processor is predictable and less influenced by variations of reusers' experience.
o Our finding support use of both approaches to increase the reusability of domain analysis information in comparison with domain analysis information processor or DFD tool alone.요약문 = ⅰ 약어해설 = � 제1장 서론 = 1 1.1 연구의 배경 = 1 1.2 연구의 목적 및 범위 = 4

In 2016, AlphaGo, a Artificial Intelligence developed by Google, won the game against Professional Go player Lee-SeDol. It was the first computer program to win the professional Go Player without handicap.
Now, Artificial Intelligence has been used more widely in various fields due to great development of hardware, the emergence of big data and the improvement of algorithms such as computer vision, natural language, processing, robotics and etc, all fields that have been difficult to learn. That also includes investment banking, trading and credit evaluation methods.
In this paper, we study how to build a stock price prediction model by analyzing financial data using Multi Layer Perceptron, Convolution Neural Network. Recurrent Neural Network.
The model uses data from the top 20 stocks in the KOSPI market between July 2006 and July 2016.
Stock data, KOSPI, NIKKEI , S&P500, and NASDAQ were converted into technical indicators and used for neural network learning. Since the neural network assumes that the stock shift patterns will be different for each item, we have created different neural network models.2016년, 구글에서 개발한 바둑 인공지능 알파고가 이세돌 9단과의 대국에서 승리하며, 핸디캡 없는 맞바둑으로 프로 바둑기사를 이긴 최초의 컴퓨터 프로그램으로써 인공지능 역사에 한 획을 그었다.
이제 인공지능은 하드웨어의 발달, 빅데이터의 등장, 알고리즘의 개선으로 기존에는 학습이 어려웠던 컴퓨터 비전, 자연어 처리, 로보틱스 등 다양한 분야의 문제를 해결하고 있으며, 금융권에서는 투자, 트레이딩, 신용 평가 방법에 사용되고 있다.
본 논문에서는 인공신경망 알고리즘인 신경망 알고리즘 Multi Layer Perceptron, Convolution Neural Network, Recurrent Neural Network를 사용하여 금융데이터를 분석하여 주식 가격 예측 모델을 구축 방법에 대해 연구한다.
모델은 2006년 7월부터 2016년 7월까지의 KOSPI 시가총액 상위 20위 종목의 데이터를 이용한다. 종목 데이터와 KOSPI, S&P500, NASDAQ, NIKKEI지수의 시작가, 당일 최고가, 당일 최한가, 종가, 수정 종가, 거래량을 기술적 지표로 변환하여 신경망 학습에 사용하였다. 신경망은 종목마다 주가 등락 패턴이 다를 것이라고 가정하고, 각각 다른 신경망 모델을 만들어 학습하였다.제 1 장 서론 1 1.1 연구 배경 및 목적 1 1.2 관련 연구 2 1.3 논문의 구성 4

A named entity(NE) is an important information for various natural language processing researches and services. For the development of NE recognizer(NER), the NE resources(NE dictionary and NE tagged corpus) are needed. However, it is not easy to construct a NE resources manually since human annotation is time consuming and labor-intensive. To save construction time and reduce human labor, we propose a semi-automatic system for the construction of a NE resources. The proposed system is consist of NE dictionary construction, multilingual expansion of NE dictionary and NE tagged corpus construction. In the NE dictionary construction step, the system constructs a pseudo-document with Wiki-categories per NE class by using an active learning technique. Then, it calculates similarities between Wiki entries and pseudo-documents using the BM25 model, a well-known information retrieval model. Finally, it classifies each Wiki entry into NE classes based on similarities. In experiments with three different types of NE class sets, the proposed system showed high performance(macro-average F1-score of 0.9028 and micro-average F1-score 0.9554). If NE dictionary construction is done, Multilingual expansion of NE dictionary is easy because of the characteristic of Wikipedia. In experiments with constructing English NE dictioanry from Korean NE dictionary by multilingual expansion method, the proposed system showed macro-average F1-score of 0.8789 and micro-average F1-score 0.9036. Using the NE dictionary, I construct NE tagged corpus by the distant supervision method and two heuristics. In experiments with constructing NER using generally used features, the NER is implemented with F1-score of 0.7317.개체명은 다양한 자연어처리 연구 및 서비스에 중요한 정보로 이용된다. 개체명 인식기의 개발을 위해서 개체명 자원(개체명 사전, 개체명 부착 말뭉치)은 필수적이다. 그러나 개체명 자원을 구축하는 것은 매우 시간 소모적이고, 인력 소모적인 작업이다. 이를 완화하기 위해서 본 논문에서는 개체명 자원을 반자동으로 구축하는 방법을 제안한다. 제안 방법은 개체명 사전구축, 개체명 사전의 다국어 확장, 개체명 부착 말뭉치 구축으로 구성된다. 개체명 사전 구축 시스템은 능동 학습을 이용하여 위키피디아 분류정보로 구성된 가상 문서를 개체명 범주 당 하나씩 생성한다. 그리고 잘 알려진 정보검색 모델인 BM25를 이용하여 위키피디아 엔트리와 가상문서 사이의 유사도를 계산한다. 마지막으로 유사도를 바탕으로 각 위키피디아 엔트리를 개체명 범주로 분류한다. 서로 다른 3종류의 개체명 범주 집합에서 실험한 결과, 제안 시스템은 매크로 평균 F1-점수 0.9028, 마이크로 평균 F1-점수 0.9554이라는 높은 성능을 보였다. 이렇게 구축된 개체명 사전은 위키피디아의 특성에 의해 쉽게 다국어로 확장이 가능하다. 본 논문에서는 앞서 구축된 한국어 개체명 사전으로부터 영어 개체명 사전을 자동으로 구축하는 실험을 하였다. 그 결과 매크로 평균 F1-점수 0.8789, 마이크로 평균 F1-점수 0.9036의 결과를 얻을 수 있었다. 구축된 개체명 사전을 이용하여 원거리 감독법과 간단한 휴리스틱으로 개체명 부착 말뭉치를 구축할 수 있다. 일반적인 자질을 이용한 개체명 인식기를 구현하여 실험한 결과 F1-점수 0.7317의 성능을 보이는 개체명 인식기를 구현할 수 있었다.I. 서론 1 II. 연구배경 2 1. 인공지능 응용프로그램 2 2. 지식 구축 방법 3 3. 개체명과 지식 구축 5

Named entity is a phrase that clearly identifies one item from a set of other items that have similar attributes such as person, organization, location etc. Named entity recognition is a subtask of information extraction that seeks to locate and classify elements in text into predefined categories. Named Entity Recognition is used for various departments which receives natural language inputs.
In previous work, supervised learning method is used to recognize Named entities which needs human annotating. Recently semi-supervised learning methods are used to reduce the cost of labeling which makes extract a large amount of the labeled corpus using small seed data.
In this paper, we propose the two methods which can generate named entity training corpus automatically using knowledge base. One of the methods attaches named entity labels to data using Wikipedia. The other method crawls data from web and labels named entities to web data using Freebase.
We evaluate labeled corpus generated in proposed way. we extract sentences randomly from two corpus which called Wikipedia corpus and Web corpus then label them by hand annotating. Our labeling performance shows high precision in both corpus evaluation. we also compare the performance of named entity recognizer trained by ontoNotes corpus which is labeled by human with our automatic generated labeled corpus from Wikipedia and Web. The result showed that proposed named entity recognizer adapted well with new corpus which reflects diverse sentence structures and the newest entities.개체명(Named Entity)은 인명, 기관명, 지명 등과 같이 고유명사나 일반적인 사전에 등록되지 않은 단어를 의미하며, 개체명 인식(Named Entity Recognition)은 미리 정의된 개체 범주로 텍스트의 요소를 분류하는 과정을 의미한다. 개체명 인식 과정은 현재 자연어 입력을 사용하는 다양한 응용 분야에 널리 적용되고 있다.
기존의 개체명 인식은 사람의 수작업에 의한 코퍼스를 필요로 하는 지도 학습(Supervised Learning) 기법이 적용되어 왔다. 그러나 수작업을 이용한 레이블링은 시간적, 금전적 비용이 크다. 최근에는 레이블링 비용을 줄이기 위해 소량의 seed 데이터로 다량의 학습 코퍼스를 생성하는 준지도 학습(Semi-Supervised Learning) 방법에 대한 연구가 활발히 진행 중이다.
본 논문에서는 기반으로 한 지식 베이스(Knowledge Base)에 따라 개체명 인식 코퍼스를 자동으로 생성하는 두 가지 방법을 제안한다. 첫 번째 방법은 Wikipedia(en.wikipedia.org)를 기반으로 Wikipedia 본문의 문장에 개체명을 레이블링 하여 학습 코퍼스를 생성하는 방법이다. 두 번째 방법은 Freebase(www.freebase.com)를 기반으로 Web으로부터 문장을 수집하고 개체명을 레이블링하여 학습 코퍼스를 생성한다.
두 지식 베이스를 기반으로 생성된 학습 코퍼스의 레이블링 성능은 Wikipedia 학습 코퍼스와 Web 학습 코퍼스로부터 임의의 문장을 추출하여 수작업 레이블링 한 결과로 검증하였다. 또한 각 코퍼스로 학습된 개체명 인식 모델의 성능을 검증하기 위해서 수동 레이블링된 ontoNotes와 비교하였다. 추출된 데이터로 학습된 개체명 인식 모델은 ontoNotes로 학습된 모델과 비교해 높은 precision을 보여주었다. 특히 실제 응용에서 많이 사용되는 Web 데이터 환경에서 의미 있는 성능 향상을 보여주었다.

Advances in hardware and a large number of public datasets have brought about a renaissance of machine intelligence, with successful achievements in a variety of fields, for instance, image recognition, natural language processing, audio synthesis, and future scene prediction. In biomedical data analysis, various studies have also attempted to apply machine learning to effectively analyze biomedical big data, which are massively accumulated.

On the contrary, there are several issues to be overcome to effectively apply machine learning techniques to biomedical data. The first issue is that the rationale of machine predictions must be interpretable for machine learning to be applied as a diagnostic aid. The second issue is that the size of the biomedical data for a particular disease may be insufficient to train models that require large amounts of training data, such as deep neural networks. Furthermore, the lack of annotated data for training of the model can be considered as another issue. In fact, patient data accumulate over time, but the ground-truth data are insufficient to train machine learning-based models. In the case of biomedical data, it is very difficult to obtain ground-truth data because the efforts of a domain expert such as a medical doctor are inevitable. Finally, when vast amounts of biomedical data are analyzed, such as the human genome, the input/output (IO) pattern of the analysis tool can act as a bottleneck and affect the overall analysis time.

In this dissertation, we present the proposed approaches in four chapters to address each issue.
First, we describe a pyramid gradient-based class activation mapping (PG-CAM) technique that allows users to understand the rationale behind the prediction results when using a deep learning-based model as a diagnostic aid. Second, we introduce a method to successfully train a deep learning-based model and improve the robustness of the model in the case of lack of training data. To address the third issue, we propose a novel lesion detection method through a weakly supervised approach. Lastly, we perform an in-depth profiling of 23 bioinformatics applications and its IO pattern analysis through hierarchical clustering to discover IO patterns that can accelerate massive biomedical data analysis in storage devices. This dissertation proposes a variety of machine learning-based analysis and acceleration methodologies for an effective analysis of biomedical data.하드웨어의 발전과 방대한 크기의 데이터셋의 공개로 인공지능 분야는 황금기에 접어들었다. 인공지능에 기반한 연구들은 오랜 시간 답보 상태에 있던 영상 인식, 객체 검출, 자연어 처리, 기계 번역 및 자율주행 자동차와 같은 다양한 분야에서 성공적인 결과를 보여주었다. 생체 의료 데이터 분석 분야에서도 방대하게 축적되는 생체 의료 빅데이터를 효과적으로 분석하기 위해 기계 학습을 적용하려는 다양한 연구가 시도되고 있다.

한편, 생체 의료 데이터에 기계학습 기법을 효과적으로 적용하기 위해서는 극복해야만 하는 몇 가지 이슈가 존재한다. 첫번째 이슈는 기계 학습 기법이 의료 현장에서 진단 보조도구로 적용되려면 기계의 예측 결과와 그에 대한 추정 근거가 해석이 가능해야 한다는 것이다. 두번째 이슈로 특정 질환에 대한 생체 데이터의 크기가 딥러닝과 같은 대량의 학습데이터를 요구하는 기계학습 모델의 학습에는 부족할 수 있다는 것이다. 더 나아가, 모델의 학습을 위한 그라운드 트루스 데이터의 부족도 또 하나의 이슈로 여길 수 있다. 생체 의료 데이터의 경우 그라운드 트루스 데이터를 생성하기 위해서는 의사를 비롯한 전문가의 노력이 불가피하여 이를 확보하기란 매우 어려운 일이기 때문이다. 마지막으로 인간 유전체와 같이 방대한 양의 생체 데이터를 분석해야 하는 경우 분석 도구의 입출력 패턴이 장애물로 작용하여 전체적인 분석 시간에 영향을 줄 수 있다는 점이다.

본 학위 논문에서는 각각의 이슈들을 해결하기 위해 제안한 접근법들을 4개의 챕터에 걸쳐 제시한다. 첫번째로는 딥러닝 기반의 모델을 진단 보조도구로 사용하였을 때, 사용자가 모델의 판단 근거를 시각적으로 피드백 받을 수 있도록 하는 pyramid Grad-CAM을 제안하였다. 두번째로는 학습 데이터가 부족한 상황에서 딥러닝 모델을 성공적으로 학습시키고, 모델의 강인함을 향상시키기 위한 방법을 소개한다. 학습데이터 부족을 극복하기 위하여 가우시안 노이즈 기반의 왜곡을 활용한 데이터 증강 기법을 사용하였으며, 학습된 모델을 보조할 수 있는 신호 처리 기법 기반의 방법론을 상보적으로 융합하였다. 세번째 이슈를 극복하기 위해서 약한 지도 학습법에 기반한 새로운 병변 검출 기법을 소개한다. 마지막으로는 방대한 생체 의료 데이터 분석 기법을 저장장치 단에서 가속화 할 수 있는 입출력 패턴을 발견하기 위하여, 23개의 생물정보학 어플리케이션에 대한 심층적인 프로파일링과 계층적 군집화 기법을 통한 입출력 패턴 분석을 수행하였다. 본 학위 논문에서는 이와 같이 생체 의료 데이터의 효과적인 분석을 위한 다양한 기계 학습 기반의 분석 기법과 가속화 방안을 제안하였다.Abstract iii List of Figures x List of Tables xi 1 Introduction 1 2 Background 8

Abstract


Construction of Record Retrieval System based on Topic Map

Kwon, Chang-ho
Department of Archival Science
The Graduate School of Archival Science
Myongji University

Recently distribution of record via internet and coefficient of utilization are increase. Archival information service using website becomes essential part of record center. The main point of archival information service by website is making search easy. Matching user request and representation of record resources is basic mechanism of retrieval system and accuracy of matching is required. Archivist and record manager use various information representation tools from taxonomy to recent thesaurus, still, the accuracy of information retrieval has not solved. Nowadays ontology is remarkable that originated in natural language processing of AI because of the fundamental limit of matching user request and information resources. Topic map, language for ontology, is proposed as solution to retrieve unstructured large-sized information by finding, connecting, and organizing relevancies between information resources. This study builds retrieval environment that records are retrieved accurately and promptly in the topic map system which are based on semantic relation.
The target user of the system is general internet users and its range is limited to the president related sources in the National Archives Portal Service. The procedure is as follows; 1) Design an ontology model for archival information service based on topic map which focusing on description metadata of the records. 2) Buildpractical record retrieval system with topic map that received information source list, which extracted from the National Archives Portal Service,by editor. 3) Check and assess features of record retrieval system based on topic map through user interface.
Through the practice, relevance navigation to other record sources by semantic inference of description metadata is confirmed. And also, records could be built up as knowledge with result of scattered archival sources.최근, 인터넷을 이용한 기록정보의 유통량 증가하고, 정보적 활용 가치가 제고되어 웹사이트를 이용한 기록정보서비스가 기록관의 중요업무로 부각되고 있다. 웹사이트를 이용한 기록정보서비스의 핵심은 이용자가 원하는 기록정보의 검색을 용이하게 하는데 있다. 검색을 용이하게 하기 위해서는 검색시스템의 기본 메커니즘인 이용자질의와 기록자원표현의 매칭의 정확성이 요구된다. 이를 위해 기록관리전문요원(archivist)과 문서관리자(record manager)들은 텍사노미(Texonomy)에서 최근의 시소러스까지 다양한 정보표현 도구를 이용하고 있지만, 검색의 정확성 문제는 해결되지 않고 있다. 웹상의 기록정보의 양이 급격히 증가함에 따라, 이용자 질의와 정보자원과의 매칭의 문제는 심화되고 있어, 최근에는 인공지능(AI)분야의 자연어처리 기술에서 유래한 의미기반 언어인 온톨로지가 해결방안으로 새롭게 주목받고 있다. 온톨로지의 기술표현 언어인 토픽맵은 정보 자원간의 상호 연관성을 도출, 연결 및 조직하여 대용량의 구조화되지 않은 정보를 효율적으로 검색하기 위한 해결 방안으로 제시되고 있다. 이에 본 연구에는 의미관계를 기반으로 한 토픽맵 기반의 검색시스템으로 설계하고, 구축하여 이용자들이 보다 정확하고 신속하게 기록물을 검색할 수 있는 검색환경조성을 마련해 보고자 한다.
시스템 구축은 인터넷을 이용한 일반이용자를 대상으로 하고, 구축의 범위는 국가기록포탈의 대통령관련 자원의 일부로 제한하였으며, 구축단계는 다음과 같다. 1)기록물의 기술정보 메타데이타를 중심으로 한 토픽맵 기반의 기록정보서비스를 위한 온톨로지 모델을 설계한다. 2)설계한 온톨로지 모델을 바탕으로 국가기록포탈에서 추출한 정보자원리스트를 에디터를 이용해 토픽맵으로 반입하고, 웹프레임워크를 통해 기록물검색시스템으로 구현한다.3) 구축된 검색시스템의 사용자 인터페이스에서 테스트질의를 통해 토픽맵기반 기록물검색시스템의 특징을 확인하고 구축의 의미를 평가한다.
구축한 토픽맵을 테스트 질의하여 기록물건의 기술정보메타데타의 의미적 추론에 의한 다른 기록자원으로의 연관 네비게이션 검색을 확인할 수 있었고, 분산된 기록정보자원간의 연계된 결과값을 통해 기록의 지식화도 도모할 수 있었다.제 1장 서론 1 1.1 연구의 배경 및 목적 1 1.2 사례 및 선행연구 7 1.3. 연구방법 및 한계 10

Chapter 1. Introduction 1 1. 1. Background of a research 1 1. 2. Objectives and Necessity of a research 7 1. 3. Contents and Scope of a research 12 Chapter 2. The Theoretical Background 14

Throughout the thesis, we study state complexity for regular languages and reduction of automata. State complexity is one of the most intensively studied topics in automata and formal language theory in recent years. The size of FAs used in those applications increases steadily. State complexity is a measurement of a FA size by the number of states in the FA. Since measuring the size of FAs become more and more important, state complexity also becomes important in automata theory. For instance the estimated upper bound of the state complexity is useful in practice since it may help to manage resource efficiently. Moreover, it is a challenging quest to verify whether or not an estimated upper bound can be reached. The state complexity of an operation for regular languages is defined as the number of states that are necessary and sufficient in the worst case for the minimal DFA to accept the language resulting from the operation, considered as a function of the state complexities of operands. We focus on the state complexity of prefix-free regular languages and non-returning regular languages. First, we consider the state complexity of prefix-free regular languages. We compute state complexity of several combined operations such as k-union, k-intersection, boundary languages for prefix-free regular langauges. We also consider the state complexity of some basic operations on non-returning regular languages. And we compute the state complexity of some combined operations on suffix-free regular languages.
It becomes more and more important to reduce nondeterministic finite-state automata (NFAs) as NFAs in practice get larger and larger. However, since it is PSPACE-complete to obtain a minimal NFA, instead of computing a minimal NFA, we design an efficient algorithm that reduces the number of states of an NFA. Our algorithm keeps merging equivalent states until there are no more equivalent states by relying on the fast equivalence test and the computation tree. We improve the practical run-time of our algorithm using logical rules and the structure of computation trees, which enables us to skip many computations of the state equivalence checking for merging two states. We demonstrate the efficiency of our algorithm by experiments with respect to the number of reduced states and the run-time. In addition, we study the equivalence test between two nondeterministic weighted automata (NWAs) and reducing NWAs.결정적 유한 오토마타(deterministic finite automata), 비결정적 유한 오토마타(nondeterministic finite automata), 가중 오토마타(weighted automata), 트랜드듀서(transducer) 등의 유한 오토마타는 자연어 처리(Natural Language rocessing), 음성 인식(Speech recognition) 등의 다양한 분야에서 널리 사용되고 있다. 유한 오토마타를 사용하는 어플리케이션의 크기는 점점 더 커지고 있고, 이에 따라 사용되는 오토마타의 크기 역시 점점 더 커지고 있다. 우리는 본 졸업 논문에서 오토마타 간의 연산이 발생하였을 때 생기는 유한 오토마타의 크기를 예측하는 문제를 연구한다. 또한 오토마타에서 동치 상태(state)들을 찾아 합함으로써 오토마타의 크기를 줄이는 문제를 연구한다. 우리는 먼저 유한 오토마타에서 발생할 수 있는 여러 연산이 일어났을 때 생기는 유한 오토마타의 크기를 예측하는 연구를 한다. 상태 복잡도(State Complexity)는 주어진 오토마타의 상태 개수를 나타내는 수치로써, 오토마타의 크기를 나타내는 데에 많이 사용되고 있다. 우리는 연산 후 발생하는 오토마타의 상태 개수를 예측하는 연구를 한다. 우리는 크게 두 가지 특성을 가진 정규 언어(regular language)에 대하여 고려하는데, 첫째는 전위부자유 정규 언어(prefix-free regular language)이고, 둘째는 논리터닝 정규 언어(non-returning regular language)이다. 어떤 정규 언어 L에 있는 문자열(string)들 중에 어떤 한 문자열이 다른 문자열의 앞부분에 해당하는 것이 없을 경우, 우리는 이런 언어를 전위부자유 정규 언어라고 부른다. 논리터닝 정규 언어의 경우, 정규 언어를 나타내는 결정적 유한 오토마타의 시작 스테이트(start state)로 향하는 전이(transition)이 없을 경우, 즉 시작 스테이트에서 출발한 후에는 절대로 시작 스테이트로 돌아올 수가 없는 경우를 말한다. 전위부자유 정규언어와 논리터징 정규 언어 모두 특정한 성질을 가지고 있으며, 우리는 이러한 특정 성질까지 고려하여 상태 복잡도를 계산한다. 우리는 연산자 하나로 이루어진 단순 연산 뿐아니라, 연산자 여러 개를 하나의 연산자로 인식하는 복합 연산자(combined operation)에 대하여서도 상태 복잡도를 계산한다.
그 다음에 고려하는 문제는 유한 오토마타의 상태를 줄이는 연구이다. 주어진 유한 오토마타에서 동등한 역할을 하는 상태들을 찾아 하나로 합침으로써 오토마타의 상태 개수를 줄이는 연구를 한다. 우리는 비결정적 유한 오토마타와 가중 오토마타의 상태를 줄이는 연구를 한다. 먼저 비결정적 유한 오토마타에서 두 상태가 동치인지를 테스트하는 효율적인 알고리즘을 설계한 후 이를 모든 상태 쌍에 적용하여 모든 상태 쌍들에 대한 동치 관계를 알아낸다. 그리고 동치인 상태를 하나로 합하는 연구를 한다. 그 다음에 이러한 알고리즘을 가중 오토마타로 확장한다. 가중 오토마타는 비결정적 유한 오토마타에서 전이마다 가중치(weight)를 가지고 있기 때문에 이러한 것을 더욱더 고려해야한다. 우리는 가중치를 고려할 수 있는 새로운 알고리즘을 고안한다.

For a long time, digital image processing algorithms have been adopting filters and masks to perform images feature extraction and transformations. These filters and masks were carefully computed using mathematical methods through each pixel of the image and return the features that extracted out of the input image. One drawback of conventional image processing using prepared filters and masks is not generalized enough to apply universally to all images. For many tasks, it is difficult to decide what kernels should be used as well as what features should be extracted. The accuracy of the image processing procedure depends heavily on which filters and methods it used during the process. Moreover, such filters and masks cannot be learned automatically.
Recently, the development of computer power and the availability of graphic processing units(GPUs) enable neural networks to be deployed widely particularly in an image and visual processing problems and overcome the limitation of computation complexity that neural networks faced in the past. The term “deep learning” has been introduced, which is a branch of neural networks. It has acquired its reputation for successfully applying to fields including computer vision, speech recognition, natural language processing, bioinformatics.
The learning process of deep learning can be supervised, semi-supervised or unsupervised. So far, the most striking achievement in deep learning has involved discriminative models, with the remarkable success of convolutional neural networks (CNNs). The disadvantage of the discriminative learning process is it relies on labeled data where that information is expensive to collect. While unsupervised and generative learning does not have such disadvantages, but it tends to be less accurate. Deep generative models have earned less influence since it is difficult to approximate intractable probabilistic computation that arises in maximum likelihood estimation and related strategies. The most common representation of generative models is an autoencoder.오랜 시간 동안 디지털 이미지 프로세싱 알고리즘은 이미지 특징 추출 및 변환을 수행하기 위해 필터와 마스크를 채택해 왔다. 필터와 마스크는 이미지의 픽셀을 수학적 방법을 통해 계산되어 출력값으로 반환한다. 전통적인 이미지 처리의 단점은 모든 이미지에 보편적으로 적용할 수 있는 일반화된 필터가 존재하지 않는다는 점이다. 그 이유는 이미지에서 어떤 특징을 추출해야 하는지, 어떤 커널을 사용하는지 결정하는 것이 어렵기 때문이다. 이미지 처리에서 정확성은 사용된 필터 및 방법에 크게 의존하기 때문에 이러한 필터와 마스크는 자동으로 학습될 수 없다.
최근 컴퓨터 성능의 발전과 그래픽 처리장치 (GPU)의 가용으로 인해 과거 신경 네트워크 (Neural Network)에서 직면한 계산의 복잡성 한계를 극복할 수 있으며, 그로 인해 이미지 및 시각 처리 문제에 광범위하게 적용 될 수 있었다. 심층 학습 (Deep Learning)이라는 용어는 신경 네트워크에서 파생되었으며, 컴퓨터 비전과 음성인식, 자연어처리, 생물정보학 등의 분야에 성공적으로 적용되어 명성을 얻었다.
심층 학습 분야는 감독학습과 비감독학습, 준감독학습으로 분류할 수 있다. 지금까지 가장 두드러진 성과는 CNN (Convolutional Neural Network)과 같은 분류 모델 (Discriminative Models)에서 주로 이루어졌다. 분류학습 모델의 단점은 분류된 데이터를 의존한다는 것인데, 그 이유는 데이터를 분류하는데 비용이 많이 들기 때문이다. 비감독학습으로 분류되는 생성 모델 (Generative Models)에서는 분류 모델에서 발견되는 단점은 없지만 정확도가 분류 모델에 비해 높지 않다는 단점이 있다. 심층 생성 모델 (Deep Generative Models)은 최대 기능도 추정 (Maximum Likelihood Estimation) 및 다른 전략을 이용하여 확률 분포를 근사화하는 것이 어렵기 때문에 그로 인해 심층 학습 분야에 크게 영향을 주지 못했다. 생성 모델의 대표적인 모델은 Auto-Encoder이다.Chapter 1 Introduction 1 Chapter 2 Preliminaries 3 2.1 Dark Channel Prior 3 2.2 Convolutional Neural Network Based Architecture 4 2.3 Image Deconvolution 6



2008년 금융위기 이후 신자유주의에 대한 반발이 커지고, 2009년 용산참사 후 성장에 대한 회의와 복지에 대한 요구가 높아졌다. 이에 따라 경제민주화 등 사회가치를 추구하는 경향이 제도적으로도 강해졌다. 하지만 이 연구는 금융위기나 용산참사로 인한 시장가치의 위기와 사회가치의 부상이 오히려 시장 자유주의 통치성이 금융통치성의 대두를 통해 강화되는 계기가 됐다고 주장한다. 즉 시장가치를 중시한 신자유주의적 태도는 물론 서민 복지를 지향하는 담론조차 금융화(financialization)의 통치성(governmentality)에 포섭되고 있음을 대규모 서민주택담론분석을 통해 밝히고자 한다.
먼저 이론적으로는 푸코의 통치성연구(governmentality studies)를 재해석하여 시장 자유주의 통치성의 현대적 형태로서 금융화와 행동경제학(behavioral economics)에 따른 금융통치성(financial governmentality)의 개념을 도출했다. 시장 자유주의 통치성(market liberal governmentality)은 시장인의 이기적인 본성에 따라 시장메커니즘을 우선시하여 시장과 사회를 통치하는 합리성을 뜻한다. 금융통치성은 기존의 신자유주의(neoliberalism) 통치성이나 푸코가 분석한 시장 자유주의 통치성 의 미국모형이 심화된 형태이다. 금융통치성이란 시장 자유주의 통치성의 하나로 금융메커니즘에 따라 주체와 시장과 사회를 구성하고 개입하는 합리성이다. 1차적으로는 시장•사회•정부의 금융화에 따른 시장•주체•사회의 재구성을 의미하며, 2차적으로는 이론적으로 신고전학파 경제학이 아닌 행동경제학을 중심으로 시장주체를 교환•경쟁•축적의 주체에서 선호(preference)의 주체로, 시장을 효율시장(efficient market)에서 심리시장(psychological market)으로, 사회를 기업사회와 시민사회(civil society)에서 금융사회와 대중사회(mass society)로, 주체•기업•국가•사회를 시장에 접목하는 개입술로는 신용평가(credit rating) 또는 등급화(grading)를 활용하는 통치합리성을 의미한다. 한국은 금융통치성이 대두하는 단계로 1차적 의미에서 시장이 산업자본 중심에서 금융자본 중심으로, 간접금융 중심에서 직접금융을 적극 활용하는 방향으로 발전하는 추세에 맞춰 주체와 사회를 재구성하고 정부의 한계를 극복하려는 시도가 엿보인다. 이는 담론 수준에서 시장담론과 사회담론 간 대립 속에서 금융통치성의 자원으로서 금융담론의 타당성이 높아지는 담론의 금융화(financialization of discourse)로 나타난다.
분석대상을 살펴보면, 한국언론진흥재단 뉴스아카이브인 카인즈(KINDS)에서 제공되는 6년치(2008~2013년) 서민주택 관련 기사를 대상으로 분석했다. 서민주택은 생필품이자 자산으로서 사회적 측면과 시장적 측면을 모두 갖고 있다는 점에서 금융통치성연구의 적절한 대상이 된다. 특히 분석기간 중에 통치성의 산업적사회적금융적 측면을 잘 보여주는 소재로, 도시재개발사업인 뉴타운(New Town) 사업, 부동산간접투자상품인 주택리츠(real estate investment trusts, REITs), 주택프로젝트금융(project finance, PF), 보금자리론에 대한 기사를 분석했다.
효율적인 분석을 위해 뉴스정보원연결망분석(news source network analysis, NSNA)의 알고리즘을 비롯하여 빅데이터분석(big data analytics), 자연어처리(natural language analytics), 사회연결망분석(social network analysis), 시각화(visualization) 등의 기술을 활용한 컴퓨터 보조 질적 데이터분석 소프트웨어(computer assisted qualitative data analysis software, CAQDAS)로서 뉴스소스(NewsSource) 프로그램을 개발하여 활용했다. 뉴스소스는 정보원과 인용문과 기사를 체계적으로 선별한 뒤, 주요 개인실명정보원의 인용문을 중심으로 담론의 전개를 살펴보는데 도움을 주었다.
연구문제는 다음과 같다. 첫째, 2008년부터 2013년까지 NSNA를 통해 본 서민주택담론으로서 뉴타운담론과 주택금융담론(주택PF담론, 주택리츠담론, 보금자리론담론)의 주요정보원은 연도별, 소속별로 누구이며 그 비중은 어떻게 변화했는가?
둘째, 분석기간 중 서민주택담론으로서 뉴타운담론은 어떻게 변화했는가? 즉 뉴타운산업담론과 뉴타운사회담론은 어떻게 서로 대립하며 성장하고 위축되고 수렴했는가?
셋째, 분석기간 중 서민주택담론으로서 주택금융담론은 어떻게 변화했는가? 즉 금융위기담론과 심리순환담론은 금융담론에 어떤 영향을 주었으며 간접금융 중심의 담론으로서 주택PF담론, 주택담보대출담론, 주택청약담론과 직접금융 중심의 주택리츠담론, 보금자리론담론은 어떻게 위축되고 성장했는가? 특히 분석기간 중 산업담론과 사회담론의 대립 속에서 금융담론은 어떻게 이들 담론과 접합되면서 성장했는가?
넷째, 이상의 논의에 따라 한국 서민주택에 나타난 금융통치성은 시장, 주체, 사회를 어떻게 재구성하는가?
분석결과는 다음과 같다. 첫째, 뉴타운담론의 경우 컨설턴트 산업자본주의라고 부를 수 있을 만큼 국회의원 등 입법부 정보원과 컨설턴트가 중시됐다. 반면 주택금융담론에서는 엘리트 금융자본주의라고 부를 만한 현상, 즉 장관이나 시장 등 행정부 정보원과 금융인이 중요하게 인용됐다.
둘째, 정보원 비중을 통해 산업담론, 사회담론, 금융담론의 흐름을 근사적으로 살펴보고, 대중매체담론분석을 통해 재검토한 바에 따르면, 뉴타운산업담론은 2008년 총선 당시 패권화되었다가 2009년 용산참사 이후 급속도로 위축됐고, 뉴타운사회담론은 용산참사 이후 활성화되어 뉴타운출구전략담론으로 발전했다. 뉴타운사회담론은 뉴타운담론 전반을 위축시켰고, 뉴타운산업담론과 뉴타운사회담론 모두 정부의 재정투입담론으로 수렴했으며, 결국 재정부족 논란으로 한계에 직면한다.
셋째, 금융위기담론과 시장심리위기론으로 시장 중심의 주택금융담론은 위기를 맞는다. 특히 서민주택 분양을 중심으로 하는 주택PF담론이나 주택 직접 소유를 중시하는 주택담보대출담론처럼, 산업과 관련성이 높은 간접금융담론은 위기에 봉착한다. 반면 주택리츠담론이나 보금자리론담론처럼 직접금융담론은 서민주택 관련 사회문제 해결에 활용될 수 있다는 점이 부각되면서 정부 주도의 직접금융 육성책이 정당화된다.
전체적으로 보면 뉴타운산업담론과 뉴타운사회담론의 대립 속에 뉴타운담론 자체가 재정담론으로 수렴하면서 위축되고, 뉴타운산업담론과 결부된 간접금융담론 역시 위축된다. 그리고 재정부족의 해법이자 간접금융의 대안인 동시에 사회문제의 해결로서 직접금융을 활용하자는 주장의 타당성이 강화되는 담론의 금융화가 나타난다. 담론의 금융화는 크게 두 가지 역설적 과정을 거쳐 나타난다. 우선 금융위기담론에서 촉발되어 뉴타운담론, 시장심리위기담론, 주택청약담론, 간접금융담론에서 보이는 산업담론과 금융담론의 타당성 위기는 오히려 금융통치성이 사회담론과 적극적으로 접합하여 직접금융담론으로 고도화될 수 있는 계기를 제공했다. 다음으로 사회담론은 시장담론인 산업담론과 대립하면서 시장담론을 위축시키는 듯 보이지만, 또다른 시장담론인 금융담론에 접합되면서 금융담론이 간접금융담론에서 직접금융담론으로 고도화되는 발판이 되고 있다.
넷째, 이상의 논의를 금융통치성의 대두에 따른 담론구성체로서 시장, 주체, 사회의 재구성 측면에서 기술하면 다음과 같다. 한국 서민주택 통치에서 금융통치성 대두는 아직 초기단계이지만, 서민주택 및 서민주택담론의 금융화와 함께 금융통치성에 의한 시장•주체•사회를 재구성하려는 시도가 엿보인다.
우선 시장 측면에서 서민주택 관련 직접금융시장을 육성해야 한다는 주장이 힘을 얻는다. 즉 산업자본과 연관성이 큰 주택PF는 유명무실해지고, 금융자본을 끌어들이는 주택리츠 활성화가 대안으로 제시된다. 보금자리론처럼 주택저당증권으로 유동화하여 직접금융시장에서 주택담보대출의 자금을 조달하는 방안이 모색된다.
주체 측면에서는 서민과 자산가가 직접금융 방식에 맞는 형태로 재구성된다. 뉴타운사업은 무주택 서민을 1가구 1주택자로 전환하는데 목표를 두고 있다. 또 뉴타운담론에서 자산가는 다수의 주택을 직접 소유한 다주택자로 그려진다. 그러나 직접금융담론에서 서민은 세입자로, 자산가는 주택리츠 등을 통해 주택을 간접 소유하는 금융투자자로 재구성된다. 시장에 세입자가 늘면서 장기적으로 임대료가 오르고, 그 결과 주택리츠 등의 수익성이 장기적으로 개선된다. 자산가는 주택을 직접 매입하는 산업자본가에서 주택 관련 금융상품에 투자하는 금융자본가가 된다.
끝으로 사회 측면에서 1주택자가 아닌 세입자를 위한 사회를 만들기 위한 노력이 강화된다. 이러한 노력은 사회적 가치를 지향하고 복지를 확대하는 의도를 갖고 있을지는 모르지만, 그 효과 측면에서는 직접금융시장이 발달한 사회를 뒷받침한다.
이 연구는 국내 언론학에서 금융의 문제를 본격적으로 다루었다는 점에서 의의를 갖는다. 이론적으로는 푸코 통치성연구를 체계화하고 발전시켜 금융통치성의 개념을 제안하고 이를 한국 현실에 분석적으로 적용해보았다. 방법론적으로는 NSNA를 제안하고 이를 바탕으로 한 프로그램을 직접 개발하여 한국언론진흥재단 미디어가온(www.mediagaon.or.kr)에 일반 공개했으며, 구체적인 절차를 명시하고 이를 바탕으로 처음으로 대규모 담론분석을 실시하여, CAQDAS로서의 유용성을 확인했다.제 1장 서론 1 1절 문제제기 1 2절 논문의 구성 10 제 2장 선행연구검토 12 1절 푸코의 통치성 연구 12

본 연구의 목적은 영어 듣기 능력 향상을 위한 방안으로써, 원어민이 가지는 '영어식 사고 방식'의 습득에 그 주안점을 두고 빠른 속도의 읽기가 가능하면 빠른 속도의 듣기도 가능하게 할 것이라는 가정 하에 그 효과를 알아보는데 있다.
듣기 능력의 향상을 위해서 개발 연구된 학습 방법은 여러 가지가 있지만 그 중에서도 직청 직해를 위한 직독 직해의 학습법의 개발이 필요하며, <말하는 속도>가 <읽으며 이해하는 속도>보다 훨씬 빠르기 때문에, 영어를 모국어로 하지 않는 사람들이 영어로 말하는 것을 알아듣기 위해서는 우선 그 내용을 <읽으며 이해하는 속도>가 <말하는 속도>를 능가해야 한다. 따라서 1분당 최소란 150단어(150wpm)이상의 속도로 쏟아져 나오는 영어 문장을 이해하기 위해서는 150wpm 이상의 속도로 문장을 이해할 수 있는 능력을 갖추면 가능하리라는 가정 하에 한국 고교생들의 평균 영문 독해력 속도1)-75wpm과 이해도2) 45%(정철 English Revolution. 2OO3)라는 문장 이해력수준을 150wpm 이상으로 향상시킨다면 일반적인 영어 문장 듣기에서 그 효율성을 증대시킬 수 있을 것이라는 가설에서 출발하기로 한다.
연구의 대상으로는 김해시내 인문계 고등학교 1학년을 대상으로 하여 비교반과 실험반으로 나눈 뒤에 사전 영어 듣기 능력 검사를 실시하여 이를 기준으로 하고, 실험반을 대상으로 사전 독해력 속도와 이해도를 알아 본 뒤 2002년 9월에서 2003년 5월에 걸쳐 속독 훈련 향상 프로그램을 자체 개발하여 적용 실험해 보기로 한다.
연구의 방법으로는 속독력 향상을 꾀하기 위한 학습 방법으로서 chunking 개념 익히기, 끊어읽기, 단어군 속독 연습, 훑어읽기(skimming) 등 다양한 독해 학습 방법을 이용하여 지루함을 피하고, 교재의 내용도 실험 대상자들의 흥미를 유발시킬 수 있는 재미있고 유익한 내용들을 선정하여 흥미로운 독해 훈련이 되도록 하였다.
그 결과 실험 초 독해 속도 평균 56wpm, 이해도 32% 정도의 독해력 수준과 사전 영어 듣기 능력 평가에서 20점 만점에 평균 9.75 (비교반 평균 9.40)정도의 점수를 보이던 학생들이 12주간의 독해 속도 향상 프로그램을 거친 뒤에는 독해 속도 평균 128wpm, 이해도 43%정도의 수준까지 향상되었다. 이는 실험 초에 목표로 했던 150wpm에는 미치지는 못하지만 실험기간이 짧고, 실험 대상자들의 수준이 중하위권의 고등학교 1학년 학생들이라는 점을 감안한다면 그리고 그들의 실험 초 독해 속도와 비교해 볼 때 무려 2배 이상의 향상을 보인 점을 고려한다면 짧은 기간이지만 체계적인 지도를 통하여 독해력을 증진시킬 수 있음을 보여준다. 또한 사후 영어 듣기 능력 평가 결과에서도 실험반 평균 12.80, 비교반 평균 10.67을 나타내어 독해 속도의 증진이 듣기 능력의 향상에도 효과가 있음을 입증하고 있다.
위의 연구 결과로 살펴볼 때 듣기 능력 향상을 위한 다양한 학습 방법들이 있지만, 자연어 습득 과정의 유아기도 아니며 언어습득장치가 아직 사라지지 않은 저 연령층의 어린이가 아닌 일반 중고교생 이상의 학습자들이 영어 문장을 듣고 쉽게 이해하기 위해서는 영어 문장을 무조건 듣기만 하는 것보다는 영어 문장에 대한 구조적, 통사적 이해를 기반으로 하여 빠른 속도의 직독 직해(直獨直解)를 해내고 영어식 사고 방식을 익힌다면 일반적인 속도로 들려오는 영어 문장의 직청 직해(直請直解)가 가능하리라고 생각된다.Language learning includes four skills such as listening, speaking, reading, writing. Among them Listening comprehension has been considered as more important part than any others. For it can affect other skills and improve them.
In this study, improving Listening comprehension was examined through rapid reading drill. It is considered that people can only hear what he can understand. And 'speaking speed' is more rapid than 'Reading speed'. So this study has the hypothesis that if one can read and understand more than 150 words in a minute, he would be able to hear and understand flowing sentences whose speed is more than 150 words in a minute.
The experiment to improve listening comprehension ability through rapid reading was carried on two groups, the experimental group and the control group. The experimental group was trained by exercising eye movement and learning chunking, skimming, meaningful words groups, etc. On the other hand, the control group was taught by traditional teaching method, grammar-translation method.
As the result of this study, the listening ability of the experimental group was improved more than twice than at the beginning of this study and they understood long paragraph easily. Students could hear and understand meaningfully on the basis of structural and syntactic understanding.
If students can achieve thinking in English and read more rapidly, their listening comprehension will be improved.
In conclusion, to improve the ability of listening comprehension for students whose LAD are no more at work, there must be need of reading a lot, especially rapid reading.목차 = ⅲ Ⅰ. 서론 = 1 1. 연구의 필요성 = 1 2. 연구의 목적 = 3 3. 연구의 가설 = 4

본 논문은 프랑스어와 한국어의 감탄에 관한 의미론적 고찰이다. 우선 감탄의 형태통사론적 특징과 문장 층위의 담화 층위에서의 감탄의미들을 검토하여 감탄의 정의를 제시하고 그로부터 얻어진 감탄발화체(enonce exclamatif)들을 진리조건적(veri-conditionnelle) 접근 속에서 R. Martin의 Univers de croyance이론에 근거하여 형식화를 시도한다.
전통문법을 비롯한 대개의 감탄문 연구들은, 감탄의 본질로. 화자의 감정표현이라는 정서적 측면(aspect affectif)만을 주목하여 감탄의 정의를 규명하기 때문에 많은 혼선을 유발시킨다. 그래서 의문(interrogation)과 부정(negation) 등이 의미논리적 관찰 대상이 되는 반면, 감탄은 일찍부터 제외되었다. 그러나 이같은 연구들이 갖고 있는 문제점들로부터 감탄의 정의를 재검토해 보면 감탄이라는 의미가치가 진리조건적으로 접근가능할 뿐 아니라 다양한 유형의 감탄발화들이 일정언어이론 속에서 형식화까지 될 수 있다는 것을 알 수 있다.
제 1장에서는 한국어와 프랑스어의 비교 연구를 위해, 출발어(langue de depart)인 프랑스어의 감탄발화체에서 나타나는 특징들을 살펴서 감탄 정의를 규명한다. 즉, 감탄이라는 의미가치는 화자의 단어의 힘(force assertive)과 모순된 긴장(tension contradictoire)이라는 두 요소로 구성된다고 정의한다. 이처럼 감탄은 이 두 요소가 동시에 작용하여 이뤄지는 것이다. 그리고 이 두 요소는 전형적인 감탄문,
예문 (111)
a) Comme c'est agreable!
b) Que je suis vieux!
c) Ce que I'on respire mal ici!
d) Combien vous avez du souffir!
e) Qu'est-ce qu'il est interessant!
f) Quel visage!
뿐 아니라 형태통사적으로는 의문문 구조를 취하나 더 이상 순수의문의 의미가치를 갖지 못한 발화체들, 즉, 전통적 수사의문문 구조 발화체,
예문(113)
a) Qui le connait mieux que toi!
b) Qui ne souscrirait a cela!
c) Qu'ai-je a faire maintenant de cette lonhue confession!
감탄의문문 구조 발화체,
예문 (115)
a) Qu'est-ce que je te disais!
b) A qui le dites-vous!
c) A quoi en sommes-nous reduits desormais!
강조 수사의문문 구조 발화체,
예문 (117)
a) Combien de fois te l'ai-je dit!
b) Combien ai-je deja depense pour toi!
c) A quel prix l'a-t-il emporte!
들이 어떻게 감탄의 의미가치를 갖게되는 지도 설명 가능케 한다. 후자의 유형들은 의문의 중화현상(neutralisation de l'interrogation)의 문제로 형태통사적 의문구조 속에서 더 이상 의문의 의미가치가 작용을 하지 못하는 것이다.
이와 같은 고찰은 제 2 장에서 감탄을 진리조건적 관점에서 설명하는데 뒷받침이 된다. 다시 말해, 감탄도 의미논리적 고찰 대상이 될 수 있는 것이다. 본 논문은 Univers de croyance 이론을 통해 형식화를 시도하는데, 감탄발화체와 부정형 단언발화체를 구별하기 위해 temps de dicto라는 개념을 사용한다. 이는 자연어의 진리치는, 전통 논리에서의 진리치와 달리, 시간의 흐름 속에서 계속 동요한다는 것을 잘 설명해줄 수 있는 개념으로, 감탄의 모순적 긴장과 부정의 모순을 구별할 수 있는 것도 바로 이 temps de dicto 라는 시간축에 달려 있는 것이다.
제3장에서는 앞서 살펴 본 프랑스어의 감탄발화체들에 대한 연구를 근간으로, 비교 연구의 도착어(langue d'arrive)로서의 한국어 감탄발화체들을 분석한다. 특히 한국어의 경우, 감탄보어문에 대한 연구를 제외하면 감탄발화체에 대한 통사의미론적 연구가 거의 없는 상황임을 고려할 때, 본 논문의 한국어 연구는 큰 의의를 찾을 수 있다. 한국어와 프랑스어 사이에 어원적으로 큰 차이가 있고, 각각 언어의 감탄 연구 동향이 다름에도 불구하고 앞서 규명한 논리의미론적 감탄 정의에 근거하여 우리는 7개의 한국어 감탄발화체 유형을 제시할 수 있다. 즉, {-구나
]유형, {-다니}유형, {-ㄹ 수가}유형, 감탄부사 '얼마나', '어찌나', '어떻게나'와 '-ㄴ 지'의 연결 구조유형, {무슨}유형, {웬} 유형, '얼마나' 와 의문어미의 연결 구조유형 등 7개를 제시한다. 본 연구를 통해서 특히 전통적으로 전형적인 감탄어미로 분류되어 오던 {-구나} 는 우리의 감탄 정의에 의하면 더 이상 감탄 어미가 아니라 오히려 논술화(rhematisation) 표지로서 기능하고 있음을 밝혀 냈다. 그리고 이 어미대신 {-다니}와 {-ㄹ 수가}를 감탄 어미로 제시한다.
한편, 한국어의 감탄발화체에서도 프랑스어의 경우와 마찬가지로 의문의 중화현상이 검증된다. 이는 전형적인 의문사인 '무슨' 또는 '얼마나' 와 의문어미가 결합된 문형구조가 더 이상 순수의문은 파악되지 않은 채, 본 논문의 감탄정의에 의한 감탄의미가 파악되는 경우로, 기존에는 특수의문문으로 분류하던 것이다.
이와 같은 두 언어 사이의 공통된 현상이외에 서로 다른 특성들이 관찰된다. 첫째로, 프랑스어에서는 형용사의 정도에 대해서 의문을 품을 수 없는 반면, 한국어에서는 가능하다. 그래서 한국어의 경우, 강조수사의문이 형용사나 부사에서 파생된 정도 감탄에 대해서 흔히 관찰된다(예문 유형 371-b : 자유! 얼마나 좋은가!). 둘째로, MILNER(1978)에 따르면, 의문과 감탄은 동일한 통사구조에서 파악될 수 있지만, 이들끼리 결코 겹쳐 파악되지는 않는다고 한다. 그러나 이는 프랑스어의 경우이고 한국어의 경우, {웬} 유형에서는 이 두 의미가치가 공존하는 것이 관찰된다. 비록 감탄만을 파악하는 화자들도 있지만, 이 유형으로부터 화자가 언급한 사건의 추이 과정에 대한 호기심으로 표현되는 의문의 의미가치가 여전히 파악될 수 있다. 더욱이 이 유형은 ��은이들의 언어 형태에서 점차로 의문의 가치가 상실되어 가는 것을 볼 때 수식어(modificateur) '웬' 은 현대 한국어에서 감탄사로 전환되어 가고 있는 과정에 있다고 본다. 셋째로, 프랑스어의 정도 감탄에서는 부정의 의미가치가 중화되는 반면, 한국어에서는 부정의 중화현상은 일어나지 않으며(예문 376 : 그들은 그일로 얼마나 고생을 하였느냐!), 부정어 '않' 이나 '아니' 등은 감탄발화체에서 긍정의 짝과 여전히 부정의 의미관계를 맺고 있다.
이상으로 본 논문을 통해, 감탄은 더 이상 단순한 표현가치만으로 파악되는 의미가 아니라, 논리적 접근이 가능한 의미가치임을 알 수 있다. 그리고, 프랑스어와 한국어 감탄 발화들에, 자연어의 고유한 특성인 주관적 진리치를 근간으로하는 Univers de croyance이론을 적용함으로써 감탄발화체의 화자는 아주 강한 진리치 대립으로부터 발생되는 모순의 의미가치에 바탕을 두고 있음을 보여준다.This paper identifies the characteristics of travel behavior for use in market segmentation. For the purpose od this study,four hypothetical travel characteristics were tested using logit analysis and survey data on air travel. The results indicated that the travel characteristics such as trip, mode consumer and travel characteristics were observed as important influetial factors in choosing a certain type of air travel.

As 80 percent of tasks in finance, communications and manufacturing sectors are performed through information systems, The information systems have become large in scale and complicated, which make the range of the system users diverse and specialized. In addition, as real-time response to users is regarded imperative, the change of system users caused by corporate restructuring has frequently occurred. In order for corporations to ensure their business continuity, efficiency optimization process for system maintenance is required, which can prevent the incompleteness of business tasks by narrowing the gap of understanding in business processes between the system users and the system maintenance mangers.
As a solution to the aforementioned problems, this master thesis suggests “Modeling and Utilization for Software Enhancement Process Based on Business Scenarios”. This model uses “business scenarios” as a communication tool to resolve the difference in understanding of business process between the system users and maintenance managers. Business scenarios are manuals that describe the business rules and procedures in natural language, and this is enhanced through optimization and standardization process. Moreover, the enhanced business scenarios can be transformed into activities of software maintenance process, which enables the process of software modifications with the matching the direction of the business task flows.
The proposed model also establishes the feedback system for the modified software. This leads to the fine-grained of the software, which improves the reusability and usability of the software in the actual business settings.금융 및 통신, 제조부문까지 80%이상의 업무가 정보시스템을 통해 이루어짐에 따라 정보시스템은 복잡해지고 대형화되면서 이를 사용하는 사용자의 영역 또한 다양해지고 전문화되었다. 그리고 기업의 실시간(Real-time) 대응이 더욱 더 중요시되면서 조직 변화에 따른 사용자의 변화가 빈번하게 이루어지게 되었다. 이러한 상황에서 비즈니스의 연속성을 확보하기 위해서는 사용자의 요구사항 파악 단계에서 유지보수자와 사용자의 업무에 대한 이해의 차이(Gap)를 해소하여 업무의 누락을 방지할 수 있는 유지보수 프로세스의 효율화가 필요하다.
본 연구에서는 이러한 문제점을 해결하기 위해서 “업무 시나리오를 기반으로 한 소프트웨어 개선 프로세스 모델링 기법”을 제안한다. 본 모델은 유지보수자와 사용자의 업무 이해의 차이를 해소하기 위해서 커뮤니케이션 도구로써 “업무 시나리오”를 사용한다. 업무 시나리오는 사용자의 실제 업무 수행 규칙과 절차를 자연어로 기술한 것으로 최적화와 정형화과정을 통해서 최상의 상태로 개선된다. 그리고 개선된 업무 시나리오는 소프트웨어 유지보수 프로세스의 활동으로 전환되어 업무 수행 흐름과 동일한 방향성을 가지고 소프트웨어 변경구현 절차가 이루어진다.
그리고 변경구현 결과에 대한 반복적인 피드백(Feedback) 체계를 구축하여 점진적인 상세화(fine-grained)를 유도하여 재사용성을 높임으로서 실무 적용 결과물에 대한 활용의 기회 또한 높여 준다.
본 모델을 실무 유지보수 프로세스에 적용시 사용자 변경요구사항에 대한 업무 누락을 방지하여 비즈니스의 연속성을 확보할 수 있다. 그리고 정형화된 업무 시나리오를 기반으로 프로세스 자동화를 수행할 경우, 업무처리 시간을 단축할 수 있고 업무매뉴얼 작성에 활용하는 경우에는 기업에서 가장 소중한 업무 수행 노하우를 자산화 함으로써 기업의 경쟁력을 확보할 수 있다. 그리고 시스템 운영에 업무매뉴얼을 참조함으로써 신속한 업무 대응이 가능하다.국문초록 ⅵ 영문초록 ⅷ 제 1 장 서론 1 1.1 연구배경과 필요성 1

Title Named Entity Recognition (TNER) is the task which identifies the titles, names of books, movies, and songs. Titles are very common and widely used proper nouns.These names often become important to people, and can be an important information unit in various applications such as Information Extraction (IE) system, Question Answering (QA) system, Online Shopping, Home Agent, etc.TNER is expected to be a crucial module in those systems.

The statistical machine learning approach is a main trend in natural language processing area. Also in the Named Entity recognition task (NER), considerable amount of researches are based on supervised machine learning and some of them show near-human performance. However, these require large tagged corpus, so those NER methods are generally hard to be used in the recognition of other NE categories.

Comparing to the other common NE categories, TNER has two difficulties: The first one is few internal evidences, which are usually important features used for the traditional named entities, and the other is that there is no annotated corpus, which is essential to use supervised learning techniques. Without a solution of these problems, a development of TNER which show enough high performance can not be accomplished.

In this thesis, the researchers propose a method for TNE recognition using contextual patterns and a title dictionary which are automatically collected from mass amounts of raw corpus by bootstrapping process.
During the process, the reliability of each dictionary entities and patterns are examined using each other information at each iteration step and only reliable dictionary entities and contextual patterns over threshold are extracted and stored. Then, in the recognition step, we find TNE from input documents based on the dictionary, patterns and their reliability information.개체명 인식은 문서 내에서 특정 고유 명사나 숫자 표현을 찾아내고 분류하는 작업으로 정보 추출 (Information Extraction), 질의 응답 (Question and Answering) 등 다양한 자연어처리 분야에서 주요 모듈로 사용되고 있다.
지금까지 개체명 인식 연구는 주로 MUC-7에서 정의한 가이드라인을 기반으로 이루어졌는데, MUC-7에서는 인명, 지명, 조직명, 날짜, 시간, 금액, 백분율 (percentage), 이렇게 7종류 개체명은 인식 대상으로 삼는다. 이러한 개체명 분류로는 문서에서 좀 더 다양한 정보를 추출하기에 어려움이 따르기 때문에 개체명 분류를 확장하는 연구가 필요하다. 개체명 분류 확장에는 기존의 분류를 좀 더 세분화 하는 것과 기존에는 다루지 않던 분류를 다루는 것 두 가지 방법이 있다. 본 논문에서는 기존 개체명 인식 연구에서는 다루지 않던 책 제목, 영화 제목, 음악명을 인식하는 개체명 인식기를 제안한다. 이들 제목 개체명은 일상 생활에서 널리 사용되며 응용분야가 다양하다.

개체명 인식에서는 은닉 마르코프 모델 (Hidden Markov Model), 최대 엔트로피 (Maximum Entropy)와 같은 지도 학습을 이용한 방법들이 좋은 성능을 보여주었다. 하지만, 지도 학습은 대량의 학습 집합이 필요하다는 단점이 있다. 이는 만약 기존 MUC-7 개체명 이외의 다른 개체명을 인식하려면 새로이 학습 말뭉치를 만들어야 한다는 것을 뜻하기 때문에 제목 개체명과 같이 아직 학습 말뭉치가 없는 분야에는 적용하기 어렵다. 또한 제목 개체명은 MUC-7 개체명에는 풍부한 내부 자질이 부족하다.

이를 극복하기 위해 본 논문에서는 적은 양의 학습 데이터를 이용하는 부트스트레핑 (Bootstrapping) 프레임워크를 사용하였다. 또한, 부족한 내부 자질을 극복하기 위한 방법으로 외부 문맥 패턴과 따옴표를 이용해 제목 개체명을 인식하는 방법을 제안하였다.

웹에서 수집한 제목 사전과 따옴표 자질을 이용해 원시 말뭉치에서 초기 학습 말뭉치를 생성한 후, 이를 이용해 외부 문맥 패턴을 추출한다. 다음에는 추출된 패턴을 이용해 말뭉치에서 새로운 제목 개체명을 찾아내 제목 사전에 추가한다. 이 과정을 반복 수행함으로써 더욱 많은 외부 문맥 패턴과 제목 개체명 사전을 수집할 수 있다. 개체명 인식 단계에서는 수집된 패턴과 개체명 사전을 이용해 문서 내에서 제목에 해당하는 부분을 찾아낸다.

본 논문에서 제안한 제목 개체명 인식기는 f-measure 0.661247의 성능을 보였다.

Sense tagging is traditional task in natural language processing field, it is tagged relevant sense to target word based on the surrounding context.
Among them, all-words sense tagging is the task of determining correct senses to all content words in the given text.
Because sense list for each word is necessary to tag sense of word, dictonary including sense invectory of all words is essential language resource to all-word sense tagging.
Therefore, all studies for all-word sense tagging have been progressed to improve performance using various language resources (sense tagged corpus, WordNet, ontology, etc) additionally based on Machine Readable Dictionary (MRD).
However, the resource deficient languages cannot use the sense tagging method requiring vast resources.
The conventional sense tagging method utilizing only MRD suffers from the low recall and low precision because it determines senses only when a gloss word in the dictionary exactly matches with a context word.

In this paper, we propose two all-words sense tagging method which is effective especially for resource deficient languages.
It requires MRD which is the essential resource for all-words sense tagging and a raw corpus which is easily acquired and freely available.
The first proposed sense tagging method tries to find semantically related context words based on the co-occurrence information extracted from a raw corpus, and utilizes those words for tagging senses of the target word.
The second proposed sense tagging method utilize result of word embedding to calculate similarity between words.
The experimental results show that we can automatically tag senses to all contents words with high precision when we evaluate the proposed sense tagging algorithm on the Korean test corpus consisting of about 15 million words.
Furthermore, we also show that the semantic concordancer can be developed based on the automatic sense tagged corpus.단어 의미 부착(sense tagging)은 자연어처리 분야의 오래된 과제로서 텍스트에 함께 나타나는 문맥 단어를 기반으로 대상 단어에 적절한 의미를 부착하는 작업이다.
그 중에서 모든 단어 의미 부착(all-words sense tagging)은 주어진 텍스트에 있는 모든 내용어에 올바른 의미를 부착하는 작업이다.
단어에 의미를 부착하기 위해서는 각 단어에 대한 의미 목록(sense inventory)이 필요하기 때문에, 모든 단어에 대한 의미 목록을 포함하고 있는 사전은 모든 단어 의미 부착에 필수적인 언어자원이다.
따라서 모든 단어 의미 부착 연구는 기계 가독형 사전(Machine Readable Dictionary, MRD)을 기반으로 다양한 언어 자원(의미 부착 말뭉치, WordNet, 온톨로지 등)을 추가적으로 사용하여 그 성능을 개선하는 방향으로 진행되었다.
하지만 언어 자원이 부족한 언어에서는 다양한 언어 자원을 추가적으로 이용하기 어렵고, MRD만을 이용한 전통적인 기법은 문맥 단어(context word)와 정확히 일치하는 사전 내용 단어(gloss word)만을 의미 결정에 이용하여 재현율과 정확도가 낮은 한계가 있다.

본 논문은 모든 단어 의미 부착에 필수적인 언어 자원인 사전과 비교적 쉽게 구할 수 있고 무료로 사용 가능한 언어 자원>인 원시 말뭉치만을 추가적으로 이용하여 언어 자원이 부족한 언어에서도 효과적으로 사용할 수 있는 모든 단어 의미 부착 기법을 두 가지 제안한다.
첫 번째 의미 부착 방법은 원시 말뭉치에서 추출한 공기 정보를 바탕으로 모든 단어들 간의 관련성을 미리 계산하고, 이를 의미 부착에 활용하는 기법이다.
두 번째 방법은 단어들 간의 관련성을 계산하기 위하여 워드 임베딩 결과를 이용하는 기법이다.
1,500만 단어로 이루어진 대량의 한국어 실험 데이터를 대상으로 실험한 결과, 높은 정확도로 모든 단어의 의미를 부착할 수 있음을 확인하였다.
아울러 이렇게 구축된 의미 부착 말뭉치를 이용하여 의미기반 용례 검색기를 구축할 수 있음을 보인다.1 소개 1 2 관련 연구 7 2.1 모든 단어 의미 부착 컨퍼런스 7 2.1.1 Senseval 7 2.1.2 SemEval 9

Source code retrieval takes high proportion in software development projects and developers obtain the needed modules and code examples through code search so that they can improve the effectiveness or quality of software. In addition, developers constantly modify and expand their own or others' codes, and in this case, they also search for source code for modification and expansion. At this time, most of developers often simply search for codes with free-form query, and sometimes it takes a lot of time.
In this paper, I propose Semantic similarity-based COde seArCH engine (S_Coach), an approach that retrieves code examples based on semantic similarity and that receives an input as a code fragment. In this approach, indices are constructed based on the data of GitHub and Stackoverflow, the largest code hosting site and Q&A forum respectively to extract a large number of code samples and user code query examples. In addition, by analyzing the user input code and mapping it with the structural code elements, the performance of the search engine is improved. S_Coach has added a process to automatically search for similar questions through natural language analysis of posts related to the user input code within the Q&A forum, without directly searching for code to consider semantic similarity. To evaluate the effectiveness of S_Coach, structural similarity and semantic similarity were compared against to the results of commercial code search engines, taking input as the code fragments extracted from the answers in the most frequently searched questions in Stackoverflow. As a result, S_Coach showed structural similarity of 50% and 23% points higher compared to two commercial code search engines (Krugle and searchcode) respectively, and the result also indicated that S_Coach is capable of finding semantically similar codes without syntactic similarity or those using different libraries.소스코드 검색은 소프트웨어 개발 프로젝트에서 높은 비중을 차지하며, 개발자들은 검색을 통해 개발에 필요한 모듈, 코드 예시 등을 얻고 개발의 효율성을 증대시키거나 소프트웨어 품질을 개선한다. 또한 개발자들은 수시로 자신 또는 타인의 코드를 수정하고 확장하는데, 이 경우에도 소스코드 검색을 통해 수정, 확장 방향을 모색한다. 이때 대부분의 개발자들은 단순히 자연어 입력으로 코드를 검색하는 경우가 많고, 때때로 검색에 많은 시간을 할애하기도 한다.
본 논문에서는 코드조각을 입력으로 코드예시를 검색하는 의미적 유사성 기반의 코드 검색엔진인 Semantic similarity-based COde seArCH engine (S_Coach)를 제안한다. 본 접근법에서는 대규모의 코드샘플과 사용자 질의 예시를 추출하기 위하여 각각 가장 큰 규모의 코드 호스팅 사이트와 Q&A 커뮤니티인 GitHub와 Stackoverflow의 데이터를 기반으로 색인을 구성하였다. 또한 사용자 입력 코드를 분석해 구조적 코드요소들과 맵핑하는 방식을 통해 검색의 성능을 향상시키고, 의미적 유사성을 고려하기 위하여 코드를 직접 검색하지 않고 Q&A 커뮤니티 내에서 사용자 입력코드와 관련된 포스트들의 자연어 분석을 거쳐 유사한 질문들을 자동으로 검색하는 프로세스를 추가했다.
S_Coach의 효용성을 입증하기 위하여 Stackoverflow에서 가장 많이 검색되는 질문들내의 답변들에서 추출한 코드조각들을 입력으로 하여, S_Coach와 상용 코드검색 엔진들로부터 반환되는 결과의 정확도를 분석하고, 구조적 및 의미적 유사성을 비교하였다.
결론적으로, S_Coach가 Krugle과 SearchCode에 비해 각각 50%, 23% 포인트의 구조적 유사성이 높은 것으로 나타났으며, 의미적으로는 유사하지만 문법이나 라이브러리가 다른 코드들도 검색 가능함을 보였다.I. INTRODUCTION 1 II. RELATED WORKS 8 1. Source code search 8 2. Existing code search engines 9 3. Open source repositories and Q & A communities 13



제 1 장 서 론 1 1.1 연구배경 및 목적 1 1.2 연구 방법 6 제 2 장 관련 연구 8 2.1 구전 정보 8

Abstract

The quality of a product to be usable has become the basic requirement in consumer’s perspective while failing the requirement ends up the customer from not using the product. Identifying usability issues from analyzing quantitative and qualitative data collected from usability testing and evaluation activities aids in the process of product design, yet the lack of studies and researches regarding analysis methodologies in qualitative text data of usability field inhibits the potential of these data for more useful applications. While the possibility of analyzing qualitative text data found with the rapid development of data analysis studies such as natural language processing field in understanding human language in computer, and machine learning field in providing predictive model and clustering tool.

Therefore, this research aims to study the application capability of text processing algorithm in analysis of qualitative text data collected from usability activities. This research utilized datasets collected from LG neckband headset usability experiment in which the datasets consist of headset survey text data, subject’s data and product physical data. In the analysis procedure, which integrated with text-processing algorithm, the process includes training of comments onto vector space, labeling them with subject and product physical feature data, and clustering to validate the result of comment vector clustering.

The result shows “Volume and music control button” as the usability feature that matches best with the cluster of comment vectors where centroid comments of a cluster emphasized on appearance and button position while centroid comments of the other cluster emphasized on the button interface issues. When the volume and music control buttons are designed separately, the participant experienced less confusion and thus the comments mentioned only about the appearance and the positions of the buttons. While in the situation where the volume and music control buttons are designed as a single button, the participants experienced the interface issues regarding the buttons such as operating methods, confusion of functions and learnability of button functions. The relevance of the cluster centroid comments with the extracted feature explained the capability of text processing algorithms in analyzing qualitative text data from usability testing and evaluations.Abstract (Korean)

제품의 특징 중에 사용성이 높아야 한다는 것은 사용자 관점에 꼭 만족하여야 하는 기본 요구가 되었다. 이 요구 사항을 만족시키지 못한 경우에는 사용자가 그 제품을 사용하지 않게 될 수도 있다. 사용성평가를 통한 정량적, 정성적인 데이터를 통해 사용성 문제점을 파악할 수 있지만 정량적 데이터를 분석하는 방법론에 대한 연구는 많이 부족한 실정이다. 즉, 정량적 데이터를 분석할 수 있는 방법론이 개발되어 있지 않아 이러한 데이터의 유용성이 낮게 평가 되어져 왔다. 그러나, 데이터 분석 연구의 급속한 발전으로 정량적 텍스트 데이터를 분석할 수 있는 가능성이 높아졌다. 그 예로, 컴퓨터에서 인간 언어를 이해하는 자연어 처리 분야, 예측 모델 및 클러스터링 도구를 제공하는 기계 학습 분야가 있다.

따라서 본 연구의 목적은 사용성 평가를 통해 수집 된 정량적 텍스트 데이터의 분석을 위한 텍스트 처리 알고리즘의 응용 가능성을 연구하는 것이다. 이 연구는 LG 넥밴드 헤드셋 사용성 평가 실험을 통해 수집 된 데이터 세트를 이용하였다. 이용한 데이터 세트는 헤드셋 설문 텍스트 데이터와 사용자 데이터 그리고 제품의 물리적 데이터가 있다. 텍스트 처리 알고리즘과 통합 된 분석 절차에서 벡터 공간에 코멘트를 학습하고 사용자 및 제품 물리적 피처 데이터로 레이블을 지정하고 클러스터링을 사용하여 코멘트 벡터 클러스터링의 결과를 검증하였다.

결과에서 분류된 두개의 클러스터와 가장 일치하는 사용성 피처는 “볼륨 및 곡 이동 버튼”이다. 중심 코멘트를 살펴봤을 때, 분류된 두개 중 하나의 클러스터 중심 코멘트는 모양과 버튼 위치를 강조하는 반면, 다른 클러스터의 중심 코멘트는 버튼 인터페이스 문제점을 강조한 내용이다. 볼륨 및 곡 이동 버튼이 별도로 설계된 제품에서는 피실험자는 두 버튼에 대한 혼동이 적었으며 버튼의 위치와 모양에 대해서만 언급한 문제점을 제시했다. 반면, 볼륨 및 곡 이동 버튼이 하나로 설계된 제품에 대해서는 조작 방법, 기능의 혼란, 버튼 기능의 학습성과 같은 인터페이스 문제점을 서술했다. 클러스터 중심 코멘트 내용과 추출된 사용성 피처의 높은 관련성은 사용성평가에서 정성적 텍스트 데이터를 분석 할 때 텍스트 처리 알고리즘의 응용 가능성을 증명했다 .Chapter 1. Introduction 1 1.1 Background and Motivation 1 1.2 Research Objective 3 1.2.1 Self-organizing of qualitative text data 3 1.2.2 Quantification of qualitative text data 3

Sentiment Analysis is Natural Language Processing that aims to distinguish positive, negative, and neutral sentiments of a specific object from various text data such as reviews, news, and blogs. Sentiment analysis techniques are largely classified into lexicon based approach and machine-learning approach. In the case of lexicon based approach, general-purpose sentiment lexicon or a lexicon created by a researcher is used for sentiment analysis. On the other hand, when a large amount of documents are targeted, machine-learning approach is generally applied. In the relevant studies, there have been various statistical explorations to improve the analysis accuracy in the feature selection stage, which is an intermediate stage of the machine-learning approach. Also, there have been studies that compare the performance of sentiment analysis models by applying various classifiers required in the classification stage, or more recently, studies that apply and classify Word2vec, a representative word embedding methodology, to the feature selection stage of the machine-learning approach.
Word2vec is continuous word embedding based on a nerve network. In the case of word learning using Word2vec, words with similar contexts in the word-embedding space have close spatial distribution, which makes differences in the spatial distribution of the words depending on the kinds of documents used in a field. The majority of the sentiment analysis studies applying Word2vec are to reduce the dimension by applying the clustering technique to solve the high dimension problem of Word2vec in the feature selection stage. The Word2vec model only learns the usage information such as the syntactic information of the word in a sentence, Thus, the limitations of expressing sentiment with different polarity in a similar word embedding vector can cause a problem of impairing the accuracy of sentiment analysis. Therefore, studies suggesting word embedding that integrate the semantic information of a word have emerged.
This study aimed to propose a new sentiment classification model applying Word2vec which integrates sentiment lexicon information to sentiment analysis. In other words, the sentiment lexicon constructed by the researcher was combined with the Word2vec model learning stage to establish the sentiment lexicon-based Word2vec model, and sentiment analysis was performed by building the features of the documents to be analyzed. When the sentiment analysis was performed by the method proposed in this study, the accuracy improved by about 2% compared to the existing method. As a result, the validity of the proposed model was confirmed.
By combining the lexicon-based technique and machine learning technique to conduct sentiment analysis, this study has implications for enhancing the analytical performance to reflect the polarity information of sentiments using the sentiment lexicon constructed by the researcher. Moreover, from the perspective of sentiment analysis study using Word2vec, the word embedding method is excellent as a word expression method, however, it does not sensitively react to sentiment polarity information to learn sentence structural syntactic information in sentiment analysis Accordingly, this study has implications in that it can complement the limitations of the existing Word2vec features by combining the sentiment lexicon.감성분석(Sentiment Analysis)은 리뷰, 뉴스, 블로그 등의 다양한 텍스트 데이터로부터 특정 대상의 감성의 극성이 긍정, 부정 혹은 중립인지 찾는 것을 목표로 하는 자연어 처리(Natural Language Processing) 분석 기법이다. 감성분석의 기법으로는 크게 사전 기반 기법(lexicon based approach)과 기계학습 기법(machine-learning approach)이 있다. 사전 기반 기법의 경우 범용 감성사전 혹은 연구자가 직접 제작한 사전을 이용하여 감성분석을 수행하게 되며, 대량의 문서들을 대상으로 할 경우, 기계학습 기법을 적용하는 것이 일반적인 흐름이다. 이와 관련한 연구로는, 기계학습 기법의 중간단계인 자질 선택(feature selection)단계에서 분석 정확도를 높이기 위해 다양한 통계적 방법을 이용하는 연구가 있었다. 그리고 분류단계에서 필요한 분류기를 다양하게 적용하여 감성분석 모형의 성능을 비교하거나, 최근에는 대표적인 워드임베딩 방법론인 Word2vec을 이용해 기계학습 기법의 자질 선택 단계에 적용하여 분류를 수행한 연구들이 많이 있었고, 딥러닝을 이용한 감성분석에서도 자질 선택 단계에서 Word2vec을 적용하고 있는 추세이다.
Word2vec은 신경망 기반의 연속 워드임베딩(continuous word embedding)로서, Word2vec을 이용해 단어들을 학습할 경우 워드임베딩 공간에서 비슷한 문맥을 가진 단어들은 서로 가까운 공간 분포를 가지게 되는데, 어떠한 분야에서 사용되는 문서들을 대상으로 학습했는가에 따라 단어가 공간 분포는 달라지게 된다. Word2vec을 적용한 감성분석 연구로는 자질 선택 단계에서 Word2vec의 고차원 문제를 해결하기 위해, 클러스터링 기법을 적용하여 차원을 축소시키는 것과 관련한 연구가 주류를 이루었고, Word2vec 모델이 문장 내에서 단어의 통사적 정보와 같은 쓰임새 정보만 학습하여, 감성적으로 극성이 다른 단어도 유사한 워드임베딩 벡터로 표현되는 한계점이 있는데 이는 감성분석의 정확도를 떨어뜨리는 문제점을 야기하기에 단어의 의미정보를 결합한 워드임베딩 방법을 제안하는 연구도 계속해서 등장하고 있다.
본 연구에서는 감성사전 정보를 결합한 Word2vec을 자질을 감성분석에 적용하는 새로운 감성 분류 모형을 제안하고자 한다. 곧, 연구자가 구축한 감성사전을 Word2vec 모델 학습 단계에 결합하여 감성사전 기반 Word2vec 모델을 구축하고 이를 통해 분석하고자 하는 문서의 자질을 구축하여 감성분석을 시행하였다. 본 연구에서 제안한 방법으로 감성분석을 시행했을 때, 기존 방법보다 분류 정확도가 약 2.5% 향상된 것을 확인하여, 제안모형의 유효성을 확인하였다.
본 연구는 기존의 감성분석 기법인 사전 기반 기법과 기계학습 기법을 혼합하여 감성분석을 수행하므로 연구자가 구축한 감성사전을 활용하여 감성의 극성 정보를 반영함으로써 분석 성과를 높인데 의의가 있었다. 또한, Word2vec을 적용한 감성분석 연구의 관점에서 바라봤을 때, 워드임베딩 기법이 단어 표현 방법으로는 우수하지만, 감성분석에 있어서 문장 구조적 통사적 정보를 학습하여 Word2vec이 감성의 극성 정보를 예민하게 반응하지 못하는 점이 있기에, 감성사전을 결합하여 기존의 Word2vec 자질의 한계점을 보완할 수 있는 연구를 수행했다는 점에서 의의가 있다.Ⅰ. 서론 1 A. 연구의 배경 및 목적 1 Ⅱ. 선행연구 4 A. 온라인 리뷰(Online Review)를 이용한 감성분석 4 B. 워드임베딩을 적용한 감성분석 11

In this paper, we developed a hybrid based recommender system that combines existing recommendation system with deeplearning to efficiently recommend fashion products considering user 's preference among newly emerging fashion products. Deep Learning is a technology that shows outperform in various fields such as image, video, natural language processing, speech recognition, etc., and there is an increasing number of studies to incorporate deep learning to effectively model the interaction between user and product in the recommender system domain . In the case of the SPA brand in the apparel fashion market, new apparel products are poured every week. Accordingly, the recommender system should recommend the product considering the preference to the user. However, since there is the evaluation of the user about the product, the user can learn the recommendation system model, which causes the problem of 'Cold Start'. In this study, suppose that the user considers the visual features of the apparel product, and coped with the 'Cold Start' problem by using the apparel product image data. In addition, this model is hybrid with Deep Learning model MultiLayer Perceptron to further improve the performance, thus suggesting a hybrid based deeplearning recommender system that can be used in the fashion market.
The data set to learn the recommended system model was created by extracting only the users who evaluated 'Tradesy.com', which was crawled from the second-hand clothing site, and evaluated the product more than 20 times. The data set consists of 8,355 users and 258,943 products and 492,350 interactions. We also divide the proposed model into 'All items' and 'Cold items' settings to see if the 'Cold start' problem is solved well. The data set uses one data evaluated by each user as a test set and the rest as a training set The experiment was carried out. To develop the model, we used Python, one of the programming languages, and Keras, one of the deeplearning frameworks, and experimented with intel® Core ™ i5-8600 @ 3.10Ghz, 32GB, and 1080Ti, respectively, for cpu, memory and gpu. To evaluate the performance of the proposed Visual Generalized Matrix Factorization, we compared the models of Random, Itempop, Matrix Factorization, and Generalized Matrix Factorization. Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) were used as the evaluation metrics for comparing each model. The proposed VGMF model showed 10.9% and 33.5% improvement in performance in the 'All items' setting compared to GMF and MF (NDCG), and 18.7% and 71.0% performance improvement (NDCG) in the 'Cold items' setting respectively. In the case of the model made by hybrid of VGMF and MLP, the performance improvement (NDCG) was 8.8% and 7.9% in the 'All items' and 'Cold items' settings, respectively, compared to VGMF. If you use the image of the product when recommending the fashion product, you can see that the overall performance of the model is increased. Also, if you use the image of the product when the 'Cold start' occurs because there is no evaluation information about the product, It can be seen that it can cope well with existing models.
In this study, we proposed a fashion product recommender system model considering user 's preference by combining deeplearning which is actively studied recently with recommender system. Although we tried to improve the performance by using only image information among the metadata of the product, in future studies, if we combine various metadata such as the user's review data, the number of clicks of the product web page, the age of the user and the gender of the user into the recommender system model, Personalized recommendations will be made.본 논문에서는 매일 쏟아지는 패션 상품들 중에서 사용자의 취향을 고려한 패션 상품을 효과적으로 추천해주기 위해 기존의 추천시스템과 딥러닝을 결합한 하이브리드형 추천시스템을 개발하였다. 딥러닝은 최근 이미지, 영상, 자연어처리, 음성인식 등의 다양한 분야에서 뛰어난 성능을 보이는 기술이며 추천시스템 도메인에서 사용자와 상품사이의 상호작용을 효과적으로 모델링하기 위해 딥러닝을 접목하려는 연구가 많아지고 있다. 의류 패션시장의 SPA 브랜드 같은 경우 매주 새로운 의류상품이 쏟아지고 있고 이에 따라 추천시스템은 사용자에게 취향을 고려한 상품을 추천해줘야 하지만 상품에 대한 사용자의 평가가 존재해야 추천을 해줄 수 있는 협업필터링 추천시스템 특성 때문에 ‘Cold start’ 라는 문제가 발생하게 된다. 본 연구에서는 사용자가 의류 상품을 구매할 때 시각적 특징을 고려하여 구매한다고 가정하고 의류 상품의 이미지 데이터를 사용함으로써 이 ‘Cold start’ 문제에 대처했다. 또한 이 모델을 다시 딥러닝 모델인 Multi Layer Perceptron과 하이브리드 하여 성능을 더욱 향상시킴에 따라 패션시장에서 사용할 수 있는 하이브리드형 딥러닝 추천시스템을 제안한다.
추천시스템 모델을 학습시킬 데이터세트는 중고의류사이트에서 크롤링한 ‘Tradesy.com’ 를 사용하였고 상품에 대해 20회이상 평가한 사용자만을 추출하여 만들었다. 데이터세트는 8,355명의 사용자와 258,943의 상품과 492,350의 상호작용으로 이루어져 있다. 또한 제안한 모델이 ‘Cold start’ 문제를 잘 해결하는지 보이기 위해 ‘All items’와 ‘Cold items’ 세팅으로 나누고 데이터 집합은 각 사용자가 평가한 1개의 데이터를 테스트집합으로 사용하고 나머지를 훈련집합으로 사용해 실험을 진행하였다. 모델은 개발하기 위해 프로그래밍 언어 중 하나인 파이썬과 딥러닝 모델을 학습하기 위해 딥러닝 프레임워크 중 하나인 케라스를 사용하였고 cpu, memory, gpu 각각 intel® Core™ i5-8600 @3.10Ghz, 32GB, 1080Ti 에서 실험을 진행하였다. 본 연구에서 제안하는 Visual Generalized Matrix Factorization의 성능을 평가하기 위해 Random, Itempop, Matrix Factorization, Generalized Matrix Factorization의 모델들과 비교를 하였다. 각 모델들을 비교하기 위한 평가지표로는 Hit Ratio(HR) 와 Normalized Discounted Cumulative Gain(NDCG)를 사용하였으며 제안한 VGMF 모델이 ‘All items’ 세팅에서 GMF와 MF에 비해 각각 13.7%, 33.5%의 성능 향상을(NDCG) 보였고 ‘Cold items’ 세팅에서는 각각 18.7%, 71.0%의 성능향상을(NDCG) 보였다. 또한 VGMF와 MLP를 하이브리드 하여 만든 모델의 경우 VGMF에 비해 ‘All items’, ‘Cold items’ 세팅에서 각각 8.8%, 7.9%의 성능향상을(NDCG) 보였다. 이를 통해 패션제품을 추천할 때 상품의 이미지를 활용할경우 모델의 전체적인 성능이 올라감을 알 수 있고 또한 상품에 대한 평가정보가 없어서 발생하는 ‘Cold start’ 일 때 상품의 이미지를 사용하면 이 문제에 대해 기존의 모델들에 비해 잘 대처할 수 있음을 알 수 있다.
이처럼 본 연구에서는 최근 활발하게 연구가 이뤄지고 있는 딥러닝을 추천시스템에 결합하여 사용자의 취향을 고려한 패션제품 추천시스템 모델을 제안하였다. 비록 상품의 메타데이터 중 이미지정보만을 사용하여 성능을 높이고자 했지만 추후 연구에서 사용자의 리뷰데이터, 상품 클릭 횟수, 사용자의 나이, 사용자의 성별 등의 다양한 메타데이터들을 추천시스템 모델에 결합 한다면 사용자에게 더욱 개인화된 추천을 해줄 수 있을 것이다제1장 서론 ....................................................................................................................1 제1절 연구 배경 및 목적 .........................................................................................1 제2장 이론적 배경 및 선행연구 .............................................................3 제1절 이론적 배경 .......................................................................................................3

본 연구는 박경리 소설에 나타난 공간을 이차모델 형성 체계의 한 언어로 보고 그것을 구체적으로 소설의 텍스트 안에서 어떻게 작용하고 있는지를 분석하여 작가의 공간의식이 어떻게 나타나고 있는지를 중심으로 살펴보고자 하였다.
박경리에 관한 기존의 연구서들은 박경리 소설에 나타난 주제적인 측면에 초점을 맞추어 논의를 전개하고 있는데 본 연구에서는 이러한 기존의 연구들을 논리 전개의 출발점으로 하되, 공간을 이차모델 형성체계의 한 언어로 보고 그것이 구체적으로 소설의 텍스트 안에서 어떻게 작용하고 있는지를 분석함으로써 박경리 소설의 공간 의미를 밝히고자 논의를 전개시켰다.
그리고 그것의 실천성과 유효성을 검증하기 위해 많은 작품을 기반으로 공간 의미를 밝히고자 하였다. 기호론의 기본을 이루고 있는 것은 차이고 그 차이는 이항대립적 관계에서 생겨나는 것이다. 그러므로 음운 체계와 같은 방법으로 공간의 이산적 단위를 추출하면 수직, 수평으로 분절되고 그것은 다시 上/中/下와 內/境界/外의 하위분별이 가능해진다. 그리고 그러한 공간 체계는 성속(聖俗)이나 생사(生死)와 같은 양극화된 관념, 그리고 긍정· 부정과 같은 가치의 대립체계와 연계된다. 이같은 공간이 이산성과 그 의미 작용을 박경리의 서사구조 속에서 분석하고 그 공간이 의미하는 것을 비교하면 그 구조의 의미를 좀더 명확하게 파악할 수 있다.
Ⅱ장에서는 A, Ｂ로 나누어 논의를 심화시키고 있으며 시기적인 순서에 따라 주요 텍스트를 가려 논의를 전개하고자 하였다. A에서는 박경리의 초기 소설에 나타나는 /내/공간의 부정성을 제시하고자 하였다. /내/공간은 /외/공간에 대립되는 것으로 수평 공간에서는 인간의 공간 이동에 의해 텍스트가 동태적인 것이 되고 서사적인 성격을 띠게 된다. 그러나 박경리의 초기 소설에서는 내/외 공간의 이동이 자유롭지 않고 /내/공간의 부정성을 강조하는 것으로 공간의 의미가 생성되고 있다. 특히, 병에 의해 안락과 평화의 /내/공간이 해체되고 가족 구성이 완전하지 않음에 따른 /내/공간의 부정성을 강조하는 것으로 내/외공간의 의미를 제시하고자 하였다.
B에서는 동태적인 텍스트(서사체 문학)에서 주인공=화장가 /내/공간에서 /외/공간으로 향해 나가는 이동과 /외/공간에서 /내/공간으로 들어오는 이동에 의한 공간 기호의 의미를 분리할 수 있다. 여기에서는 /내/공간과 /외/공간의 대립과 /내/공간에서 /외/공간을 향해 나가는 텍스트의 분석을 통해 /외/공간이 탈출이나 도주의 공간임을 언급하면서 /내/공간의 부정성을 극복하기 위한 생성의 /외/공간으로 나아감을 밝히고자 하였다. 그러나 점점 후기 작품으로 갈수록 여성 인물의 공간 이동에 대응하는 남성 인물의 공간이 뚜렷하게 제시되고 있으며 이는 수평 구조에 대립되는 수직 공간으로 제시되고 있음을 알 수 있다.
그러나 여기에서는 특히 여성 인물에 초점을 맞추어 논의를 전개하고 그들의 이동에 따른 공간 생성에 초점을 맞추어 논의를 심화시켜 나가고자 하였다. 왜냐하면 박경리는 여성 인물의 /외/공간으로의 나아감을 지향하고 있으며 그를 통한 가족 공간의 회복, /내/공간의 부정성 회복 등이 가능하기 때문이다. 특히 박경리의 경우, 주로 여성 인물의 수평 공간 중심으로 논의를 전개해 나가는 것이 특징이며 또한 이러한 공간 확장이 서사 구조에 미치는 영향은 매우 크다.
Ⅲ장에서는 수평 공간에서의 경계 영역은 매개항의 기능을 한다는 전제하에 논의를 전개하였다. /內/와 /外/의 교환이 이루어지는 ‘항구’, ‘역’, 그리고 ‘길’ 같은 곳은 박경리의 텍스트에서 다의적인 기호를 생성하고 있는 공간이 된다. 따라서 이 부분에서는 ‘역’과 ‘항구’ 등이 갖는 의미를 텍스트의 분석을 통해 추출해보고자 하였다.
그러나 ‘역’과 ‘항구’가 같은 /境界/ 공간이나 이들의 갖는 의미는 다소 차이가 있다. 역의 경우, 텍스트 내에서 /내/→/외/,/외/→/내/의 공간 이동이 자유로이 나타나며 반드시 /내/공간으로의 회귀를 전제한 /외/공간으로의 나아감이 이루어지고 있다 따라서 /境界/공간의 의미가 /내/공간의 부정성에서 벗어나지 못한 다소 닫힌 공간으로 제시되고 있다. 그러나 항구의 경우에는 /외/공간 지향의 의지를 분명하게 나타내고 있으며 여성 인물의 나아감과 바다 건너의 /외/공간 즉, 국가적 차원의 /외/공간을 지향하는 것이 특징이며 이는 역의 공간에 비해 열려진 공간으로 제시되고 있는 것이 특징이다.
Ⅳ장에서는 박경리 작품에 나타난 공간이 시기적으로 의미하는 것과 공간의 기호가 어떻게 변모하고 있는지를 밝히고자 하였다. 박경리의 경우 동일한 공간 기호를 사용하여 다른 의미를 나타내는 경우가 많다. 처음에는 ‘집’, ‘병원’, 등의 공간이 주로 사용되고 있으나 점차 중·장편 소설로 이어짐에 따라 점점 ‘통영’, ‘부산’과 같은 구체적인 지명 공간을 통해 공간 기호 체계를 형성하면서 이에 소속된 구체적인 ‘집’,의 공간과 /외/ 공간을 대립시키고 있는 것이 특징이다. 또한 같은 공간 기호더라도 작품에 따라 다르게 나타나는 변모 양상과 /외/공간 지향의 작가 의지를 좀더 다루어보고자 하였다. 또한 /내/공간의 부정성을 극복하기 위한 /외/공간으로 나아감이 도주, 탈주의 공간이었음을 드러내면서 /내/공간의 긍정성을 회복하기 위한 작가의 의지를 여성 인물의 /외/공간 지향을 통해 밝히고자 노력하였다.
이와 같이 박경리의 공간 분석을 통하여 수직· 수평의 이같은 이산적 단위들이 결합하여 다양한 공간 기호를 형성하고 그것들의 기호작용을 통해서 자연어로서는 기술할 수 없는 서사적 의미를 산출하는 공간의 의미를 정립하는데 의의가 있다고 할 수 있다.This thesis surveys how the author's awareness of space is realized by assuming the space in park Kyung-Lee's novels as a language of a secondary modelling structure and analyzing how it actually works in the text.
And at the same time to test its practicability and effectiveness, I tried to show the meaning of space based on a large number of novels. The basis of semiotics is the difference, which comes from the binominal contrastive relations. Thus, if we draw the separate unit of space as we do in phonetic system, it can be divided into vertical and horizontal and then sublivided into high/mid/low and inner middle/outward. And this system of space can be related to polarized concepts such at sacred and common or life and death, or contrastive system such affirmation and negation. Thus by analyzing the dispersion of space and its scape of meaning within the description structure of Park and by comparing what the space means, we will be able to understand the meaning of that structure more clearly.
In chapter Ⅱ, divided innntopart A and B, the discussion deepens and main texts are discussed in chronological order. In part A, I tried to present negativeness of inner space shown in Park's initial novels. Inner space is opposite to outer space and in horizontal space text becomed dynamic and descriptive by movement of prople in space. But in Park's early novels, this transfer is not free and the meaning of space is generated emphasizing the negation of inner space, Particularly, comfortable and peaceful inner space is disorganized by diseases and the negative meaning of inner space is emphasized due to incompleteness of a family.
In part B, it is possible to recognize the difference between a hero going from inner to outward and outward to inner space in a dynamic transformational system, Here, inner and outward space are contrasted and the text is analyzed as going from inner space to outward space. Through this analysis, outward space is referred as a space of escape or desertion and overcomes the negativeness of inner space. However as we move up to her later works, along with female characters'movement the vertical structure corresponding.
However the focus is on female move outward to restore space for family and recover from the negativeness of inner space. Therefore her story developes putting emphasis on the horizontal movements if female characters. And this extension of space has a great influence on the narrative structure of a story.
In chapter Ⅲ, It is thought that the middle space within a horizontal space has mediating function. So this middle space with its double references creates a space which generates polysemic codes such as a port, a station and a road. Therefore I tried to endure the meanings of a station and a port through text analysis.
Chapter Ⅳ defines what space means in different periods and explains how the signs for space changed. In case of Park, she uses the same symbol to express different meanings. At first, spaces such as a house, a hospital are used but as they move to middle or long novels specitic names such as Tong-young and Pusan are used and the house which belongs to these places contrasts with outward spaces. Additionally in different works and the authors outward oriented intentions. Also it shows that running to outward space means overcoming negativeness of inward space and this is realized by the female characters orientation for outward spaces.
Through the spatial analysis of Park's we can see that those dispersive units such as horizontal and vertical combine and construct various spatial signs and through that, we can produce narrative mraning of space which is impossible from the natural language.목차 논문개요 = iii I. 서론 = 1 A. 문제 제기 및 연구 목적 = 1 B. 연구사 검토 및 연구의 방법 = 3

In the urban transformation process, many cities experience decline, which they often overcome through urban regeneration projects. Most urban regeneration methods vitalize declining communities by attracting tourists, but research on urban regeneration that uses tourism is insufficient.
On the one hand, as tourists often check destination images before visiting, destination images have a strong impact on the selection of tourist destinations. In addition, since destination images that is felt after a visit can lead to another visit, the images of tourist destinations have long been an interest area for tourism researchers. Most studies on tourist destination images have focused on quantitative research using surveys. Survey methodologies can differ based on measurement factors and can change based on the investigation period. However, content analysis of social media data using big data can help to overcome such limitations of existing tourist destination image research. The desired data can be sampled through a systematic algorithm, and the collected data can be visualized by frequency analysis and proportion analysis.
Therefore, this study aims to analyze the differences in the tourist destination images of Namdaemun Market before and after the urban regeneration project of Seoullo 7017. In addition, this research intends to expand the study of urban regeneration in the field of public administration and combine text mining, a big data analysis method which is used to supplement the limits of existing survey research in the tourism field.
Naver blogs and cafés were selected as the data collection channels because they occupy 70% of the Korean web. The amount of data collected is 108,585 articles before the establishment of Seoullo 7017 and 4,610 articles after its opening. The collected text-data was converted into data that can be analyzed through step document preprocessing, NLP, and term-document matrix. Using the converted data, frequency analysis, proportion analysis, and visualization through infographics were conducted.
The related word frequency analysis results showed that before the opening of Seoullo 7017, the most frequently used words related to Namdaemun Market were related to food tourism, including “famous restaurant,” “Seoul,” “eat,” “place,” “purchase,” “galchi jorim” (Korean spicy braised beltfish), “alley,” “hotteok” (honey-filled Korean pancake), and “sightsee.” Other frequently used words included “bargain,” “experience,” “big hit,” “browse,” “honsu” (wedding gifts of household goods), “crowd,” and “crowded.” Correlation analysis results showed that the correlative relationship between food-related words and Namdaemun Market was high. Following food-related words were shopping tourism, purchasing, and browsing words. These results show that tourism at Namdaemun Market is mostly for food and shopping tourism, and tourists’ purpose of visiting is related to purchasing. Therefore, the destination image of Namdaemun Market before the opening of Seoullo 7017 can be interpreted as a place for food and shopping and with less perception as a travel destination.
Second, the related word frequency analysis for the Namdaemun Market on social media changed after the opening of Seoullo 7017 to words such as “Seoullo 7017,” “Seoul Station,” “people,” “good,” “overpass,” “connection,” and “course.” As a result, words related to the changed space after the opening ceremony appeared at a high frequency, as did tourist destination-related words such as “sightsee,” “night view,” and “date.” This analysis shows that the perception and expectations of Namdaemun Market as a tourist destination, which was relatively low before the opening of Seoullo 7017, significantly increased after the urban regeneration project’s establishment.
Third, there was no significant difference in the number of positive words used before and after the establishment of Seoullo 7017. However, the content of the words changed from shopping-related words such as “active” and “liveliness” before Seoullo 7017’s opening to words such as “rebirth” and “transformation,” as well as “healing,” “rest,” and “hot place,” revealing new characteristics post-opening. As a result, it can be seen that Namdaemun’s destination image differed from one of shopping and food before Seoullo 7017’s opening to one of healing, rest, and popular sightseeing spots after the urban regeneration project’s establishment.
Based on the results of this analysis, the following implications are suggested. First, this study has expanded methodologies used in the tourism field. Second, this study not only analyzes the frequency of unstructured data but also lowers the subjectivity of text analysis through correlation analysis. Third, image research using text mining, which differs from image surveys conducted through questionnaires, shows what images visitors perceive in more detail. In addition, this study can be used in various practical ways. Finally, this research shows that urban regeneration using tourism influences areas in decline.
The main limitation of this study is that the subject of the research was limited to blogs and cafés based on the Korean portal site Naver. This online data collects in targeting only Korean, it can’t generalize too much except other target. Second, in the text mining process, emotions expressed through single vowels and emoticons are not accounted for. In modern Korean language, however, the influence of unstructured data, such as Internet initialisms and emoticons, cannot be ignored. Thus, in the future, this research needs to be conducted using unstructured data consisting of emoticons, single consonants, and single vowels.도시의 변화 과정에서 많은 도시들이 쇠퇴를 경험하게 되고 이를 극복하기 위한 도시재생의 필요성이 요구된다. 많은 도시재생사업은 관광객 유치를 통해 진행되고 있다. 그러나 관광을 활용한 도시재생에 대한 연구는 매우 부족하다.
한편 관광객은 사전에 가지고 있는 이미지를 통해 관광지를 방문하며, 관광지이미지는 관광목적지 선택에 중요한 영향을 미친다. 또한 방문 후에 느끼는 관광지 이미지는 재방문으로 이어질 수 있다. 따라서 관광지 이미지에 대한 연구는 관광 연구자들의 오랫동안 관심분야가 되어있다. 대부분 관광지 이미지 연구들은 설문조사방법을 중심으로 연구하였다. 그러나 설문조사법은 측정요소에 따라 관광지 이미지의 결과 값이 달라질 수 있으며 조사기간에 따라 결과 값 또한 변동될 수 있다. 그러나 빅데이터를 활용한 SNS데이터의 내용분석은 기존의 설문조사법의 표본에 한계를 극복할 수 있다.
따라서 본 연구에서는 서울로7017과 남대문시장을 통해 도시재생 전·후 관광지 이미지 차이를 분석하고자 한다. 또한 본 연구에서는 빅데이터연구 방법을 관광분야에 접목시키고 관광분야에 도시재생연구를 확장시키고자 한다.
본 연구는 데이터 수집 채널을 국내 점유율 70%이상인 네이버 블로그, 카페를 선정하였다. 수집된 데이터량은 서울로7017 개장 전, 108,585이고 개장 후 4,610이다. 수집된 텍스트데이터는 전처리, 자연어처리(NLP), 단어-문서행렬을 통해 정형데이터로 변환하였다. 변환된 정형데이터는 빈도분석, 상관관계분석, 시각화를 진행하였다.
분석결과 SNS상의 서울로7017 개장 전 남대문시장에 대한 관련 단어 빈도분석 상위 값은 ‘맛집’, ‘서울’, ‘먹다’, ‘쇼핑’, ‘구입하다’. ‘갈치(조림)’, ‘골목’, ‘호떡’, ‘구경하다’, ‘안경’, ‘아동복’ 등이었다. 이를 통해 남대문시장을 방문하는 목적이 쇼핑관광과 음식관광임을 해석 할 수 있다. 이외에도 ‘흥정하다’, ‘체험’, ‘대박’, ‘둘러보다’, ‘혼수’, ‘인파’, ‘북적하다’, 등 다양한 단어들이 나타났다. 또한 텍스트의 상관관계분석결과는 남대문시장과 음식 관련 단어들의 상관관계가 높았으며, 이후 ‘쇼핑’, ‘구입하다’, ‘구경하다’, ‘여행’ 순으로 나타났다. 이는 남대문시장이 음식관광과 쇼핑관광의 목적이라는 빈도분석결과의 해석을 뒷받침한다. 그리고 남대문시장의 방문목적이 구매와 관련 있다는 것을 알 수 있다. 따라서 서울로7017 개장 전 남대문시장의 이미지는 음식관광 쇼핑관광 적은 숫자의 관광목적지로 해석 할 수 있다.
둘째, SNS상의 서울로7017 개장 후 남대문시장에 대한 관련단어 빈도분석은 ‘서울로7017’, ‘서울역’, ‘사람’, ‘좋다’, ‘고가’, ‘연결’, ‘코스’ 등 이었다. 이를 통해 개장 후 변화된 공간에 대한 단어들이 높은 빈도로 나타났으며, ‘구경하다’, ‘야경’, ‘데이트’와 같은 관광지와 관련된 단어들을 통해 개장 후 남대문시장에 대한 이미지는 새로운 모습과 기대에 대한 관심과 개장 전 낮았던 관광목적지에 대한 단어들이 높게 나타나는 것을 알 수 있다.
셋째, 개장 전·후 긍정어의 개수 차이는 없었다. 그러나 단어의 내용이 개장 전에는 쇼핑과 관련된 단어와 시장에 대한 ‘활기참’, ‘생기’ 등의 단어에서 개장 후 ‘재탄생,’ ‘재생’, ‘탈바꿈’과 같은 단어와 ‘힐링’, ‘휴식’, ‘핫플레이스’ 등과 같은 새로운 성격의 단어들이 나타나기 시작했다. 이를 통해 개장 전·후 남대문시장에 대한 이미지가 차이가 있음을 알 수 있다.
본 연구의 시사점은 다음과 같다. 첫째, 설문조사법이 가지고 있는 표본의 한계를 극복하고 관광연구의 조사 방법을 확장시켰다. 둘째, 본 연구에서는 비정형화된 텍스트 데이터의 빈도분석에 그치지 않고 상관관계분석을 통해 텍스트 해석에 대한 주관성을 낮추었다. 셋째, 텍스트 마이닝을 활용한 이미지연구는 방문객이 본인스스로 연구대상임을 인식하지 않기 때문에 연구의 인식으로 나타나는 오류를 줄일 수 있었다. 마지막으로 본 연구를 통해 도시재생이 주변 대상지역에 이미지 차이가 있다는 사실을 알 수 있었다.
본 연구의 한계점은 조사대상을 국내 포털사이트 네이버 블로그와 카페의 작성글로 한정시켰다는 점이다. 이는 국내 관광객들이 가지고 있는 남대문시장에 대한 결과만으로 관광지 이미지를 일반화에 제약이 있을 수 있다. 후속연구에서는 국외 관광객들이 가지고 있는 도시재생 전·후 남대문시장의 관광지이미지 연구가 필요하다. 둘째, 텍스트마이닝의 전처리 과정에서 모음과 이모티콘으로 나타난 감정표현이 사라진다. 한국어의 특성상 ‘ㅋㅋ’, ‘ㅎㅎ’, ‘ㅜㅜ’, :)와 같은 비정형 데이터의 영향력도 무시할 수 없기 때문이다. 이후 연구에서는 이모티콘, 자음모음으로 이루어진 비정형데이터를 전처리 과정에 포함한 연구가 진행되어야 할 필요성이 있다.제 1 장 서 론 1 제1절 연구의 배경 1 제2절 연구의 목적 3 제3절 연구의 방법 및 범위 4 제4절 연구의 구성 5

Recently, artificial intelligence (AI) technologies has been widely used in various fields such as finance, medical care, and distribution due to the development of deep learning algorithm, the increase of data quantity, improvement of computing power and storage, and improvement of network performance. The AI market is expected to reach $ 19.1 billion in 2018, growing at a CAGR of 46.2% and estimated at $ 52.2 billion in 2021. Accordingly, developed countries such as the United States, Canada, and Japan are developing and promoting various policies to foster AI technology as a new growth engine in the future. As a result, Korea has also announced its AI R&D strategy for the realization of i-Korea 4.0 in May 2018. However, Korea's AI technology is inferior to major competitors such as the US, Canada, and Japan Therefore, in order to cope with the 4th industrial revolution, it is necessary to allocate AI R&D budgets efficiently through selection and concentration so as to gain competitive advantage under a limited budget. In this study, the importance of each AI technology was evaluated in multi-dimensional way through the questionnaire of expert group using the evaluation index derived from the literature review
The results of this study are as follows. First, the indicators to be considered for AI technology development is shown in the order of market growth potential, urgency of technology development, ripple effect of technology development, technological gap compared to advanced technology, import-substituting effect, possibility of export product. Second, in terms of the importance of AI technology by evaluation index, development of AI technologies such as cognitive computing, data mining, deep learning, and virtual reality is most urgent in terms of urgency of technology development, and in terms of technology gap compared to advanced technology, deep learning, cognitive computing and data mining were the biggest. From the viewpoint of import-substituting effect, substituting effect is the greatest in order of deep learning, data mining, and cognitive computing, and in terms of ripple effect of technology development, cognitive computing, data mining, and virtual reality are the largest. In terms of market growth potential, cognitive computing, deep learning, and virtual reality are the biggest growth potentials. And the most likely export products are virtual reality, data mining, and deep learning. Third, the priorities of AI technology development were cognitive computing, deep learning, data mining, virtual reality, natural language processing, computer vision, image recognition, speech recognition, machine learning, and pattern recognition. Finally, as a result of evaluating AI technology in terms of market growth potential, urgency of technology development, and ripple effect of technological development, AI technologies such as cognitive computing, data mining, and virtual reality have great ripple effect, urgent technology development, and market growth potential. However, AI technologies such as pattern recognition, machine learning, image recognition, speech recognition and computer vision are not urgent and have low ripple effect on technology development, and low market growth
Therefore, in order to successfully establish the AI technology development strategies, it is necessary to prioritize the cognitive computing technology that has great market growth potential, ripple effect of technology development, and the urgency of technology development according to the principle of selection and concentration. To this end, it is necessary to find creative ideas, manage assessments, converge multidisciplinary systems and strengthen core competencies. In addition, since AI technology has a large impact on socioeconomic development, it is necessary to comprehensively grasp and manage scientific and technological regulations in order to systematically promote AI technology development.최근 딥러닝 알고리즘의 발전, 데이터의 양적 증가, 컴퓨팅 파워 및 스토리지(Storage)의 개선, 네트워크의 성능 향상 등으로 인공지능(AI) 기술이 금융, 의료, 유통 등 다양한 분야에서 널리 활용되고 있다. 이러한 AI시장은 2018년 191억달러 규모로 전망되며, 연평균 46.2% 성장하여 2021년에는 522억 달러의 규모로 성장할 것으로 추정됨에 따라 미국, 캐나다, 일본 등 선진 주요 국가들은 AI 기술을 미래의 성장 동력으로 키우기 위해 다양한 정책을 개발 및 추진하고 있다. 이에 따라 우리나라에서도 2018년 5월에 I-Korea 4.0 실현을 위한 인공지능(AI) R&D 전략을 발표 및 정책을 추진하고 있다. 그러나 우리나라의 AI 기술력은 미국, 캐나다, 일본 등 주요 경쟁국 대비 부족한 상황이다. 따라서 4차 산업혁명에 대응하기 위하여 한정된 예산하에서 경쟁우위를 얻을 수 있도록 선택과 집중을 통해 효율적으로 AI R&D 예산을 배분할 필요가 있다. 이를 위해 본 연구에서는 문헌 연구 통해 도출된 평가지표를 이용하여 전문가 그룹의 설문을 통해 다차원적인 방법으로 AI 기술별 중요도를 평가하였다.
본 연구의 분석결과를 살펴보면 다음과 같다. 첫째, AI 기술 개발을 위해 고려해야 할 지표는 시장 성장성, 기술 개발 시급성, 기술 개발 파급 효과, 선진 기술 대비 기술 격차, 수입대체 효과, 수출 제품 가능성 등의 순으로 나타났다. 둘째, 평가지표별 AI 기술의 중요도를 살펴보면, 기술 개발 시급성 관점에서 인지컴퓨팅, 데이터마이닝, 딥러닝, 가상현실 등의 AI 기술 개발이 가장 시급하며, 선진 기술 대비 기술 격차 관점에서는 딥러닝, 인지컴퓨팅, 데이터마이닝 순으로 가장 크게 나타났다. 수입 대체 효과 관점에서는 딥러닝, 데이터마이닝, 인지컴퓨팅 순으로 대체 효과가 가장 크며, 기술 개발 파급 효과 관점에서는 인지컴퓨팅, 데이터마이닝, 가상현실 순으로 가장 큰 것으로 나타났다. 또한 시장 성장성 관점에서는 인지컴퓨팅, 딥러닝, 가상현실 순으로 시장 성장성이 가장 큰 것으로 나타났으며, 수출 제품 가능성은 가상현실, 데이터마이닝, 딥러닝 순으로 가장 가능성이 높은 것으로 나타났다. 셋째, AI 기술 개발의 우선순위는 인지컴퓨팅, 딥러닝, 데이터마이닝, 가상현실, 자연어 처리, 컴퓨터비전, 이미지인식, 음성인식, 기계학습, 패턴인식 순으로 나타났다. 마지막으로, 시장 성장성, 기술 개발 시급성, 기술 개발 파급 효과 관점에서 AI 기술을 평가한 결과, 인지컴퓨팅, 데이터마이닝 및 가상현실 등의 AI 기술은 기술 파급 효과가 크고, 기술 개발이 시급하며, 시장성장성이 크다. 그러나, 패턴인식, 기계학습, 이미지인식, 음성인식 및 컴퓨터비전 등의 AI 기술은 기술 개발이 시급하지 않을 뿐 만 아니라 기술 개발 파급 효과가 낮으며, 시장성장성 또한 낮다.
따라서 성공적으로 AI 기술 개발 전략을 수립하기 위해서는 선택과 집중의 원칙에 따라 시장성장성, 기술 개발 파급 효과, 기술 개발 시급성이 큰 인지컴퓨팅 기술을 우선적으로 추진할 필요가 있다. 이를 위해 창의적 아이디어 발굴, 평가관리, 다학제 융합 및 핵심 기초역량을 강화할 필요가 있다. 또한 AI 기술은 사회 경제적으로 파급 효과가 크기 때문에 AI 기술 개발을 체계적으로 추진하기 위해서는 과학기술 규제를 종합적으로 파악하여 관리할 필요가 있다.제1장 서론 １ 제1절 연구의 배경 １ 제2절 연구의 목적 ２ 제3절 연구의 구성 ３ 제2장 이론적 배경 ４

This thesis aims to analyze how after six nuclear experiments North Korean authorities have been using propaganda internally regarding these experiments. Until now, researches regarding North Korean nuclear experiments focused on nuclear policies and the cause for development of nuclear weapons, and researches regarding North Korean propagandas revolved around propagandas for North Korean regime, policies against South Korea and discourses about unification. Through six nuclear experiments since 2006 North Korean techniques for propaganda using nuclear weapons have evolved as much as the North Korean nuclear capability, but researches in this field have been lacking. Therefore, this research focuses on North Korea’s media coverage and internal propagandas regarding nuclear experiments which have not been addressed in depth before.
As target of analysis for North Korean internal propaganda as regarding nuclear experiments, 『Rodong Sinmun』, the newspaper published by Worker’s Party of Korea which represents the North Korean authorities has been selected. The scope of research has been limited to a time span beginning from 9th of October 2006, when the first nuclear experiment took place, to 31st of December 2017. The reason behind limiting the temporal scope of research is because 2017 was the most recent year in which North Korea conducted a nuclear experiment, as well as the year immediately preceding North Korea’s opening of conversation on the premise of denuclearization. For the method of research, Text mining Analysis, a technique which has recently started being used in the field of North Korean studies. Text mining is the process of using natural language processing technology of computers to extract certain patterns and useful information from large amounts of atypical texts and analyze them. The aim of this analysis is to process documents written with certain intent in order to find repeating and correlated patterns in between words and phrases. Text mining can automatically extract intentions which are not shown in texts by computer algorithms and is one of the methods suitable for literature analysis. The specific goals this research aims to achieve using text mining analysis on articles from 『Rodong Sinmun』 related to nuclear experiments are; ① Degree of interest North Korean authorities have regarding nuclear experiments ② Degree of similarities in articles regarding nuclear experiments ③ Keywords in articles regarding nuclear experiments ④ Main subject of articles regarding nuclear experiments.
In order to achieve goals previously mentioned, text mining analysis through stages of data collection, data preprocessing, data analysis, and visualization, and extra analysis on the content of 『Rodong Sinmun』 articles were done. Foremostly in the data collection stage, through searching the database (DB) provided by Information Center on North Korea of the Ministry of Unification, and reading the printed articles of 『Rodong Sinmun』, 2088 articles which contain ‘nuclear experiment’in title and main text were collected. Data cleaning process was performed against the collected data. For the data cleaning process, data preprocessing and morphological analysis were done. 13,529 nouns were extracted through data preprocessing. In the next stage, the data analysis stage, 『Rodong Sinmun』 articles published after the 1st-6th nuclear experiments and related to the experiments, were sectioned into six documents for each experiment, and text clustering analysis was performed to point out similarities and diversions between the documents. Keyword frequency analysis and topic modelling analysis were performed for each cluster automatically categorized by text clustering algorithm. The main results of this research are as follows:
First, after six instances of nuclear experiments, North Korean authorities’degree of interest in nuclear experiment increased steadily. Analyzing number of 『Rodong Sinmun』 articles containing the keyword ‘nuclear experiment’, an increase in the number of articles was shown in the year a nuclear experiment took place. In 2013, when the 3rd nuclear experiment took place, the number of articles rose to a noticeable amount of 639. During the period of nuclear experiments, the term of regime for Kim Jong-Il and Kim Jong-Eun were both around 6 years, but 1766 articles related to nuclear experiment, which are equivalent to 85% of all the articles, were published during the Kim Jong-Eun’s regime.
Second, through text clustering analysis which groups documents based solely on their similarities, the articles were categorized into three groups. 『Rodong Sinmun』 articles under cluster A are from the period of 1st nuclear experiment during Kim Jong-Il’s regime and the period after 6th nuclear experiment during Kim Jong-Eun’s regime. Articles under cluster B are from the period of 2nd nuclear experiment and the period after 4th and 5th experiment to 31th of December 2017. Examining the situation within North Korea revealed that the three clusters categorized by text clustering analysis were related to ‘stability of North Korean regime’. In relation to the stability of North Korean regime, cluster A was related to the most stable period, followed by cluster B and cluster C which was related to the least stable period.
Third, through measuring terms which appear at high frequency or have high importance in 『Rodong Sinmun』 articles regarding nuclear experiments, keywords were inferred. Term Frequency (TF) analysis which shows the number of times a term appears in 『Rodong Sinmun』 articles regarding nuclear experiments, and TF-Inverse Document Frequency (TF-IDF) analysis which filters out common terms that appear in all articles from the length of analysis and identify the keywords, were performed. Based on TF analysis, ‘USA’, ‘people’, ‘Choseon’, ‘war’, ‘South Choseon’, ‘Republic’ and ‘World’ appeared most frequently in all clusters. Based on TF-IDF analysis, in cluster A, ‘USA’, ‘practice’, ‘war’, 'South Choseon’, ‘unification’, ‘military’, ‘reverence’, ‘nation’, ‘motherland’ and ‘general’ had highest TF-IDF value in descending order. ‘Sanctions’ had a low TF value but a high TF-IDF value. In cluster B, terms such as ‘USA’, ‘General’, ‘people’, ‘Choseon’, ‘revolution’, ‘reverence’, ‘youth’, ‘South Choseon’, and ‘victory’ had high TF-IDF value. Terms such as ‘Park Geun-Hye’, ‘UN’, ‘Science’ were also used with high importance. In cluster C, terms with high TF-IDF value were ‘puppet’, ‘practice’, ‘war’, ‘construction’, ‘South Choseon’, ‘military’, ‘provocation’, ‘revolution’ and ‘North-South’ in descending order, and terms such as ‘Kim Jong-Il’, ‘economy’, ‘Park Geun-Hye’ and ‘parallel’were also used with high importance. By analyzing stability of regime and keywords of each cluster, it was shown that cluster A was related to propagandas of the regime at its plateau, cluster B was related to propagandas for the regime’s strategies for managing crisis, and cluster C was related to propagandas for Kim Jong-Eun’s proclaiming of regime.
Fourth, 『Rodong Sinmun』 articles regarding nuclear experiments dealt with subjects related to ‘advertising’ North Korean system and ‘criticizing’ hostile forces. Using Latent Dirichlet Allocation (LDA)during Topic Modeling analysis, 9 subjects of; ① advertising nuclear experiments ② advertising ‘Songun’ politics ③ advertising achievements of the supreme leader ④ advertising effort mobilization ⑤ advertising economy-nuclear parallel politics ⑥ criticizing anti-unification forces ⑦ criticizing UN-USA sanctions ⑧ criticizing South Korean government ⑨ criticizing South Korea-USA military practice, were extracted.
Fifth, main subjects addressed by 『Rodong Sinmun』 articles regarding nuclear experiments differed for each cluster. In cluster A where the regime was relatively most stable and cluster C where the regime was least stable, subjects related to criticism were addressed more than subjects related to advertisement. In cluster B where stability and instability coexisted, subjects related to criticism and subjects related to advertisement were equal in amount.
This research focused on North Korea’s propaganda regarding nuclear experiments which has not been the focus of North Korean studies. Text mining analysis based of North Korea’s 『Rodong Sinmun』 was performed to minimize subjective judgement and increase consistency, systematicity and verifiability. I believe that text mining technique as used in this research has presented a new possibility for researches related to North Korea. Therefore, follow-up studies should be done in order to expand and develop the technique into diverse fields of researches regarding North Korea.본 논문은 여섯 번의 핵실험 이후 북한 당국이 핵실험에 대하여 대내적으로 어떻게 선전선동하고 있는지 알아보고자 하였다. 그 동안 북한의 핵 관련 연구는 핵정책이나 핵을 개발하게 된 원인 분석에 초점을 맞추었고, 북한의 선전선동 연구도 북한 체제에 대한 선전이나 대남 정책 및 통일 담론에 대한 연구 위주로 이루어졌다. 2006년 이후 여섯 번의 핵실험을 거치며 북한의 핵 능력 발전만큼 핵을 이용한 선전선동의 기술도 진화하였지만 이 분야의 국내 연구는 미비했었다. 따라서 본 연구는 지금까지 깊이 있게 다루지 않았던 북한의 핵실험에 대한 보도와 대내 선전선동에 주목하였다.
북한의 핵실험 관련 대내 선전선동의 분석을 위하여 조선노동당의 당 기관지로서 북한 당국의 입장을 대변하는 노동신문 기사를 분석 대상으로 선정하였다. 연구 범위는 첫 핵실험을 진행한 2006년 10월 9일부터 2017년 12월 31일로 제한하였다. 시간적 범위 제한의 이유는 2017년은 가장 최근에 북한이 핵실험을 진행한 해이기도 하며, 비핵화를 전제로 대화 공세를 펼치기 직전의 시기이기 때문이다. 그리고 연구 방법으로는 최근 북한학 연구에서도 다양하게 시도되고 있는 텍스트 마이닝 분석(Text mining Analysis) 기법을 활용하였다. 텍스트 마이닝이란 방대한 양의 비정형 텍스트를 컴퓨터의 자연어 처리 기술을 이용하여 텍스트에서 일정한 패턴을 파악하고 유용한 정보를 추출하여 분석하는 작업을 의미한다. 이 분석 작업의 목적은 의도를 가지고 작성한 문서의 데이터를 가공한 뒤 단어와 어구 사이에서 반복 및 연관성을 가진 패턴을 찾는 것이다. 텍스트에서는 드러나지 않는 의미를 컴퓨터의 알고리즘으로 자동 추출하는 방식으로 문헌 분석에 적합한 방법 중의 하나이다. 본 연구는 노동신문의 핵실험 관련 기사에 대한 텍스트 마이닝 분석의 구체적인 목표는 ① 북한 당국의 핵실험에 관한 관심도 ② 핵실험 관련 기사의 유사성 ③ 핵실험 관련 기사의 주제어 ④ 핵실험 관련 기사의 주제는 무엇인지 알아보는 것이다.
이를 위하여 자료수집과 데이터 전처리, 자료 분석 및 시각화 등의 단계를 거쳐 텍스트 마이닝 분석을 수행하고, 노동신문 기사의 내용 분석을 추가로 실시하였다. 먼저 자료수집 단계에서는 통일부 북한자료센터에서 제공하는 데이터베이스(DB) 검색과 노동신문 인쇄물을 직접 읽고 제목과 본문에 ‘핵실험’을 포함하는 기사 2088건을 수집하였다. 수집된 자료에 대하여 데이터 클리닝(Data cleaning) 작업을 수행하였다. 데이터 클리닝 작업은 데이터 전처리(data preprocessing)와 형태소 분석(morphological analysis)을 수행하였다. 데이터 전처리 과정을 통하여 추출된 명사는 모두 13,529개이다. 다음으로 자료 분석 단계에서는 1-6차 핵실험 이후 노동신문의 핵실험 관련 기사를 핵실험 차수별로 여섯 개의 문서로 구분한 후, 문서 간 유사도 및 분기점을 파악하기 위하여 텍스트 클러스터링 분석(Text Clustering Analysis)을 실시하였다. 텍스트 클러스터링 알고리즘이 자동적으로 분류한 군집별로 주제어(Keyword) 빈도 분석과 토픽 모델링 분석(Topic Modeling Analysis)을 수행하였다. 본 연구의 주요 결과는 다음과 같다.
첫째, 여섯 번의 핵실험 이후 북한 당국의 핵실험의 관심도는 꾸준히 증가했다. ‘핵실험’ 키워드를 포함한 노동신문 기사의 게재수를 분석한 결과, 핵실험을 실시한 해당 연도에는 노동신문의 핵실험 관련 기사도 함께 증가하는 양상을 보였다. 특히 3차 핵실험이 있었던 2013년에 639건으로 기사가 눈에 띄게 증가했음을 확인할 수 있었다. 핵실험 진행 시기에 김정일-김정은 집권 기간은 약 6년으로 비슷했지만, 전체 기사 중 약 85%에 해당하는 1766건의 핵실험 관련 기사가 김정은 정권에서 보도되었다.
둘째, 오로지 개체들 간의 유사성(Similarity)에만 기초하여 문서를 자동적으로 그룹화하는 텍스트 클러스터링(Text Clustering) 분석을 실시한 결과, 세 개의 군집으로 구분되었다. A군집에 해당하는 노동신문의 기사는 김정일 정권에서 진행한 1차 핵실험과 김정은 정권에서 진행한 6차 핵실험 이후 기사이다. B군집에는 2차 핵실험과 4차‧5차 핵실험 이후부터 2017년 12월 31일까지 기사가 해당된다. 북한 국내 상황을 살펴 본 결과, 텍스트 클러스터링 분석으로 구분 된 세 개의 군집은 ‘북한 정권의 안정 정도’와 관련되었다. 군집별로 북한 정권의 안정도를 살펴보면, A군집은 군집 중에서 상대적으로 가장 정권이 안정되었고, 그 다음은 B군집의 정권 안정도가 높았으며 마지막으로 C군집이 정권 안정도가 가장 불안했다.
셋째, 노동신문 핵실험 관련 기사에 자주 등장하는 단어와 중요도가 높은 단어를 측정하여 주제어를 알아보았다. 노동신문 핵실험 관련 기사에 등장하는 단어의 횟수를 나타내는 단순 ‘단어 빈도(Term Frequency: TF)’와 분석 기간 모든 기사에 나오는 상투적 단어를 걸러내고 주제어를 파악할 수 있는 ‘단어 빈도-역문서 빈도(TF-Inverse Document Frequency: TF-IDF)’ 분석을 수행하였다. TF를 기준으로 분석했을 때, ‘미국’, ‘인민’, ‘조선’, ‘전쟁’, ‘남조선’, ‘공화국’, ‘세계’ 등이 군집별 공통적으로 출현 빈도가 높았다. TF-IDF 기준으로 분석한 결과, A군집은 ‘미국’, ‘연습’, ‘전쟁’, ‘남조선’, ‘통일’, ‘군사’, ‘경애’, ‘민족’, ‘조국’, ‘장군’ 순으로 TF-IDF의 값이 높았다. ‘제재’는 TF값은 높지 않았지만 TF-IDF 값이 높은 것으로 나타났다. B군집은 ‘미국’, ‘장군’, ‘인민’, ‘조선’, ‘혁명’, ‘경애’, ‘청년’, ‘남조선’, ‘승리’와 같은 단어의 TF-IDF 값이 높았다. 또한 ‘박근혜’, ‘유엔’, ‘과학’ 등의 단어도 중요하게 다루었던 것으로 확인되었다. C군집에서 TF-IDF값이 높은 단어는 ‘괴뢰’, ‘연습’, ‘전쟁’, ‘건설’, ‘남조선’, ‘군사’, ‘도발’, ‘혁명’, ‘북남’ 등의 순으로 나타났고, ‘김정일’, ‘경제’, ‘박근혜’, ‘병진’도 중요하게 거론되었다. 군집별 정권 안정도와 주제어 분석을 토대로 하면 A군집은 정권 안정기의 선전선동, B군집은 위기관리 전략의 선전선동, C군집은 김정은 시대 선포의 선전선동과 관련된 것으로 나타났다.
넷째, 노동신문의 핵실험 관련 기사는 북한 체제 ‘선전’과 적대 세력 ‘비난’과 관련된 주제를 다루었다. 토픽 모델링(Topic Modeling) 분석 중 잠재 디리클레 할당(Latent Dirichlet Allocation: LDA) 기법을 활용하여 분석 결과, 노동신문 핵실험 관련 기사에서 ① 핵실험 선전 ② 선군정치 선전 ③ 최고지도자 업적 선전 ④ 노력 동원 선전 ⑤ 경제-핵 병진노선 선전 ⑥ 반통일 세력 비난 ⑦ 유엔-미국 제재 비난 ⑧ 남한 정부 비난 ⑨ 한미 군사연습 비난 등의 아홉 가지 주제가 추출되었다.
다섯째, 노동신문 핵실험 관련 기사에서 중요하게 다룬 주제는 군집별로 차이가 나타났다. 상대적으로 정권이 가장 안정되었던 A군집과 정권이 가장 불안정했던 C군집에서는 비난 관련 주제를 선전 관련 주제보다 많이 다루었다. 정권의 안정과 불안이 공존했던 B군집은 선전과 비난 관련 주제가 동일한 것을 확인할 수 있었다.
본 연구는 북한학계가 그 동안 주목하지 않았던 북한의 핵 관련 선전선동에 주목하였다. 그리고 북한 노동신문을 기반으로 텍스트 마이닝 분석을 시도함으로써 연구자의 주관적 판단을 최소화하고 일관성과 체계성, 검증 가능성을 높이고자 했다. 본 연구에서 활용한 텍스트 마이닝 기법은 새로운 북한 관련 연구의 가능성을 제시했다고 생각한다. 따라서 후속 연구를 통하여 북한 연구의 다양한 분야로 확대 발전시킬 필요가 있을 것이다.Ⅰ. 서론 1 A. 연구의 배경 1 B. 연구의 목적 및 차별성 5 Ⅱ. 이론적 배경 8 A. 북한 핵무기 개발 및 핵실험 전개 과정 8

A Study on a Linguistic Consciousness in Formative Period of Modern Korean Linguistics

Yang, Geun-Yong
Dept. of Korean Language and Literature
Graduate School, University of Incheon

The purpose of this study is to find linguistic consciousness about the objects of a language study produced in the formative period of modern Korean linguistics. This means that this study investigate the observation attitude and the stream concerning the study objects which was ignored by the history of Korean linguistics.
The period of the modern Korean linguistics was divided into four sections : the period of Dae-Han Empire, the period under Japanese colonization, the period of post-Independence, and the period of pre-Korean War. In this study, the generality on the linguistic consciousness in the Dae-Han Empire period was considered as materiality and a regulation for spoken and written language. The generality in the period under Japanese colonization was considered as a regulation for spoken and written language pertaining to dialects, loan words, orthography, dictionaries, standard language, and linguistic consciousness which emphasized the mutual relations with a people, culture, and science. The generality in the periods of post-Independence and pre-Korean War was considered as an interest about scientific study of Korean linguistics based on general linguistics and about national unique spoken language and Korean education in order to escape from the influence of Japanese. The results from the contents discussed in each chapter based on the generalities are as follows.
The linguistic consciousness of Dae-Han Empire period can be summarized into four sections. First, the consciousness on a part of speech was to classify a language itself, and there were 6 parts of speech, 7 parts of speech and 8 parts of speech. Because of differently dealt with postpositions, these various classifications took place. Second, a vocabulary 'Korean(National) Language' was resulted from combination of 'a nation' with 'a letter', which excluded Chinese letters, with regarding them an obstacle after setting binarity relationship between Korean and Chinese. And the identity of spoken and written language was an outcome on recognizing the mutual relationship of the spoken language and written language as identity and restoration, which means that the spoken language is the written language and the written language is the spoken language.
Third, a style mixed Korean with Chinese was to arrange Chinese letters in the Korean word order after excluding Chinese writing by setting Chinese letters as an assistance of Korean. The style was recognized as a style which embodied the identity of spoken and written language, However, it used to regarded as a problem which disturbed the embodiment of the identity of spoken and written language. It was resulted from two contrary recognitions. One is that Chinese letters in the style caused double illiteracy, and the other is that they were assistant means of Korean. Fourth, phonogram was recognized as a letter which embodied homography. And the classification system of a letter was regarded same as that of initial, medial, and final sound. This resulted from setting the initial sound and final sound as a same character based on the regulation that initial sound is used again in the location of final sound. Ju, Sikyeong set unchangeable original forms, fundamental sounds and fundamental bodies, which distinguishes an ending of the word from a stem. This is origin of orthographic principle which sought for representing the meanings of the phonogram.
The linguistic consciousness under the period of Japanese colonization can be summarized as follows. First, Korean study that emphasized the national culture set the mutual relationship between the national culture and the language as a mental category, and it performed toponymy which stressed community of a cultural area in Northeast Asia and features of Korean culture. Second, the consciousness that enforces natural character of Esperanto was more perfectly recognized Esperanto than natural language. On the other hand, the consciousness that Esperanto is superior artificial language made us recognize natural character of language as a artificial product, and it regarded the artificial character of language as essence of language. Third, a Draft for Unified Loanwords Orthography premised the fact that international pronunciation symbols embodied homography, with looking squarely marking limitation of native letters about loanwords sound. And the treatment on the Chinese formed a linguistic consciousness that clearly made difference of the concept between a foreign language and a loanword.
Fourth, a horizonal writing and a meaning representation of phonogram based on a phoneme notion were stressed on Hangeul Proclamation Day. Fifth, a discussion in the movement for improving the system of spelling handled the problems of rationality about agreement between linguistic theories and linguistic facts. Futhermore, scientific language study on Korean made people naturally accept divided speech function between specialists and common people. And it made linguistic consciousness detailed based on the general linguistics and created a domain of Korean Study. Sixth, the geographical, era, and social class standards were introduced to set the terms of the standard language, which formed the consciousness that differentiates dialects from the standard language. Seventh, the interest in the orthography brought about the consciousness which classifies into 'grammatical expressions' and 'habitual expressions' according to the grammatical rules and pronunciation systems. Eighth, a discussion about the orthography created a possibility which was able to think other study viewpoints on given linguistic facts, with causing linguistic consciousness which excluded each other's viewpoints.
The linguistic consciousness of pos근대국어학 형성기의 언어의식 연구

본 연구는 근대국어학의 형성기에 중요하게 다루었던 국어학의 연구대상들에 관한 언어의식을 밝히는 데 목적을 둔다. 이는 기존의 국어학사에서 경시한 연구대상에 관한 관찰태도와 그 양상의 흐름을 고찰하는 것이기도 하다.
근대국어학의 시대범위는 대한제국시대와 일제강점기, 그리고 광복 이후와 한국전쟁 이전까지로 세분해서 설정하였다. 본 논문에서는 대한제국시대의 언어의식에 관한 일반성을 언어의 유형화와 어문규범화라고 보았다. 일제강점기의 언어의식에 관한 일반성은 민족, 문화, 그리고 과학과의 상호관계를 강조한 연구태도와 표준어, 방언, 외래어, 표기법, 사전에 관한 어문규범이라고 보았다. 광복 이후와 한국전쟁 이전의 언어의식에 관한 일반성은 일반언어학을 토대로 한 국어학의 과학적 연구와 일본어의 영향으로부터 탈피하기 위한 민족 특유의 음성언어와 국어교육에 대한 관심이라고 보았다. 이상의 근대국어학에 나타난 언어의식의 일반성을 바탕으로 각장에서 논의한 내용을 정리하면 다음과 같다.
대한제국시대의 언어의식은 네 가지로 요약될 수 있다. 첫째, 품사인식은 언어 그 자체를 분류한 어종의 유형화로서 6품사, 7품사, 그리고 8품사로 나타났다. 이러한 분류유형의 차이는 토(吐)를 품사로 처리하는 방식이 서로 달랐기 때문에 발생한 것이다. 둘째, 국문이라는 단어는 ‘국가’와 ‘문자’의 개념을 결속한 결과인데, 이 언어의식은 국문과 한문을 이항대립의 관계로 설정한 다음에 한자를 문명의 장애물이라고 배척했다. 그리고 언문일치는 언(言)과 문(文)의 상호관계를 동일성과 환원성으로 인식한 결과로서 글이 곧 말이고 말이 곧 글이라는 언어의식이었다.
셋째, 국한자교용체는 한자를 국문의 보조물로 설정하고 한문을 배제한 다음에 우리말의 어순으로 한자를 배열한 문장형태이다. 국한자교용체는 언문일치를 구현하는 문체로 인식되기도 하지만 그것의 구현을 저해하는 문제의 대상으로도 간주되었다. 이는 국한자교용체의 한자가 이중의 문맹을 야기한다는 부정적 인식과 국어의 보조수단이라는 긍정적 인식이 대립된 결과이다. 넷째, 표음문자는 그 자체로 일자일음(一字一音)을 구현하는 언문일치의 문자로 인식되었다. 그리고 자모(子母)의 분류방식은 초·중·종성의 분류방식과 동일한 것으로 간주되었다. 이는 훈민정음의 ‘종성부용초성’을 근거로 삼아서 초성과 종성을 동일한 성질로 설정한 결과이다. 주시경은 어미와 어간을 구별하는 본음(本音)과 본체(本體)라는 고정불변의 원형을 설정하였는데, 이는 표음문자의 표의화를 추구한 표기법원리의 기원이다.
일제강점기의 언어의식은 다음과 같이 요약될 수 있다. 첫째, 민족문화를 강조한 조선어연구는 민족문화와 언어의 상호관계를 심적 범주로 설정하였고, 동북아문화권의 공통성과 조선문화의 특색을 강조한 지명연구도 등장시켰다. 둘째, 에스페란토의 자연성을 강화는 인식은 에스페란토를 자연어보다 완벽한 자연어로 인식하였다. 반면에 에스페란토가 우수한 인공어라는 인식은 언어의 자연성을 인공의 산물로 파악했고, 언어의 인공성을 언어의 본질로 보았다. 셋째, 외래어표기법통일안은 자국문자의 외래어음에 관한 표기 한계를 직시했으며, 국제음성기호가 일자일음의 구현이라는 사실을 전제하였다. 그리고 외래어로서의 한자어에 관한 처리문제는 외국어와 외래어의 개념을 명확하게 구분하는 언어의식을 강조하였다.
넷째, 한글날에는 음소개념을 바탕으로 가로풀어쓰기와 표음문자의 표의화를 강조하였다. 다섯째, 철자법개량운동의 논쟁은 언어이론과 언어사실의 일치여부에 관한 합리성의 문제를 다룬 것이었다. 게다가 조선어에 관한 과학적 언어 연구는 연구자와 학습자로 분화된 언어활동을 자연스런 현상으로 받아들이게 만들었고, 일반언어학을 바탕으로 한 언어의식의 세밀화와 조선어에 관한 연구 영역을 창출시켰다. 여섯째, 표준어의 설정조건은 지리적 표준과 시대적 표준, 그리고 계급적 표준으로 구분되었으며, 방언과의 차별성 인식을 형성시켰다. 일곱째, 표기법에 대한 관심은 문법적 규칙을 적용한 ‘문법의 말’과 발음현상을 그대로 표기하는 ‘습관의 말’로 구분되는 언어의식을 초래하였다. 여덟째, 어문규범에 대한 논쟁은 주어진 언어사실에 관해서 서로 다른 연구관점으로 사유하는 가능성을 창출하면서 동시에 서로의 관점을 배척하는 언어의식을 초래하였다. 아울러 사전편찬은 어문규범의 표상이자 집대성으로 인식되었다.
광복 이후와 한국전쟁 이전의 언어의식은 다음과 같이 정리될 수 있다. 첫째, 정렬모의 문법모형은 낱뜻론과 감말론으로 양분된 체계인데, 이는 종래의 품사분류에서 전개한 감말과 낱뜻의 혼동인식과 품사의 상위분류와 하위분류를 혼용한 상태를 바로잡은 것이다. 그러나 이 문법모형은 마쓰시다 다이사부로(松下大三郞)의 문제의식을 공유한 산물이었다. 둘째, 실험음성학은 음성의 생리적 관계를 연구하는 자연과학의 학문분야로 인식되었다.목차 <국문요약>

Since the advent of social media, more people have been consuming news through Social Network Services(SNS) such as Facebook and Twitter rather than through traditional media such as television and newspapers. News Consumption using SNS have the advantage of being accessible and less expensive. However, it also has some serious disadvantages. First, because anyone can generate information on social media, false information can be easily generated. Secondly, because any information can be spread quickly on social media, there is always the risk of exposure to fake news for SNS users. Fake news is problematic because it can cause political and economic problems to news consumers and the entire community. For example, if SNS is misused for political purposes, voters using SNS can make wrong decision by being affected by fake news. In addition, if fakes news about certain individuals or companies spread, there is a possibility that their economic value will decrease. Concerns over the seriousness of the fake news problem have spread around the world since the 45th U.S. presidential election in 2016. Therefore, the importance of detecting fake news has emerged and studies to prevent fake news on social media have been actively conducted overseas.
On the other hand, domestic research on fake news detection is not enough even though fake news problems are rampant in Korean society as well. Therefore, in this study, we aim to detect Korean fake news on SNS. Specifically, the goal of this study is to build a classification model that identifies the authenticity of the Korean Twitter. To achieve the goal, this research extracts a variety of linguistic input variables that can represent the textual characteristics of news on Twitter rather than extracting feature vectors using “Term Frequency(TF)” or “Term Frequency-Inversed Document Frequency(TF-IDF)” techniques that are commonly used in feature extraction. In addition, a meaningful group of variables was selected by conducting feature selection using Stepwise Regression.
According to the study, the proposed model showed 70.53% accuracy, 85.76% recall, 71.18% precision, and 77.24% F-measure by outperforming the performance of TF-based model and TF-IDF based model in general.
The main contribution of this research is to build a Korean fake news classifier since there has not been enough research about Korean fake news detection before. Also, the proposed model using content-based features outperformed the models using TF and TF-IDF, which identifies the importance of various types of linguistic features in fake news classification.
On the other hand, our research has a limitation of not having enough data. Therefore, it is necessary to collect more data in future studies to explore ways to make the research more reasonable. Furthermore, it is expected to be a meaningful follow-up study to conduct semi-supervised learning-based research because it can build a high-performance automated classifier even when data size is not large.최근 TV나 신문과 같은 전통적인 매체보다 페이스북, 트위터 등의 소셜 네크워크 서비스(Social Network Service, SNS)를 통해 뉴스를 소비하는 인구가 증가하고 있다. SNS를 이용하면 언제 어디서나 편리하게 뉴스에 접근할 수 있으며, 비용을 절감할 수 있기 때문이다. 하지만 SNS에는 가짜 뉴스가 쉽게 생성될 수 있으며, 한번 생성된 가짜 뉴스는 빠르게 확산되어 SNS 이용자들에게 노출된다는 문제를 야기할 수 있다. 이와 같은 가짜 뉴스의 확산은 뉴스 소비자 개인과 전체 사회에 정치적, 경제적으로 악영향을 끼칠 수 있기 때문에 ‘SNS 상의 가짜 뉴스’는 최근 심각한 문제로 평가되고 있다. 구체적으로, 정치적 목적으로 SNS에 유포된 가짜 뉴스는 잘못된 여론 형성을 유발하여 SNS를 사용하는 수 많은 유권자들의 의사를 왜곡하는 문제가 발생할 수 있다. 또한, 개인이나 기업에 대한 가짜뉴스가 확산될 경우 브랜드의 가치가 하락하여 막대한 경제적 손실이 발생할 위험이 있다.
한편, 가짜 뉴스는 전세계적인 차원의 문제로 인지되고 있다. 2016년 45대 미국 대통령 선거에서 소셜 미디어를 통해 유출된 가짜 뉴스가 트럼프의 승리에 영향을 주었다는 연구 결과가 발표되었고, 이를 계기로 국외에서는 가짜 뉴스를 탐지하는 연구의 중요성이 대두되었다. 그 결과, 페이스북과 구글이 ‘가짜 뉴스’와의 전쟁을 선포하는 등 가짜 뉴스 문제를 해결하기 위한 연구들이 국외에서 다각도로 활발히 진행되고 있다. 국내에서도 가짜 뉴스 문제가 심각한 사회적 이슈로 자리잡고 있다. 하지만, 국외에 비해 가짜 뉴스 탐지에 대한 연구가 미흡한 실정이다. 따라서, 본 연구에서는 SNS 상의 국내 가짜 뉴스를 대상으로 가짜 뉴스 탐지 연구를 진행하여 국내 가짜 뉴스 문제 해결에 기여해보고자 한다.
구체적으로, 본 연구는 한국어 트위터의 진위 여부를 파악해주는 분류 모형을 구축하는 것을 목표로 한다. 이를 위해, 텍스트의 문체적 특징을 대변해 줄 수 있는 언어학적 변수들을 추출하는 ‘콘텐츠 기반 변수 추출 방법(Content-based Feature Extraction Method)’에 의거하여 분류 모형을 구축하고자 한다. 텍스트 분류 연구에서는 비정형 텍스트로부터 정형화된 자질 벡터를 추출하기 위해 주로 단어 빈도(Term Frequency, TF) 혹은 단어 빈도-역문서 빈도(Term Frequency-Inverse Document Frequency, TF-IDF) 기법을 사용하는데, 이들은 텍스트의 어휘 특성만을 기반으로 하여 입력 값을 산출한다는 한계점을 지니고 있다. 따라서, 본 연구에서는 콘텐츠 기반 변수 추출 방법을 통해 어휘, 문장 구조, 문법 등 보다 다양한 범주의 언어학적 특징들에 주목하여 텍스트의 진위 여부를 파악해주는 우수한 성능의 분류기를 구축하는 것을 목표로 하였다.
연구 결과, 제안 모형은 정확도 70.53%, 재현율 85.76%, 정밀도 71.18%, F값 77.24%의 성능을 보여 전통적인 텍스트 자질 추출 방법인 TF나 TF-IDF를 적용한 모형보다 전반적으로 우수한 성과를 도출하였다. 이를 통해, 자연어처리 기반의 가짜 뉴스 분류 문제에서는 다양한 범주의 언어학적 변수들을 사용하는 것이 효과적임을 확인할 수 있다.
본 연구는 텍스트의 언어학적 특징에 주목하는 ‘콘텐츠 기반 변수 추출 방법’을 사용하여 우수한 성능의 가짜 뉴스 탐지 모형을 구축하였다. 가짜 뉴스가 사회적으로 심각한 문제로 자리잡고 있는 현 시점에서, 본 연구는 SNS 상의 가짜 뉴스의 문체 특징을 분석하여 이를 기반으로 가짜 뉴스 여부를 알 수 있는 우수한 성능의 분류 모형을 구축하였다는 점에서 의미가 있다. 또한, 국내 가짜 뉴스 연구가 거의 전무한 상황에서 한국어 대상의 가짜 뉴스 분류 모형을 구축하였기에 학술적으로 의미가 있다고 사료된다. 나아가, 영어 대상의 가짜 뉴스 뿐만 아니라 한국어 대상의 가짜 뉴스에서도 ‘콘텐츠 기반의 변수 추출 방법’을 적용한 모형이 우수한 분류 성과를 보였으므로, 가짜 뉴스 도메인에서 다양한 범주의 언어학적 특징들을 변수로 추출하는 것이 중요함을 확인할 수 있다.
한편, 본 연구의 한계점은 다음과 같다. 우선, 데이터 확보의 어려움으로 인해 데이터의 양이 크지 않았다는 아쉬움이 따른다. 따라서 향후 연구에서는 데이터를 추가적으로 수집하여 연구의 타당성을 보다 높일 수 있는 방안을 모색할 필요가 있다. 나아가 레이블링된 데이터가 부족한 상황에서도 우수한 성능의 자동화된 분류기를 구축할 수 있는 준지도학습(Semi-supervised Learning) 기반의 연구를 진행하여 가짜 뉴스 도메인에서 발생할 수 있는 Cold-start 문제를 해결하는 것이 의미 있는 후속 연구가 될 것으로 예상된다. 또한, 콘텐츠 기반의 언어학적 변수들을 세분화하여 더욱 다양한 범주의 변수들을 고려한다면 더욱 우수한 성능의 분류 모형을 구축할 수 있을 것으로 보인다.Ⅰ. 서론 1 A. 연구 배경 1 B. 연구 목적 4 Ⅱ. 선행연구 5 A. 콘텐츠 기반 가짜 뉴스 탐지 연구 5

The research on Word Sense Disambiguation(WSD) that distinguishes the meaning of words in context using computers has been a subject of research in the field of Natural Language Processing(NLP) and has been a subject of interest since the early 1950's, when computer language processing began. Especially, about 30% of Korean vocabularies are identified as homonyms, so it is necessary to resolve the ambiguity of words for automatic translation and parsing.
Although Korean studies have been consistently conducted in Korea, there are less number of studies on the Korean language than overseas studies on the English language, deriving statistical rules from supervised learning of small-scale sense tagged corpus, and a mixture of dictionary based approaches. The supervised learning approach has a relatively high performance in terms of WSD. However, there is a great limitation in that it is difficult to obtain or produce high-quality corpus that human beings attach meaning to.
Outside of Korea, various unsupervised learning approaches have been studied such as graph-based approach and machine learning. After Word2Vec, which applied Deep learning to text in 2013, has been published, Word Sense Induction(WSI) studies have been conducted to extract the meaning of word by learning and clustering context. However, there has yet to have studies related with WSD using it.
In order to overcome the limitation of the supervised learning method, this study attempted to solve the semantic ambiguity of nouns by unsupervised learning of large corpus without a knowledge base. This study also proposed a new method to link WSD, which labels the meaning of words after word sense inference, using a Multi-Prototype Skip-Gram model that extends Word2Vec's Skip-Gram model.
The training corpus used in the study consisted of a combination of Korean news articles, Korean Wikipedia, Sejong modern written Korean corpus, and KAIST raw corpus. This study collected the machine-readable dictionary, 'Korean Basic Dictionary' on the web and used it to attach the dictionary meaning. Also, this study used Korean data from the SENSEVAL-2, almost the only open evaluation data of the Korean ambiguity resolution.
In this study, sense disambiguation proceeded as follows. After pre-processing all learning documents, context-learning was performed using a multi-prototype Skip-Gram model. The resulting prototypes (meanings of clustering of ambiguous words) were semantic labeling according to the degree of correspondence with the words in the example sentence in Korean Basic Dictionary. Next, each of the evaluation examples was mapped with the prototype in the same manner as the dictionary example mapping, and the dictionary meaning associated with the prototype was tagged as a final meaning.
As a result of the experiment, when learning the entire corpus containing more than half of the news articles was performed, the accuracy of semantic attachment was rather improved when the scale was reduced to 1/2 or 1/4. In order to reduce the specificity of the message type called news articles, the proportion of the article inquiry was reduced to ½ and ¼, and as a result, the accuracy rate improved to 71.03% even though the size of corpus was smaller than before.
In the word window size experiment, which means the extent of the context of the word in learning, the performance was improved with widening the range of 5, 7, and 10, and the accuracy was 76.91% at the maximum. Finally, in the performance comparison experiment according to the type of POS tag set used in the experiment, cases using only 6 real morphemes were better in the performance than cases using 15 parts including formal morphemes.
As a result of the study, this study confirmed that the Deep learning based unsupervised learning method can be applied to WSD. By using the unsupervised learning and machine-readable dictionary with no knowledge base, this study achieved about 10% to 15% higher accuracy than previous unsupervised learning based studies. In addition, by extending Word2Vec, which is easy to use for contextual learning, beyond word sense inference to sense disambiguation research, this study is worth noting that even more objective performance evaluation is possible and it is applicable to various fields such as automatic translation.컴퓨터를 이용하여 문맥 내에서 단어의 의미를 구별하는 단어 의미 중의성 해소(Word Sense Disambiguation) 연구는 자연어처리 분야에서 핵심적인 연구 주제이며, 컴퓨터로 언어 처리를 하기 시작한 초창기인 1950년대부터 많은 관심의 대상이 되어 왔다. 특히 한국어는 어휘의 약 30% 가량이 동음이의어로 파악되고 있어 자동 번역이나 구문 분석 등을 위해 단어의 중의성을 해소하는 것이 반드시 필요하다.
국내에서도 한국어를 대상으로 한 연구가 꾸준히 수행되어 왔지만 영어를 대상으로 한 국외 연구들에 비해 그 개수가 적고, 소규모 의미 부착 말뭉치를 지도 학습해 통계적 규칙을 추출하고 여기에 사전 기반 접근을 혼합한 것들이 많았다. 지도 학습 접근법은 단어 의미 중의성 해소 성능이 상대적으로 우수하지만 사람이 직접 의미를 부착한 양질의 말뭉치는 구하거나 제작이 어렵다는 큰 한계가 있다.
국외에서는 그래프 기반 접근, 기계학습 등 국내에 비해 다양한 비지도 학습 접근법들이 연구되어 왔다. 2013년 딥 러닝을 텍스트에 적용한 Word2Vec이 발표된 후 이를 이용해 문맥을 학습하고 군집화하여 단어의 의미를 추출하는 단어 의미 추론(Word Sense Induction) 연구들이 수행되고 있으나 아직 이를 이용한 단어 의미 중의성 해소 연구는 거의 없는 실정이다.
본 연구는 지도 학습법의 한계를 극복하기 위해 지식베이스가 없이 대량의 말뭉치를 비지도 학습하여 명사를 대상으로 의미 중의성 해소를 시도하였다. 또한 Word2Vec의 Skip-Gram 모델을 확장한 다중 프로토타입(Multi-Prototype) Skip-Gram 모델을 이용하여 단어 의미 추론 이후 사전적 의미를 라벨링하는 단어 의미 중의성 해소까지 연결시키는 새로운 방법을 제안하였다.
연구에서 사용한 학습 말뭉치는 한국어 뉴스기사, 한국어 위키백과, 세종 현대 문어 말뭉치, 카이스트 원시 말뭉치의 병합으로 구성되었으며, 사전적 의미 부착을 위해서 기계가독식 사전인 ‘한국어 기초사전’을 웹에서 수집하여 사용하였다. 또한 성능의 평가에는 거의 유일하게 공개된 한국어 의미 중의성 해소 평가 데이터인 SENSEVAL-2 대회에서의 한국어 데이터를 이용하였다.
본 연구에서 의미 중의성 해소는 다음과 같은 절차로 진행되었다. 우선 학습 문헌들을 모두 정제한 후 다중 프로토타입 Skip-Gram 모델을 이용해 문맥 학습하였다. 그 결과로 얻어진 각각의 프로토타입(중의 단어의 군집화된 각 의미)을 한국어 기초사전의 예문 내의 단어들과 일치하는 정도에 따라 의미 라벨링을 하였다. 다음으로 평가용 예제 각각에 대해 사전 예문 매핑과 동일한 방법으로 프로토타입과 매핑시킨 후 이와 연결된 사전적 의미를 최종 의미로 부착하였다.
실험 결과, 뉴스기사가 절반 이상 포함된 말뭉치의 전체를 학습했을 때는 규모를 1/2, 1/4로 줄여나갈 때 오히려 의미 부착 정확률이 향상되는 모습을 보였다. 뉴스기사라는 문종(文種)의 특수성을 줄이기 위해 기사문의 비중을 1/2, 1/4로 줄인 결과, 이전보다 말뭉치의 규모가 줄었음에도 정확률은 더욱 향상되어 최고 71.03%를 기록하였다.
학습 시 단어의 문맥 범위를 얼마나 넓게 할 것인지를 뜻하는 단어 윈도우 크기 실험에서는 5, 7, 10으로 그 범위를 넓혀 갈수록 성능이 향상되어 최고 76.91%의 정확률을 보였다. 마지막으로 실험에 사용한 품사 집합의 종류에 따른 성능 비교 실험에서는 실질 형태소 6개만을 사용한 경우가 형식 형태소까지 포함한 15개 품사를 사용한 경우보다 성능이 더 좋았다.
연구 결과, 여러 실험들을 통해 딥러닝 기반 비지도 학습법이 단어 의미 중의성 해소 연구에 적용될 수 있음을 확인하였다. 본 연구는 지식베이스가 없는 상태에서 비지도 학습과 기계가독형 사전만을 이용해 선행 비지도 학습 기반 연구들에 비해 약 10~15% 높은 정확률의 향상을 이루었다. 또한 문맥 학습에 용이한 Word2Vec을 단어 의미 추론을 넘어 의미 중의성 해소 연구까지 확장함으로써, 보다 객관적인 성능 평가가 가능하고 자동번역 등 다양한 분야에 적용이 가능하도록 하였다는 데에서도 본 연구의 가치를 찾을 수 있다.

Requirements traceability is one of the quality attributes in software engineering field and requirements engineering field. It has been considered as an essential attribute of software systems. For about 20 years in the past, many researchers have been trying to improve requirements traceability. To put it simply, if we can assure the stakeholders that requirements traceability has been perfectly constructed, we should be able to follow and describe the requirements’ whole life. In other words, we should be able to trace the requirements’ life throughout all the phase, from the customers’ needs in the early phase of the projects to requiremtns specification phase, to development phase, to deployment phase, and to maintenance phase. And also, it includes not only the traceability from requirements to development artifacts but also reverse-directional traceability from development artifacts to the original customers’ needs or requirements. In the software industry, requirements traceability is especially essential in safety-critical domain which does not tolerate any defects or faults. If we have excellently constructed traceability in a software project, we are able to easily understand the relationships between software process artifacts, and easily manage the change requests from the stakeholders. Although requirements traceability affects the success of the software projects significantly, many field workers ignore requirements traceability. This is mainly because there is not many practical methodologies and tools, and also constructing requirements traceability demands huge additional costs and additional human resources. Therefore, there is a need to develop a cost-effective traceability methodology or tool. In that respect, traceability automation is a key to solve this problem.
In this thesis, especially I dealt with the traceability between functional requirements and non-functional requirements. It is vital to ensure that all functional requirements have considered the apporopriate non-functional requirements and all the quality attributes are satisfied by the specific functionalities. I tried to construct this traceability by using information retrieval techniques (or it can be called text mining techniques). There have been many researches that made use of information retrieval techniques to automate traceability. Because requirements specification documents are usually written out with natural languages (usually English), it is very suitable for establishing traceability by using information retrieval techniques. Among the many information retrieval techniques, I decided to utilize Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) in my research. Ultimately, I conducted an experiment on constructing traceability by using aforementioned two techniques, and I analyzed and evaluated the experiment results. And finally, I conducted a comparative study between two information retrieval techniques.소프트웨어공학 및 요구공학 영역에서의 품질 속성 (Quality Attributes) 중 하나인 요구사항 추적성 (Requirements Traceability) 은 소프트웨어 시스템의 유지 및 보수에 필수적인 속성으로 여겨져 왔으며, 20년간 많은 연구자들에 의해 끊임없이 연구되어 왔다. 훌륭한 소프트웨어 프로젝트라면 이 요구사항 추적성이 확실히 보장되어야 한다. 요구사항 추적성이 보장되었다면, 프로젝트 초기의 고객의 필요 (Needs) 에서부터 요구사항 명세화 단계, 개발 단계, 배포 단계, 그리고 유지 및 보수 단계의 전 과정에 걸쳐 요구사항의 일생을 따라가고 표현할 수 있어야 한다. 또한, 요구사항으로부터 수반되는 개발 산출물을 추적하는 것뿐 아니라, 개발 산출물로부터 그와 연관된 최초 고객 요구사항을 추적하는 역방향으로의 추적성도 포함한다. 특히, 추적성은 실무에서 작은 결함도 용인되지 않는 안전 필수 도메인 (Safety-Critical Domain) 에서 핵심적인 속성이다. 추적성을 훌륭하게 구축한 소프트웨어 프로젝트는 소프트웨어 프로세스 산출물 (Software Process Artifacts) 간의 연관성 이해와 변경 요청 관리 (Change Request Management) 가 용이하게 된다.
요구사항 추적성이 소프트웨어 프로젝트의 성공에 주요한 영향을 미침에도 불구하고, 요구사항 추적성은 실용적인 방법론 및 도구 (Tools) 의 부족하고 추가적인 비용 및 인력이 요구되기에 자주 무시된다. 그렇기 때문에 효율적이고 비용 효과적인 (Cost-Effective) 추적성 방법론이 연구되어야 한다.
이처럼 추적성이 실무에 있어서 비효율적이고 성가신 업무가 되지 않기 위하여, 요구사항 추적성의 자동화를 달성하는 것에 대한 연구가 중요시되었다. 요구사항 추적성 연구의 지도적 위치의 있는 연구자들이 요구사항 추적성 연구의 과제들을 제시하였는데, 그 중에서도 궁극적인 과제 (Grand-Challenge) 를 제시하였다. 그것은, 편재성 (Ubiquitousness) 이다. 요구사항 추적성은 실무자들이 추적성에 대해 신경 쓸 필요도 없이, 저절로 항상 달성되어야 한다는 것이다. 즉, 요구사항 추적성은 아무런 노력 없이도 (with near zero effort) 달성되고, 유지되어야 한다는 것이다. 그러기 위해서는 요구사항 추적성과 관련된 업무가 소프트웨어 공학 프로세스 (Software Engineering Process) 에 완전히 정착되어 있어야 한다고 했다. 이 추적성의 편재성 달성을 위해서, 요구사항 추적성의 자동화 (Traceability Automation) 가 한 방편이라고 할 수 있다.
추적성 자동화를 위해 정보검색 (Information Retrieval), 텍스트마이닝 (Text Mining), 혹은 자연어 처리 (Natural Language Processing) 을 이용한 요구사항 추적성 접근법이 많이 시도되었다. 본 논문에서는, 정보검색 기법을 이용하여 요구사항 명세서 (Requirements Specification Documents) 내에서의 기능 요구사항 (Functional Requirements) 과 비기능 요구사항 (Non-Functional Requirements) 간의 추적성을 구축하는 것에 대한 연구를 진행하였다. 기능 요구사항과 비기능 요구사항의 추적성 검증 (Traceability Assurance) 은 소프트웨어 시스템의 위험 관리 (Risk Management) 에 필수적이다. 즉, 모든 기능 요구사항이 그에 해당하는 적절한 비기능 요구사항들을 고려했다고 확증할 수 있어야 한다. 또한, 모든 비기능 요구사항에 맞는 기능 요구사항 테스팅 계획이 세워지고, 품질 속성이 충족되었다는 것을 확증할 수 있어야 한다.
요구사항 문서는 자연어로 작성된 경우가 많기에, 정보검색 기법을 이용한 추적성 연구가 효과적이다. 많은 정보검색 기법들 중에서, 잠재 의미 색인 (Latent Semantic Indexing) 기법과 잠재 디리클레 할당 (Latent Dirichlet Allocation) 기법을 사용하여 연구를 진행하였다. 최종적으로, 두 가지 기법을 이용한 추적성 구축을 실험해 본 후, 결과를 평가하고 두 기법의 비교 연구를 진행하였다. 정보검색 기법들이 도출하여 주는 기능 요구사항과 비기능 요구사항 간 유사도의 정확도를 추적성 매트릭스와 비교해 보았다. 실험 결과 잠재 의미 색인 기법이 잠재 디리클레 할당 기법에 비해 상대적으로 조금 더 높은 정확도를 보였다. 또한, 정보검색 기법이 추적성 구축에 활용되는 데에 있어서 도출된 유사도 정확도가 확실한 추적성으로 인정되기에는 전반적으로 부족한 정확도 수치를 보였다고 할 수 있다. 정보검색 기법의 유효성 측면에서, 어떠한 요인들이 낮은 유사도 정확도가 도출되는 데에 영향을 미쳤는지 분석해 보았다.제 1 장 서 론 1 제 1 절 연구 배경 1 제 2 절 연구 목적 및 목표 3 제 3 절 논문 구성 6 제 2 장 배경지식 및 관련연구 7

Since the introduction of data mining, more efficient and expansive mining algorithms aimed at detecting patterns and rules from mass data have been proposed, and more advanced research performance has been demonstrated. In particular, association rule mining has been widely used in diverse areas including commerce, communications, insurance, and bioengineering. However, together with such performance improvements, the sizes and dimensions of actual commercial databases have been greatly increased, and thousands—or even millions—of association rules requiring users’ analyses have been generated. Moreover, recent efforts to find useful knowledge from the huge volumes of data accumulated on the internet as well as countless electronic documents have focused on the potential problems of association rule mining and have been expanded to define different forms of interestingness measures.
Interestingness measures, i.e., representative research on data mining post-processing techniques, which help users to interpret the meaning of knowledge by improving the quality of the generated rules and patterns, provide only useful rules to users. At the core of the quality evaluation of association rules, which are aimed at defining useful knowledge, lie users’ expressions of knowledge, which seek to screen out interesting knowledge they did not expect. In other words, the issues that should be resolved by this study include the knowledge environment of domains that objective measures to evaluate information theory-based association rules fail to consider, and users’ knowledge collection and application that may change according to age and experience.
This study utilizes common sense for users’ expressions of knowledge. Common sense knowledge may be expressed as basic knowledge that is necessary for social communications but is uninteresting because it is predictable and already known to people. Therefore, the interestingness measures proposed by this study evaluate how close association rules are to common sense knowledge itself. These measures may be defined by utilizing the similarity between association rules and common sense knowledge expressed through natural language by common people. This similarity estimation is proposed so as to overcome the problems of similarity techniques, which are much used in existing text mining, and to more precisely measure semantic relatedness and the similarity of rules and common sense knowledge with different structures.
In order to embody common-sense measures, interestingness measures based on common sense knowledge, association rules and common sense knowledge are defined as vectors in a vector space model. In order to find the common sense knowledge method closest to the association rules, common sense matching techniques of association rules using semantic networks are used. Such matching techniques enable the detection of common sense knowledge with the highest semantic-related similarity through common sense networks with consideration to the structural characteristics of association rules. The matched common sense knowledge outcomes are not restricted to mere sentence forms but include knowledge that may be inferred through common sense networks. In order to calculate the semantic-relatedness similarity between matched association rules and common sense knowledge in a vector space model, the cosine distance similarity technique with distance in semantic networks as weight is used.
According to the results of the estimating similarity proposed by this study, higher evaluation results were obtained in a similarity evaluation of structured knowledge, like association rules and common sense knowledge, than in the existing similarity techniques used for text mining. The experimental evaluation result of the common-sense measures was compared with the result of the association rule evaluations by domain experts. This experimental result showed that, as we predicted, the common-sense measure and usefulness evaluation by experts were in inverse proportion. In other words, rules with a high common-sense measure were evaluated as not being useful rules by experts. The experimental result of the common-sense measures was consistent with 76 percent of experts, and 87 percent of those rules whose common-sense measure was 0.5 or higher were evaluated as not useful. The result also showed that unnecessary rules not removed by the existing objective measures were able to be screened out effectively. Therefore, it is anticipated that the common-sense measures proposed by this study will be used as a data mining post-processing technique for the quality evaluation of association rules in order to support analyses of users’ rules and patterns and to screen out only useful knowledge.데이터 마이닝이 소개된 이후로 대용량 데이터로부터 패턴과 규칙들을 발견하기 위해 보다 효율적이고 확장성 있는 탐사 알고리즘들이 제안되어 왔고 한 차원 진보된 연구 성과를 보여 주었다. 특히, 연관 규칙 마이닝은 상업, 통신, 보험, 생명공학 등과 같은 다양한 분야에서 폭넓게 활용되어 왔다. 그러나 이와 같은 성능 향상과 더불어 실제 상업 데이터베이스들의 크기와 차원은 크게 증가하여, 사용자의 분석을 필요로 하는 수천 개 또는 수백만 개의 연관 규칙이 생성되게 되었다. 또한, 인터넷에 축적된 방대한 데이터인 빅 데이터(big data)와 수많은 전자 문서들로부터 유용한 지식을 찾고자 하는 최근의 노력들은 이와 같이 연관 규칙 마이닝의 잠재적인 문제에 초점을 맞추어 다양한 형태의 유용성 척도(interestingness measure)를 정의하는 것으로 확장해 가고 있다.
데이터 마이닝 과정을 통해 생성된 규칙 및 패턴의 품질 향상을 통해 사용자가 그지식의 의미를 잘 해석할 수 있도록 도와주는 역할을 하는 데이터 마이닝 후처리 기법의 대표적인 연구인 유용성 척도는 사용자에게 유용한 규칙만을 제공하기 위한 척도이다. 유용한 지식을 정의 하기 위한 연관 규칙의 품질 평가는 사용자가 기대하지 않았던 흥미로운 지식을 선별하기 위해 사용자의 지식 표현을 그 핵심으로 한다. 즉, 통계와 확률, 정보이론을 기반으로 연관 규칙을 평가하는 객관적 척도들이 고려하지 못하는 도메인의 지식 환경과 시대, 경험에 따라 변화 할 수 있는 사용자의 지식 수집 및 적용이 본 연구에서 해결해야 하는 문제이다.
본 연구에서는 사용자의 지식 표현을 위해 상식을 활용한다. 상식은 일반적으로 사회적 의사소통을 위해 필요한 기본 지식으로, 예측 가능하고 이미 사람들에게 알려져 있어 흥미롭지 못한 지식으로 표현할 수 있다. 따라서 본 연구에서 제안하는 유용성 척도는 상식을 기반으로 연관 규칙이 얼마나 상식에 가까운지를 평가하는 척도이다. 이 척도는 연관 규칙과 일반 사람들에 의해 자연어로 표현되는 상식의 유사도를 이용해 정의 될 수 있다. 이 유사도 측정은 기존의 텍스트 마이닝에서 많이 사용되는 유사도 기법들의 문제점을 극복하고, 서로 다른 구조를 갖는 규칙과 상식의 의미 관계 유사도를 보다 정밀하게 측정할 수 있도록 제안한다.
상식 기반의 유용성 척도인 상식 척도를 구현하기 위해 벡터 공간 모델에서 연관 규칙과 상식을 벡터로 정의하고 연관 규칙과 가장 가까운 상식을 찾기 위해 시맨틱 네트워크를 이용한 연관 규칙의 상식 매칭 기법을 사용한다. 이와 같은 매칭 기법은 연관 규칙의 구조적 특징을 고려하여 상식 네트워크를 통해 가장 의미 관계 유사도가 높은 상식을 찾도록 해준다. 매칭된 상식은 단순한 문장 형태에 제한되지 않으며 상식 네트워크를 통해 추론될 수 있는 지식까지도 포함한다. 매칭된 연관 규칙과 상식은 벡터 공간 모델에서 의미 관계 유사도를 계산하기 위해 시맨틱 네트워크 상의 거리를 가중치로 하는 코사인 거리 유사도 기법을 사용한다.
본 연구에서 제안한 유사도 평가 결과 연관 규칙과 상식과 같이 구조화된 지식의 유사도 평가에서는 기존의 텍스트 마이닝에서 사용되었던 유사도 기법보다 높은 평가 결과를 보여주었다. 상식 척도의 실험 평과는 도메인 전문가의 연관 규칙 평가 결과와 비교 평가 하였다. 이 실험의 결과는 우리가 예측한 바와 같이 상식 척도와 전문가의 유용성 평가가 반비례함을 보여 주었다. 즉, 상식 척도가 높은 규칙이 전문가들에 의해 유용하지 않은 규칙으로 평가 되었음을 보여주었다. 상식 척도의 실험 결과는 76% 전문가들의 평가 결과와 일치하며, 상식 척도가 0.5 이상인 규칙들의 87%가 유용하지 않은 규칙으로 평가되었다. 또한, 기존의 객관적 척도들에 의해 제거되지 못했던 불필요한 규칙들을 효과적으로 선별할 수 있음을 보여주었다. 따라서 본 연구에서 제안한 상식 척도가 데이터 마이닝 후처리 기법으로 사용자의 규칙 및 패턴 분석을 지원하고, 유용한 지식만을 선별할 수 있도록 연관 규칙의 품질 평가에 사용될 수 있을 것이라 기대한다.I. 서론 1 1.1 연구 배경 1 1.2 연구 목적과 내용 5 1.3 논문 구성 7 II. 관련 연구 9

The purpose of this study is to analyze and categorize the design patterns of virtual reality games and to suggest a pattern-based method to create virtual reality games in the aspects of game design. In this study, the term, a virtual reality game is used to address the games with the head-mounted display.
This study argues that the available actions for the players to choose serve as the main elements of the design patterns in virtual reality games. Any stories or play experiences could be disassembled into smaller elements. Then the elements could be combined to generate a new story or play experience. Vladimir Propp, Mark O. Riedl and Chris Crawford tried to define those elements and all concluded that verbs are one of the main elements which make the story progress. Similarly, player actions in games make the game progress into the next stage. Thus the design patterns in virtual reality games could be defined in terms of verbs. Those patterns could be used together to create a whole experience in the virtual reality game, like Christopher Alexander argued in his book A Pattern Language that combined small independent patterns make a complex system.
Chapter 2 categorizes the spatial levels in virtual reality into three: game space, background space and interactive space. Then it defines the design patterns in virtual reality games as verbs that are able to change the game states.
Chapter 3 analyses five virtual reality games in the aspects of design patterns. Those five games are categorized into three groups based on the navigation system in the games. The first group, where the player's position is fixed includes <GUNJACK> and <Esper>. This group tends to use more design patterns with contact verbs so that the player can change or select the objects in the game. The second type is the virtual games where the player can freely navigate across the virtual world, like <Anshar Wars 2>. In this group, design patterns with possess verbs are mainly observed. The third group allow players to be at certain points in the virtual world. Players move between those points with design patterns with motion verbs.
Chapter 4 examines the narrative effects derived from the certain design patterns. Virtual reality games with contact verbs are effective to convey protagonist's growth as the players' skill grows. Possess verbs are effective with exploring protagonists since it continuously offer new goals to reach. Motion verbs make the player to look for clues in every play point, thus effectively convey mysterious stories. This study aims to suggest a method to design virtual reality games in the aspects of those verbs and the design patterns. With virtual reality device developing, more design patterns certainly will appear, thus they could be explored in future.본 연구의 목적은 가상현실(virtual reality) 게임의 디자인 패턴을 찾아 유형화하고 이러한 패턴의 조합으로 가상현실 게임을 창작할 수 있는 가능성을 모색하는 것이다. 이를 위해 플레이어 행동을 중심으로 가상현실 게임의 디자인 패턴을 정의한 후, 가상현실 게임 사례들의 디자인 패턴을 분석했다. 그리고 이를 기반으로 디자인 패턴의 각 유형이 가상현실 게임에서 어떤 서사적 효과를 전달하고 있는지 살펴보았다. 단, 본 연구에서는 가상현실 게임이라는 용어를 헤드 마운티드 디스플레이(Head Mounted Display)를 이용한 가상현실 게임에 한정하여 사용한다.
가상현실 환경은 스크린과 플레이어가 일체화되어 마치 인터페이스가 사라진 것 같은 느낌을 제공하며, 플레이어에게 실제로 그 가상의 장소에 가있는 것 같은 현존감을 불러일으킨다. 또한 현실 시야를 완전히 차단하지는 않는 기존의 컴퓨터, 비디오, 모바일 게임 등과 달리, 가상현실 게임은 현실의 시야를 차단하고 가상현실만을 보여준다. 이때 플레이어와 가상세계 사이의 상호작용은 가상현실의 몰입감을 높이는 중요한 역할을 한다.
따라서 본 논문에서는 가상현실 게임과 플레이어간의 상호작용, 즉 플레이어가 취할 수 있는 행동을 중심으로 가상현실 게임을 분석하고자 한다. 이때, 모든 소프트웨어 사용자의 상호작용을 동사로 보는 접근 방식과, 독립적인 디자인 패턴들을 조합하여 복잡한 체계를 디자인 할 수 있다고 보는 패턴 언어 이론을 바탕으로 가상현실 게임에서의 디자인 패턴을 정의하고 분석했다.
2장에서는 플레이어의 상호작용 가능 범위를 중심으로 가상현실 게임 공간의 층위를 구분하고 플레이어가 취할 수 있는 행동, 즉 동사를 중심으로 디자인 패턴을 정의했다. 크로퍼드는 동사와 다른 문장 요소들이 조합되어 이야기를 생성하는 규칙을 만듦으로써 인터랙티브 스토리월드를 구현하고자 했다.
그러나 본 연구에서의 동사는 자연어로 된 스토리의 일부가 아니라 가상현실 공간에서 플레이어가 취할 수 있는 행동을 뜻하므로, 각 동사들의 집합이 문장이 아닌 가상현실 게임이라는 시스템을 만드는 것이라 보았다. 따라서 독립적인 원소들을 연결하여 하나의 시스템을 만들 수 있다는 크리스토퍼 알렉산더(Christopher Alexander)의 패턴 언어 이론을 기반으로 가상현실 게임에서의 디자인 패턴을 정의했다.
3장에서는 앞선 논의를 바탕으로 가상현실 게임에서 어떤 디자인 패턴이 나타나는지를 분석하고 유형을 분류했다. <건잭(GUNJACK)>, <에스퍼(Esper)>, <데드 시크리트(Dead Secret)>, <랜즈 엔드(Land's End)>, <앤샤르 워즈 2(Anshar Wars 2)>를 중심으로 그 외의 가상현실 게임들을 분석했다. 플레이어의 위치가 고정된 공간에서는 플레이어가 오브젝트의 위치 및 상태 등을 직접 변화시키는 접촉 동사 위주의 패턴들이, 플레이어의 위치 이동이 연속적으로 일어나는 공간에서는 획득 및 소비 등의 소유 관련 패턴이, 플레이어가 지정된 플레이 지점을 불연속적으로 오가는 공간에서는 이동 관련 패턴이 플레이의 주요하게 나타났다.
4장에서는 앞선 장에서의 텍스트 분석을 바탕으로, 각 디자인 패턴이 어떤 형태로 가상현실 게임의 공간에서 활용되고 있으며 그 결과 어떤 특징적인 서사가 발생하는지를 도출했다. 접촉 패턴이 주를 이루는 가상현실 게임의 경우, 플레이어가 가상현실 공간 속의 오브젝트들을 조작하는 것에 익숙해지면서 게임 속의 주인공 또한 함께 성장하는 성장 서사가 함께 진행되었다. 획득을 주요한 목표로 하는 가상현실 게임에서는 목표물을 찾기 위해 가상공간을 돌아다니는 과정에서 새로운 이벤트가 계속해서 발생하는 탐험 서사가 진행되었다. 각 플레이 지점마다 새로운 오브젝트와 새로운 장소를 마주하게 되는 세 번째 유형에서는 각 플레이 지점마다 새로운 단서들이 주어지는 추리 서사에 가까운 스토리텔링이 이루어졌다.
본 논문은 가상현실 게임 디자인의 특수성을 도출하고 가상현실 게임 디자인의 방식의 한 가지 가능성을 제시한 시도로서 의의가 있다. 다만 분석과정에서 사용했던 HMD 기기가 한정적이었다는 한계가 있다. 이외의 HMD 기기의 게임 플랫폼에서는 또 다른 디자인 패턴을 발견할 수 있을 것이며, 가상현실 게임 전용의 컨트롤러를 사용한 여타 게임에서는 가상현실이 아닌 게임에서는 발견할 수 없었던 차별화된 디자인 패턴을 더욱 더 많이 찾아볼 수 있을 것이다. 이러한 점에 있어서는 후속 연구가 필요할 것이다.Ⅰ. 서론 1 A. 문제 제기 및 연구사 검토 1 B. 연구 대상 및 연구 방법 7 Ⅱ. 가상현실 게임의 공간 층위와 디자인 패턴 15 A. 플레이어 중심의 상호작용 공간 15

대부분의 경우 일부 개념이 여러 사람들에게 제시 될 때, 그들은 관찰, 경험 및 과거의 지식을 바탕으로 자신의 관점을 수립한다. 다른 사람들이 그 개념에 대한 그들의 신념에 동의하게 하기 위해 사람들은 논쟁과 반론을 주고받습니다. 때로는 사람들이 확신을 가지기도 하고, 또 때로는 의견이 일치하거나 주제에 대해 제시된 논거에 대해 의견이 일치하지 않는 경우도 있습니다. 만약 이러한 갈등이 장기간 지속된다면 추가적으로 사람들이 참여하게 되고, 이는 논쟁을 불러일으키게 될 것이다. 개념이나 주제가 논쟁을 일으킬 때마다 해당 주제 또는 해당 개념과 관련된 정보와 갈등을 구분하기 어려워 진다. 논쟁을 해결하는 것은 매우 중요하고, 정보를 필요로 하는 사회의 레벨 어디에서나 이러한 논쟁의 해결을 필요로 한다. 정보 기술 시대에 있어, 우리는 우리가 생각할 수 있는 거의 모든 주제와 관련된 정보 제공을 위한 다양한 데이터 포맷을 포함하고 있는 수 백 억 개의 웹 페이지를 지나쳐 온다. 이 웹 페이지는 정보를 제공하고 사회의 특정 대중들을 대상으로 부적절하거나 논쟁의 여지가 있는 내용을 포함할 수 있다. 대게 우리는 어떤 주제에 관한 정보에 접근하려고 할 때마다 텍스트 데이터를 접하게 되므로 논쟁의 여지가 있는 내용의 경우, 해당 주제에 관한 일반 정보와는 분리되어야 한다. 텍스트 데이터의 논쟁 여부를 결정하고 더 큰 차원에서 이 문제를 해결하기 위해 많은 노력이 기울여지지 않은 것은 매우 놀라운 일이다. 우리가 시도해 볼만한 것은 위키피디아 기사의 논쟁 여부, 트위터와 같은 일부 소셜 미디어의 논쟁 여부 그리고 뉴스 표제에 대한 논쟁 여부를 결정하는 것이다. 위의 솔루션들은 매우 구체적이며, 텍스트 데이터의 타겟 도메인에 집중되어 있다. 다른 말로 표현하자면, 모든 유형의 데이터 소스에 대한 텍스트 데이터로부터 논쟁 여부를 판단할 수 있는 일반적인 솔루션은 없다. 검색 엔진 및 위키피디아의 출현으로 누구나 쉽고 자유롭게 정보에 접근할 수 있게 되었다. 위키피디아는 거대한 협력의 효과적인 결과물이며, 위키피디아 문서의 생성과 유지 보수 및 진화와 같은 동료 검토 방법은 높은 품질과 신뢰성을 보장한다. 그러나 위키피디아의 “누구나 편집 가능한” 정책은 비전문가의 참여로 인해 제공되는 정보의 내용과 신뢰성에 대한 갈등, 문서의 파괴, 논쟁 및 의심과 같은 다양한 문제를 야기했다. 위키피디아는 최근 주로 사용되는 지식을 참고하기 위한 공간이며, 위키피디아 문서가 높은 품질을 가지고 있다는 것은 널리 알려져 있다. 그럼에도 불구하고, 시간이 지남에 따라 편집자들 사이의 의견 불일치로 인해 종종 논쟁의 여지가 있는 일부 문서들이 나타난다. 종교, 역사, 정치를 포함하는 것과 같은 인기있는 주제 분야의 특정 문서의 상당수가 편집자 스스로가 논란의 대상이 되는 것을 지적하고 있지만, 이는 미봉책일 뿐이다. 또한 위키피디아에서는 다른 소셜 미디어와 비교하여 의견의 불일치, 편견 그리고 갈등이 매우 다양하게 나타나기 때문에 기존의 접근법으로는 다룰 수 없다. 한편, 위키피디아를 편집하기 위한 사회적 절차는 부분적으로 문서들의 편집 기록으로 남게 되고, 이는 새로운 전략을 위한 시발점이 될 수 있다. 사람들은 양적 데이터를 사용하는 다양한 기법을 통해 위키피디아 문서의 논쟁 여부를 식별하고 순위를 매기려고 시도했으나, 이러한 기법들은 편집자 간의 갈등에 대한 의미적 중요성을 무시했다. 위키피디아에는 “낙태”, “지구 온난화”, “동성애”와 같이 의견이 극과 극으로 갈리는 논쟁적인 문서와 “윌리엄 셰익스피어”, “실레시아”와 같이 논쟁의 여지가 전혀 없는 문서가 있다. 논란이 많은 문서에 대한 위키피디아의 편집 계획에 따르면, “정기적으로 편견을 가진채로 고쳐졌거나, 자연적으로 분쟁의 대상이었던 문서는 분명 미래의 논쟁에 영향을 줄 것이다. 이 주제들은 위키피디아 편집자들 사이에서 상당한 압력으로 작용하고 있으며, 이러한 주제에 대한 시각은 편집자의 시간, 장소, 문화와 같이 다양한 요소로부터 영향을 받는다.” 라고 한다. 방문자들은 위키피디아 관리자가 추가한 태그를 통해 해당 문서가 논란의 여지가 있거나 내용에 다양한 분쟁 내용이 포함되어 있다는 경고를 확인할 수 있다. 문서 내에서 논쟁의 여지가 있는 내용을 자동으로 파악하게 된다면, 현재 존재하는 위키피디아의 품질 관리 프로세스가 크게 향상될 수 있다. 예를 들어 이는 편집자가 작업을 쉽게 수행할 수 있게 되고, 더 많은 조사를 위해 논쟁의 여지가 있는 내용을 제안하게 될 것이다. 최근에 발견된 위키피디아의 특정 관점[3, 4]에 대한 편향과 은밀한 캠페인과 관련된 문제의 개발은 기존의 수작업 프로세스가 더 이상 확장 할 수 없다는 것을 암시한다. 또한 자동 논란 인식 도구는 주제를 분석하고 논쟁을 일으키는 편집자 간의 협업 패턴을 탐지하는 데 유용할 수 있다. 도전. 위키피디아가 어떻게 구축 되었는가에 따라 논쟁을 진정으로 새로운 도전으로 여기고 있다. 편집 절차에 따라, 위키피디아에 있는 대부분의 기사는 합리적으로 감정적이고 중립적인 언어로 쓰여지고, 다른 공적인 광고보다는 미묘하게 지적되는 논쟁과 생각의 방아쇠를 당긴다. 따라서 특징적인 용어 기능에 의존하는 전통적인 정서와 극성 검사는 적용되지 않는다. 예를 들어, 몇몇 논쟁과 논란, 편견들은 “대부분의 학자들은 그것을 생각한다.”와 “몇몇 학자들은 그것을 생각한다.”와 같이 사실상 동일할 수도 있는 두가지 버전의 비유사성에서만 분명하다. 또한 기사의 기록이나 대화 사이트에 대안이나 정당성을 제시하지 않고도 이전 버전의 텍스트를 삭제할 때 편집자의 행동이 일관적이지 않을 수 있다. 논쟁의 여지가 있는 주제를 다루는 문서는 편집자가 효과적으로 협력하고 중립적인 형태로 이 기사를 작성하여 반대되는 모든 견해를 다루는 논란의 여지가 없는 문서가 될 수 있다. 반대로 논쟁의 여지가 없는 주제에 대한 문서는 편집자 간의 의사 소통 및 협업이 불충분하다는 이유로 논쟁의 대상이 될 수 있다. 이 때문에 위키피디아의 논쟁 탐지에 대한 대부분의 작업은 텍스트나 그들의 견해를 이해하는 것과는 대조적으로 이 기사의 편집 배경에 기록된 편집 과정에서의 증거와 분쟁을 확인하는 데에 집중한다. 기존의 자연어 이해 기술의 도움을 받지 못하면서 초기 저작물은 주로 각 기사의 편집 내역과 거기에 나열된 편집자의 작업에 중점을 두었다. 그러나, 이전의 상태를 해결하려는 시도는 주로 편집자의 연결을 완전히 해제하거나 되돌리기, 또는 단어 지우기와 같은 작업이 제한되는 기본 편집 작업 그룹에 포함시키는 간단한 경험적 방법에 의존했다[3, 20, 36, 43]. 그러나, 실제로 이러한 간단한 전략은 위키피디아에서 논쟁을 탐지하는데 효과적이지 못했다[31].
이 논문에서는 자연어 처리 기술을 사용하여 논쟁 여부를 식별하는 문제를 다루었다. 제안하는 방법은 문서의 주제와의 관계와 함께 새로운 편집 프로세스로 인해 텍스트의 본래 의미에 미치는 영향을 파악한다. 실험 결과로 도출한 정밀도(0.901), 리콜(0.901), 정확도(0.908), F-척도(0.901) 값은 제안된 방법의 효율성을 입증한다. 이 기술은 기존 문서 내용에 새로 도입된 논쟁 여부를 자동으로 식별하는 데 유용하며, 같은 주제에서 논란을 포함하거나 제외할 지 여부를 결정하는데 도움이 될 수 있다.Mostly it happens that when some concept is presented to a number of people they establish their own point-of-view (POV) about it, based on their observations, experiences and the past knowledge. To make the others agree on their beliefs about that concept people give arguments and counter arguments. Sometimes people get convinced and sometimes they disagree and get indulged in conflicts over the presented concept or the arguments given in support or against the topic. If this conflict carries on for a longer period of time an additional number of people get involved in it and thus leads to controversy. Whenever a concept or a topic gets controversial it becomes difficult for a common people to distinguish between information and conflicts regarding that topic or concept. Resolving controversies is very important, present at all level of society, wherever there is some information need. In this era of information technology, we come across millions of billions of web pages that contain different format of data to provide information related to almost every topic that is possible to think of. These web pages provide information and may also be containing the inappropriate or controversial contents for certain masses of the society. Since mostly we come across textual data whenever we strive for accessing information about any topic so it is of utmost importance that controversial contents must be separated from the ordinary information about that topic. It is very surprising that not many efforts have been put for determining the controversy of textual data and resolving this great issue at a broader level. The only attempts we see are only determining the controversy of Wikipedia articles, some social media based controversies such as twitter and a few about controversial news headings. All the above solutions are very specific and targeted domain of textual data is focused. In other words, there is no general solution that could be used to get controversy in textual data on any type of data source.
The advent of search engines and wikis has made access to information easy and almost free. Wikipedia is the efficacious outcome of an enormous collaboration, and its peer review-like methods of creation, maintenance, and evolution of contents ensure high quality and reliability. However, the “anyone-can-edit” policy of Wikipedia has created many problems such as trolling, vandalism, controversies, and doubts about the content and reliability of the information provided due to non-expert involvement. Wikipedia is probably the mostly used knowledge reference point nowadays, and the high quality of its articles is extensively recognized. Nevertheless, disagreement among editors often cause some articles to be controversial as time passes. These articles course a large number of popular subject areas, including religion, history and politics, to mention a few, are personally tagged as controversial by the editors, which is evidently sub-optimal. Additionally disagreement, bias and conflict are indicated quite diversely in Wikipedia in comparison to other social media, rendering previous approaches inadequate. On the other hand, the social procedure for editing Wikipedia is partially captured in edit history record of the articles, beginning the entranceway for novel strategies. People have tried to identify and rank controversies in Wikipedia articles through various techniques that use quantitative data, ignoring the semantic significance of conflicts among authors.
There are lots of types of controversial articles on delicate, polarized issues in Wikipedia, including "Abortion", "Global Warming", "Homosexuality", along some examples of controversial articles on less regarded as debated matters such as "William Shakespeare", and "Silesia" (a historical region in Central European countries). According to the Wikipedia editorial plans, controversial articles are the ones that "regularly become biased and have to be fixed, or are articles which were once the subject of a natural point of view dispute and will probably are affected future disputes. ... These subjects are in charge of significant amounts of pressure among Wikipedia editors, and perspectives on these themes are influenced by the time, place, and culture of the editor". Visitors are warned about such articles by tags, by hand added by Wikipedia administrators (henceforth referred to as admins), indicating this article is controversial or has disputed content. Automatically figuring out controversial content within articles could substantially enhance the current quality control process in Wikipedia. For example, it could ease editors from the task, or at least suggest possibly controversial articles for even more inspection. The development of problems related to bias and the covert campaign of specific viewpoints in Wikipedia discovered recently [3, 4] suggests the existing manual process is not scalable. Moreover, automatic controversy recognition tools can even be useful in analyzing topics and detecting collaboration patterns among editors that lead to controversy. Challenges. Just how Wikipedia is built makes detecting controversy a truly new challenge. As per its editorial procedures, most articles in Wikipedia are written in reasonably sentiment-neutral language, triggering arguments and thoughts to be indicated more subtly than in other public advertising. Thus, traditional sentiment and polarity examination, which count on characteristic terminology features, do not apply. For instance, some arguments, debates, and bias are only apparent in the dissimilarities of two versions that may be virtually identical, such as exchanging "Most scholars think that ..." with "Some scholars think that...". Disagreements might also be implicit in actions of editors when they, for example, remove text from an earlier version without providing an alternative or some justification in the article's record or talk site. It ought to be noted an article covering a controversial theme is definitely not a controversial article as editors might have were able to collaborate effectively and write this article in an neutral form, covering all opposing viewpoints. Conversely, an article on a topic for which there is absolutely no controversy might get tagged as controversial credited to insufficient effective communication and collaboration between editors. Because of this, most of the work on controversy detection in Wikipedia concentrate on identifying proof conflict and dispute in the editorial process, recorded in the edit background of this article, as opposed to understanding the text or the viewpoints in it. With little help from existing natural language understanding techniques, earlier works have focused mainly on the revision history of each article, and the actions of the editors listed there. However, earlier attempts to resolve the condition have relied mainly on simple heuristics that either completely dismiss editor connections or count on a restricted group of basic edit operations such as reverts or erased words [3, 20, 36, 43]. In practice, however, these simplistic strategies are not powerful in discovering controversy in Wikipedia [31].
In this paper, we have addressed the problem of identifying controversy using natural language processing techniques for the first time. The proposed method spots the impact on existing meanings of the text due to new editing processes along with their relationship to the topic of the article. The experimental results for precision (0.901), recall (0.901), accuracy (0.908), and F-measure (0.901) demonstrate the effectiveness of the proposed method. The technique is deemed useful for automatic identification of conflicts newly introduced into existing article contents, and could prove helpful in making decisions for inclusion or exclusion of controversies under the same topic.Chapter 1. Introduction 1 Chapter 2. Related Works and Background 5 2.1 Revert Statistics 5 2.2 Mutual Collaboration-based Methods 6 2.3 Visualization of Revisions 8

Due to the importance of strategic R&D planning, the analysis of technological information for strategic R&D planning is getting more important. Different from the initial technology analysis which is expert dependent methods called a qualitative approach, the existing quantitative approach, represented as keyword-based approach, provided the improvement to ensure better support for effective decision-making within strategic R&D planning and objective information without experts’ intervention. However, three points for technology analysis should be complemented in that the keyword-based approach cannot fully support strategic R&D planning. They are 1) dependency on domain experts’ knowledge in defining the patterns of keywords, 2) limitation on only providing superficial technology information, and 3) limitation on the usage of the keyword-based approach for supporting strategic R&D planning.
This research uses the concept of function for strategic R&D planning. Savaransky defines “function” as “The action changing a feature of any object”. As concept of function can be useful for technology analysis, the function-based approach is a new approach for supporting strategic R&D planning in the perspective of the quantitative approach. To represent a function, a Subject-Action-Object (SAO) structure is commonly used.
This paper proposes an SAO-based approach to patent analysis for strategic R&D planning. The proposed approach supports a strategic R&D planning process by providing the technology function information which is not provided by the keyword-based approach. The objective of this paper is to suggest a new quantitative approach to strategic R&D planning; applying SAO based approach to solve problems such as only dependency on experts’ knowledge and limitation of a keyword-based approach. To achieve the objective, four specific issues were defined. They are 1) Function-based technology database (FTDB), 2) SAO-based technology analysis methods regarding SAO network analysis, 3) SAO-based technology tree, and 4) An SAO-based technology roadmapping (TRM) for strategic R&D planning.
The first issue, ‘Function-based Technology Database’ deals with the method of how to construct FTDB. For constructing well-structured FTDB, we define the four important factors of FTDB and utilize a fact-oriented ontological approach to SAO-based function modeling of patents. The second issue, ‘SAO network analysis of patents for technology trend analysis’, deals with the method of how to identify trends in technological innovation from patents. Using Subject-Action-Object as nodes, and co-occurrences as links, an SAO network can be generated. Using Actor Network Theory, the method analyzed technological implications of indicators such as degree, centrality, and sub-network analysis in the SAO network. The third issue, ‘An SAO-based text mining approach to develop a technology tree’, deals with the method of how to develop a technology tree using SAO-based text mining techniques. The proposed method is to develop a technology tree representing various technology perspectives such as product taxonomy, technology taxonomy, and function taxonomy of technology. The fourth issue, ‘SAO-based TRM for strategic R&D planning’, deals with the method of how to develop SAO-based TRM with objective and quantitative information. In this research, I suggest the procedure of TRM and product-function-technology maps. I also propose the methods regarding how to interpret the proposed maps and how to apply the interpretation to the SAO-based TRM procedure.
Through these issues, this research provides the following contributions: 1) Providing SAO based technology analysis methods for strategic R&D planning, 2) Extending the R&D manager’s views regarding product and technology in strategic R&D planning process, and 3) Supporting strategic R&D planning for managerial decision making경제학자 슘페터는 기업이나 국가의 새로운 성장 동력은 혁신에서 온다고 주장했다. 슘페터가 밝힌 혁신은 기술혁신뿐만 아니라 시장, 조직 등 다양한 혁신을 포괄하지만 역사적으로 기술혁신의 충격은 그 어느 것보다 컸다.
오늘날 기술혁신의 중요성으로 인해 전략적 기술 기획에 관한 연구가 많은 관심을 받게 되었으며, 선진국을 중심으로 이를 지원하기 위한 다양한 연구가 활발히 진행되고 있다. 전략적 기술 기획은 연구개발 테마의 발굴과 연구개발을 선정하는 의사결정 과정에서 발생하는 불확실성을 줄여주고, 체계적이고 신뢰성 있는 연구기획을 가능하게 한다는 점에서 그 필요성이 부각되고 있다.
전략적 기술 기획을 위한 기존의 접근들이 기술 전문가의 정성적 노력에 따른 많은 비용과 시간이 수반되기 때문에, 특허와기술문서의 자동화된 분석을 통해 기술기획의 생산성을 높이는 방법에 대한 연구가 기업과 정부기관들의 주요 관심사 중의 하나이다. 비록 키워드 기반의 접근방법이 제시된 바 있으나, 이는 미리 정의된 키워드의 출현정보에만 기반하므로 기술요소들간의 명시적 연관관계를 담지 못한다. 즉, 키워드 기반의 접근은 기술의 목적, 구성, 영향 (Objective, Structure, Effect: OSE)에 대한 정보를 표현하지 못하기 때문에, 해당 접근법을 바탕으로 한 정보는 전략적 기술 기획에 그대로 활용하기에는 한계점을 지닌다.
이러한 한계점을 해결하기 위해 본 연구는 기능 (Function) 기반의 접근법을 활용하여 전략적 기술기획을 지원하는 체계를 제시한다. 기능이란 기술의 OSE 정보를 담고 있으며 Subject-Action-Object (SAO) 문법적 구조로 표현될 수 있기 때문에, 본 연구에서 제시되는 방법은 기술문서의 자연어처리분석을 통해 기술의 OSE 정보를 자동으로 추출하여 분석할 수 있도록 한다. 이를 위해 본 연구는 네 가지 연구 이슈 1) Function-based technology database, 2) SAO-based technology analysis methods regarding SAO network analysis, 3) SAO-based technology tree, 4) An SAO-based technology roadmapping (TRM) for strategic R&D planning. 를 수행한다.
본 연구의 방법을 R&D 기획단계에 적용함으로써, 전략적 기술 기획에 따른 비용과 시간의 절감이 가능하며, 제품/기술 OSE에 대한 R&D 기획전문가의 시야를 넓혀 보다 효과적인 기획의사결정을 지원할 것으로 기대된다.CHAPTER I. INTRODUCTION 1 1.1. Background and Motivation 1 1.2. Research Objectives and Scope 7 1.3. Thesis Organization 11 CHAPTER II. LITERACTURE SURVEY 13

The development of human civilization was made along with play. The play is distinguished from the 'routine' life of the subject by the place of play and its persistence. This is an active third characteristic of play and can be defined as the isolation of space and the limitation of time. The limit of space is not limited to the space in reality, but can be extended to ideally closed spaces. Therefore such space, distinguish the play from everyday life and fenced from the surroundings of the subject's daily life. Another characteristic of play is that the subject who plays the game does not take any materialistic interest. The characteristics of these plays are called 'no purpose', therefore the player can entirely 'immerse' in the play as a “fulfilent” who satisfies their desire through fiction, or plays in order to maintain a sense of personality.
In order for humans to immerse themselves in the narration, in other words, narrative of play, they need a medium that connects the narrative and the subject, and it is the character that performs this role. Therefore, the character can be viewed as an interface between narrative and human in an invisible form.
Having simple interaction functions with users, in other words, through case studies of office assistant, Tamago Kochi, Aibo, Pepper, etc., the limits of emotional interaction between virtual characters and users were derived, therefore, for virtual character to fascinatingly approach the users through interaction, it requires emotional interaction elements.
Not a passive communication that derives just amusing sensation and limited results to deliver users with 'attraction' but proactive communication that can give users a constant new experience, and through such, three elements of extensional narratives between a user and a virtual character were defined as emotional interaction enhancement factor, and through the virtual character AR emoji and artificial intelligent speaker which is recently being developed and commercialized, the study has analyzed how the three elements mentioned above enhances the fascination of virtual characters through emotional interaction with the users.
As a result, AR Emoji has derived the stare and attraction in the appearance of virtual character imitating the appearance of the user, brought proactive communications by delivering the feeling of users to the consumers, and as these experiences accumulates, it was found that the fascination of the virtual character has strengthened. And the artificial intelligence speaker provided auditory attraction through communication based on user-friendly natural language processing methods, and in conjunction with user's surroundings through active communication using mechanisms such as cloud center and machine learning, it was found that the emotional interaction occurs through an expanding narrative.
Users experience 'attraction' in other words a pleasurable sensation from the virtual character through the sense of sight, hearing, and tactile sense. And as experiences accumulate due to constant interaction with virtual characters through active communication, it forms the symbolization as a virtual character within the notion of the user. In addition, by deriving an expanding narrative from the user,
in other words, by allowing new attraction according to the user's needs, it not only enables continuous interaction, but gives the interaction extensibility with other virtual characters. Therefore, the three elements for emotional interaction have a mutual cyclic structure, and in such process of circulation, by enabling continuous interaction between the user and the virtual character, it was found that the attractiveness of the virtual character has strengthened.인간의 문명은 놀이와 함께 발전해왔다. 놀이는 놀이의 장소와 그 지속성에 의해 주체자의 ‘일상적인’ 삶과는 구분된다. 이것이 놀이의 적극적인 제 3의 특징으로, 그것은 공간의 격리성과 시간의 한계성으로 정의할 수 있다. 공간의 한계성은 현실에서의 공간에만 국한되지 않고, 관념적으로 폐쇄 된 공간으로까지 확장할 수 있다. 그러므로 그 공간은 일상생활로부터 놀이를 구별하고 주체자의 일상의 주변으로부터 울타리 쳐져있다. 또 하나의 놀이의 특성은 놀이를 하는 주체자가 놀이를 통해 아무런 물질적 이해관계를 취하지 않는다는 점이다. 이러한 놀이의 특성을 ‘무목적성’이라고 하며, 그렇기에 놀이의 행위자는 온전히 놀이에 ‘몰입’하며 현실에서 이룰 수 없는 소망을 허구를 통해 만족시키는 “소망실현(with fulfilent)”로서, 혹은 인격감을 유지하기 위해 놀이를 행하게 된다.
인간이 놀이의 서사 즉 내러티브에 몰입하기 위해서는 내러티브와 주체자를 이어주는 매개체가 필요한데, 이 역할을 수행하는 것이 캐릭터이다. 따라서 캐릭터는 비가시적인 형태로도 서사와 인간을 이어주는 인터페이스적 역할을 한다고 볼 수 있다.
사용자들과의 단순한 인터랙션 기능을 갖춘 즉 오피스 도우미, 타마고치, 아이보, 페퍼 등의 선행사례 연구를 통하여 가상 캐릭터와 사용자간의 감성적 인터랙션의 한계점을 도출 할 수 있었고, 따라서 가상 캐릭터가 사용자와의 인터랙션을 통하여 매혹적으로 다가갈 수 있도록 하기 위해서는 감성적인 인터랙션 요소들이 필요하다고 분석되어졌다.
사용자들에게 ‘이끌림’을 선사하기 위한 유희적 센세이션, 한정적인 결과값만을 도출하는 수동적 커뮤니케이션이 아닌 사용자에게 끊임없이 새로운 경험을 줄 수 있는 능동적 커뮤니케이션, 이를 통해 사용자와 가상 캐릭터 사이에서 발생하는 확장적 내러티브 세 가지 요소를 감성적 인터랙션 강화 요소로 정의하였으며, 최근 개발이 활발히 이루어지고 상용화 되고 있는 가상 캐릭터인 AR이모지와 인공지능 스피커를 통하여 앞서 언급한 세 가지 요소들이 어떻게 사용자와의 감성적 인터랙션을 통하여 가상 캐릭터의 매혹성을 강화시키는지에 대해 분석하였다.
그 결과로 AR 이모지는 사용자의 외형을 모방한 가상 캐릭터의 모습에서 응시와 이끌림을 끌어내고 사용자의 감정을 수용자에게 전달해 능동적인 커뮤니케이션을 선사하고 이러한 경험이 누적됨에 따라 가상 캐릭터의 매혹성이 강화되는 것을 알 수 있었고, 인공지능 스피커는 사용자에게 익숙한 자연어 처리방식을 바탕으로 한 커뮤니케이션을 통해 청각적 이끌림을 선사하고 클라우드 센터, 머신러닝 등의 매커니즘을 활용한 능동적 커뮤니케이션, 사용자의 주변 환경과 연동되어 확장적인 내러티브를 통하여 감성적 인터랙션이 발생하는 것을 알 수 있었다.
사용자들은 시각, 청각, 촉각의 감각을 통해 가상캐릭터로부터 ‘이끌림’, 즉 유희적 센세이션을 경험하게 되며, 능동적인 커뮤니케이션을 통하여 가상 캐릭터와의 지속적인 상호작용으로 경험들이 누적됨에 따라 사용자의 관념 안에서 가상 캐릭터로서의 상징화를 이루게 된다. 또한 사용자로부터 확장적 내러티브를 이끌어내 즉 사용자의 니즈에 따른 새로운 이끌림을 가지게 하여 지속적인 상호작용을 가능하게 해줄 뿐 만 아니라 다른 가상 캐릭터들과의 인터랙션 확장성을 부여할 수 있다고 보았다. 따라서 감성적 인터랙션을 위한 세 가지 요소들은 상호간의 순환 구조를 가지고 있으며, 이러한 순환과정 속에서 사용자와 가상캐릭터 사이에 지속적인 상호작용이 가능하게 하여 가상 캐릭터의 매혹성이 강화된다는 것을 확인 할 수 있었다.제 1 장 서 론 1 제 1 절 연구의 필요성 및 연구 목적 1 제 2 절 연구범위 및 연구방법 2 제 2 장 선행연구 4 제 1 절 이론적 선행 연구 4

자연어 처리, 번역, 시계열 데이터 분석 등의 분야에서 순환신경망(Recurrent Neural Network, RNN) 알고리즘을 이용한 연구가 활발히 이루어지고 있다. 본 논문에서는 순환신경망 알고리즘을 이용한 시계열 데이터 변화점 탐지 분야에 초점을 맞추려고 한다.
기존에 이루어진 연구들은 시계열 데이터를 활용해 값을 예측하거나, 평균, 기울기, 혹은 분산과 같이 육안으로 파악이 가능한 데이터의 변화점을 찾는데 초점이 맞춰졌다. 하지만 자기회귀과정의 특징을 가지는 데이터에서 자기회귀이동평균과정의 특징을 가지는 데이터로 변하는 경우 그 변화점은 육안으로 판단할 수 없다.
본 연구에서는 순차적 데이터를 분석하는 데 특화된 순환신경망 알고리즘을 이용해 시계열 데이터를 학습하고, 이후 육안으로 파악이 불가능한 시계열 특성이 변하는 변화점을 탐지하는 연구를 진행하였다. 또한 변화점 이후와 이전의 데이터 특성까지 검출하는 모형을 만들어 제안하려고 한다.Recurrent neural network has been widely studied in various fields such as natural language processing, translation and time series data analysis. This thesis is concerned with change point detection in time series data using recurrent neural network algorithm. Previous studies were focused on predicting stock price and detecting change point on mean, variance or slope which is observable with the naked eye. However, the change point in data cannot be determined visually if the feature of time series data changed from autoregressive process to moving average process.
In this thesis, recurrent neural network algorithm that is specialized in dealing with sequential data will be used to detect change point which is impossible to find visually. Also, to detect the characteristics before and after the change point.Ⅰ. 서론 = 1 1. 연구의 배경 = 1 2. 연구의 내용 = 2 3. 논문의 구성 = 3

자연어를 Computer로 처리하기 위해서는 면밀한 분석과 체계적인 구조형식을 도출하는것이 중 요한 문제이다. 따라서 의미의 규칙과 지식을 가능한 한 형식화 해서 알고리즘화하지않으면 안 본 연구는 한개의 시스템으로 한일 쌍방향의 번역을 가능하게 하는것이다. 본 연구에서는 양언어공통성과 차이점을 추출해, 용언의 많은 변격활용과음운조화, 구문 ·의미해석과 생성에 필요한 처리 알고리즘을 Pattern 화해서,동일 알고리즘과 처리 Routine, 개념정보에 의한 시스템을 개�

Non-monotonic logical systems are logics in which the introduction fo new axioms invalidate old theorems.
Commonsence reasoning is non-monotonic in the sence that we of ten draw on the basis of partial information, conclusions that we later retract when we are given more complete information.
some of the most interesting products of recent attempts to formalize non-monotonic reasoning are the non-monotonic logics of Mc- Dermott and Doyle.[l,2].
These logics, however, all have peculiarities that suggest they do not quite succeed in capturing the intuitions that prompted th-eir development.
In this paper I will introduce about non-monotonic logic and reconstruct non-monotonic logic as a model of an ideally rational agent's reasoning about the non-monotonic logics of McDermott and Doyle.목차 = ⅰ ABSTRACT = ⅱ Ⅰ. 서론 = 1 Ⅱ. Non - monotonic Logic = 4 A. Default logic = 4





본 연구는 한국어 문서에서 흔히 발생하는 용언의 격이 생략되는 현상 즉 무형 대용어 (Zero Anaphora) 현상을 해결하는 문제를 다룬다. 이와 유사한 문제로 영어권에서는 대용어 해결 (Anaphora Resolution) 문제가 있다. 대용어 현상은 명사구가 이미 앞에서 나타난 경우에는 이를 다시 반복하지 않고 대명사를 사용하는 것을 말한다. 대용어 해결은 대용어와 동일한 개체를 지시하는 즉, 상호참조 관계에 있는 선행어를 찾는 문제이다. 하지만 무형 대용어 현상은 대용어조차 생략되는 것을 말한다. 대용어가 생략되어 있기 때문에 형태가 없는 이의 선행어를 찾는 작업이 되어 대용어 해결 문제보다 더 어려운 작업이다.
본 무형 대용어 해결에서 먼저 수행하여야 하는 작업은 무형 대용어 탐지이다. 이를 위해 본 논문은 휴리스틱 (heuristic) 방법을 사용하고 있는데 입력된 문서의 문장에 대해 먼저 구문분석을 실행한다. 이 결과를 이용하여 필수격 (주격, 목적격)이 채워지지 않은 용언이 있는지 탐색하는 방법을 사용한다.
무형 대용어 해결에서 본 논문이 채택한 핵심 기법은 구조적 (Structural) SVM (Support vector machine)을 이용하는 시퀀스 레이블링 (Sequence Labeling)을 이용하는 것이다. 무형 대용어를 채울 가능성이 있는 명사구들이 존재하는 범위는 문서의 처음부터 무형 대용어가 발생한 용언까지이다. 이 범위의 모든 명사구들을 선행어 후보로 삼는다.
추가적인 연구로 표제어 복원이 있다. 이 문제는 선행어의 인식여부에 따라서 스트링 매칭과 이진 분류를 통해서 표제어 복원을 결정한다.
본 연구에서는 무형 대용어 탐지에 대한 시스템의 성능은 F1(F1-score : 조화 평균을 균등하게 나눈 평균 값)에 대해 약 79.39%로 나타났다. 무형 대용어 해결을 포함하는 전체 시스템에 대한 성능은 F1에 대해 58%로 나타났다. 이는 세계에서 가장 좋은 성능을 보이는 일본의 시스템보다 더 좋은 결과이다. 위키피디아 백과사전에 대해서 표제어 복원까지를 포함한 전체 시스템에 대한 성능은 F1에 대해 68%로 나타났다. 표제어 복원을 포함하는 무형 대용어 해결 문제는 본 논문의 연구가 유일한 것으로서 성능을 비교할 대상이 없는 실정이다.

임상검사명 및 임상 검사항목들은 임상의가 환자의 내과적 외과적 질환을 진단하기 위해 사용하는 기본정보이다. 그러나 이러한 정보들의 대다수는 정형화되어있지 않으며, 특정 질병의 진단에 중요한 검사 항목들은 임상의들이 환자들을 진료하는 동안에 진료기록지에 서술하여 기록된다. 이러한 문서내의 정보들은 사람에 의해 기록되므로, "cholesterol"을 "chol"로 줄여 쓰는 것처럼 편의상 축약되어 등장하거나 오타가 발생 할 수 도 있다. 이런 이유로 중요한 임상 정보들이 다른 예기치 못한 형태로서 등장할 수도 있으며, 이는 기존의 표준 용어를 활용한 사전기반의 정보추출 방식의 성능을 떨어뜨리는 요인이 된다. 대량의 문헌 정보들을 일일이 찾아서 이런 예기치 못한 표현의 패턴들을 일일이 찾아 사전 또는 정규표현식으로 구축하는 것은 신뢰도 높은 결과를 얻을 수는 있지만, 시간이 오래 걸리며, 연구자에게 긴 노동시간을 강요하게 된다. 따라서 본 논문에서는 임상검사명 및 그 항목을 찾아내기 위해, 축약된 표현을 포함한 유사문자열을 자동으로 찾아낼 수 있는 새로운 알고리즘들을 개발하고자 한다.
이를 위해서 본 연구는 유사문자열을 찾아내는 기존의 알고리즘 중 q-gram기반의 카운터 필터링(counter filtering)을 기반으로 새로운 알고리즘들을 단계적으로 개발하였다. q-gram 기반의 카운터 필터링이란 문자열을 q만큼의 길이로 잘라내어 새로운 문자열들을 생성한 후 공통된 문자열의 개수가 역치(threshold)를 넘으면 유사하다고 판정한다. 그러나 고전적인 카운터 필터링은 너무 낮은 역치로 인해 너무 높은 위양성을 생성한다는 에러를 지니고 있다. 이를 해결하고자 본 논문에서는 수정된 q-gram 필터링(modified q-gram filtering)을 제안한다. 그 후 축약된 표현을 찾아낼 수 있는 orthogonal distance filtering을 제안하고 높은 위양성을 낮출 수 있는 triangular area filtering, modified triangular area filtering을 제안한다.
실험 결과, i2b2 학습 데이터에서 기존의 사전 매칭보다 제안한 modified triangular area filtering의 f1-score가 3.91 향상되었고(Precision / Recall / F1-score : (89.76 / 73.51 / 80.83) → (85.39 / 84.11 / 84.74)), 평가 데이터에서는 6.29가 향상되어 가장 큰 상승 폭을 보였다. (Precision / Recall / F1-score : (88.68 / 69.34 / 77.83) → (86.26 / 82.08 / 84.12)). 서울대학교 병원에 내원한 당뇨환자들에 대한 학습 데이터에서도 기존의 사전 매칭보다 제안한 modified triangular area filtering의 f1-score가 2.74 더 높았으며 (Precision / Recall / F1-score : (95.2 / 76.76 / 84.99) → (91.26 / 84.47 / 87.73)), 평가 데이터에서는 2.86이 향상되는 것을 확인하였다(Precision / Recall / F1-score : (95.08 / 80.92 / 87.43) → (92.69 / 88.02 / 90.29)).제 1 장 서론 1 제 1 절 연구의 배경 및 필요성 1 제 2 절 연구 배경 및 주안점 2 제 3 절 연구 순서 4



Recently, online transactions are becoming more common due to factors
such as IT technology development and smart device dissemination, and
online review has a big influence on potential buyer's purchase decision.
Therefore, analyzing online reviews is important not only for consumers
but also for companies. Thus, this study presents a set of analytical
methodologies for understanding the meaning of customer reviews of
products in online customer review analysis. To this end, this study
analyzes customer reviews of laptops sold in domestic online shopping
malls. Based on the topic modeling, we analyzed detailed feedback from
all customer reviews. Based on this, we applied two experimental
techniques and classified them. We conducted experiments to classify
online customer reviews with lessons using deep learning techniques and
95% and 91% classification accuracy. Therefore, this study classified the
unstructured text data in the semantic analysis of the offline customer
evaluation by the experimental method and confirmed the practical
application possibility of the review analysis process of this study.최근 IT 기술과 발전과 스마트 기기의 확산으로 온라인 거래가 활발히 이루어 지고 있으며 구매한 제품에 대한 리뷰가 끊임없이 생산되고 있으며 고객 리뷰는 잠재적 구매자의 구매 의사결정에 큰 영향을 미치는 요소로 소비자뿐만 아니라 기업에게도 분석 대상으로 중요하게 인식되고 있다. 따라서 본 연구는 온라인 고객 리뷰의 분석에 있어 제품에 대해 고객 리뷰가 가지고 있는 의미에 대해 세세하게 파악할 수 있는 일련의 분석 방법론을 제안한다. 이를 위해 본 연구는 국내 온라인 쇼핑몰에서 판매되고 있는 노트북의 고객 리뷰를 대상으로 분석을 진행한다. 토픽 모델링을 통해 제품에 대한 전체 고객 리뷰의 세세한 의견을 파악한 뒤 이를 바탕으로 두가지의 실험적인 기법을 적용하여 카테고리화 하였으며 클래스를 부여한 온라인 고객 리뷰를 딥러닝 기법을 활용하여 분류하는 실험을 진행해 두개의 방법론에 대해 각각 95%, 91%의 분류 정확도를 얻었다. 이처럼 본 연구는 온라인 고객 리뷰의 의미적 분석에 있어 비정형화된 텍스트 데이터를 실험적 방법으로 카테고리화 하고 이를 딥러닝으로 분석하여 본 연구의
리뷰 분석 프로세스의 실무적 적용 가능성을 확인했다.

최근 자연어 처리, 자율주행, 컴퓨터 비전 등 다양한 분야에서 복잡한 문제를 해결하기 위해 딥러닝을 이용하는 연구가 활발히 이루어지고 있다. 또한 지상에서 신경망을 이용한 채널 추정에 관한 연구가 활발히 이루어지고 있다[8][9]. 본 논문에서는 딥러닝의 한 종류인 심층신경망을 이용해 수중 채널 추정에 관한 연구를 수행하였다.
본 논문의 2장에서는 모의실험을 위한 수중 채널 모델링과 OFDM 시스템, 심층 신경망에 관해 설명한다. 3장에서는 OFDM 시스템에서 사용하는 기존의 채널 추정방식인 LS 알고리즘과 심층 신경망을 이용한 채널 추정방식을 설명한다. 4장에서는 심층신경망의 수중 채널 추정의 가능성을 알아보기 위한 실험과 OFDM 심벌 구조에 따른 채널 추정 성능을 비교하기 위해 파일럿 심볼이 일정 간격으로 할당된 상태에서 기존 LS 채널 추정방식과 comb-type 채널 추정방식을 비교하고, OFDM 심벌에 파일럿 심볼을 할당하지 않고 온전히 데이터 심볼로 이루어진 상태에서 채널 추정 성능을 확인한다. 마지막 장에서는 논문의 결론을 서술하고 향후 연구의 방향을 제시한다.Recently, studies using deep learning to solve some difficult problems are increasing. In the underwater communication environment, since the propagation medium is water, multipath occurs due to the medium fluctuation, and the propagation speed changes due to the water temperature or the like, which makes it difficult to estimate the channel. In this paper, QPSK symbols are OFDM modulated and transmitted thorough the underwater channel simulation environment. And the channel estimation algorithm using deep neural network in the underwater channel environment is studied.
We modeled the underwater channel environment in the West Sea using the bellhop ray tracing method.
 First, we confirmed that the larger the size of the neural network, or the more the learning data, the better the performance of the channel estimation.
 Next, we propose a channel estimation method using a large number of deep neural networks and compared with the existing LS channel estimation algorithm. When 128 pilot symbols were allocated, the neural network-based channel estimation scheme performed better than the conventional LS scheme. In a further experiment, the proposed scheme without any pilot symbols is better than LS algorithm using 64 pilots. Also, we confirmed that the more the number of pilots, the better the performance of the proposed neural network based channel estimation algorithm.
In this paper, the proposed neural network scheme is constructed using only the Fully Connected Layer which is the most basic form of the neural network. However, if Covolutional Layer is used, it is expected that the performance will be improved.

화행은 자연어 대화시스템이 발화를 이해하고 생성하는데 필수적인 요소로 국내외적으로 많이 연구되어 왔다. 화행분류에 대한 연구는 크게 규칙에 기반한 화행분류(Rule based Speech Acts classification)와 통계에 기반한 화행분류(Statistical Speech Acts classification)로 나누어 볼 수 있으며, 최근에는 영역 이식성 및 시스템 유연성이 좋은 통계에 기반한 화행분류 방법이 많이 연구되고 있다. 통계에 기반한 화행 분류는 일반적으로 정답이 부착된 대화 말뭉치 구성, 말뭉치로부터 화행 분류에 필요한 자질 추출, 통계학습을 위한 추출된 자질 표현, 분류를 위한 기계학습, 학습된 분류모델을 이용한 화행 분류의 과정을 거친다.
본 논문은 통계적 한국어 화행분류 시스템의 자질추출과 추출된 자질을 통계적 학습에 이용하면서 발생하는 여러 문제에 대한 개선 방법을 제안하고 이를 실험을 통해 증명한다. 논문은 먼저 한국어의 특성 및 대화에 나타난 여러 현상들을 모델링하여 화행 분류를 위한 새로운 발화 내 자질과 발화 간 자질 추출 방법을 제안한다. 그 동안 한국어 화행분류를 위한 발화 내 자질로는 구문분석을 이용한 구문유형이 사용되어 왔다. 그러나 구문유형은 한국어 화행 분류를 위한 정보를 완전히 표현하지 못해 한국어 화행 분류의 성능을 저하시켰다. 따라서 본 논문에서는 형태소분석 결과를 이용한 두 단계 발화간 자질 추출 방법을 제안하며 이 방법을 이용해 한국어 화행분류의 성능을 개선하였다. 또 기존 연구에서 사용한 이전 발화를 이용한 발화간 자질 추출 방법은 대화에서 자주 발생하는 부대화 발생시 적절하지 못하다. 본 논문에서는 이 문제를 처리하기 위해 대화말뭉치를 분석하여 구현이 편리한 담화 스택을 설계하고 담화 스택을 발화간 자질 추출에 이용하여 한국어 화행 분류의 성능을 높였다. 두번째로 본 논문은 제안된 자질 추출 방법이 기존에 연구되어온 다양한 자질 선택 방법과 결합하여 한국어 화행분류에 적절하게 작용하는 가를 실험하였고, 실험을 통해 자질 선택 방법이 자질의 수를 효과적으로 감소시켜 시스템의 속도를 개선할 수 있음을 보인다.
마지막으로 본 논문은 통계에 기반한 화행분류에서 가장 큰 문제점으로 지적한 데이터 희박성 문제(Data Sparseness Problem)와 데이터 내 각 화행 분포의 불균형에서 오는 성능 저하를 해결하기 위한 새로운 가중치 부여 방법을 제안한다. 이 방법은 분류 모델에서 가장 높은 성능을 보인 SVM에 화행 분류 체계와 쉬링크지(shrinkage) 방법을 이용하여 최적의 가중치를 부여하는 방법으로 전체적인 성능을 증가 시킬 뿐만 아니라 화행 분포의 불균형에서 오는 정확성 저하 문제를 보완할 수 있다.
본 논문에서 제안 되어진 자질 추출과 가중치 부여 방법은 일반적인 한국어 화행 분류 시스템의 정확도와 속도 향상에 기여 할 뿐만 아니라 상용대화 시스템을 위해 좋은 특성을 보인다. 형태소 분석을 이용한 자질 추출 방법은 기존의 방법에 비해 언어분석단계를 한단계 줄임으로서 속도 증가 뿐만 아니라 시스템 구현에도 많은 이점을 가지게 되어 실제 운영되는 상용대화 시스템에 적합하며, 화행 분류 체계와 쉬링크지를 이용한 새로운 가중치 부여 방법 역시 상용대화 시스템의 제약 조건 중에 하나인 충분한 양의 정답 말뭉치를 구성할 수 없다는 문제를 보완한다.Speech acts classification is an essential part of natural language dialogue systems to understand and generate an utterance. Many researchers have studied the speech acts classification. The techniques of speech act classification can be roughly divided into two folds; one is rule based speech acts classification techniques, the other is statistical speech acts classification techniques. Recently, statistical speech acts classification has been focused because it is more flexible and portable. The statistical speech acts classification generally has five steps; The first step is to construct dialogue corpus annotated with speech acts, the second step is to extract features from utterances in the annotated dialogue corpus, the third step is to represent utterances for statistical learning, the fourth step is to learn statistical classification model, the fifth step is to classify new utterance by using the statistical classification model.
This dissertation proposes an improved method for feature extraction and representation for statistical learning. First, we propose new intra-utterance and inter-utterance feature extraction methods that are obtained by modeling the various characteristics of Korean language dialogue. Syntactic patterns from a syntactic parser have used as intra-utterance features for Korean speech acts classification, but syntactic patterns do not completely represent all information for Korean speech acts classification. Therefore, they have reduced classification performance. We suggest to applying the intra-utterance feature extraction method using morphological analyzer to the proposed speech acts classification system. The new feature extraction method improved the Korean speech act classification systems by reducing the problem of syntactic patterns. Inter-utterance features of previous works, that have used the information of previous utterances, have a difficulty to classify speech acts when sub-dialogue is happened. In order to solve this problem, we analyze the dialogue corpus and design the easy-made discourse stack. We apply them to the extraction of inter-utterance features. As a result, we could achieve the improvement of the Korean speech act classification. In addition, we integrate the proposed feature extraction methods with conventional feature selection methods. The integrated system can decrease the training and response time and show the better classification performance.
For another improvement, we propose a new feature weighting method to solve data sparseness problem. The lower performance is based on ill-balanced distribution of speech acts. The new feature weighting method uses the hierarchy information of speech acts and a shrinkage technique. We apply it to the SVM classifier that has shown the best performance in Korean speech acts classification. The new feature weighting method can improve overall classification performance and compensate the problem of ill-balanced distribution of speech acts.
Since the speech acts classification systems in real application field have suffer from long training and response time, and data sparseness problem, the proposed feature extraction and weighting method suggests the solution for the problems. The proposed feature extraction method uses only morphological analyzer; it does not need high-level linguistic analyzers such as syntactic parser. Therefore, we can save training and response time for high-level linguistic analyzers and easily implement the system. Moreover, the new feature weighting method can compensate the data sparseness problem that real field systems have mainly suffered from.

대화 시스템이란 자연어를 이용하여 인간과 정보를 교환하거나 업무를 수행하는 프로그램이다. 자연언어는 인간이 사용할 수 있는 쉽고 효율적인 인터페이스이기 때문에 이를 이용한 대화 시스템의 필요성이 증대되고 있다.
본 논문에서는 지금까지 주로 인식에 초점이 맞추어져 연구되어 왔던 계획 기반 대화 모델을 이용하여 발화를 생성하는 시스템을 설계하고자 한다. 발화의 생성은 곧 시스템의 행위의 수행을 의미한다. 시스템의 행위는 Lambert의 Tripartite 모델에 기반하여 인식된 사용자의 계획을 통하여 결정되며, 결정된 행위는 현재의 상황에 대한 믿음과 사용자의 믿음을 고려하여 발화를 생성하게 된다.
본 논문에서 설계하고자 하는 대화 시스템은 사용자의 질의에 응답할 뿐 아니라, 자신의 행위를 능동적으로 수행할 수 있는 협조적 시스템이다. 또한 대화의 효율성을 고려하여 사용자가 필요로 하는 정보를 능동적으로 제공하는 시스템이다. 대화의 효율성을 고려한 발화를 위해 본 논문에서는 새로운 시스템의 행위들을 정의하고 실제 가능한 대화 예를 보인다.Dialogue system is a program that uses natural language to exchange information and/or do tasks. Since natural language is the easiest and most effective means of communication between human and computer, the importance of dialogue system is increasing.
In this paper, unlike most of previous works, which focused on recognition, we propose a natural language generation system using plan-based dialogue model. The generation of utterance is the system's action. The action is decided by the user's plan, recognized by Lambert's Tripartite model. The utterance is generated after checking the beliefs of the current situation as well as the beliefs of the user.
The system we propose a collaborative system that can answer user's questions, and actively take part in the dialogue. Also, to increase the efficiency it provides information that the user needs without having to wait for the user to ask for it. To handle such efficient dialogue, we define new communicative acts and show an example of a possible dialogue.

목차 1. 서론 = 1 2. 정보검색시스템의 기본고찰 = 4 2.1 정보검색시스템의 개요 = 4 2.2 정보검색시스템의 구성 = 5

본 논문은 자연어 질문에 대한 정확한 응답을 찾아내는 방법에 관하여 기술한다. 다양한 사용자의 질문의 유형을 분석하고, 그 유형에 따른 응답의 패턴을 구축한다. 질문에서 추출한 질의어들을 기반으로 검색엔진을 이용하여 검색한 문서들과 작성된 응답 패턴과의 매칭 여부를 검사하여 일치하는 답을 찾아낸다. 설문 조사에 의하여 질문을 수집하고, 수작업으로 질문 유형을 분류하였다. 질문 유형 별로 질문들을 선택하여 그 질문에 포함된 질의어들과 예상 응답을 키로 하여 검색하여 결과로 나온 문서로 응답의 패턴을 만들었다.
이 패턴들을 이용하여 새로운 질문에 대한 응답을 찾아내고 찾아낸 응답의 정확도를 계산함으로써 질문의 유형과 응답 패턴 타당성을 평가한다.This paper describes a method for finding correct answer about user's question with natural language. We analyze the question, classify the question type and construct answer patterns according to this question type. When user asks a question with natural language, the search engine collects related documents on the web using queries which are extracted from user's question by the system and the answer patterns are compared with these documents by QA pattern matching. Finally the system pinpoints answer boundaries precisely and identifies the answer from the documents. We collected the proper questions by users' survey, classified question types manually and used the extracted queries from the question and virtual answer in order to construct answer patterns.
We apply these patterns and question types to find the answer from new question and evaluate the precision of the results.목차 요약 = ⅷ Ⅰ. 서론 = 1 Ⅱ. 관련연구 = 3 Ⅲ. 질문 응답 시스템 = 5

최근 기계 학습 및 자연어 처리 기술이 발달하며 인공지능이 자동으로 뉴스 기사를 생산하는 ‘로봇 저널리즘’ (Robot Journalism 혹은 Algorithmic Journalism)이 새롭게 연구되어야 할 주제로 주목받고 있다. 로봇 저널리즘은 자동화된 뉴스 생성 시스템을 통해 텍스트 기반 뉴스 기사를 실시간으로 제작하고 대중들에게 게재하는 알고리즘을 일컫는다. 하지만 텍스트 기반 기사는 현재 뉴스를 소비하는 독자 패턴에 맞지 않는다. 현재 뉴스 독자들은 간결하고 이미지 지향적인 기사를
선호하는 경향이 있고, 실제 현재 언론사에서는 이미지, 인포그래픽, 도표 등 다양한 시각적 요소를 포함한 뉴스 기사를 생성하고 있다. 그러나 다양한 시각적 요소를 기존에 있는 로봇 저널리즘 알고리즘에 적용하려면 각종 정보 유형에 적절한
시각적 형태가 무엇인지 자동으로 파악할 수 있는 채점 방법(scoring method)가 필요하다. 현재 게재되고 있는 시각적 뉴스 기사는 그래픽 디자이너 혹은 편집자의 충분한 경험과 지식을 바탕으로 다양한 기사 내용에 적절한 시각적 요소가
결정된다. 하지만 자동 뉴스 생성 시스템에 시각적 요소를 포함하게 되면 각 상황마다 적절한 시각적 요소를 결정할 수 있는 디자이너가 없기 때문에, 이 과정을 자동화할 때 다양한 컨텍스트에 적절한 시각적 요소를 평가할 수 있는 채점 방법이
필요한 것이다.

기사에 포함되는 시각적 요소는 텍스트와 달리 다양한 속성 (i.e. 색깔, 크기, 모양, 등)을 통해 시각적 현저성 정도 (visual saliency level)를 구축하게 되는데, 이때 시각적 현저성은 제시되는 정보에 시각적 계층 구조(visual hierarchy)를 만들어
사용자의 주의와 정보회상에 영향을 미치게 된다. 따라서 이 연구에서는 시각적 요소가 사용자의 주의와 정보 회상에 미치는 영향을 관찰함으로써 뉴스 기사에 포함된 시각적 요소를 다양한 컨텍스트에 대비해 시각적 현저성 정도로 수치화하여
자동 뉴스 생성 과정에 필요한 채점 방법을 제시하였다. 그 결과, 각 컨텍스트 마다 사용자의 주의 및 정보 회상이 다양한 현저성을 나타내었다. 이러한 결과는 시각적 요소의 현저성을 다양한 컨텍스트에 대비해 관찰하게 되면 자동 뉴스 생성에
필요한 수치가 생성된다는 뜻이며, 이 수치는 향후 시각적 요소를 자동화에 포함할 때 필요한 채점 방법의 가능성을 시사한다.Automated news generation systems have been generating text-based news articles ever since the advancement of Natural Language Processing and machine-learning algorithms. Although automatic generation of text-based articles are effective in communicating information to users, the next step in the
automated production of articles would be including visual elements such as images, tables, and infographics. However, the challenge of incorporating visual elements into automated news generation relies on translating the human designer’s iterative design process into numerical values, which involves selecting pertinent set of visual elements according to the context in which the information will be displayed on. In order to automate this process, the visual elements must be scored to be weighed and selected according to context. One
way to assign values on the visual elements in news articles is through determining the level of saliency on each element, since it reflects the hierarchy of information. Accordingly, this research measured the saliency of visual elements by observing its effect on user’s visual attention and information recall. Also, this study chose ‘attentional state’ and ‘information behavior’ as two contexts in which the visual elements must be weighed against. The results indicate that each context provided a different range of saliency scores for visual
attention and information recall. This suggested a possibility in using saliency scores as a method to implement visual elements in automatic visual news generation.TABLE OF CONTENTS 1. INTROUDUCTION 1 2. LITERATURE REVIEW 9 2.1. AUTOMATED JOURNALISM 9

사람과 언어로 의사소통하는 기계를 만드는 것은 튜링테스트를 통한 인공지능 연구자들의 오랜 꿈이다. 그러나 언어의 수많은 예외와 불확실성으로 인해 전통적인 규칙 기반 방식으로는 모델링에 한계가 있었다. 최근 급격히 발전하고 있는 딥러닝 알고리즘을 이용하여 그 한계를 극복한 언어 모델링 연구가 이어지고 있다. 그 중 TextQA는 지문(context)과 질문(question)을 보고 답(answer)을 생성하는 연구로, 언어 모델이 얼마나 언어를 잘 이해하였는가를 테스트하기에 적합하다. 그러나 많은 연구들에도 불구하고 아직 사람의 수준에는 미치지 못하고 있다.
본 논문에서는 TextQA 모델의 성능 향상을 위한 Augmented TextQA를 제안한다. Augmented TextQA는 답을 이용하여 질문을 생성할 수 있는 모델을 만들고, 지문 속에서 추출한 키워드를 입력으로 질문 생성 모델을 통해 질문을 생성한다. 마지막으로, 새로 생성한 데이터를 추가하여 기존 TextQA 모델의 성능 향상을 시도한다. SQuAD(Stanford Question Answering Dataset)와 Seq2Seq 기반 TextQA 모델을 구현하여 실험한 결과, 전반적으로 Augmented TextQA를 사용했을 경우에 성능이 향상되는 것을 확인할 수 있었다. 나아가 Augmented TextQA를 활용하면 외부 자료를 사용하지 않고 도메인 상에서 데이터를 증강시키기 때문에 특정 도메인에서 단어의 표상을 더 적절히 학습할 수 있으며, 학습 데이터에만 국한되지 않은 확장된 TextQA가 가능해지는 효과를 기대할 수 있다.I. 서 론 1 II. 이론적 배경 5 2.1. 데이터 증강 기법 5 2.2. Seq2Seq 모델 7



1980년대 후반 이후 소프트웨어는 자연어에서 응용언어로의 변화를 보였다. 이때 소프트웨어는 사용자가 편리하도록 기능을 강화시켰기 때문에 소프트웨어 성능과 특성을 파악하기란 더욱 힘들어졌다. 소프트웨어 신뢰성 영역은 관리적인 측면의 소프트웨어 프로세스(Software Process)와 공학적인 측면의 소프트웨어 신뢰성 모델링(Software Reliability Modeling) 및 테스팅(Testing)으로 분류할 수 있다.
본 논문에서는 소프트웨어 신뢰성 구현을 위해 비-동질 안정 프로세스인 NHPP(Non-Homogeneous Poisson Process)모델을 적용하였으며 테스팅 실험은 실제 임베디드 시스템 소프트웨어가 탑재된 상용 제품(Commercial Product)에 적용하여 테스트 지그(Zig)를 구성하였다.
실시간 기반 임베디드 시스템 소프트웨어의 소프트웨어 신뢰성 예견과 평가 모델을 수립하기 위해 누적 결함 수, 고장 강도, 신뢰성의 예견 수치와 MTTF, 결함률에 대한 평가 수치를 산출하였다.
보다 높은 실시간 기반 임베디드 시스템 소프트웨어의 신뢰성을 구현하기 위해서는 안정된 소프트웨어 프로세스와 운영체제를 구축하고 보다 높은 테스팅 커버리지(Testing Coverage)를 확보한다면 실시간 기반 임베디드 시스템 소프트웨어의 성능과 신뢰성 특성을 향상시킬 수 있다.Since then 1980's, software is changed from natural language to application language. It is very difficult for us to understand software performance and characteristics because of adding a few powerful function to the software in order to a familiar user. The scope of software reliability can be classified to the software process of management side, and the software reliability modeling & testing of engineering side.
In this paper, we apply NHPP(Non-Homogeneous Poisson Process) model example to software process for software reliability. Test examination is constructed by test zig of commercial product loaded real embedded system software.
We are established to software reliability prediction and estimation of real-time embedded system software. We are computed for prediction value of cumulative failures, failure intensity,
reliability and estimation value of MTTF(Mean time to failure), failure rate.
To the more realization of high reliability in the real-time embedded system software, If the embedded system software is ensured to test coverage and constructed to stable software process & operating system, we can improve for performance & reliability characteristics of the real-time embedded system software.제 1 장 서 론 제 2 장 본 론 2.1 소프트웨어 신뢰성의 정의 2.2 소프트웨어와 하드웨어 신뢰성 비교 2.3 소프트웨어 신뢰성의 중요성

SNS is a web-based platform that helps to build or to keep relationships among people. SNS platforms in early stage including Friendster and MySpace were implemented for the desktop and laptop users. As more people access wireless internet using their mobile phones, SNS platforms can also have some important features such as "real-time access" and "location information". These two features make it possible to let people share their activities, interests, and observations in real-time at any places. Recently, most of SNS platforms including Twitter, Facebook, and Yelp use the location information of users. Therefore, if we consider a SNS user as a sensor that reports its observations at a specific location, it would be possible to detect events by analyzing their social contents. There have been already numbers of research on this topic have been published or still ongoing. Twitter has been widely used for conducting the research because it has important three features which are required to detect an event: time, location, and content. However, the most approaches struggle with detecting the location which is related to an event correctly. In this paper, we introduce a system that detects an event with its location in real-time based on increment of tweets that mention a specific location frequently. The result of performance evaluation shows that the proposed system detects an event in real-time. We also improved the system performance by reducing some noises from our system.SNS는 사용자들의 관계형성을 도와주는 웹 기반의 온라인 플랫폼 서비스이다. 기존 SNS 사용자들은 이를 이용하기 위해 주로 데스크톱이나 노트북을 이용하였다. 그러나 최근 스마트폰의 보급으로 인해 웹 접근성이 확대되면서 SNS 사용자가 크게 증가하였다. SNS를 이용하는 사용자들은 주로 자신의 일상이나 경험한 일들을 다른 사용자들과 공유한다. 이때 사용자 개인을 하나의 센서로 가정하고 그들이 남긴 콘텐츠를 분석할 수 있다면, 이를 이용해 현실에서 발생한 이벤트를 탐지할 수 있다. 이러한 시도는 이미 많은 연구에서 진행되고 있다. 특히 트위터의 경우 이벤트 탐지에 적합한 구조적 특징들로 인해 관련 분야에서 다양하게 활용되고 있다. 그러나 대부분의 선행 이벤트 탐지 연구들은 키워드에 의존하며 이벤트가 발생한 지역을 탐지하는 것에 있어 명확한 한계점을 지니고 있다. 이에 본 논문에서는 트위터에서 언급된 빈도가 급증한 지역들을 기반으로 이벤트가 발생한 지역을 실시간으로 탐지하는 TRED(Twitter based Realtime Event-location Detector) 시스템을 제안하였다. 이후 성능평가를 통해 제안하는 시스템이 실시간으로 동작할 수 있는지를 확인하였고, 실제 발생한 이벤트를 탐지함으로서 효율성을 입증하였다. 또한 노이즈 제거를 통한 탐지율 향상을 언급하였으며 향후 보다 높은 성능의 시스템에 대한 가능성을 보였다.목 차 감사의 글 ···························································· iv 그림 목차 ···························································· vii 표 목차 ····························································· viii 초록 ··································································· ix

본 연구는 한국어 자연어 처리를 통해 해당 문장이 정확한 사실을 말하는 문장인지 거짓을 말하는 문장인지 표현하기 위한 연구이다. Hedge는 언어의 표현에 있어서 불확실한 내용을 나타내기 위한 언어적 표현으로, 저자가 자신의 글에 내포된 내용이 불확실하거나 의심이 갈 때, 혹은 공손함을 표현할 때 자주 사용되는 표현이다. 이러한 불확실성 때문에 hedge가 포함된 문장은 사실이 아닌 문장으로 간주 할 수 있다. 또한 hedge 문장과 non-hedge 문장을 이용하여 정보검색, 정보추출, 질의응답 시스템 등의 여러 응용 분야에서 전처리 과정에 적용되어 보다 더 정확한 결과를 얻게 할 수 있는 효과를 볼 수 있다.
언어 표현의 정확하지 않은 표현과 공손함을 나타내는 표현은 영어권 보다는 한국어에서 많이 표현된다. 또한 한국어 특성상 한 가지 표현이 많은 다른 응용표현으로 연결되며, 한국어의 특성으로 인해 어순이 자유로운 문맥 자유 언어이고, 한국어의 사용에 있어서 주어의 생략 등과 같은 많은 생략이 자유롭게 이루어지는 특성을 지니고 있다. 이러한 특성으로 인해 한국어의 처리는 어려움이 많기 때문에 형태소 분석이나 구문 분석에서 어려움이 많다.
본 논문에서는 처음으로 시도되는 한국어 hedge 문장 인식을 위해 한국어 hedge 말뭉치를 구축하고, 이로부터 hedge 단서어구들을 추출하여 일반화된 단서어구 패턴을 구축하며, CRF(Conditional Random Fields), SVM(Support Vector Machines)과 같은 기계학습 기법을 이용하여 한국어 hedge 인식 실험하였다. 실험을 통하여 77.28%의 F-measure 값을 얻었으며, 이결과는 영어를 대상으로한 실험 결과인 61.91%보다 15% 가량 높은 수치로 이는 한국어와 영어의 언어적 차이에서 비롯된 결과이다.Ⅰ.서론 1 1.연구의 배경 1 2.연구의 목적과 방법 2 3.논문의 구성 2 Ⅱ.배경 지식 및 관련 연구 3



A dialogue system is a software program by means of which a user interacts with a system using natural language. An essential task of the dialogue system is to correctly understand user’s utterance and is to naturally generate a proper response. For the former the system should be able to correctly identify the domain action (i.e. the pair of a speech act and a concept sequence) that implies user’s intention. For the latter the system should be able to predict the next domain action based on dialogue history and domain knowledge. The domain action prediction of user''s utterance is useful to reduce the search space of an ASR (Automatic Speech Recognizer), and the domain action prediction of system''s utterance is useful to increase the flexibility of a response generator. In this thesis, we propose a model to predict a domain action of the next utterance by using CRFs (Conditional Random Fields). The proposed model predicts the next domain action by using three types of information as input features; sentential information, contextual information, and domain frame information. In the experiment, the proposed model showed the precision of 76.25%(speech act) and 64.21%(concept sequence) in domain action prediction of user’s utterance and the precision of 88.11%(speech act) and 87.19(concept sequence) in domain action prediction of system’s utterance. Based on these experiments, we believe that the proposed model showed a good possibility of reliable speech recognition and flexible response generation.

본 논문은 언어적 등계(Linguistic scales)가 자연어에서 어떻게 이용되는 지 를 연구하고 있다. 여기서 언어적 등계라함은 정보성(Informativeness)의 강약에 따라서 순위를 매길 수 있는 대체표현들(Alternatives)의 집합을 의미 한다. 이런 연구를 위하여 본 논문은 다음과 같은 현상을 중점적으로 다루고 있다. 즉 동계 함축(Scalar Implicature), 배타성(Exhaustivity), 영어 'eve n'에 의해서 유발되는 강조 단언(Emphatic assertion)을 다룬다. 이들은 공통적으로 언어적 등계에 기초하여 발생하는 언어 현상이다. 본 논문은 n-자리 이접표현(n-place disjunction)으로 부터 일어나는 단 계함축을 기존의 이론으로는 설명할 수 없음을 보이고 이에 대한 해결책을 제안한다. 또한 종래의 이론에서는 배타성의 현상을 의미론적인 것으로 취급 했으나, 본 논문에서는 그 부당성을 지적하고 이 배타성은 Grice-격률(Gricea n Maxims)의 기초가 되는 Grice의 협력원리(Gricean Cooperative Principle) 에 근거한 화용론적 현상임을 강조한다. 그리고 강조 단언에 대한 이해를 더 하기 위해서 영어의 "even"에 해당하는 한국어의 특수 조사인 "-마저",-"조 차",-"까지", -"도"와 "심지어"를 분석한다 이 한국어 특수 조사를 분석함으로서 강조 단어의 의미가 크게 두 부분으로 구성되어 있음을 밝힌다. 다시 말해서, 한 부분은 대화 참석자에게 놀라운 혹은 극단적인 상화을 전달하고 있다. 다른 하나는 그 놀라운 상황과 대조가 되는 놀랍지 않은 상황이 전제되어 있음을 전달한다. 위에 언급한 것외에도, 본 논문은 다음과 같은 주장을 한다. 첫째, 위에서 언급된 현상, 즉. 등계 함축, 배타성, 강조 단언 등은 대화참석자의 심상에 해당 언어 등계를 떠오 르게 하는 촉발자(Activators)가 있어야 한다. 촛점화작용(Focus), 극성어(P olarity items), 최상급표현(Superlatives)등이 이런 촉발자의 역할을 한다. 이들 촉발자들은 대화자의 세상 지식과 관련 언어표현의 의미와 어우러져 여 러 가지의 언어적 등계를 대화자의 심상에 도입한다. 둘째, 언어현상에 따라 서 이용되는 언어등계도 각각 다르다. 가령, 배타성의 효과는 대체표현들 중 오직 한 표현만이 해당 화맥에서 성립할 때 발생한다.This dissertation investigates the exploitation of linguistic scales in natural
language. Here, linguistic scales mean sets of alternatives which can be ordered in
terms of their informativeness. To this end, this dissertation will focus on the
phenomena of scalar implicature and exhaustivity both of which arise from the
use of terms from linguistic scales. In addition, the emphatic assertion triggered
by the particle even will be discussed. This work will show that previous analyses cannot account for scalar implicature arising from n-place disjunctions, and will propose solutions. Arguing against previous views on exhaustivity that treated it as a semantic phenomenon, the present work will claim that exhaustivity is rooted in the Gricean Cooperative Principle, which in turn consists of the well-known Gricean maxims. These phenomenon should be accounted for within pragmatics. To gain a better understanding of emphatic assertion, the current work will analyze the particles simcie, -to, -kkaci -cocha and- mace, which fill the role of the English even in an emphatic assertion. The analysis of those Korean particles helps us to see that the interpretation of emphatic assertion consists of two components: One conveys that the situation described by the expression in question is more or less extreme or surprising to interlocutors, the other corresponds to the presupposition that there is at least one alternative situation which is nonsurprising.
Furthermore, the present work will make the following claims: First, the
occurrence of the above mentioned phenomena requires activators of linguistic
scales. Typical activators are focus, polarity items, superlatives, etc. These
linguistic cues activate various scales based on the meaning of the constituent in
question and on interlocutors world knowledge. Second, different exploitations of
scales result in different phenomena. For example, the exhaustiveness effect
arises when only one out of a set of alternatives holds against the background.
Third, with regard to the activation of scales and the characteristic ways of
exploiting scalar information, current theories of focus are shown to be suitable for accounting for the above phenomena. Last, assuming current focus theories,
different focus-sensitive operators are argued to be involved in different phenomena ( i.e. scalar implicature, exhaustivity ). By giving an appropriate interpretation rule to each relevant focus-sensitive operator, a relevant effect can be captured.

토픽 모델은 문서 집합으로부터 토픽을 추출하는 모델이며 자연어 처리 등에 사용되고 있다. 대표적인 방법으로는 잠재 디리클레 할당과 단어 군집화 기반 토픽 추출방법이 있다. 그러나 이러한 방법의 문제점으로는 토픽 중복 문제와 토픽 혼재 문제가 있다. 토픽 중복 문제는 특정 토픽이 여러 개의 토픽으로 추출되는 문제이며, 토픽 혼재 문제는 추출된 하나의 토픽 내에 여러 토픽이 혼재되어 있는 문제이다.
이러한 문제를 해결하기 위하여 본 논문에서는 잠재 디리클레 할당을 기반으로 하여 단어 간 유사도를 이용하여 토픽을 보정하는 방법을 제안한다. 본 논문에서는 토픽 중복 문제 대해 강건한 잠재 디리클레 할당으로 토픽을 추출하고 단어 간 유사도를 이용하여 토픽 분리 및 토픽 병합의 단계를 거쳐 최종적으로 토픽을 추출한다. 실험 결과 제안 방법이 잠재 디리클레 할당 방법과 단어 군집화 기반 토픽 추출방법에 비해 토픽 중복 문제 및 토픽 혼재 문제에 대해 좋은 성능을 보였다.A topic model is a model to extract a topic from a set of documents, which has been used in natural language processing, etc. The representative topic extraction methods include Latent Dirichlet Allocation (LDA) and word clustering-based methods. However, there are problems with these methods, such as repeated topic and mixed topic. The problem of repeated topics is one in which a specific topic is extracted as several topics, while the problem of mixed topic is one in which several topics are mixed in a single extracted topic.
In order to solve these problems, this study proposes a method of correcting topics using the similarity between words based on the LDA. This study extracts topics with LDA that is robust regarding the problem of repeated topic, goes through the steps of separating and merging the topics using the similarity between words, extracting the final topics. As a result of the experiment, the proposed method showed better performances for the problems of repeated topic and mixed topic as compared to the LDA method and the word clustering-based topic extraction method.국문초록 ⅵ 영문초록 ⅶ 제 1 장 서론 1 1.1 연구 배경 및 개요 1

딥 러닝 기술이 컴퓨터 비전, 자연어 처리 등 여러 어플리케이션에 적용되어 큰 성공을 거두고 있다. 특히 강화학습 알고리즘에 딥 러닝 기술을 접목시킨 심층 강화학습은 아타리 게임을 이미지만으로 학습시킬 수 있게 하면서 주목을 받게 되었다.
이러한 딥 러닝 어플리케이션들은 대부분 많은 계산량을 요구하며 주로 데이터 병렬성을 이용해 GPU로 가속화했는데, 최근 들어 저전력 시스템에 대한 요구가 증가하면서 FPGA에서의 딥 러닝 가속화에 대한 연구가 활발히 진행되고 있다. FPGA는 어플리케이션에 따라 하드웨어를 재구성을 할 수 있는 디바이스이기 때문에 GPU에 비해 전력을 적게 사용할 수 있고, 특히 임베디드 시스템에서 활용도가 높다.
본 논문은 FPGA를 기반으로 심층 강화학습을 가속화하는 방법을 제시했다. 특히 아타리 게임 플레이를 위한 CNN(convolutional neural network) 모델을 가속화했다. CNN을 포함한 대부분의 딥 러닝 어플리케이션들은 계산량 뿐만 아니라 메모리 전송량이 많아서 메모리 대역폭과 온-칩 메모리가 제한된 FPGA에서 효과적으로 가속화하기 어렵다. 그러나 아타리 게임 플레이를 위한 CNN 모델은 FPGA 온-칩 메모리에 모두 들어갈 수 있는 크기이기 때문에 데이터의 재사용을 최대화할 수 있었다. 또한 C++, OpenCL 등의 상위 레벨 언어의 고수준 합성 툴을 사용하지 않고 하드웨어 레벨 언어인 Verilog로 프로그래밍해서 하드웨어 자원을 최대로 활용할 수 있도록 했다. 결과적으로 하드웨어 자원 90%이상을 활용했고, GPU와 비교했을 때 14배 이상 성능 향상을 보였다.Deep learning technology has been successfully applied to various applications such as computer vision and natural language processing. In particular, deep reinforcement learning combining deep learning with reinforcement learning algorithms has attracted attention by allowing Atari games to be learned with images alone.
Most of these deep learning applications require large amounts of computation and have been accelerated on GPUs, mainly using data parallelism. In recent years, there has been a growing demand for low-power systems, and studies are actively being conducted to accelerate deep learning on FPGAs. Since FPGA is a device that can reconfigure hardware according to applications, it can use less power than GPU, especially in embedded system.
This paper presents a method to accelerate deep reinforcement learning based on FPGA. In particular, we accelerated the CNN (convolutional neural network) model for playing Atari game. Most deep learning applications, including CNN, have large memory footprint as well as computational complexity, making it difficult to accelerate effectively on FPGAs with limited memory bandwidth and on-chip memory. However, the CNN model for playing Atari was able to fit in the FPGA on-chip memory, minimizing data transfer. As a result, it utilized more than 90% of hardware resources, and showed a 14x performance improvement compared to GPU.제 1 장 서론 1 제 2 장 관련 연구 4 제 3 장 강화학습 5 제 4 장 구현 상세 8 4.1 아키텍처 8

A conversation model is a system that responds appropriately to input utterances. Recently, the sequence-to-sequence framework has been widely used as a conversation-learning model. However, the conversation model learned in such a way often generates a safe and dull response that does not provide appropriate information or sophisticated meaning. In addition, this model is also useless for input utterances appearing in various forms, such as with changed ending words or changed word order. To solve these problems, we propose a method for modeling natural conversation using noise injection. During the training process, the proposed method injects noise into original input and trains model to generate true target response. The proposed method creates a model that will stochastically experience new input made up of items that were not included in the original data during the training process. This data augmentation effect allows the realization of a robust model by regularizing the model. We evaluate our model using 90k input utterances-responses from Korean conversation pair data. The proposed model achieves better results compared to a baseline model on both quantitative evaluations and qualitative evaluations by human annotators.I. 서 론 1 II. 관련 연구 6 III. 잡음 추가 기법을 활용한 대화 모델링 11

최근 몇 년간, 딥러닝은 음성인식, 자연어 처리, 이미지 분류 등 여러 많은 분야에 서 성공적인 성과를 내였다. 대부분 딥러닝은 유클리디안 공간에 대한 연구들이 주를 이루었지만, 많은 데이터들이 비율크리드에 존재 함에 따라 최근에는 유클리디안 도 메인이 아닌 그래프와 다양체 같은 비유클리디안 구조에 대한 연구도 진행 중이다. 이 논문은 비유클리디안 도메인인 그래프에 중점을 두어 스케터링 네트워크와 하-웨이블 릿을 사용한 하네트워크에 대해 살펴보았다. 또한, 그래프 컨볼루션 대해 살펴 보았으며 CNN 프레임 워크를 따르는 2D 하-스케터링 네트워크를 만들어보았다. 우리는 하-웨이 블릿을 사용하는 하-네트워크에 대해 실험을 하였다. 우리는 하-스케터링 네트워크에 분류기로 보편적인 딥러닝 방식인 LSTM을 사용 하여 MNIST 와 Fashion MNIST에 대해 테스트를 해보았다.In recent decades, Deep Learning approaches showed tremendous performance on a variety of different fields. Despite of the results obtained, research on Deep Learning techniques has mainly focused on data defined on Euclidean domains. However, there are lots of data with an underlying structure that is not of Euclidean domain. The adoption of Deep Learning on non-Euclidean domains has been lagging behind until very recently. In this thesis, we review a variety of approaches for graph convolutional networks and the scattering network representation. In particular, Haar scattering networks follow Deep Neural Network architecture. Furthermore, we suggest 2D Haar scattering network which is following the CNN framework. As an expriments parts, we introduce a classifier of Haar scattering networks by applying LSTM, which has been recently proven to be a powerful tool in Deep Learning. These algorithms are implemented using MATLAB and applied to MNIST and Fashion MNIST data.

일반적으로 사람들이 가진 지식 베이스를 컴퓨터에 적용하여 자연어 처리에 이용하면 많은 효과를 거둘 수 있다. 이러한 지식 베이스를 개념 체계(Ontology) 또는 시소러스(Thesaurus)라고 하는데 이는 대개 각 단어들을 그 의미에 따라 분류하여 그들 사이의 여러 가지 관계를 정리해 놓은 것을 의미한다. 영어권에서는 이미 WordNet이라는 사람들이 직접 구축한 의미 개념 체계가 존재한다.
기존의 영어 WordNet을 이용하여 한국어에 해당하는 의미 개념체계를 자동으로 구축해보려는 방법이 시도되었는데 이는 자동으로 구축하는 것이 수동으로 하는 방법에 비하여 적용할 수 있는 단어의 수가 많지 않고 정확도도 떨어지지만 일관된 기준으로 구축할 수 있고 비용이 적게 듦으로 유리한 점이 많다.
기존에는 단순히 한영 대역어 사전만을 가지고서 한국어 단어를 영어 WordNet에 연결하려는 시도가 있었으나 본 논문에서는 한영 대역어 사전 이외에 이미 구축된 한국어 개념 체계를 이용하여 영어 WordNet에 연결하는 방법을 제안한다. 이 과정은 먼저 대역어 사전을 이용하여 한국의 의미 계층 체계에서 각 명사에 해당하는 영어 WordNet 안의 의미 개념에 대한 연결 후보를 찾고 여기에서 양쪽 계층 구조를 반영하여 의미 애매성 해소를 하게 되어 올바른 연결을 구하게 된다.
이와 같이 양쪽 다국어 의미 계층 체계를 대역어 사전과 양쪽 계층 체계내의 관계를 이용하여 연결함으로써 새롭게 유용한 한국어 명사 개념 체계를 만들 수 있다. 이를 통하여 기존의 개념 체계 안의 상하의 관계뿐만 아니라 영어 WordNet에 있는 다른 많은 정보를 활용할 수 있게 된다.The human knowledge base has been used in Natural Language Processing. We call this knowledge base ‘ontology/thesaurus’. This ontology/thesaurus is composed of the well-classified words in a language, and arranges the various relationship among words in practice. In the English society, there already exists a well-defined lexical knowledge base called WordNet, which is constructed manually.
There have been made concerted efforts that automatically establish Korean ontology using the English WordNet and a Korean-English dictionary. Although the automatic construction of Korean WordNet is less accurate and does not cover more words than the manual construction, the former is preferable to the latter since it can be made in a relatively short time with less costs. Furthermore, it can keep consistency. Thus far, major researches has been using only a Korean-English dictionary to build Kore an thesaurus. Departing from this tradition, this study proposes that it is more appropriate to integrate the pre-existing Korean noun hierarchy into the English WordNet and a Korean-English dictionary in order to build a Korean noun thesaurus.
We first use a Korean-English dictionary to obtain each translation of nouns in the Korean noun hierarchy and the English WordNet. Then, we compare each structure of the Korean noun hierarchy and the English WordNet in order to derive correct sense mapping. We expect that these s processes may resolve word sense disambiguation to get the correct senses of Korean nouns that match synonym sets in WordNet (synset). As a result, we can obtain a new and well-constructed Korean thesaurus which can be useful in many fields.목차 = ⅰ 표 및 그림 차례 = ⅱ Abstract = ⅲ 요약 = ⅳ 제1장 서론 = 1

본 논문은 여러 데이터모형 중에서 최근의 경향인 자연어에 가까운 질의어를 위한 사용자 인터페이스(User Interface)를 제공하는데 매우 적절한 모형으로 평가되고 있는 관계질의어를 기존의 네트워크 데이타베이스 관리시스템(NDBMS)인 IMAEG/3000을 사용하여 수행할 수 있는 관계질의어 인터페이스인 UFS(User Friendly System)와 이 시스템을 위한 관계질의어 UFSQL(User Friendly System’s Query Language)을 설계 및 구현하였다.
UFS는 크게 사용자 인터페이스, 상부 인터페이스, 하부 인터페이스로 구성되어 있으며 이들 중 하부인터페이스가 본 논문의 주요 연구대상이다.
하부인터페이스는 동시에 여러 사용자가 사용할 수 있도록 한 병행수행기 법, 데이타베이스 보호를 위한 시스템 보호기법, 상부 인터페이스에 필요한 정보를 제공하고 질의어를 처리하는데 필요한 정보를 가지고 있는 시스템카타로그 그리고 질의어를 처리하는 UFSQL처리로 나뉘어져 있다. UFS를 위한 데이타베이스는 IMAGE/3000에 의해 네트워크 모델로 구축되어 있기 때문에 기억장소의 이용효율이 좋을 뿐 아니라 다양한 데이타 접근방법을 제공할 수 있어서 속도가 빠르고, 관계 질의어인 UFSQL은 사용자가 이해하기 쉬우며 사용하기 편리하다는 이점을 갖는다.This thesis studies the design and implementation of both relational query interface UFS and relational query language UFSQL for this system: that processes relational query language evaluated as the very good model supporting user interface for query language containing the nearest tendency of natural language by applying IMAGE as the present network database system. UFS consists of three-level database interfaces: user interface, high-level interface, and low-level interface. Among those, the database low-level interface is the main focus of this thesis. The low-level interface is further subdivided into concurrency executing mechanism for many computer users to use in the same time, system security mechanism for database protection, system catalog supplying the necessary information to high-level interface, UFSQL process for query language.
There are four advantages in the application of the UFS: the economical utility of storage space because of being composed network model by IMAGE, the high-speed supplying the vertical data approach methods, the good understandability for computer user to use UFSQL, the easy application of this system.목차 초록 = 2 1. 서론 = 3 2. 기본개념 = 5 2.1 관계 데이타베이스 = 5

무선인터넷 환경이 보편화되면서 언제 어디서나 인터넷에 접속할 수 있는 모바일은 현대인의 삶에서 가장 중요한 정보 검색 수단이 되었다. 최근에는 하드웨어 성능의 발전에 힘입어 휴대성이 강화되면서 모바일은 기존의 PC를 넘어서는 중요한 서비스 모델로 성장하고 있다. 모바일의 등장과 함께 가장 성공적으로 영향력을 확대한 분야는 SNS(Social Network Service)다. 최근 등장한 페이스북(Facebook), 트위터(Twitter), 인스타그램(Instagram) 등은 10년 안팎의 짧은 역사에도 전세계 수억 명의 사용자를 보유한 글로벌 IT 기업으로 성장했다. 모바일의 대중화, 보편화 흐름과 함께 성장한 SNS의 영향력이 날로 증가되면서 이를 효과적으로 활용한 서비스들이 늘어나고 있다.
본 논문에서는 모바일에서 획득한 GPS(Global Positioning System)를 활용하여 사용자의 위치 주변에서 발생한 SNS 데이터를 수집하고 분석을 통해 사용자가 원하는 장소를 추천하는 시스템을 제안한다. 이를 위해 트위터에서 위치정보를 포함하는 게시글을 표본 집합으로 정하고 모바일의 위치정보와 함께 활용했을 때, 사용자의 검색의도에 부합하는 양질의 정보를 제공할 수 있음을 실험을 통해 증명하였다. 이를 위해 2015년 11월부터 12월까지 수집한 트윗(Tweet)을 대상으로 임의의 위치정보와 검색어로 구성된 질의를 구성하고 형태소 분석을 거쳐 분석에 적합한 형태의 데이터로 변환하였다. 또한 장소 추천을 위해 감정사전을 구축하여 긍정 및 부정을 의미하는 극성 키워드들을 정의하고 레이블을 구성한 후, 감정사전과 극성키워드를 이용해 개별 트윗의 추천 점수를 도출하였다. 논문은 추천 점수와 사용자의 현재 위치, 트윗이 작성 된 위치와 사용자 위치 사이의 거리 계산을 통해 가까운 거리 순으로 10개의 장소 정보를 정렬하여 결과를 보인다. 또한 성능평가를 위해 감정 분석 된 트윗에 대한 정밀도와 재현율을 도출하여 시스템의 성능을 확인한다.
실험은 ‘맛집’, ‘공연’ 2개의 키워드와 10개 지역을 기준으로 수행하였다. 실험 결과 키워드 1개당 수집된 트윗은 평균 10.5개였으며, 실험에 사용된 210개의 트윗 중 긍정 또는 부정의 단어를 포함한 트윗의 개수는 122개였다. 또한 감정 분석을 통해 긍정 또는 부정으로 분류된 트윗은 65개였으며 그 중 실제로 긍정 또는 부정의 의미를 담은 트윗은 46개였다. 이를 통해 시스템은 38%의 재현율로 감정 요소를 담은 트윗을 탐지하고, 71%의 정밀도로 감정 분석을 수행했음을 확인했다.With the generalization of wireless Internet environment, mobile has become the most important searching measure in the life of modern people. SNS is a field which expanded the influence successfully due to the mobile emerged. For this reason, services which have utilized SNS effectively are increasing.
This paper proposes a system recommending spatial information what user wants with collecting and analyzing tweets around the user's location by using the GPS information acquired in mobile. This system has built an emotion dictionary and then derive the recommendation score of morphological analyzed tweets to provide not just simple information but recommendation through the emotion analysis information. The system also calculates distance between the recommended tweets and user's latitude-longitude coordinates and the results showed the close order.
This paper evaluates the result of the emotion analysis in a total of 10 areas with two keyword 'Restaurants' and 'Performance'. In the result, the number of tweets containing the words positive or negative are 122 of the total 210. In addition, 65 tweets classified as positive or negative by analyzing emotions after a morphological analysis and only 46 tweets contained the meaning of the positive or negative actually. This result shows the system detected tweets containing the emotional element with recall of 38% and performed emotion analysis with precision of 71%.1. 서론 1 2. 관련 연구 5 2.1 관련 연구 5 2.2 자연어 처리 기법 7

In this paper, with finding the information defined in advance from the web document the information was extracted the information extraction technology from the document of the heterogeneous and was converted in the form of one isomorphism quality. And the technology which irradiated the structure of being hidden in HTML and improves the knowledge access was designed and it implemented.
Presently, the information extraction refers to the technology which recognizes clearly the information in which a user wants only the information of the principal agent defined from high-capacity information datas in advance or the field of interest and which it processes to the template form.
The natural language processing base information extraction stores useful information in the subject area analyzing the content of a document the natural language processing technique to the ground to the format configurated like a database.
It effectually finds the suitable part in a text and the information system for extracting implements the technology in which the additional processing is possible unlike the natural language understanding.
The Wrapper base information extraction is the technology which makes the various strategies handling an internet possibly. It implements the technology showing about the internet information source of the heterogeneous unitized as one example of such strategy.목 차 Ⅰ. 서론 1 1. 연구 배경 및 목적 1 2. 논문의 구성 2

본 논문은 기존 전문가 시스템의 상담 형태에서 받아들여지는 자연어의 애매모호한 질의와 응답에 퍼지 값을 부여하여 불명확한 지식을 좀 더 명확하게하는 방향의 연구이다.
따라서, 본 논문에서는 전문가인 내과 의사의 경험적인 지식을 습득하여 전문가 시스템의 구축 도구인 Insight2+에 입력한 후 화면 운영 방식으로 환자가 컴퓨터를 통하여 대화하므로써 자신의 질환에 대한 질환 명을 알 수 있도록 해주는 의료 진단 시스템인 MESF(Medical diagnosis Expert System Introduced Fuzzy theory in Knowledge-based)를 구축하여 구현 방안을 제시하였다.
MESF을 병원에서 의료 진단에 도입될 시 전문가들의 개별적 지식보다 우월하므로 오진률을 좀 더 줄일 것으로 기대된다.This paper concerns with clarification of uncertain knowledge endowing fuzzy value to ambiguous querry and answer given by natural language in the field of consultation mode of present expert system.
In this paper, acquisition of heuristic knowledge of physicians as an expert and inputting that knowledge to insight2+ which is a building tool for the expert system were studied. The communication between expert system and patient was performed through computer terminal.
MESF(Medical diagnosis Expert System introduced Fuzzy theory in consultation mode) was suggested and implemented through menu-driven method, by which patient is told about identification of his disease.
Due to fuzziness introduced in consultation mode, suggested system will be better than existing systems with his own knowledge of a physician. As a result, if and when this MESF system is employed in hospital as medical diagnosis device, it is expected that physicians can reduce their fault-diagnosis rate.목차 ABSTRACT = iii 제1장 서론 = 1 제2장 전문가 시스템의 환경 2.1 전문가 시스템의 고찰 = 4

텍스트 마이닝은 객관적인 정보를 필요로 하는 많은 분야에서 쓰이는 기술이다. 특히 자연어 처리 기반 텍스트 마이닝은 반정형 또는 비정형 텍스트 데이터를 정형화하고, 그 특징을 추출하기 위한 기술이 핵심이다. 이 기술을 통해서 추출해낸 특징으로부터 사용자가 원하는 정보를 얻어낼 수 있도록 하는 것이 주된 목적이라고 할 수 있다. 다만 어떤 언어로 텍스트가 구성되었는지에 따라 특징을 추출하는 기술은 어느 정도 다를 수 있다. 특히 한글은 표현의 자유도가 높기 때문에 표준어가 정해져 있는 단어라도 조금씩 형태가 바뀌어서 사용되기도 한다. 또한 인터넷 상에서는 상대방에게 자신이 말하고자 하는 의미를 더 편하게 전달하는 데에 초점이 맞춰졌기 때문에 굳이 맞춤법을 완벽하게 맞출 필요가 없어졌고 이로 인해 발생하는 한글 파괴 현상이 한글 텍스트를 분석하는데 문제가 되기도 한다.
본 논문에서는 한글 텍스트를 음소단위로 분할하여 시그니처 비트를 생성하는 트라이그램 시그니처(Trigram-Signature) 기법과 구문 패턴을 분석하여 사용하는 구문 태그 패턴 기법을 통합한 새로운 상태 분류 기법을 제안했다. 그리고 기존 연구에서 분류 했던 만족, 불만, 의문, 흥분 4가지 상태에 추가적으로 낙담 상태를 추가하여 분류를 시도했다. 또한 기존 연구에서 만족/불만 분류에만 적용했던 극성 사전 기반의 기법을 나머지 상태 분류에도 동일하게 적용할 수 있도록 수정했다. 이에 추가적으로 제안한 시각화 시스템에서는 사용자의 ID와 글의 내용 및 게시날짜 데이터를 분석하여 사용자의 현재 상태 및 상태 변화와 활동 변화를 초기 화면에서 보여준다. 그리고 ID를 클릭하면 앞서 제공한 정보 외에도 사용자가 몇 주차에 어떤 상태의 글을 몇 건 게시하였는지를 원형 그래프 및 방사형 그래프로 나타내주며, 각각의 상태 글의 빈도와 극성 값을 확인할 수 있도록 꺾은선 그래프와 막대 그래프로 나타낸다. 이에 추가적으로 해당 ID의 사용자가 작성한 글도 취합하여 상태 별로 확인할 수 있도록 해준다.국문 요약 ------------------------- 01 제 1장 서 론 ---------------------- 03 제 1절 연구 배경 및 내용 ---------- 03

한국어 화행 분석 시스템은 발화에 담겨있는 화행을 분석해 내는 시스템이며 자연어를 통해 컴퓨터와 대화하고자 하는 여러 응용에서 필수적인 부분이다.
본 논문에서는 화행을 분석하는 방법을 제안한다. 먼저 사용 가능한 자질은 문장 자질과 문맥 자질 두 가지로 나누어 볼 수 있다. 문장 자질은 화행을 분석하고자 하는 해당 문장으로부터 추출되는 자질이다. 문맥 자질은 이전 문장들과 현재 문장으로부터 추출되는 자질이다. 이러한 자질들 중에서 카이 제곱통계량을 이용하여 화행을 결정하기 위한 적절한 자질들을 선택한다. 제안하는 시스템은 선택된 자질들에 대해서 적절한 가중치를 부여한 후에 Support Vector Machine을 이용하여 사용자로부터 입력되는 문장의 화행을 분석하게 된다. 실제 영역에서 수집되어 전사된 말뭉치에 대해 제안된 시스템을 이용하여 실험하여 기존 시스템들에 비해 좋은 성능을 얻을 수 있었다.A speech act analysis system for Korean dialogue sentences finds the speech act of a sentence in a dialogue. It is essential for the applications.
This thesis proposes an analysis system of speech act for Korean. First, usable features are divided into two parts as sentence features and context features. The sentence features are extracted from the sentence which is asked to speech act analysis. The context features are extracted from the current sentence and previous sentences. Among these features, appropriate features for speech act analysis are selected by Chi square statistics. After suitable weighting to selected features, proposed system analyzes speech act of the sentence input from user using Support Vector Machine. The proposed system was tested in Korean dialogue corpus transcribed from recording in real fields, and the proposed system showed better performance than existing systems.목차 = ⅰ 표 및 그림 차례 = ⅲ Abstract = ⅳ 요약 = ⅴ 제1장 서론 = 1

In this study, we conducted a technique for normalize unstructured data, one of the main concerns of text mining, and analyzed the prediction of a company's profitability by document classification algorithm. Starting with collecting unstructured data, preprocessing of collected data, and classification analysis by document classification algorithm is described after converting to a word-document matrix. The document classification algorithm used is a logistic regression model, a convolution neural network model, and gradient boosting model. The three models are used to provide a classification analysis for reports published by securities companies in accordance with the procedures previously introduced.I. 서 론 1 II. 본 론 7 1. 자연어 처리 단계 7 1.1자료 수집 8 1.2전처리(pre-processing) 10

요약



패러프레이즈란 같은 의미를 나타내는 여러 가지 표현을 말한다. 패러프레이즈는 자연어 처리 분야에서 다양하게 활용할 수 있다. 특히 최근에는 기계 번역 분야에서, 데이터 부족 문제를 보완하여 통계적 기계 번역의 성능 향상을 위해 패러프레이즈를 활용한 연구가 많다.
패러프레이즈 추출을 위한 기존 연구는 크게 단일어 병렬 말뭉치를 이용하는 연구와 이중 언어 병렬 말뭉치를 사용하는 연구로 나눌 수 있다. 이때 사용한 단일어 병렬 말뭉치는 한 가지 소설을 여러 버전으로 번역한 경우로, 구축이 매우 어렵고 도메인이 한정된다. 따라서 이중 언어 병렬 말뭉치를 사용하는 연구가 늘어나고 있으며, 이중 언어 병렬 말뭉치를 이용하여 패러프레이즈를 추출하는 것이 유용하다고 기존 연구에서 입증하고 있다.
이중 언어 병렬 말뭉치를 이용하는 패러프레이즈 추출 과정에서는 일반적으로 다른 언어를 피봇으로 이용하여 단어 및 구 정렬 과정을 두 번 거친다. 따라서 단어 정렬의 오류 전파 문제가 큰 단점이 된다. 특히 한국어와 영어와 같이 언어의 구조적인 차이가 큰 경우, 단어 정렬 오류가 더 많고 이로 인해 잘못된 피봇 프레이즈가 선정되는 문제가 더욱 심각하다. 이런 문제를 보완하기 위해, 본 논문에서는 패러프레이즈 추출 과정에서 피봇 프레이즈를 차별화하는 방안으로서, 올바른 피봇 프레이즈에 더 높은 가중치를 부여하는 방법을 제안한다. 실험 결과, 기존의 패러프레이즈 추출 방법에 제안하는 피봇 가중치 부여 방법을 추가적으로 적용했을 때, 패러프레이즈 추출 정확률과 재현율이 모두 향상됨을 확인할 수 있었다. 또한, 제안한 피봇 가중치 적용한 패러프레이즈 추출 방법을 번역 모델에 적용 시 기존 패러프레이즈 추출 방법보다 성능이 미미하게 향상됨을 확인 할 수 있었다.목 차 요약 vi 제 1 장 서론 1 제 2 장 관련 연구 4

컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식기술은 많은 진보가 있었다. 그러나 아직까지 대어휘 자연어 연속음성인식 시스템을 구축되기 위해서는 해결하여야 할 많은 연구과제들이 남아있다. 그 중의 하나는 연속음성에 대한 음성인식 단위로의 최적 자동분할이다. 현재 대부분의 연속음성 인식 시스템은 인식대상 어휘에 따른 언어모델, 발음모델, 음향모텔을 이용하여 탐색 네트워크를 구성하고 입력된 음성을 의사 분할한 후, 비터비 (Viterbi) 탐색으로 최적경로를 찾는다. 이 방법에서 발생하는 의사 분할의 문제점은 세그먼트 기반 연속음성인식으로 보완된다. 이 때, 연속음성인식 시스템을 구축하기 위한 필수적인 선결과제가 연속음성의 최적 자동분할이다. 그리고 음성인식의 대표적인 전처리로 음성구간검출이 있는데 최근 들어 잡음에 강인한 음성구간검출 방법이 요구되고 있다.
본 논문은 우리말 연속음성인식을 위한 음성구간검출과 음절과 음소단위로의 최적 자동분할 방법에 관한 연구이다.
음성구간검출은 에너지등을 이용하는 규칙-기반 방법을 이용하는 대신 분류-기반 방법을 이용한다. 분류-기반의 검출기가 잡음 환경에 적응하여 음성을 실시간으로 검출하기 위해서는 적응화 과정이 필요하다.
본 논문에서는 실시간 적응화 기법을 적용하기 위한 선결과제인 다차원 음성 특징을 축소하는 방법을 제안한다. 이 방법은 잡음과 음성 클레스에 대한 특징벡터를 확률 우도값으로 매핑시켜 차원을 비선형적으로 축소하는 방법이다. 제안된 축소 방법은 선형판별분석법에 의하여 선형적으로 차원을 축소된 결과와 비교하였으며 우도비 검증(LRT: Likelihood Ratio Test)을 이용하여 음성/비음성을 분류하였다.
연속음성의 최적 자동분할은 음향-음소 정보와 기존의 음성 라벨링 시스템과 결합하면 정확도나 견고성에서 확연한 개선이 기대되므로 지식기반 정보인 음향-음소 연결정보를 이용하여 음성을 자동 분할하고 라벨링하는 방법을 제안한다.
음절단위 분할은 유한상태 오토마타로 설계된 파서을 이용한 방법을 제안한다. 이 방법은 일차적으로 음향-음소 특징들을 이용하여 라벨링하고 라벨링된 정보를 유한 상태 오토마타에 적용하여 스캐닝한 다음 마지막으로 음절단위로 파싱하는 순서로 이루어진다.
음소단위 분할의 경우, 전통적인 GMM과 최근 많은 연구가 이루어지고 있는 SVM을 이용하여 자동분할하고 후처리를 거쳐 분할을 완성하였다.Technology of speech recognition has been improved by the development of computer science. Despite that, there are a lot of problems to implement large vocabulary natural language continuous speech recognition system without any restriction. One of those problems is optimal automatic segmentation which divides continuous speech into the unit of speech recognition. Until now, most continuous speech recognition systems make a search network by using speech, pronunciation and acoustic model for recognition, then find the optimal path through viterbi search. In this approach, the problems of pseudo segmentation above are solved by continuous speech recognition based on segment. Therefore, the prior problem in implementing continuous speech recognition system is the optimal automatic segmentation of continuous speech.
This dissertation research Voice Activity Detection(VAD) of Korean continuous speech recognition and the optimal automatic segmentation method as a unit of a syllable and a phoneme. VAD is achieved from a classifier-based method instead of a rule-based method. This method needs adaptation procedure for the real-time speech detection in noise environment. To implement adaptation procedure, this dissertation use a dimension reduction of multi-dimensional speech feature. This method that reduces dimensions non-linearly to map the likelihood of feature vector of speech and noise is compared with Linear Discriminant Analysis(LDA) with linear dimension reduction. The implementation of both methods in classification of speech and non-speech is by Likelihood Ratio Test(LRT).
The optimal continuous speech automatic segmentation uses connection information of acoustic-phoneme based on knowledge, divides speech automatically and labels it.
Segmentation as a unit of syllable uses a parser method with finite state automata, which labels acoustic-phoneme feature, applies and scans the information to finite state automata, and parses as a unit of phoneme in order. In segmentation as a unit of phoneme, this dissertation performs automatic segmentation by GMM and SVM and implements post-processing.국문초록 = ⅰ 目次 = ⅲ Ⅰ. 서론 = 1 Ⅱ. 우리말의 음향학적 특징분석 = 6 1. 음성의 발생 = 6

Recently, peer-to-peer live broadcasting services which allow individual users to produce content directly to be shared with others have become increasingly popular. ”One-man media” is one of the terms used to refer to this relatively new phenomenon in the field of media. However, it is often not easy for retuning viewers or new audiences to fully understand each piece of content in its entirety because of the dynamic environment in which many people simultaneously interact with one another. In addition, it is also difficult to categorize which parts of video are more important than other parts. In this study, we propose a system of making video clips based on timestamp comments in live video streaming. An One-man media within Korea that generates the greatest amount of real-time communications is selected. Afterwards, timestamp comments are collected within the service to identify the key topic words in clips. The clips will be clustered together based on the corresponding topic words and they are provided to users with appropriate keywords. We evaluated how appropriately the topics were defined for each piece of content and how accurately video clips were matched with suitable keywords, and the results confirmed that the proposed system performed better and solved many problems of the existing method. Using the proposed method, it is possible to automatically identify content topics and provide users with appropriate video clips on past broadcasts. Therefore, it can assist in enhancing overall understanding of broadcasting and also enable unprejudiced video editing.최근에 개인이 직접 콘텐츠를 제작하고 온라인을 통해 사용자들과 공유하는 1인 미디어의 인기가 높아지고 있다. 하지만 이러한 1인 미디어는 수많은 사람들이 동시에 소통하는 환경이기 때문에 그들이 작성한 모든 내용을 이해하기는 힘들며, 시간이 지나거나 새롭게 방송을 보는 시청자들은 방송의 흐름을 파악하기가 어렵다. 또한, 동영상 내에서 다른 부분에 비하여 중요한 부분을 파악하여 구분 짓는 것은 어려운 일이다. 본 연구에서는 국내에서 가장 실시간 소통이 활발한 1인 미디어 플랫폼을 선정하고, 실시간 비디오 스트리밍 환경에서 타임스탬프 코멘트들을 수집하여 클립들에 존재하는 주제를 파악한다. 이후 파악한 주제를 기반으로 클립들을 군집화시켜 전체 비디오 영상에서 다른 부분보다 중요한 부분을 구분하여 이해를 도울 수 있는 적절한 키워드들과 함께 제공하는 비디오 클립 영상 제작 시스템을 제안한다. 제안된 시스템은 전체 영상에서 다루고 있는 주제를 얼마나 잘 파악하고 정확하게 클립 영상을 예측했는지 평가하였고, 본 연구에서 제안한 시스템이 기존 방법의 문제를 해결하고 결과가 향상되었음을 확인하였다. 본 연구의 제안 방법을 활용한다면 다루고 있는 여러 개의 주제들을 자동으로 식별할 수 있고, 이미 지나간 방송 내용을 사용자에게 제공함으로써 전체적인 방송 영상의 이해를 돕고 객관적인 비디오 편집이 가능할 것이라고 생각된다.제 1장 서론 1 제 2장 관련 연구 6 2.1 1인 미디어 6 2.2 자연어 처리 8 2.3 주제 분석 10



인공 지능 연구가들의 최대 목적은 인간이 할 수 있는 것처럼 어때한 경험으로 부터 효율직으로 학습하는 시스템을 만드는 것인데, 자연어 처리에서 부터 회로 디자인등의 여러 분야에서 많은 시스템이 개발되었고 진행 중에 있다.
본 논문은 논리프로그래밍 환경에서 시스템의 성능을 증가시킬 수 있는 학습 시스템에 환하여 연구하였다. 학습은 설명을 기반으로 하는 일반화(Explanation-based Generalization:EBG)와 직접수행가능화(operationalization)에 의해서 이루어진다. 이러한 학습의 성질을 Prolog에 첨가한 결과는 다음과 같이 요약된다. 원시의 질문에서 많은 탐색 (search)과 서브루틴 호출(subroutine calling)이 필요한 경우 일수록 학습한 후에는 수행 시간이 많이 줄어든다.The ultimate objective of artificial intelligence(AI) researchers is to make system that learn from their experience as effectively as human do. A number of systems have been and are still being developed in a broad spectrum of domains from natural language processing to circuit design.
This paper discusses on learning to improve system's performance. Learning consists of explanation-Based Generalization and operationality. The effects of adding a learning component to Prolog can be summarized as followes : the more search and subroutine calling in the original query, the more reduce execution time after learning.목차 * 초록 = III 1. 서론 = 1 1.1. 학습에 관한 역사적인 관점 = 1 1.2. 기계 학습의 분류 = 2

This research purpose of this study is to analyze the research trends and changes of issues about social media engagement by using social network analysis and a machine learning method.
The imprtance of social media is growing rapidly through diverse fields of our lives. Most companies make use of social media as a marketing tool and hence it has become important for those who make use of it day by day. Since social media is mainly used with their participation such as putting posts on their web, rating, reposting, commenting, and etc, critical efforts are made to make online users to participate on the social media. Many researches proved relationship between social media and influential elements that foster participation. But there are few meta-analysis, and none that analyzed the trend with big academic data.
This study used main path analysis, social network analysis, and machine learning to conduct meta-analysis regarding social media participation. As a resul of main path analysis, 37 researches were extracted and it was found that the main path of social media participation consists of online community and new media networks.
Majority of papers was discussing the decisive elements that promote social media participation.
Periodic social network analysis and k-means clustering results revealed that the data mainly is in form of topics regarding social network service, online community, political participation, online business, healthcare, and game. The academic significance of this study is that it conducted macroscopic-microscopic analysis of social media participation using network analysis and machine learning.네트워크 분석과 클러스터링 기법을 활용한 메타 분석을 실시하였다. 소셜 미디어(Social Media)의 중요성은 다방면에서 매우 중요해지고 있다. 특히 기업들에게는 없어서는 안 되는 마케팅 채널로써 그 역할이 증대되고 있다. 이에 따라 소셜 미디어 참여와 관련된 연구의 수도 해마다 증가세를 보이고 있다. 하지만 온라인 커뮤니티, 소셜 네트워크 서비스 등 다양한 소셜 미디어 구성 요소들과 온라인 참여를 입증하는 연구들은 다수 선행되어왔으나, 이들을 종합적으로 분석한 연구는
부재하였다.
본 연구는 기존에 연구자의 주관에 따라 연구를 구분하는 문헌 분석과 달리 주경로 분석과 네트워크 분석, 기계 학습을 동원해 메타 분석을 실시하였다. 주경로 분석 결과 총 37개의 주요 연구들이 추출되었으며 온라인 커뮤니티 관련 네트워크와 뉴 미디어 관련 네트워크로 구분되는 것으로 나타났다. 전반적으로 각각 온라인 커뮤니티와 뉴 미디어와 온라인 참여를 결정 짓는 주요 요인을 추출하는 문헌들이 주류를 형성하였다.
사회 연결망 분석과 K-평균 클러스터링 분석은 시기를 세 가지로 구분하여 분석을 실시하였는데 크게 소셜 네트워크 서비스, 온라인 커뮤니티, 온라인 정치 참여, 온라인 비즈니스 관련 키워드와 클러스터가 형성된 것으로 분석 결과 나타났다. 본
연구는 소셜 미디어 참여와 관련한 학술 빅데이터를 활용해 소셜 미디어 참여의 연구 동향을 거시적으로 파악하고, 네트워크 분석과 기계학습을 활용해 분석에 활용해 주요 쟁점을 미시적으로 분석하였다는 학술적 의의를 가진다.



최근 소셜 네트워크 서비스의 영향력이 증대되면서 이에 대한 효과적인 분석법에 대한 관심이 높아지고 있다. 그중에서도 정치, 특히 선거 분석은 소셜 네트워크 기반 연구들 중 가장 활발하고 분석 요구가 높은 분야로 기존의 종이나 전화 여론조사를 대체할 충분한 잠재력을 가지고 있다. 그러나 한편으로는 아직까지 낮은 분석 정확도와 부족한 연구 성과들로 최근까지도 분석에 어려움을 겪고 있는 분야이기도 하다. 이에 본 논문에서는 항구성을 띄는 인간의 정치적 성향에 착안하여 이를 분석 설계에 적용할 경우 정확도 향상에 기여할 수 있음을 가정하고 실험을 통해 증명하였다.
본 논문에서는 정치적 성향을 개별 트윗에 의존하여 분석하는 기존의 방법들 대신 전체 타임라인을 통해 사용자 별로 분석하는 방법을 제안한다. 이를 위해 2012년 4월 11일 제19대 국회의원선거 기간 동안 발생한 트윗을 이용하여 트윗 코퍼스를 구성하였다. 실험에 앞서, 트윗 코퍼스에서 선거 도메인을 대표하는 검색 키워드들과 긍정 및 부정을 의미하는 극성 키워드들을 추출하고 레이블을 구성한다. 다음으로 각 계정의 타임라인에 존재하는 모든 트윗에 레이블을 적용하여 극성을 분류하고 사용자의 정치 성향을 도출한다. 마지막으로 분석 성능을 살펴보기 위해 개별 트윗 분석과 타임라인 분석의 정확도와 재현율을 도출하고 사용자 타임라인 분석의 정확도 향상을 확인한다.
실험 결과, 보수와 진보 성향으로 분류된 계정은 각각 295개, 293개였고 양쪽 성향이 동등하게 나타나 상쇄된 계정은 1,026개였으며, 실제 선거 결과와 비교했을 때 상당히 유사함을 확인할 수 있었다. 성능평가는 개별 트윗 분석과 계정별 성향 분석으로 나누어 평가하였는데, 개별 트윗 분석의 경우 75.4%의 정확도와 34.8%의 재현율을 보였다. 반면 계정별 성향 분석의 경우 85.7%의 정확도를 보여 약 10%의 성능 향상을 보였다. 다음으로 사용자의 성향과 일치하는 트윗 비율을 알아본 결과, 계정의 성향과 같은 극성을 따르는 트윗은 80.9%, 반대의 극성을 따르는 트윗은 19.1%였다. 이를 통해 트위터에서도 항구성을 띄는 인간의 정치적 성향이 그대로 반영된다는 사실을 증명할 수 있었으며 개별 트윗 수집을 통한 코퍼스 분석보다 사용자 계정 단위의 분석이 정치적 성향 분석의 정확성을 향상시킬 수 있음을 확인하였다.The interest in the effective methods for analysis of social network services has increased due to the rapid growth of influence of social media. Among them, politics, especially election analysis is a field that is most active among social network-based studies and the demand for analysis is high and has an enough potential to replace existing paper or telephone poll. However meanwhile, it is a field going through difficulty in analyzing even lately due to low accuracy rate of analysis and insufficient study results. With this, this thesis paid its attention to human's political attitudes showing permanence and assumed that if applying it to the analytic design, it would contribute to the increase of precision and demonstrated it through the experiment.
This thesis suggested a way to analyze the political attitude by users through entire Timeline rather than existing ways to analyzing by relying on the individual Tweet. For this, it composed Tweet corpus using Tweet that happened during the 19th National Assembly election on April 11, 2012. Prior to the experiment, it extracts search keywords representing election domain and polarity keywords meaning positiveness and negativeness in Tweet corpus and composes label. Next, polarity is classified by applying label to all Tweets existing in Timeline of each account and deduces user's political attitude. Finally, it deduces analyzed political attitudes of users, precision and recall of individual Tweet analysis and Timeline analysis and the increase of precision of user Timeline analysis is confirmed.
As a result of experiment, the number of accounts classified into conservative attitude and progressive attitude was each 295, 293, and the number of accounts offset as both attitude appear similarly was 1,026 and it could be known to be considerably similar compared to actual election result. Performance evaluation was accomplished by dividing into individual Tweet analysis and by-accounts attitude analysis, the precision of 75.4% and recall of 34.8% was shown in case of individual Tweet analysis. On the other hand, the precision of 85.7% was shown in by-accounts attitude analysis so the performance improvement of approximately 10% was shown. As a result of examining the ratio of Tweet corresponding to user's political attitude, the Tweet following the same polarity as account's political attitude was 80.9%, the Tweet following the opposite polarity was 19.1%. Through this, the fact that human's political attitudes bearing permanence is reflected to Twitter as it was could be demonstrated, and the fact that the analysis of user account unit could improve the precision of political attitude analysis better than corpus analysis through individual Tweet collection was confirmed.감사의 글 ······················································································ iv 초록 ································································································ x 1. 서론 ···························································································· 1

본 논문은 상호정보와 문장의 구간 분할을 이용하여 효과적인 한국어 구문 분석을 목적으로 한다.
오늘날 자연어 구문 분석 기술은 만족할 만한 수준에 도달하지 못하고 있고 한국어 구문분석 기술 역시 만족할만한 수준과는 거리가 멀다. 특히 문장의 길이가 긴 문장의 경우 구문분석기가 너무 많은 계산 량으로 인해 제대로 동작하지 못하는 경우가 빈번히 발생하고, 비록 구문구조 결과를 내더라도 정확도가 낮은 경우가 많다. 그 이유는 문장의 길이가 길어질수록 중의성이 매우 증가하여 많은 수의 구문분석 결과가 가능하기 때문이다. 이 중에서 정확한 구문구조를 선택하는 문제는 매우 어려워서 기존의 긴 전체 문장에 대한 구문구조를 한번에 계산하려는 시도는 앞으로도 계속 좋은 결과를 기대하기 어렵다.
따라서 우리는 문장의 길이에 상관없이 항상 안정적으로 결과를 내며, 구문분석에 소요되는 시간이 비교적 짧고, 정확도 역시 높은 구문분석기를 개발하고 자한다.
이를 위하여 전체 문장에 대해서 전처리 과정을 거친 뒤 구간으로 부르는 어절단위로 분할하여 각 구간을 독립적으로 구문 분석한다. 전처리 과정에서는 실질적으로 구문분석 과정에서 불필요한 어절들을 미리 다른 어절과 묶어서 좀더 구문분석을 용이하게 하였다. 구간분할 과정에서는 구문분석 과정에서 발생하는 많은 구문 중의성을 해결하기 위해서 좀더 긴밀한 연관관계를 갖는 명사-용언에 대한 구간으로 분할하여 구문분석을 돕는다. 구문분석에는 CYK기법을 사용하여 구문분석을 수행하였고 구문분석 시 발생하는 구문 중의성을 해결하기 위해서 공기관계정보에서 추출한 상호정보와 격 관계 정보, 다양한 휴리스틱을 이용하였다. 그 다음 각 구간의 구문분석 결과를 통합하여 전체 문장에 대한 결과를 생성하는 기법을 택하였다.
본 시스템의 성능을 평가하기 위해서 다양한 분야에서 임의로 추출한 100문장에 대해서 실험하였고, 실험 결과 정확률이 만족할 만한 결과(87%)를 얻음을 알 수 있었다.Successful results could not be achieved in structural analysis of Korean sentences because of the characteristics of Korean including little striction on the order of words. The parsers experienced difficulty especially in analyzing long sentences. To cope with this problem our research took the approach of sentence segmentation. In this approach a sentences are split nto segments and the segments are parsed independently. The parsing results of segments are integrated to produce the parse of the whole sentence. The segmentation is done in such a way that nouns and verbs with dependency relation do belong to the same segment as much as possible to reduce the amount of ambiguity. The CYK parsing technique was used for structural analysis of segments. Mutual information obtained from co-occurrence data, case relations and various heuristics were used for resolving ambiguity. The total structure for a sentence was constructed by connecting the structures of the segments. Applying this approach showed that the rate of decrease of analysis accuracy was cut down much compared with other methods.목차 그림차례 = ii 표차례 = iii 국문요약 = iv 제1장 서론 = 1

Natural language Processing system is a n important part in studying artificial intelligence. In this thesis, conceptual dependency theory and the structure of Korean are described for understanding N.L.P.S.
Conceptual dependency theory is used to construct a system for understanding Korean. It has 13 basic forms, but it is reconstructed as having 11 forms in order to be adapted to Korean in this thesis. In C.D. theory, verb is regarded as the core of a sentence. Therefore, the sentence is classified with the case of verb in it.
The knowledge base called semantic network is needed to understand natural language, and its construction method is studied in it for this system. At last of this thesis, some problems of this system and some of their solutions afe described.목차 = ⅰ ABSTRACT = ⅲ Ⅰ. 서론 = 1 Ⅱ. 이론적 배경 = 3 A. 자연어 처리시스템 = 3

본 논문에서는 검색효율을 향상시키기 위한 시소러스 관리 시스템과 시소러스를 이용한 질의어 확장 시스템의 설계 및 구현에 관하여 살펴본다.
자연어 기반의 정보검색시스템에서는 색인어와 탐색어의 불일치 및 형식의 다양성으로 인한 재현율 및 적합율의 저하를 가져온다. 그러한 문제를 해결하기 위하여 시소러스를 통한 질의어 확장방법을 구현함으써 해결을 시도하였다.
또한 단순한 질의어확장 뿐만 아니라 원자료의 색인시 적용한 형태소분석 방법을 확장된 질의어에도 다시 적용함으로써 재현율을 더욱 높일 수 있도록 구현하였다. 즉, 한국어 자료의 띄어쓰기를 해결하기 위하여 복합어로 구성된 시소러스를 임의로 띄어쓰기 하는 것보다는 정보검색엔진의 형태소분석기를 활용하는 것이 더 효율적이라고 볼 수 있다.
따라서 향후 시소러스를 활용하여 검색효율을 더욱 향상시키기 위하여는 한국어의 특성에 적합한 형태소분석에 대한 끊임없는 개선과 그래프 구조의 복잡한 시소러스 구조에 대한 깊이 있는 연구를 통해서 달성될 수 있다고 본다.In this thesis, a thesaurus management system and information retrieval system using thesaurus for effective retrieval are desinged and implemented.
Information retrieval system based on natural language has inferior recall ratio and precision ratio because of inconsistency between indexing terms and searching terms and various types of word forms. The problem can be solved by the query expansion using thesaurus.
In addition to that, retrieval efficiency can be enhanced by re-expanding the query that is expanded by thesaurus. For using morphological analyzer of Information Retrieval engine is more efficient than using manual method to solve leaving space of compound nouns in Korean database.
Therefore, it is more necessary to research morphological analyzer adapted to Korean database and to design the thesaurus database consisted of complicated graphic structure for more effective retrieval.차례 = i 그림차례 = iii 표 차례 = iv 국문요약 = v 1. 서론 = 1

대화형 AI 봇은 정보에 대한 새로운 Interface이며 사용자가 일방적으로 정보를 소비하는 앱과 다르게 주도적인 역할이 가능하다. 본 논문은 한글 자연어 처리가 가능한 혼합 주도형 대화형 AI 봇을 Bot Framework 기반으로 설계한다. 그리고 주제 무제한 패턴 처리로 주제 확장이 용이하도록 하며, 생성 모델 기반으로 입력발화와 출력 발화의 변환 규칙을 통해 다양한 발화 생성이 가능하도록 구상한다. 또한 학습 방식 혹은 설계 구조로 달라지는 성능 차이를 실험을 통해 확인하고 시사점을 도출한다.Conversational AI bot is a new interface to information, and unlike apps where users consume information unilaterally, they can take a leading role. In this paper, we design a mixed initiative conversational AI bot base on bot framework which can handle hangul natural language. In addition, it is easy to expand chat-oriented dialogue, and it is possible to generate various utterances through conversion rules of input utterance and output utterance based on generative models. and we can identify the performance differences that varies depending on the learning method or design structure through experiments and implications are derived.

일반적인 관계 추출 기술은 두 개체 사이의 상관관계 추출을 목표로 하며 다양한 도메인의 여러 자연어 처리 응용에서 중요한 역할을 하고 있다. 지식그래프를 활용하는 원거리 감독법이 나타나면서 단일 문장에서의 관계 추출 연구가 활발히 이루어지고 있으나, 실제 텍스트 문서의 유의미한 관계를 뽑아내기 위해서는 단일 문장이 아닌 다중 문장에서의 개체 간 구조적 관계를 필수적으로 파악해야만 한다. 따라서 기존의 단일 문장 관계 추출 기법들은 많은 양의 컨텍스트를 고려해야하는 다중 문장 관계 추출에 활용하기 적합하지 않다. 최근 다중 문장에 대한 몇몇의 관계 추출 기법이 새로이 제안되었으나, 이들 또한 문장에 내재된 심층적인 컨텍스트를 고려하지 못함으로써 부정확한 결과를 초래하게 된다. 이러한 문제를 해결하기 위해 본 논문에서는 트랜스포머 기반의 문장 인코더에 지식 어텐션을 결합한 모델(KDCN;Knowledgeaugmented
Deep Context-Aware Network)을 제안한다. 또한, 컨텍스트 관계 경로를 관계 추출에 이용하기 위해 LSTM 기반의 경로 표현 인코더를 설계하고 활용한다. 우리는 위키피디아와 지식그래프를 이용한 데이터셋을 통해 제안하는 방법론과 다중 문장 대상의 최신 관계 추출 기술을 비교 실험하였고, 이를 통해 관계 추출 성능의 우수성을 다방면으로 검증하였다.Extracting a semantic relation between mentioned entities participate sentences is a crucial task of many natural language processing applications. However, most literature on relation extraction has focused on the single sentence relation extraction, and the cross-sentence relation extraction has been largely ignored. Unlike the single sentence relation extraction, cross-sentence relation extraction helps understanding the full-content of document. In this paper, we introduce a cross-sentence relation extraction model, Knowledge-augmented Deep Context-Aware Network (KDCAN). By incorporating deep context and external knowledge, our model can capture explicit and implicit features of complex and sparse-context sentence. We employ the Transformer-based sentence encoding and use the relation path and type information of knowledge graph. We extend existing cross-sentence relation extraction benchmark datasets for evaluation. The proposed model achieved the state-of-the-art performance compared to other relation extraction tasks across multiple sentences.

텍스트 마이닝과 오피니언 마이닝은 모두 텍스트 데이터를 입력 데이터로 사용할 뿐 아니라 파싱, 필터링 등 자연어 처리 기술을 사용한다는 측면에서 많은 공통점을 갖고 있다. 특히 문서의 분류 및 예측에 있어서 목적 변수가 긍정 또는 부정의 감성을 나타내는 경우에는, 전통적 텍스트 마이닝, 또는 감성사전 기반의 오피니언 마이닝의 두 가지 방법론에 의해 오피니언 분류를 수행할 수 있다. 따라서 텍스트 마이닝과 오피니언 마이닝의 특징을 구분하는 가장 명확한 기준은 입력 데이터의 형태, 분석의 목적, 분석의 결과물이 아닌 감성사전의 사용 여부라고 할 수 있다. 따라서 본 연구에서는 오피니언 분류라는 동일한 목적에 대해 텍스트 마이닝과 오피니언 마이닝을 각각 사용하여 예측 모델을 수립하는 과정을 비교하고, 결과로 도출된 모델의 예측 정확도를 비교하였다. 오피니언 분류 실험을 위해 영화 리뷰 2,000건에 대한 실험을 수행하였으며, 실험 결과 오피니언 마이닝을 통해 수립된 모델이 텍스트 마이닝 모델에 비해 전체 구간의 예측 정확도 평균이 높게 나타나고, 예측의 확실성이 강한 문서일수록 예측 정확성이 높게 나타나는 일관적인 성향을 나타내는 등 더욱 바람직한 특성을 보였다.목 차 그림 차례 Ⅰ 표 차례 Ⅱ

감성 분석(Sentiment Analysis) 또는 오피니언 마이닝(Opinion Mining)은 텍스트로부터 사람들의 의견, 감성 및 성향을 분석하는 자연어 처리 기술의 일종이다. 텍스트는 주제에 대한 긍정이나 부정적인 감정을 내포하고 있으므로 텍스트 감성 분석을 수행한다면 주제에 대한 집단의 평가나 의견을 알아낼 수 있다. 일반적으로 감성 분석은 대중의 트렌드(Trend) 분석, 상품 가치 분석 등 다양한 방식으로 연구되고 있다. 하지만 대다수의 감성 분석은 영문 중심으로 연구되고 있으므로 한글을 위한 감성 분석 연구가 부족한 실정이며 기존의 감성 분석 연구들은 사람의 감정 자체에 대한 분석보다는 감성 분석을 통해 다른 주제를 분석하려는 경향을 보였다. 따라서 한국어를 위한 감성어휘사전(Sentiment Lexicon Dictionary)의 구축이나 한국어 감성 분석 알고리즘이 요구되며 사람의 감정 자체에 초점을 맞춘 감성 분석의 연구가 필요하다.
이 문제들을 해결하기 위하여 본 논문에서는 텍스트마이닝 기법을 이용한 한글 감성 분석 방법에 대해 연구한다. 먼저 데이터 수집을 위해 트위터 크롤링을 수행했으며 텍스트 전처리를 위해 한국어를 위한 Stemming Algorithm과 한국어 형태소 분석기를 적용하였다. 그 다음 전처리 된 데이터를 이용하여 한국어 감성 사전을 구축하였고 KNN, SVM 알고리즘을 사용하여 한국어 감성 분석 알고리즘을 구현하였다.
또한 상기 방법들을 적용하여 행복이라는 감정에 초점을 맞춘 지역별 행복도 분석 시스템을 개발하였다. 서버는 매 시간마다 트위터로부터 지역별로 수집된 텍스트를 분석하여 행복도(%)와 행복 키워드를 추출하며 분석 결과는 웹서비스를 통해 클라이언트로 전송된다. 클라이언트는 한국 지역들의 감정 상태를 모니터링 하기 위한 도구로 사용되며 실시간, 지역별, 기간별의 세 가지 방식 행복도 분석 결과를 사용자에게 서비스한다.Sentiment Analysis or Opinion Mining is a kind of natural language processing technology that analyzes people's opinions, emotions and tendencies from texts. Since texts contain positive or negative emotions on the subject, the text sentiment analysis can be used to identify the group's evaluation or opinion on the subject. In general, sentiment analysis is being studied in a variety of ways including public trend analysis and product value analysis. However, most of the sentiment analysis is focused on the English, so there is a lack of research on emotional analysis for Korean language, and existing sentiment analysis studies tend to analyze sentiments on other subjects rather than human emotions itself. Therefore, the construction of Sentiment Lexicon Dictionary and sentiment analysis algorithm for Korean language is required, and sentiment analysis focusing on human emotions is needed.
To solve these problems, this paper studied the sentiment analysis method for Korean language using text mining techniques. We first performed Twitter Crawling for data collection, and applied Stemming Algorithm and Korean morpheme analyzer for text preprocessing. Next, Korean Sentiment Lexicon Dictionary is constructed using preprocessed data and sentiment analysis algorithm for Korean language is implemented using KNN and SVM algorithm.
We also developed a regional-based happiness analysis system focused on the happiness by applying the above methods. The server extracts happiness percentage and happiness-related keywords by analyzing the text collected from each region by Twitter every hour, and the analysis result is transmitted to the client through the web service. The client is used as a tool to monitor the emotional state of Korean regions and provides the user with the results of three types of happiness analysis by real time, region, and period.∙국문초록 i ∙Abstract iii Ⅰ. 서론 1 Ⅱ. 관련 연구 3

인공신경망(artificial neural network)모델은 발전된 컴퓨터의 계산 능력을 통해 이미지 인식, 음성 언어, 자연어 처리 등 여러 분야의 문제를 해결하기 위한 방법으로 사용되고 있다. 인공신경망의 보이지 않는 은닉 층(hidden layer)의 존재는 신경망 특유의 계산법으로서 성공을 이루었지만, 이처럼 많은 은닉 층을 활용하는 복잡한 딥러닝(deep learning) 모델에선 오버피팅(overfitting) 문제가 발생하게 된다. 따라서 인공신경망 안에서의 오버피팅 문제를 완화하기 위한 다양한 시도가 있어왔는데, 최근에는 Hinton 등 (2012)에 의해 이전과 다른 새로운 정규화 형태인 드랍아웃(Dropout)이 소개되었다. 드랍아웃은 간단한 아이디어로 인공신경망의 성능향상을 불러오고, 정의된 훈련과정 만으로도 충분히 좋은 성능을 나타내지만, 알고리즘의 훈련 방법을 변형함으로써 모델의 변동성을 줄일 수 있는 여지를 가지고 있다.
본 연구는 드랍아웃의 훈련과정 간 문제점을 인식하고, 이를 개선한 드랍아웃 에버리징(Dropout averaging)방법을 제안한다. 드랍아웃 에버리징은 일반적인 드랍아웃에서의 미니배치(mini-batch) 학습 간 적용되는 드랍아웃 구조의 개수를 기존의 하나에서, k개로 늘리는 방법이다. 이를 통해 학습 간 일반적인 드랍아웃 보다 안정적으로 모델 업데이트가 가능하게 되고, 나아가 모델의 성능이 개선되어진다. 이번 연구를 통해 완전 연결 신경망 구조 속에서 드랍아웃 에버리징의 학습 방법을 제시하고, 세 가지의 모의실험을 통한 일반적인 드랍아웃과의 성능을 비교한다.제 1 장 서 론 1 제 2 장 인공신경망(Artificial neural network) 3 2.1 신경세포 뉴런(neuron)과 퍼셉트론(perceptron) 3 2.2 퍼셉트론의 확장을 통한 인공신경망 4

데이터 셋을 생성하는 집단의 노력 덕분에, 주어진 글을 읽고 정답을 답하는 TextQA 모델의 성능은 큰 성장을 보이고 있다. 그러나, 최근 이러한 성장을 보인 모델이 실제로 자연어를 이해하고 있는지에 대한 의문들이 제기되고 있다. 테이블을 활용하여 질문의 정답을 찾는 TableQA에서는 이러한 의문들에 대해 연구되지 않았다.
TableQA 모델을 훈련하고 평가하기 위한 기존의 데이터 셋은 두 가지 방법으로 구성되어 있으며, 이는 단순히 질문과 그에 대한 정답으로만 구성되어 있거나(weak supervision datasets), 정답과 함께 정답을 찾는 과정을 담고 있는 SQL 문도 함께 포함하고 있는 것이다(strong supervision datasets). 약한 지도 데이터 셋들(weak supervision datasets)은 옳은 정답을 유도하는 잘못된 풀이 과정인 "거짓 프로그램(Spurious programs)"을 이끌어낸다.
이러한 "거짓 프로그램"을 해결하기 위해, 우리는 올바른 정답을 찾기 위해 선택해야 하는 테이블의 셀(cell) 들인 '피연산자 정보(operand information)'를 제안한다. '피연산자 정보' 를 이용하여 모델에게 올바른 테이블의 셀들을 선택하게 훈련시킬 수 있으며, 이러한 훈련 방법을 '테이블 셀 주의 지도(cell attention supervision)'라고 한다. 또한, 우리는 올바른 테이블의 셀들을 선택하는 것을 배우고 다중 계층 반복 네트워크(multiple-layer recurrent network)인 '신경 연산자(Neural Operator)' 모델을 제안한다. 이 모델은 해석 가능성(interpretability)와 성능을 높이기 위하여 여러 개의 선택 반복 모듈(selective recurrent unit)들로 이루어져 있다.
우리는 또한 여러 도메인을 다루는 TableQA인 다중 도메인 테이블 질의 응답 MTQA(Multi-domain TableQA)를 연구한다. 다중 도메인 시나리오 들에서 새롭게 등장하는 두 가지 문제를 해결함으로써 MTQA를 달성할 수 있다. 첫째, 특정 도메인의 훈련 데이터가 존재하지 않을 수 있다. 둘째, 한 도메인에서의 훈련은 다른 도메인에 기여해야 한다. 이 두 문제를 해결하기 위해, 우리는 올바른 셀을 선택하도록 하는 훈련 방법과 테이블을 통해 얻어진 도메인 별 지식을 이용하여 질문을 잘 이해하도록 하는 훈련 방법을 함께 사용하는 이중 주의 지도(Dual Attention Supervision)를 제안한다. 또한, 이중 주의 지도를 이용하여 훈련하는 도메인 독립적 적응 학습자(Domain Independent Adaptive Learner)를 제시한다.
마지막으로, 다음과 같은 실험 결과를 보인다.
1. 신경 연산자(Neural Operator)는 MLB 데이터 셋에 대해 다른 이전 모델들 보다 성능이 월등히 높다.
2. 피연산자 정보(operand information)을 이용하여 훈련하는 것은 TableQA 모델의 성능과 해석 가능성을 크게 향상시킨다.
3. 신경 연산자(Neural Operator)는 다른 TableQA 모델들에 비해 적대적인 예시들(adversarial examples)에 대해 강하다.
4. 이중 주의 지도(Dual Attention Supervision)를 학습하는 도메인 독립적 적응 학습자(Domain Independent Adaptive Learner)는 WikiOPs 데이터 셋에 대해, 다른 모든 모델들을 능가한다.
5. 하나의 도메인에서 훈련된 도메인 독립적 적응 학습자(Domain Independent Adaptive Learner)는 추가적인 훈련 없이 새로운 다른 도메인에 잘 적응한다.The task of answering a question given a text passage has shown great developments on model performance thanks to community efforts in building useful datasets. Recently, there have been doubts whether such rapid progress has been based on truly understanding language. The same question has not been asked in the table question answering (TableQA) task, where we are tasked to answer a query given a table.
Existing datasets for supervising and evaluating TableQA range within two extremes: those annotating just answers (weak) to full SQL statement (strong) that can supervise both outcome and attention. Weak supervision datasets lead to creating “spurious” programs that accidentally lead to the correct answers even by using the wrong operations.
To get around this “spurious” programs, we propose correct cell supervision as a new unit of supervision and evaluation to provide additional information for answer finding, called ‘operand information’, and a multiple- layer recurrent network, called Neural Operator (NeOp) that learns the correct operands to achieve the answer to the query given a table. NeOp uses multiple Selective Recurrent Units (SelRUs) to further help the interpretability of the answers of the model.
We also study the problem of supporting Multi-domain TableQA (MTQA), improving upon available resources and models. MTQA accomplishes this by addressing the following two questions that newly emerge in multi-domain scenarios. First, training resources for the specific domain may not exist, while models trained by out-of-domain resources suffer in performance. Second, training in one domain should contribute to another. To answer these questions, we propose Dual Attention Supervision (DuAS), a training method using Attention Supervision based on domain-specific knowledge extracted from tables themselves. Also, we present Domain Independent Adaptive Learner (DIAL), a neural network that learns to understand question and to find answer using DuAS.
Experimental results show as follows:
1. NeOp outperforms all the previous models by a big margin on the MLB dataset.
2. The use of operand information to train the model significantly improves the performance and interpretability of TableQA models.
3. NeOp is robust to adversarial examples over other TableQA models.
4. DIAL that learns DuAS, outperforms all the previous models on WikiOPs dataset.
5. The use of DuAS to train the model in one domain enables to transfer other domains with no additional training.

인공 신경망은 머신 러닝 알고리즘의 하나로 주로 특징 추철, 특징으로부터의 패턴을 발견하고, 학습하며, 학습된 데이터를 기반으로 새로운 데이터를 인식하는 방법을 사용하여 기존 문제를 해결한다. 인공 신경망은 이미지 분류, 화질 개선, 자연어 처리, 음성 인식, 동작 예측 등의 신호 처리 분야에 널리 사용되는 기술이다. 인공신경망은 전통적인 알고리즘에 비해 높은 성능으로 많은 곳에 도입되고 있다. 그러나 인공 신경망은 훈련, 추론 과정이 필요하고, 훈련과 추론 모두 매우 많은 연산량을 필요로 하기 때문에 CPU만으로는 처리하기가 어렵다. 다행히도 인공 신경망은 높은 데이터 수준 병렬성을 갖고 있기 때문에 GPU를 위시하여 ASIC, FPGA를 활용한 다양한 가속기가 등장하고 있다. 이미 CNN의 합성곱 계층과 완전 연결 계층을 위한 가속기를 많이 제안된 상황이지만, RNN의 활성화 계층의 활성화 함수는 여전히 쌍곡 탄젠트 함수 (hyperbolic tangent)를 사용하여, 이를 위한 가속기는 존재하지 않는다.

일반적으로 RNN은 완전 연결 계층과 활성화 계층으로 이루어져 있기 때문에 완전 연결 계층은 병렬 처리를 통해 쉬운 가속이 가능하다. 그러나 활성화 계층의 쌍곡 탄젠트 함수는 병렬 처리가 어렵기 때문에 RNN의 병목이 된다.

본 논문에서는 인공신경망의 높은 병렬성을 활용하여 CNN의 합성곱 계층과 완전 연결 계층, 그리고 RNN의 완전 연결 계층의 가속을 위한 SIMD 구조와 RNN의 활성화 계층을 가속하기 위한 고정 소수점 초월함수 가속기 구조를 제안한다. 가속을 위해 인공신경망 알고리즘의 병렬화를 진행하였으며, 테일러 급수를 활용한 근사법을 활용한 고정 소수점 가속기를 구현하였다. 이를 통해 CNN, RNN 알고리즘의 성능 향상을 달성하였다.An artificial neural network is a machine-learning algorithm that solves problems by extracting features from input data, obtaining patterns from features, as well as learning and recognizing answers based on learned data. Artificial neural networks are widely used in areas of digital signal processing such as image classification, image quality improvement, natural language processing, speech recognition, and motion estimation. Artificial neural networks are introduced in many places and exhibit better performances than conventional algorithms. However, artificial neural networks require training and reasoning that need extremely high computational complexity, thereby rendering it difficult to be applied with a central processing unit alone. Fortunately, artificial neural networks exhibit high data-level parallelism; therefore, various accelerators using application specific integrated circuit (ASIC), field programmable gate array (FPGA), and graphic processing unit (GPU) are emerging. Many accelerators have been proposed for the convolutional layers and fully connected layer. However, the activation function of the recurrent neural network (RNN) activation layer still uses a hyperbolic tangent function.

In general, because the RNN consists of a fully connected layer and an activation layer, the fully connected layer can be accelerated easily through parallel processing. However, the hyperbolic tangent function of the activation layer is a bottleneck for the RNN because of the difficulty in parallel processing and its computing complexity.

We herein propose a single instruction multiple data (SIMD) structure for accelerating the convolutional layer of the CNN, a fully connected layer, a fully connected layer of the CNN, and a fixed-point transcendental function accelerator structure for accelerating the activation layer of the RNN by utilizing the high parallelism of the artificial neural networks. We parallelized the artificial neural network algorithm for acceleration and implemented a fixed-point accelerator using the Taylor series approximation method. Hence, performance improvement in the CNN and RNN algorithms is achieved.

의미 역 결정(Semantic Role Labeling)은 문장의 술어와 논항의 의미 관계를 결정하는 것으로, 정보추출(Information Extraction)과 기계번역(Machine Translation)과 같은 여러 자연어 처리 응용분야에서 의미 역 결정의 필요성은 널리 알려져 있다. 의미 역 결정은 격틀 사전 기반 방법과 말뭉치 기반 방법으로 나눌 수 있다. 하지만 한국어의 경우 한국어 격틀 사전 및 의미 역 부착 말뭉치와 같은 언어 자원이 부족하며, 영어와 비교했을 때 매우 다른 언어 구조를 가지고 있기 때문에 지금까지 개발된 방법들을 사용하여 좋은 성능을 얻기 힘들다. 이러한 문제를 해결하기 위해, 본 논문에서는 추가적으로 조사나 어미와 같은 접미사 정보를 이용하였다. 한국어는 일본어와 같은 교착언어 중 하나이며, 단어에 접사를 부착하여 문법적 기능을 나타낸다. 본 논문에서는 격틀 사전방법과 접미사 구조를 이용한 구조 기반 방법과 데이터 기반 방법을 통합하여 의미 역 결정을 하였고, 데이터 기반 방법으로는 CRF와 SVM을 사용하였다. 데이터 기반 방법은 불확실하며 부정확하기 때문에, 본 논문은 데이터 기반 방법이 적용되는 경우를 줄이고 구조 기반 방법의 적용 범위를 늘리고자 하였다. 실험에서는 15,224개의 논항을 이용하였으며, 데이터 기반 방법만 사용한 경우보다 4.85% 개선된 83.24%의 f1-score를 얻을 수 있었다. 이로써 본 논문은 접미사 정보를 이용한 방법이 한국어 의미 역 결정의 성능을 향상시킬 수 있음을 보여주었다.Semantic Role Labeling (SRL) is to determine the semantic relation of a predicate and its arguments in a sentence. SRL is wide known for its necessity in many natural language processing applications such as Information Extraction (IE) and Machine Transition (MT). SRL can be divided into the case frame-based method and the corpus-based method. But Korean has the lack of linguis-tic resources like Korean case frame dictionaries and semantically annotated corpora. Also Korean semantic role labeling has faced to difficulty due to its different language structure compared to English, which makes it very hard to use appropriate approaches developed so far. That means that the case frame-based method and machine learning based methods could not show a satisfied per-formance, compared to English and Chinese. To complement these problems, we focus on suffix information analysis, such as josa (case suffix) and eomi (verbal ending) analysis. Korean language is one of the agglutinative languages, such as Japanese, which have well defined suffix structure in their words. In this paper, we label the semantic roles by integrating data driven method and struc-ture driven method using a case-frame and suffix analysis. Data driven method was used as the CRF, and we also test SVM to compare its performance to CRF’s. Our method is intended to in-crease the scope of structure driven method at the same time reduce the cases to which data driven methods should apply, because data driven method is uncertain and inaccurate. In experiments, we used 15,224 arguments and we are able to obtain 83.24% f1-score, it is more improved results than using only the machine learning based method. We demonstrated that suffix structure can improve the performance of Korean semantic role labeling.TABLE OF CONTENTS iii LIST OF TABLES v LIST OF FIGURES vi 1. Introduction 1 1.1 Background 1

With the advent of the 4th Industrial Revolution, more attempts have been made to integrate artificial intelligence into existing services. Recently, services such as personalization and recommendation using personality analysis system have been commercialized. On the other hand, in the domestic recruitment field, due to the expansion of the "based on the contents of self-introduction letters", many companies and public institutions are increasingly selecting job seekers based on the contents of their self-introduction letters without looking at specifications.

The scope of using AI in the recent recruitment process is focused on minimizing the time to review the resume without analyzing the self-introduction letters. Others are specialized in AI interviews, where video and voice analysis machines progress interviews instead of the interviewers. This is because the existing personality analysis research and systems are learning based on the contents written in social media or community.

The system that learned the social media data which has the experience and feelings at the moment of writing can not properly analyze the inclination toward refined contents which people write like self-introduction letter with suppressed emotions. Also, the recruiter recalls a close relative or a similar co-worker when he reads their self-introduction letters. In other words, when you look at the words and sentences of self-introduction letters, you sort and evaluate the applicants unconsciously based on past experience. This shows the possibility of analyzing the words and sentences of self-introduction and identifying and classifying human personalities.

Therefore, in this paper, it is presupposed that I express my own tendency in self introduction letters for job search activity. For this purpose, we focus on how to search for a word and analyze the word. For this purpose, self-introduction letters provided by large companies or public companies are utilized. Using these documents, we study how to classify Personality Corpus, TF-IPF formula, morphological analysis and Named Entity Recognition, and appropriate propensity for self-introduction letters based on Tensorflow-based Deep Learning Regression model.4차 산업혁명의 등장으로 기존 서비스에 인공지능을 접목하는 시도가 늘어나고 있으며 최근에는 성향 분석 시스템을 이용한 개인화, 추천 등의 서비스가 상용화 되었다. 한편 국내 채용분야에서는 블라인드 채용의 확대로 인해, 많은 기업들과 공공기관들이 스펙을 보지 않고 자기소개서의 내용을 기반으로 구직자를 뽑는 비율이 높아지고 있으며 이를 A.I에 접목하는 시도도 늘어나고 있다.

다만 채용과정에 A.I를 사용하는 범위가 자기소개서가 아닌 구직자의 이력서 검토를 최소화하는 것과 온라인 영상 및 목소리 분석 등 기계가 면접을 대신 봐주는 A.I 면접관에게 특화 되어 있다. 이러한 원인은 기존의 성향 분석 연구 및 시스템들이 소셜 미디어나 커뮤니티 등에 쓴 내용을 기반으로 학습이 되어있기 때문이다.

글을 쓰던 순간의 경험 및 감정이 내재되어있는 소셜 미디어 데이터를 학습한 시스템은 자기소개서같이 감정을 억제하고 쓰는 정제된 내용에 대한 성향 분석이 제대로 이루어지지 않을 수 있다. 또한 구인자는 자기소개서를 읽을 때 가까운 지인이나 유사한 직장 동료를 떠올리게 된다. 즉, 자기소개서의 단어나 문장을 볼 때, 과거의 경험을 기반으로 무의식 중에 지원자를 분류하고, 평가하게 된다. 이는 자기소개서의 단어와 문장들을 분석하여 사람의 성향을 파악하고 분류하는 것의 가능성을 보여준다.

이에 본 논문에서는 자기소개서에서 구직 활동을 위해, 자신의 성향을 글로 표현한다는 것을 전제로 성향 별 단어를 찾고, 이를 분석하는 것을 핵심으로 다룬다. 이를 위해, 대기업이나 공공기업에서 제공하는 공개된 합격 자기소개서를 활용하여 성향 별 단어 역빈도(TF-IPF) 공식으로 만든 성향 별 말뭉치(Personality Corpus)와 자기소개서를 완벽하게 분석 하기 위한 형태소 분석 및 개체명 추출에 대한 방법 그리고 텐서플로우 (Tensorflow)로 생성된 딥러닝(Deep Learning) 회귀 모델 기반의 자기소개서별 적절한 성향을 분류하는 것을 연구한다.제 1 장 서론 1 1.1 연구 배경 및 목적 1 1.2 연구의 내용 3 제 2 장 관련 연구 및 연구 동향 4

목차 국문 요약 제1장 서론 = 1 제2장 대상 데이터베이스 언어 = 5 2.1 대상 데이터베이스 = 5

채팅 시스템(Chatting system)이란 사람과 기계 사이에서 의사소통을 수행하는 시스템을 의미한다. 채팅 시스템에서 사용하는 의사소통의 수단은 사람과 사람 사이에서만 사용하였던 자연어(Natural language)를 그대로 사용하는 것이 특징이다.
이러한 채팅 시스템을 잘 만들기 위해서는 많은 양의 채팅 말뭉치가 반드시 필요하고, 다양한 언어 분석기 및 채팅 시스템 방법론을 잘 정의하는 것이 중요하다. 본 논문에서 제안하는 것은 크게 채팅 말뭉치 확장 연구, 채팅 시스템의 성능 개선을 위한 언어 분석기와 적용법, 유사도 기반 채팅 시스템과 생성 기반 채팅 시스템을 결합한 하이브리드 채팅 시스템이다.
먼저 채팅 말뭉치 확장 연구는 대량의 발화 데이터에서 임의의 채팅 쌍을 생성하고, 기계(Machine)가 채팅 말뭉치로 사용 가능한 지 판단하여, 채팅 말뭉치를 반자동으로 확장하는 것을 의미한다. 채팅 말뭉치 확장 연구를 통해 다양한 채팅 말뭉치를 확보한 후 채팅 시스템에 유용한 영역 탐지(Domain detection)와 사용자 감정 분석(Sentiment analysis)을 수행한다. 탐지된 영역과 사용자의 감정을 통해 기존의 채팅 시스템 방법론에서 각각 처리 속도 및 성능을 향상한다. 그리고 채팅 시스템을 개발하는 두 가지 방법론인 유사도 기반 채팅 시스템과 생성 기반 채팅 시스템 각각의 장단점을 잘 결합하여 사용자 만족도 측면에서 향상된 하이브리드 채팅 시스템을 제안한다.
실험 결과 채팅 말뭉치 확장 방법에서 발화 단위 표상 생성 방법에 따라 성능을 비교하였다. 그 결과 일반적인 TF(Term Frequency)을 이용하는 것보다 CNN을 이용하여 발화 단위 표상을 생성하였을 때 정확률(Precision), 재현율(Recall), 정확률과 재현율의 조화평균인 F1에서 각각 5.16%p, 6.09%p, 5.73%p 각각 향상되었다. 그리고 유사도와 생성 기반 시스템 모두 채팅 말뭉치 확장 방법을 이용하여 채팅 말뭉치를 확장하였을 때 유사도 채팅 시스템은 Precision at k에서 1.55%p, 2.03%p, 1.82%p 각각 향상하였고, 생성 기반 채팅 시스템은 BLEU와 ROUGE-L에서 1.32%p, 0.67%p 각각 향상되었다. 유사도 기반 채팅 시스템에서 영역 탐지기를 추가로 이용하였을 때 속도 면에서 약 4.53배 향상되었고, 생성 기반 채팅 시스템에서 감정 분석기를 추가로 이용하였을 때 BLEU, ROUGE-L에서 0.46%p, 0.58%p 각각 향상되었다. 최종적으로 제안하는 하이브리드 채팅 시스템이 영역 탐지기가 포함된 유사도 기반 채팅 시스템과 감정 분석기가 포함된 생성 기반 채팅 시스템보다 사용자 만족도 측면에서 각각 평균 0.14점, 0.46점 향상되었다.Ⅰ. 서 론 1 Ⅱ. 관련 연구 5 1. 유사도 기반 채팅 시스템에 대한 연구 5 2. 생성 기반 채팅 시스템에 대한 연구 5

In this paper, we have developed an Information Retrieval system which accepts Korean Natural Query Language as an input to retrieve document in Korean Encyclopedia. we have engraded the retrieval system by using double searching methods. The first search is processed with keywords stored in inverted indexed file. The second search generates better results from the results of the first search with the words that excluded in keywords in query language. This system enables to enrich the efficiency of information retrieval system without additional syntatic and semantic analysis.목차 I. 서론 = 1 1.1. 연구 배경 = 1 1.2. 논문의 구성 = 1 II. 관련연구 = 3

현대 사회에서는 의사 결정을 위해 다양한 도구가 사용되고 있다. 대표적으로 사용되는 토의나 토론 진행 과정에서 생성되는 데이터를 분석하고 문서화하는 일련의 과정이 현재는 수작업으로 진행되고 있다. 때문에 대규모로 진행되는 토의나 토론에서는 데이터를 분석하는데 많은 비용과 시간이 소모되고 있다. 회의록 요약시스템은 기존 문서처리방법을 자동화하여 인건비절감과 처리시간을 단축하는 긍정적 효과를 기대하고 있다. 본 논문은 기존에 수작업으로 진행되었던 과정을 보다 효과적으로 운영할 수 있도록 회의록 요약시스템을 설계하고 구현한다. 제안하는 시스템은 회의과정에서 수집되는 데이터를 자동으로 분석하고, 대표 문장을 추출하여 제시한다. 회의진행자는 본 시스템을 통해 회의에서 다뤄지는 모든 안건을 확인하고 관리할 수 있으며, 시스템으로 부터 제시받은 대표문장을 기반으로 회의를 진행해 최종 결과를 도출한다. 대규모 토론이나 토의에서도 요약시스템 통해 대표의견을 제시받아 정확한 의사결정을 하여 시간절약과 비용절감 효과를 기대한다.In modern society, a variety of tools are used to make decisions. A series of processes is currently manual, analyzing and documenting the data generated during the typically used discussion or discussion. Therefore, large-scale discussions and discussions are costly and time consuming to analyze data. The minutes summary system expects the positive effects of automating existing document processing, reducing labor costs and processing time. This paper designs and implements a minutes summary system to better operate the existing manual processes. The proposed system automatically analyzes the data collected during the meeting and presents the representative sentences. The system allows the conference moderator to identify and manage all the agendas covered by the meeting, and to conduct the meeting based on the sentences presented by the system to derive the final results. Even in large-scale discussions or discussions, we expect time savings and cost savings by making accurate decisions based on the opinions of representatives through a summary system.Ⅰ. 서 론 1 Ⅱ. 관련연구 4 2.1 기존연구 조사 4 2.2 이론적 배경 5

음성 대화형 인터페이스(Voice User Interface; VUI) 디바이스 시장은 인공지능(AI) 스피커를 시작으로 웨어러블 기기, 자율주행 차량, IoT 등 타 산업과 융합하며 빠르게 성장하고 있다. 그러나 음성 인식과 자연어 처리와 같은 시스템 개발에 연구가 집중되고 있으며, 사용자 경험(UX) 측면에서의 연구는 부족한 실정이다.
이에 사용자 경험에 중요한 역할을 하는 서비스를 처음 경험하는 단계인, 초기 사용자 경험에 초점을 맞추어 본 연구를 진행하였다. 먼저, 음성 대화형 인터페이스, 인공지능(AI) 스피커와 관련된 선행 연구, 그리고 초기 사용 경험과 관련된 기존 연구를 조사하였다. 실험은 인공지능(AI) 스피커로 진행하였으며, 초기 사용 경험 분석을 통해 사용자 경험에 긍정적인 영향을 끼치고 서비스의 사용성과 만족도를 향상시킬 음성 대화형 인터페이스(VUI)를 디자인하고자 하였다.
실험의 일환으로 인공지능(AI) 스피커 초기 사용자를 대상으로 Task 분석, 관찰 그리고 심층 인터뷰를 수행하여 초기 사용 경험의 데이터를 도출하였으며, 이를 바탕으로 음성 대화형 인터페이스 설계와 전략을 제안하였다. 제안한 디자인에 대한 사용자 테스트를 진행한 결과, 기존 인터페이스와 본 연구에서 제안한 음성 대화형 인터페이스를 비교했을 때, 사용성, 신뢰성, 매력성, 검색성 측면에서 유의미한 차이가 있음을 확인하였다. 더 나아가 정성적 인터뷰를 통해 본 연구에서 제안한 디자인이 음성 대화형 인터페이스(VUI)의 이해와 제품/서비스의 사용성을 높여 사용자 경험에 긍정적인 영향을 끼치고 사용자의 실망을 줄여, 추후 디바이스 사용에 있어 지속사용의도와 만족도를 향상시킴을 확인할 수 있었다.제1장 서론 1 1. 연구 배경 및 목적 1 2. 연구 범위 및 방법 3 제2장 이론적 배경 5 1. 음성 대화형 인터페이스(VUI)의 이해 5

정보의 홍수 속에서 자신이 원하는 정보를 찾아서 과제를 완성하거나 문제를 해결하는 것은 중요한 개인의 능력으로 자리 잡았으나 과제 완성이나 문제 해결 과정에서 다른 사람의 과제를 복사하거나 표절하는 경우가 많아졌다. 이러한 현상은 평가에 대한 신뢰도를 떨어뜨리고 평가의 참의미를 퇴색시키고 있다. 이에 본 논문은 학습활동의 평가가 이루어지는 평가 영역의 중요성을 인식하고, 과정중심 평가가 이루어질 수 있도록 평가에 대한 신뢰도를 높이고자 유사 구문을 찾아내는 시스템을 설계하였다.
본 논문에서는 문서간 유사도 측정을 위해 문서를 구성하는 문장의 형태소를 분석하였다. 명사만을 내용어로 본 것이 아니라 동사와 형용사인 술어도 내용어로 추출하여 문서간 유사도 측정에 정교함을 높이고자 하였다. 문서에서 나타나는 단어의 빈도수를 가중치로 주어 문서간 유사도를 측정 하였다. 유사도 측정치에 대한 객관성 검증을 위해 50개의 문서 중 30개의 문서를 무작위로 선택하여 각각의 문서를 10대 1부터 10대 10까지 분할하여 유사도를 측정하였다. 유사도가 0.5이상이면 동일한 내용이 문서의 절반을 넘게 되어 유사 문서로 탐색 할 수 있는 근거를 마련하였다. 문서 길이가 길어질수록 오차 범위가 커지고 영어나 숫자 특수기호 등 형태소 분석시 사전에 없는 단어들이 나타날 경우 오차 범위가 커짐을 확인할 수 있었다.It's important for the learner to have an ability to complete a subject and to solve a problem by oneself when the learner searches for necessary information in the flood of information. But there are many cases that it copied the homework of another person. The reliability of evaluation goes down because of this actual condition. Thus this paper will introduce to design a copy detection system to get objective proof to measure correctly the similar degree of the paper.
A method of study is as follows. It divides every 30 documents into 10:1, 10:2, … 10:9. The copy detection system analyzes the sentence not the word. And this system uses morpheme analysis. Etymon is restored to the original form. Indexing adverb, verb and noun that are meaning word raised the elaborateness. It gives an weight according to the frequency of the word which appears in the paper. And this system measures similar degree of common keyword to calculate weight each of common keyword by morpheme analysis of paper.
The test result mentioned above is as follows that the longer length of document is, the larger an error scope is. And there is the same contents above 50% with the similar document. In addition, if there are many words in paper that doesn't exist in a dictionary as analyzing morpheme, an error scope gets larger and larger.목차 국문요약 제 1 장 서론 = 1 1.1 연구의 목적 = 1 1.2 연구의 내용 = 2

There are many ways for representation of knowledge. This paper proposes the method of logical representation of knowledge on the base of the most generalized facts and rules and describes the application of this method using well formulazied logic and logic program which are selected from various Artificial Intelligence fields.
Prolog which is considered as the general program language for processing natural language is used in this paper for representation of knowledge expressed in Korean language; Hangul. Prolog is recommended because current program languages such as FORTRAN, COBOL, PL/I, Pasa1, C-language, etc are not fitted for processing natural language, and moreover overflowing information compells to treat knowledges in the form resembling to natural language.
Simple database and an example program are used here to verify a query statement.목차 圖目次 ABSTRACT Ⅰ. 序論 = 1 Ⅱ. 지식 표현의 발전 배경 = 3

In these days, also called ‘the era of Big-Data’, many data have lost their meanings because so many data are made and used only once. These data should be analyzed. Especially a social network service, the Twitter, has a significant value in issue analysis. But traditional issue analysis methods have some problems that it shows duplicated issues and needs second searching. So, this research suggests a digital curation platform, the issue curator system.
The issue curator system extracts issue keywords from social network service data, group some keywords that imply a same issue among them and suggest a description about that. It also regenerates a curated data formed with XML document format and preserves it. According to the result of evaluation of the issue curator system, it is demonstrated that the system has a meaning in recognizing issues. In future work, the system based on various data source and machine learning methods should be considered.빅데이터 시대에 접어들며 많은 데이터가 생성되어 일회성으로 그 가치를 잃고 있다. 이러한 데이터들은 분석할 필요성이 있으며, 특히 소셜 네트워크 서비스인 트위터에서 생성되는 데이터는 이슈 분석을 위해 활용할 가치가 있다. 그러나 기존의 쿼리 기반의 이슈분석 방법은 중복되는 이슈를 보여주고, 2차 검색을 요구하는 문제점이 있다. 본 연구는 이러한 문제점을 해결하기 위하여 자료를 목적에 따라 의미 있게 구성하여 단순한 정보들을 하나의 지식으로서 재 생성하여 제공하는 디지털 큐레이션 플랫폼인 이슈 큐레이터 시스템을 제안한다.
이슈 큐레이터 시스템은 소셜 네트워크 서비스를 분석하여 이슈 키워드를 추출하고, 그 키워드들 중 같은 이슈라고 판단되는 키워드들끼리 그룹을 만들어주고, 이슈에 대한 설명을 제공하는 기능을 한다. 그리고 이러한 결과들을 각각 XML문서 하나의 큐레이션된 데이터로 재 생성하여 유지, 보관하는 기능도 한다.
시스템 평가를 통해 이슈 큐레이터 시스템이 기존의 이슈 분석의 도구이던 검색 포탈보다 사용자에게 더 의미가 있는 것을 확인하였다.
향후 연구에서는 다양한 종류의 데이터를 활용하는 것에 대한 고려가 요구되며, 빅데이터 환경의 특성을 활용할 수 있는 기계 학습에 기반한 방법을 적용하는 연구 방안에 대해 고려해야 한다.제 1 장 서론 1 제 2 장 관련 연구 및 연구 배경 7 2.1 디지털 큐레이션 7 2.2 빅데이터 10 2.3 자연어 처리 12

한국어 형태소 분석기(Korean Morphological Analysis)는 의미를 가지는 가장 작은 단위인 형태소(Morpheme)를 분석하고 알맞은 품사(Part-of-Speech)를 부착하는 단계로써 자연어 처리의 가장 기본적이고 필수적인 과정이다.
영어, 불어, 독일어 등 굴절어에서 사용되는 형태소 분석기는 띄어쓰기가 형태소이기 때문에 형태소 분리 과정이 필요 없지만 한국어는 띄어쓰기 단위가 하나 이상의 형태소 조합으로 이루어진 어절이기 때문에 형태소 분리 과정이 필요하다. 그리하여 기존의 형태소 분석은 형태소 분리 작업을 먼저 하기 위하여 형태소를 분석하여 형태소와 품사 쌍으로 후보를 두고 품사를 부착하는 두 단계로 분석을 하였다. 하지만 최근 음절 단위 입력의 형태소 분석기와 딥 러닝(Deep Learning)이 발전되면서 하나의 단계로 분석이 가능하다.
또한, 기존의 연구들은 정제된 말뭉치를 사용하여 학습 및 평가를 하였다. 하지만 최근 SNS, 웹 문서 등 발달되어 데이터가 증가하면서 빅데이터의 중요성이 대두되었다. SNS, 웹문서 등의 대량의 데이터, 말뭉치가 중요한 언어 자원으로 사용되고 있지만 정제되지 않은 데이터이기 때문에 기존에 시스템을 사용하면 올바른 분석이 될 가능성이 작다. 따라서 문법적 오류를 포함하는 연구에 대하여 성능 개선하는 연구가 중요하다.
본 논문은 한국어 형태소 분석기에 대하여 두 가지를 제안한다. 첫 번째, Bidirectional LSTM CRFs 모델을 기반으로 음절 단위 입력의 형태소 분석기를 구축한다. 또한 형태소 품사 분포를 사용하여 추가적으로 입력 확장하여 성능 개선을 한다. 그리고 기분석 사전(Pre-analyzed Dictionary)으로 명사 사전과 어절 사전을 사용하여 어절 단위 정확도를 향상시키고 형태소 불규칙 변환 사전을 사용하여 원형 복원을 한다.
두 번째, 말뭉치에 대하여 정제 과정을 거치지 않아 오타가 많은 데이터를 대상으로 알맞은 형태소 분석하는 방법을 제안한다. Bidirectional LSTM CRFs 모델의 입력으로 음절 정보뿐만 아니라 자모 정보를 추가하기 위해 음절 임베딩과 자모 임베딩을 concatenate하여 사용한다. 또한 자모 임베딩 구축 시 초성, 중성, 종성의 위치 정보를 넣어 구축하고 어절 단위 정보를 넣기 위하여 어절 단위 구분자를 추가하여 입력에 사용한다. 그리고 실생활에 자주 혼동되는 단어와 모바일 및 키보드의 입력 실수로 발생하는 오타를 분석하여 자주 발생하는 자모는 통합하였다. 그 후 통합한 자모 임베딩을 구축하여 실생활에 자주 틀리는 오타에 대해 높은 형태소 분석의 성능을 보인다.

본 논문의 학습 및 평가를 위하여 세종 말뭉치(Sejong Corpus)로 Bidirectional LSTM CRFs 모델을 기반으로 음절 단위 입력의 형태소 분석기와 오타에 효과적인 형태소 분석기를 개발하였다. 또한 오타에 효과적인 형태소 분석기는 세종 말뭉치와 음성 인식, 기계 번역 등에 유용하게 사용되는 의사 형태소 단위의 말뭉치를 사용하였다. 오타에 효과적인 형태소 분석기의 평가 데이터는 어절당 하나의 자모를 랜덤으로 발생시켜 평가하였으며, 자주 발생되는 오타에 대하여 평가하기 위하여 오타가 자주 발생되는 자모를 수정하여 평가하였다. 그 결과 자모와 음절을 concatenate하여 사용한 입력이 랜덤 오타를 내어 평가하였을 때 가장 높은 성능을 보였다. 또한 자주 틀리는 오타에 대해 분석하여 통합 자모 임베딩을 형태소 분석기에 사용하는 것이 자주 틀리는 오타에 대하여 향상된 성능을 보였다.Korean Morphological Analysis is the most fundamental and essential process in natural language processing as it is a step of analyzing morphemes, which is the smallest meaningful unit, and tagging appropriate Part-of-Speech tags. The morpheme analyzer used in English, French, German, etc., does not require morphological separation because the spacing itself is based on morphemes. Korean requires a morphological separation process because the spacing unit is composed of one or more morpheme combinations. In addition, existing studies have learned and evaluated using refined corpus. However, data from SNS, web documents, etc. have increased and the importance of big data has emerged. Thus, a large amount of data from SNS, web documents, and corpora are used as important language resources but are not refined. Therefore, it is important to improve the performance of morphemic analyzers on typographical errors in such data.
This paper proposes two approaches for Korean morphemic analyzer. First, this paper develops morpheme analyzer for syllable unit input based on Bidirectional LSTM CRFs model. We also improve the performance by inputting the morpheme part-of-speech distribution to the inputs of the bidirectional LSTM CRFs model. Using the noun dictionary and the eojeol dictionary as pre-analyzed dictionary, we improve the eojeol unit accuracy and use the irregular morpheme conversion dictionary.
Second, we propose a morphological analysis method for typing errors in the data. The input to model concatenates syllable and jamo embeddings to add syllable information and jamo (character) information. Also, when constructing jamo embeddings, character location information and eojeol unit separator is added to input eojeol unit information. And then, jamo is randomly evaluated to artificially generate an error. And the common jamo was integrated by analyzing the words often mistaken in real life, and typos caused by typing mistakes on mobile devices. By integrating jamo embedding, we show high morphological analysis performance for typos that occur frequently in real life.
The data in this paper use Sejong corpus for learning and evaluation. The proposed morphological analyzer, is effective for typos, uses a pseudo-morpheme corpus, and is useful for machine translation. The evaluation data of proposed morpheme analyzer is created by generating one jamo randomly per word. In order to evaluate the typos that frequently occur, the models correct the jamo where the typos frequently occur. As a result, the highest performance is obtained when random input is evaluated by concatenating jamo and syllable. Integrated jamo embedding also improves performance against typing errors.Ⅰ. 서론 1 Ⅱ. 관련 연구 5 1. 기존의 한국어 형태소 분석 및 품사 태깅에 대한 연구 5 2. 최근 한국어 형태소 분석기 및 정제되지 않은 데이터에 대한 연구 6

본 연구는 중앙은행 커뮤니케이션 측정을 위한 한국어 감성사전을 구축을 위한 두가지 자동 극성 분류 접근법을 제안하고 있다. 본 연구에서 제시하는 접근법은 대량의 문서에 기계학습과 워드 임베딩(Word Embedding)과 같은 최신 자연어 처리 기법을 적용하여 비지도학습 방식으로 연속된 형태소의 조합(n-gram)이 지닌 극성(polarity)을 분류한다. 그러한 과정에서 경제⋅금융 관련 어휘를 잘 인식하지 못했던 기존 형태소 사전의 단점을 보완한 경제⋅금융 형태소 사전(eKoNLPy)을 만들어 제공하고 있다.

본 연구에서 제시한 텍스트 마이닝 방법론을 활용하여 한국은행 금융통화위원회 의사록에 담긴 어조를 추출하여 수치화하고 기준금리 변동에 대한 설명력과 예측력을 검정하였다. 연구결과에 따르면, 금통위 의사록에서 추출한 어조 지수는 여타 변수에 비해 기준금리에 대한 설명력과 예측력이 매우 높은 것으로 나타났다. 기존 테일러준칙의 GDP갭률과 인플레이션율 등과 함께 본 연구에서 제시한 금통위 의사록 어조 지수를 설명변수로 추가할 경우 과거 및 향후 금리에 대해 상당부분을 설명하는 것으로 나타났다. 또한 텍스트 기반의 한국의 불확실성지수(EPU)나 데이터 기반의 거시 불확실성지수(UI) 등에 비해서도 기준금리에 대한 설명력과 예측력이 뛰어난 것으로 나타났다. 금통위 의사록을 영문으로 번역한 후 기존의 영문 감성사전으로 분석한 결과와 비교한 결과, 본 연구에서 제시한 어조 지수의 설명력과 예측력이 더 높은 것으로 나타났다. 이는 한글 문서의 어조를 직접적으로 측정하는 감성사전의 중요성을 시사한다.

본 연구는 또한 금통위 기준금리 결정회의 전⋅후 기사의 어조 변화를 통해 통화정책의 충격을 측정하고 이것이 금융시장에 미치는 영향을 분석하였다. 한국은행 기준금리 변화는 만기 1년 이내의 단기금리의 변화를 잘 설명하는 반면에 텍스트 기반의 통화정책 충격 지표는 장기금리의 변화를 잘 설명하는 것으로 나타났다. 이러한 결과는 텍스트 기반의 통화정책 충격 지표가 중앙은행의 향후 통화정책 방향에 대한 시장의 기대를 반영하고 있다는 사실을 암시한다.This study proposes two novel automated polarity classification approaches to build the field-specific Korean dictionaries to measure the subtlety of central bank communication better. These approaches use the state-of-the-art natural language processing (NLP) technologies such as machine learning and word embeddings along with a large corpus to classify hawkishness/dovishness of n-grams the in an unsupervised way. In doing so, this study presents a tailor-made NLP library called eKoNLPy (Korean NLP Python Library for Economic Analysis), which is specifically designed for this purpose. With this text mining tool, this study quantifies the Monetary Policy Committee (MPC) minutes of the Bank of Korea (BOK).

This study finds that the lexicon-based indicators help explain the current and future BOK monetary policy decisions when considering an augmented Taylor rule. These indicators remarkably outperform English-based textual classifications, a media-based measure of economic policy uncertainty, and data-based macroeconomic uncertainty and a data-based measure of macroeconomic uncertainty. The results of the study also emphasize the importance of using a field-specific dictionary and the original Korean text.

This study also quantifies the tones of news articles around Monetary Policy Committee meetings and measure monetary policy surprises using the changes of those tones following monetary policy announcements. This study estimates the impact of monetary policy surprises on various asset prices. This measure of monetary policy surprise better explains changes in long-term rates while changes in the Bank of Korea's base rate is more closely associated with changes in short-term rates, whose maturity is up to one year. This result strongly suggests that monetary policy surprises constructed from text mining approach deliver the market's expectation on future monetary policy stances and information related to forward guidance.

The resignation of a nurse has been a constant problem for the healthcare industry in Korea. Of the licensed nursing staff, there are very few staff members who actually work as nurses. Therefore, it is very important to identify the current situation or cause related to the resignation of nurses and to prepare countermeasures. However, there are many difficulties in conducting surveys for nurses considering resignation. Results can be biased according to the intention of the researcher, and in some cases the subjects may not express their views frankly.
The purpose of this study is to investigate the background and cause of the resignation of nurses who are considering resignation through text mining technique. In this study, data visualization, word association analysis, and social network analysis were used for research purposes.간호사의 사직은 한국 보건의료계의 고질적인 문제로 대두되어왔다. 우리나라의 경우 면허 간호 인력 중 실제 간호사로 활동하는 인력이 매우 부족한 실정이다. 따라서 간호사 사직과 관련된 현황이나 원인을 파악하고, 그에 대한 대책을 마련하는 것이 매우 중요하다. 하지만 사직을 고려하는 간호사를 대상으로 설문조사를 하는 데에는 많은 어려움이 따른다. 연구자의 의도에 따라 결과가 편향될 수도 있고, 특수한 경우에 따라 연구 대상자들이 본인들의 의견을 솔직하게 나타내지 않을 수도 있기 때문이다. 본 연구의 목적은 텍스트 마이닝 기법을 활용하여 사직을 고려하고 있는 간호사들의 사직의 원인과 배경을 조사하는 것이다. 본 연구에서는 연구의 목적을 위해 데이터 시각화, 단어 연관성 분석 그리고 사회연결망 분석을 시행하였다.목 차 Ⅰ. 서 론 1 Ⅱ. 배경 이론 3 1. 텍스트 마이닝 3

순환신경망은 이미지 분류나 음성 인식, 단백질 이차 구조 예측 등의 순차적 데이터 분류 문제(Sequence Labeling)와 언어모델, 이미지 캡셔닝 등 자연어 처리 문제를 포괄한 많은 분야에서 높은 성능을 보여왔다. 그러나 해당 예들은 모두 분류(classification) 모형이다. 순환신경망은 이러한 분류 문제에서 다양한 성과를 이루어낸 반면에, 회귀분석 분야에서는 괄목할만한 성과나 그 적용 예제들이 많지 않다.
본 논문에서는 기존의 많은 연구들과 달리 순환신경망을 회귀분석 모형으로 사용하는 데에 활용하였다. 앞서 서술한 바와 같이 순환신경망은 순차적인 정보를 처리하기 때문에 시간 순차적 데이터의 미래값 예측 문제에 적용될 수 있다. 그리고 이는 순환신경망을 회귀분석으로 활용할 수 있는 대표적인 사례로 볼 수 있다. 그러나 순환신경망을 이용하여 회귀분석 모형을 만들고자 할 경우, 기존의 연구가 많지 않기 때문에 입출력 데이터의 형태나 그래프 구조, 훈련방법이나 손실함수에 대한 선택을 위한 참고자료가 다양하지 못하다.
이러한 문제로 인해 순환신경망이 회귀분석에 방대하게 적용되지 못하는 문제점을 해소하고자, 본 논문에서는 자기상관성을 갖는 시간 순차적 데이터의 특성과 목적에 따라 LSTM 의 다양한 훈련 방법을 제시하였고 회귀분석 문제에 적용하였다.
본 논문에서는 회귀분석을 위한 효율적인 LSTM 훈련방법을 제시하였으며 이는 세부적으로 네 가지로 나눌 수 있다. 그리고 이러한 훈련 방법들을 이용하여 세 가지 데이터에 대해 실험하였다. 그 중 가장 간단한 모형인 단일스텝, 단일층 모형의 경우, 단순한 데이터에 대한 훈련과 예측에서 좋은 결과를 보였으며, 다른 모형들보다 비용적인 측면에서도 좋은 결과를 보였다. 가장 복잡도가 좋은 다중스텝, 다층 모형에서는 복잡한 데이터에 대해 다른 모형들보다 예측 결과가 좋은 것으로 확인되었다.
또한 본 논문에서는 현실세계 데이터인 환율데이터에 대해 본 논문에서 제시한 순환신경망의 훈련방법을 적용하였으며, 이를 이용하여 실험한 결과 다층 전방전달 신경망보다 우수한 결과를 보이는 것을 확인하였다.1. 서론 1 1.1. 연구의 배경 1 1.2. 연구의 내용 4 1.3. 논문의 구성 4

시맨틱 웹의 성공 여부는 온톨로지 구축과 생성을 위해 지식을 체계화하는 시맨틱 어노테이션의 역할이 매우 중요하다. 이를 위해 각 분야의 지식을 효율적으로 온톨로지 인스턴스로 생성하고 체계화해야한다.
본 논문의 규칙기반 범용 온톨로지 인스턴스 생성 시스템은 여러 사이트의 웹문서에서 온톨로지의 적합한 정보를 정확하고 빠르게 추출하여 온톨로지 인스턴스를 생성하는 것을 목적으로 한다.
온톨로지 인스턴스 생성의 단계는 다음과 같다. 먼저 정보를 추출할 각 사이트에 대한 사이트 규칙 데이터와 규칙들을 관리하는 규칙 정보 데이터를 적용하여 웹문서를 파싱한다. 그리고 키워드 데이터에서 관리하는 추출 정보에 관련된 키워드로 파싱한 문서를 검색하여 해당하는 정보를 추출한다. 이 추출한 정보를 기반으로 온톨로지 인스턴스를 생성한다.
이 시스템에서는 사용자가 관련 정보를 찾아서 온톨로지와 대조하여 정보를 입력하는 수동적인 과정을 키워드 데이터를 이용하여 자동화하였다. 또한 추출할 정보들에 관한 키워드와 규칙들을 별도로 관리하기 때문에 소수의 키워드와 규칙들을 추가함으로써 다양한 웹문서의 정보 추출이 가능하다. 이는 곧 온톨로지 구축과 생성의 시간을 단축시키는 역할을 하게 된다.The success of semantic Web largely depends on the role of semantic annotation, which systematizes knowledge for the construction and production of ontology. For this, knowledge in each area should be generated and systematized into ontology instances efficiently.
The rule based general purpose ontology instance generation system developed in this study aims at generating ontology instances by accurate and fast extraction of appropriate ontology information from Web documents at different sites.
Ontology instances are generated through the following steps. First, parse Web documents by applying rule information data that manages site rule data and rules for each site from which information will be extracted. Second, search the parsed documents using keywords related to extracted information managed in keyword data, and extract relevant information. Third, generate ontology instances based on the extracted information.
This system automated the manual process of finding relevant information, comparing it with ontology, and entering information using keyword data. In addition, it can extract information from various Web documents by adding a small number of keywords and rules because keywords and rules related to information to be extracted are managed separately. This saves time in constructing and generating ontology.Ⅰ. 서 론 1 Ⅱ. 관련연구 3 1. 시맨틱 웹 3 가. 시맨틱 웹 3 나. OWL 온톨로지 4

위키피디아 비교 말뭉치를 이용한
한국어-영어 병렬 문장 추출

Extracting Korean-English Parallel Sentences
Using Comparable Corpora from Wikipedia

컴퓨터공학과 김 성 현
지 도 교 수 고 영 중

이중 언어 혹은 다중 언어를 다루는 자연어 처리 분야에서는 질이 좋고 양이 풍부한 병렬 문장(parallel sentence)들이 필요하다. 그러나 병렬 문장을 생성하는 작업은 시간과 비용이 많이 소요되는 작업이다. 이로 인해 비교 말뭉치(comparable corpus)에서 병렬 문장만을 자동으로 식별 및 추출하기 위한 연구가 전 세계적으로 관심을 받고 있으며, 특히 통계적인 기법을 이용한 병렬 문장 자동 추출에 대해 관심이 집중되고 있다.

본 논문에서는 현재 가장 큰 온라인 백과사전인 위키피디아(Wikipedia) 비교 말뭉치로부터 한국어․영어 병렬 문장을 자동으로 추출하기 위해 다양한 실험을 수행한다. 실험 방식은 크게 세 가지로 나눌 수 있는데, 첫 번째는 사전을 이용하는 방법, 두 번째는 번역 확률을 이용하는 방법, 그리고 세 번째는 토픽 모델(topic model)을 이용하는 방법이다. 사전을 이용하는 방법은 위키피디아의 제목으로 구성 되어있는 위키사전(Wiki dictionary)과 다음(Daum)에서 제공하는 영한 및 한영 MRD(machine readable dictionary), 그리고 여러 형식의(날짜, 서수 등 포함)들로 구성된 숫자사전을 이용한다. 번역 확률은 MRD와 세종 병렬 말뭉치, 그리고 다음 영한사전 예문에서 추출하였다. 또한 비지도 학습(unsupervised learning) 모델인 토픽 모델을 이용한 방법에 대해서도 실험을 수행한다. 이 방법들을 각자 단독으로, 혹은 서로 결합하는 방식으로 최적의 성능을 내기 위해 다양한 실험을 시도한다.

본 연구는 국내 최초의 병렬 문장 자동 추출 실험으로서 다양한 접근 방법 및 실험 결과를 제시함으로써, 실제 병렬 문장 추출에 바로 응용할 수 있을 뿐 아니라 향후 유사 연구들에 있어서도 큰 기여를 할 것으로 기대된다.


주요어 : 위키피디아, 병렬 문장, 비교 말뭉치, 사전, 번역 확률, 토픽 모델I. 서 론 1 Ⅱ. 관련 연구 6 1. 유사 문서 추출 연구 6 2. 유사 문장 추출 연구 6

본 논문에서는 고밀도 비디오 캡션 생성 문제의 해결을 위하여 새로운 심층 신경망 모델인 DVC-Net을 제안한다. 고밀도 비디오 캡션 생성은 하나의 비분할된 입력 비디오로부터 다수의 후보 이벤트 구간들을 탐지해내고, 각각의 이벤트 구간에 대한 자연어 설명 문장을 생성하는 작업이다. 기존 연구들에서 제안된 많은 모델에서는 합성 곱 신경망을 통해 추출하는 입력 비디오의 시각 특징들만을 주로 이용하였다. 그러나 본 논문에서 제안하는 DVC-Net 모델에서는 이러한 시각 특징들뿐만 아니라, 이벤트를 구성하는 중요한 요소들인 사람, 행위, 물체, 장소 등을 효과적으로 표현할 수 있는 고-수준의 의미 특징들을 추가로 이용한다. 또한, DVC-Net 모델에서는 비디오 안에 포함된 이벤트 시간 영역들을 탐지하기 위해 순환 신경망의 하나인 BLSTM을 이용한다. 또, DVC-Net 모델에서는 캡션 생성 단계에서 주의집중 메커니즘과 맥락 게이트를 적용한다. 대용량 벤치마크 데이터 집합들인 ActivityNet Captions와 MSR-VTT를 이용한 다양한 실험으로 제안한 DVC-Net 모델의 우수한 성능을 확인할 수 있었다.This paper proposes a DVC-Net model, a new deep neural network model for generating dense video captioning. Dense video captioning refers to a series of works that detect multiple events in an input video and generate natural language sentences to describe each event. In previous research, a convolutional neural network was prevalently used to extract visual features in a video. However, in this paper, the DVC-Net model uses high level semantics that can efficiently represent important elements comprising events such as a person, an action, an object and a place as well as such visual features. Additionally, DVC-Net model uses bidirectional long short-term memory network (BLSTM), a type of recurrent neural network to detect events in time. Furthermore, the DVC-Net model applies an attention mechanism and context gating to effectively make use of context information in a caption generation step. ActivityNet Captions and MSR-VTT, large-scale benchmark datasets, were used to demonstrate high performance of the DVC-Net model.제 1 장 서 론 1 제 1 절 연구배경 1 제 2 절 연구목표 4 제 2 장 이론적 배경 7

본 연구의 목적은 병원정보 이용 증대에 따른 의료기관 내 임상진료부문 정보이용자의 요구도를 파악하여 이에 적합한 시스템 구축과 정보의 효율적인 활용을 위한 기초 자료를 마련하는 것이다.
이를 위하여 부산 시 소재 일 종합전문요양기관에 근무하고 있는 의사를 대상으로 2005년 4월1일부터 5월30일까지 설문조사를 실시하였고, 조사 결과에 기초하여 임상진료 부문에 데이터웨어하우스 파일럿 시스템(Data Warehouse Pilot System)을 구현하였다. 설문 조사내용은 데이터웨어하우스의 중요활용부문, 의사결정에 필요한 정보의 유용기간, 진료와 검사관련 정보의 중요도, 수입 및 환자통계부문의 중요도와 병원경영 정보의 중요도에 대한 내용으로 설문지 530부를 배부하여 56.6%의 회수율을 보였다.
설문조사 결과, 병원데이터웨어하우스의 중요활용부문으로는 환자진료부문, 연구지원부문, 일반통계지원 부문의 순으로 나타났다. 의사결정에 필요한 정보 유용기간은 평균 3~5년이었다. 병원정보 이용의 중요도 부문에서는 진료와 검사에 관련 항목의 중요도 평균은 4.23이었고 병원경영 부문의 중요도 평균은 3.74 그리고 수입 및 환자 통계 부문의 중요도 평균은 3.72였다. 이들 조사 내용의 세부항목에서 의사의 성별, 연령별, 직종, 진료과목, 근무연수 등에 따라 다르게 나타났다.
조사 결과에 기초하여 진료 및 연구지원 부분의 데이터웨어하우스 파일럿 시스템을 구축하였으며 이 시스템은 현재까지 쌓여있는 텍스트 데이터를 기반으로 구축하였다
본 연구의 결과를 바탕으로 임상진료 및 연구지원에 필요한 시스템이 구현된다면 의료의 질 향상과 진료 및 연구에 많은 기대를 할 수 있으리라 기대된다.The purpose of this thesis is to investigate the hospital information users' degree of request in order to build up the basic data and to construct the appropriate information system, as the informational use of the clinical part in the hospitals increase.
For this research, I conducted the survey in which doctors who work in the tertiary hospitals in Busan participated during the period between April 1, 2005 to May 30, 2005. Based on the result of survey, the Data Warehouse Pilot System was implemented in the field of Clinical Part. The questionnaires of the survey included the important practice part of Data Warehouse, the length of necessary information for decision-making, the degree of importance of treatment and relevant examination information, the degree of importance of income and patient statistics, the importance of the information of the management of the hospital, A total of 530 copies of questionnaires were distributed and 56.6% of them were returned.
The important practice part of the hospital data warehouse was the patient treatment part, the research support part, and the general statistics part by the order. An average of three to five years of period were taken for the necessary decision-making. In the field of degree of use of hospital information, treatment and examination was 4.23, while hospital management was 3.74, and income and patient statistics was 3.72. Doctor's sex, age, job classification, field, length of work were the deciding factors in the details of the investigation.
Based on the outcome of the study, the data warehouse pilot system on the treatment and research support part was constructed, and this system was constructed based on the text date which has been accumulated so far. It is expected that quality medical treatment and better research will be available if the system this study embodied will be implemented in the future.Ⅰ. 서론 = 1 1. 연구의 필요성 = 1 2. 연구 목적 = 3 Ⅱ. 이론적 배경 = 4 1. 데이터웨어하우스 = 4

1. 서 론 1 1.1 연구의 필요성과 목적 1 1.2. 연구의 방법과 범위 3 2. 이론적 배경 5 2.1 이미지 커뮤니케이션 5

본 논문에서는 말뭉치에 기반한 어의 애매성을 해결하는 새로운 지도학습 방법을 제안하였다. 단어는 여러 가지 의미를 가질 수 있고, 이런 단어의 사용은 문장 내에서 애매성을 가지게 된다. 문장 내에서 사용된 애매성을 가진 단어의 의미를 결정하는 것은 정보 검색 시스템이나 기계 번역 같은 자연어 처리 응용 분야에서 중요한 일이다. 제안된 방법은 해당 단어와 문장 내에서 구문 관계를 가지는 모든 단서들에 대한 학습을 기반으로 시험 문장 내에 나타나는 애매한 의미를 가지는 단어의 올바른 의미를 구분한다. 어의 애매성 문제 해결을 위해 9개의 구문 관계를 정의하고, 이 구문 관계에 기반한 단서를 이용한 어의 애매성 해결 방법을 제시하였다. 실험 결과 본 논문에서 제안한 방법이 기준 정확도보다 정확도의 개선을 보였다.This paper presents a method for word sense disambiguation based on corpus. Words have several meanings or senses, a use of this word out of context is ambiguous in a context. The word sense disambiguation, determining which of the sense of ambiguous word is invoked in a context, is important in natural language applications such as information retrieval, machine translation.
In this paper, we present a new approach for word sense disambiguation, specially based on corpus. This approach integrates a set of clues based on syntactic relations for word sense disambiguation. After training on clues that co-occur with a specific sense of a given verb, this approach extracts a set of clues based on syntactic relations in a set of test sentences and integrates a set of clues to disambiguate the sense of given verb.
The experimental result shows that accuracy of disambiguation method on highly ambiguous verbs with fine-grained sense distinction is always better than the default strategy of assigning all occurences to the most frequent sense.요약 = ⅰ 목차 = ⅱ 표목차 = ⅳ 그림목차 = ⅴ Ⅰ. 서론 = 1

目次 一. 緖言 = 1 二. 多意語에서 본 言語經濟 = 5 1. 多意語의 成立 = 5 2. 그림으로 본 多意語 = 11

대화형 인공지능(Conversational Artificial Intelligence)이란, 음성(Voice) 혹은 문자(Text)를 통해 인간과 대화하는 목적의 프로그램이다. 이러한 대화형 인공지능들은 상대방의 입력에 대해 다음과 같은 방식의 처리과정을 통해 답변을 생성한다. 먼저, 상대방의 입력을 자연어 처리를 통해 문장을 형태소의 단위로 구분한다. 그 후, 구분된 형태소들을 통해 문장의 의미를 파악한다. 이어서, 해당 입력 문장에 대한 답변을 정한다. 마지막으로, 정해진 답변을 문장으로 만들어서 사용자에게 제공한다. 현행 대화형 인공지능은 입력의 특정 단어나 어구를 검출하여 미리 준비된 응답을 출력하는데 초점이 맞춰져 있다.
특히, 현재 가장 많이 응용되는 대화형 인공지능인 개인 비서 인공지능(Secretary Artificial Intelligence)은 최근 들어 스마트폰에 기본 사양으로 탑재되는 경우가 많다. 현재의 개인 비서 인공지능은 제조사에서 정한 캐릭터를 기반으로 하며, 다음과 같은 문제점을 갖고 있다. 첫째, 연속대화를 처리할 수 없다. 하나의 문장을 기본단위로 학습하고 처리하는 방식을 취하기 때문이다. 둘째, 대화 상황에 대한 이해보다는 정의된 기능 수행을 중심으로 진행된다. 따라서 사전에 정의되지 않은 질문에는 답변이 불가능하다. 셋째, 인공지능 스스로의 감정을 생성하고 표현할 수 없다. 하지만 사람과의 깊은 소통을 위해서는 대화의 전후 상황을 이해하고, 이를 토대로 자신의 감정 상태를 변화시킬 줄 알며, 주관적인 감정과 의견을 드러낼 줄 알아야 한다.
사람이 실세계로부터 정보를 받아들이고 그 정보를 바탕으로 행동하는데 있어서 인식과 행위를 연결해주는 가장 큰 역할이 목표를 설정하는 단계이다. Reactive 방식에 머물고 있는 현재의 대화형 인공지능의 한계를 극복하고 성능을 개선하기 위해서는 인식한 내용에 따라서 스스로 목표를 세우고 결정을 하는 Deliberative 방식의 대화형 인공지능이 필요하다. 이러한 Deliberative 방식의 대화형 인공지능은 Norman의 행동 모델과 유사한 계층 구조를 갖는 다중 에이전트 시스템을 통해 연구하고자 한다.
본 논문은 현재의 단순 반응형 수준의 Reactive 방식의 대화형 인공지능의 한계를 극복하기 위해 상황인지는 물론 주관적인 개성을 지닌 의도형 수준의 인공지능을 위한 논문이다. 이를 위해 대표적인 의도형 수준의 지능인 인간의 지능을 모방한다. 인간은 의사결정을 위해 이성뿐만 아니라 감성도 함께 이용하여 의사결정을 한다. 따라서 본 논문은 귀납추리(Inductive Reasoning)방법을 이용한 대화형 인공지능을 위한 감정 모델을 제안하고, 이를 이용해 감정 모델링을 이용한 대화형 인공지능 시스템(CAIEM, Conversational Artificial Intelligence using Emotion Modeling)을 제안한다.CAIEM은 미리 설정된 자신의 감정 모델과 훈련을 통해 획득한 상대의 감정 모델을 통해 현재 감정 상태로부터 목표 감정 상태로 이르기까지의 감정 경로를 계획하고 계획된 경로에 따라 응답을 제공하는 시스템이다. 마지막으로 사례 연구를 통해 본 시스템의 타당성을 검증하며 기존의 대화형 인공지능보다 감정의 연속성에 있어 더 나은 수준의 대화를 할 수 있음을 보인다.Conversational Artificial Intelligence is a program that aims to communicate with humans through Voice or Text. These interactive artificial intelligences generate an answer to the opponent's input through the following process. First, the input of the opponent is classified into morpheme units by natural language processing. Then, we grasp the meaning of the sentence through the separated morphemes. Next, an answer to the input sentence is determined. Finally, we make sentences of the answers and provide them to users. The current interactive artificial intelligence is focused on detecting a specific word or phrase of an input and outputting a prepared response.
In particular, Secretary Artificial Intelligence, which is the most widely applied interactive artificial intelligence, is often installed as a basic specification in smartphones in recent years. Current personal assistant AI is based on the character set by the manufacturer and has the following problems. First, you can not handle continuous conversations. This is because a single sentence is learned and processed in a basic unit. Second, it focuses on performing defined functions rather than understanding conversational situations. Therefore, unspecified questions can not be answered. Third, artificial intelligence can not create and express its own emotions. However, in order to deeply communicate with people, it is necessary to understand situation before and after conversation, to change their emotional state based on this, and to reveal subjective feelings and opinions.
The most important role that a person connects information and awareness in accepting information from the real world and acting on that information is setting a goal. In order to overcome the limitations of the current interactive artificial intelligence that is in the Reactive system and to improve the performance, we need a deliberative interactive artificial intelligence that sets goals and makes decisions according to the perceived contents. This deliberative interactive artificial intelligence is studied through a multi-agent system with hierarchical structure similar to Norman's behavioral model.
The purpose of this thesis is to study artificial intelligence with intentional level of subjective personality as well as situational awareness in order to overcome the limit of interactive intelligent artificial intelligence of simple reactive type. To do this, we imitate human intelligence, which is a typical intentional level of intelligence. Human beings make decisions by using not only reason but also emotion for decision making. Therefore, this thesis proposes an emotional model for interactive artificial intelligence using inductive reasoning, and proposes a Conversational Artificial Intelligence using Emotion Modeling(CAIEM). The CAIEM is a system for planning the emotional path from the current emotional state to the target emotional state through the previously set emotional model of the emotional model and the partner's emotional model acquired through training and providing a response according to the planned path. Finally, the case study confirms the validity of the system and shows that it can provide a better level of conversation in the continuity of emotions than the conventional interactive artificial intelligence.요 약 ⅰ 목 차 ⅳ 그림목록 ⅵ 표 목 록 ⅷ 제 1 장 서론 1

본 논문에서는 청각장애인(농아인)과 비장애인의 수화 학습의 기회를 넓히고자 스마트폰용 3D 수화교육 앱을 개발하였다. 특히 한국어 문장을 자동으로 수화구조로 변환하는 수화단어 변환 알고리즘을 제안하고 음성인식을 통한 단어검색과 문장을 수화로 표현해주는 기술을 구현하였다. 또한, 자연스러운 수화 동작을 얻기 위해 모션 캡처 시스템과 정확한 손동작 표현을 위해 데이터 글러브를 이용하여 실제 사람과 같은 3D 수화 애니메이션 DB를 구축하였다. 마지막으로 수화 애니메이션의 움직임을 실시간 3D 렌더링하여 화면에 보여주기 위해 UNITY 3D 엔진 프로그래밍이 이용되었다. 제안한 수화교육 앱 콘텐츠는 iPhone App. Store와 Android App. Store에 약 1,300단어의 3D 수화 DB를 가지는 앱으로 배포 중이다.In this thesis, we develope the smart phone App. contents of 3D sign language to widen the opportunity of the korean sign language education for the hearing-impaired and normal people. Especially, we propose the sign language conversion algorithm that automatically transform the structure of Korean phases to the structure of the sign language. Also, we implement the 3D sign language animation DB using motion capture system and data glove for acquiring the natural motions. Finally, UNITY 3D engine is used for the realtime 3D rendering of sign language motion. We are distributing the proposed App. with 3D sign language DB of 1,300 words to the iPhone App. store and Android App. store.第 Ⅰ 章 서 론 1 第 Ⅱ 章 관련 연구 3 2.1. 스마트기기 기반의 수화 교육 콘텐츠 3

개인용 컴퓨터와 인터넷의 대중화와 더불어 웹을 기반으로 한 많은 학습 시스템이 개발되어 왔다. 이러한 웹 기반 코스웨어는 협력 학습을 위한 많은 도구들을 지원하고 있는데, 이 중 채팅은 교실에서의 교사와 학생의 직접적인 상호작용의 연장이라는 측면에서 볼 때 매우 유용한 도구이다. 하지만 대화를 나누고자 하는 교사와 학생 모두가 같은 시간에 웹에 접속해야 한다는 시간적인 한계가 있으며, 채팅을 통한 대화 내용은 보존되어지지 않아, 대화 중에 생겨난 많은 학습 지식들이 대화가 끝나면 소멸되고 만다.
본 논문에서는 이렇게 대화 중 생겨난 많은 학습 지식을 보존하고 다시 활용할 수 있도록 즉, 대화에 참여했던 학생도 채팅이 끝난 이후 대화 내용을 다시 살펴볼 수 있도록 하고, 대화에 참여하지 않았던 학생도 다른 학생들의 대화 내용을 살펴보면서 궁금했던 학습 내용을 이해할 수 있도록 대화 내용을 서버에 자동으로 저장하는 시스템을 개발하였다. 본 연구에서 제안하는 시스템은 채팅으로 생겨난 학습 지식이 보존되고 활용될 수 있고, 키워드 검색과 디렉터리 검색 방법을 제공해서 채팅의 대화 내용을 쉽게 확인할 수 있다는 점에서 효과적이라 할 수 있다.
본 연구에서 제안한 시스템이 활용 가치와 학습 효과를 높이기 위해서 보완하고 추가로 연구되어야 할 사항들은 다음과 같다.
첫째, 본 연구에서는 설계 및 구현의 용이성을 위해서 키워드 추출 과정을 간단한 방법을 기반으로 개발하였다. 즉, 용어 사전을 미리 두고 용어 사전에 있는 키워드만 추출하였다. 따라서 용어 사전에서 제외되었으나 중요한 키워드는 추출될 수 없었다. 이를 보완하기 위해 대화 내용에서 빈도수가 높으면서 키워드로서의 가치 있는 용어를 추출하는 과정이 필요하다.
둘째, 본 연구에서는 텍스트만 가능한 채팅 도구를 제공했다. 따라서 채팅이 가지고 있는 한계인 타이핑이 느리거나 언어적인 이해력이 부족한 학생은 본 시스템을 활용하기에 많은 어려움이 있다. 또한 학습 내용을 이해할 때 텍스트만의 설명보다 그림 자료나 동영상 자료, 소리 자료가 합해지면 더욱 쉽게 이해가 가능하기 때문에 멀티미디어 자료들을 함께 사용할 수 있는 시스템으로 보완될 필요가 있다.
셋째, 본 시스템을 현장에서 교사와 학생들이 직접 활용하고, 시스템의 활용 효과에 대한 실제적인 연구가 부족하므로 본 시스템을 현장에서 적용하는 연구 과정이 필요하겠다.목차 = I 1. 서론 = 1 1.1 연구 배경 및 목적 = 1 1.2 연구 내용 = 2 2. 관련 연구 = 4

소프트웨어 시스템의 요구사항은 개발 초기에 수집되며 프로젝트가 끝날 때까지 계속 관리 된다. 이 중 품질 요구사항은 그들 간의 상호 이타적인 특성으로 인해 요구사항 간의 충돌을 피할 수 없다. 품질 요구사항이란 소프트웨어 시스템의 품질 속성을 기술한 요구사항으로 소프트웨어 개발 초기에 자연어인 시나리오 형태로 수집된다. 품질 요구사항 간의 충돌은 개발 초기에 발견되지 않으면 개발 후기에 커다란 비용을 지불 하며 수정해야 한다. 요구사항의 충돌을 초기에 발견하기 위한 연구들은 아키텍쳐 수준에서 ATAM, SAAM등이 알려져 있고, IEEE 1061에서는 Software quality metrics를 정의해 소프트웨어 각 개발 단계에서 품질 요구사항을 평가 하도록 지원하고 있다.
그러나, 이러한 기법들을 적용하기 위해선 품질 요구사항 수집 시에 품질 속성 별로 분류를 하는 작업이 선행 되어야 한다. 현대의 프로젝트는 요구사항의 수가 많음에도 분류 작업은 수작업으로 이루어지고 있으며, 이에 대한 연구나 도구적 지원이 미미한 실정이다.
이에 따라, 본 논문에서는 분류언어와 유사도 측정을 이용한 요구사항 자동 분류 기법을 제시한다. 도메인 별로 각 분류 속성을 대표하는 분류언어를 정의하고, 이를 기초로 유사도 측정과 2단계분류를 통해 요구사항을 분류해 낸다. 이로서 요구공학 엔지니어는 요구사항 분류에 대한 수작업의 양을 줄일 수 있다. 또한 본 기법의 활용을 위해 요구공학 공정에서 자동분류를 어떻게 사용하는지를 제시하였으며, 제시된 기법을 구현한 분류엔진을 개발하였다. 제안된 기법을 검증하기 위해, 15여 개의 프로젝트로부터 수집된 요구사항을 이용하여 실험을 수행하고 그 결과를 분석, 평가 하였다.Requirements of software system are collected in early development phase, and it is managed until project is over. Quality requirements must be conflict with each other because of their exclusive attribute. Quality requirements describe quality attribute of the system, and they are written in natural language. If conflict of quality requirements is not found in early phase, it makes big error in later phase and the project need a lot of cost. ATAM, SAAM is well known methodology about early requirement conflict detection at architecture level. And IEEE 1061 defines software quality metrics that evaluate quality attribute in all software life cycle.
However, before we apply the methodology, we must classify quality requirements into quality attribute. Modern software project have many requirements, but requirements classification is performed by manual work. And Research about this is rare. And there is no tool supporting quality requirements classification.
Therefore, this paper proposes quality requirements classification technique using classification language and similarity measurement. We can define domain-specific classification language and classify quality requirements using proposed technique. So, Requirements engineer can reduce manual work. And we suggest how proposed technique can be applied in requirements process. And we implement classification engine using proposed technique. To validate our technique, we performed experiment using requirements collected in 15 projects. And we analysis and evaluate the result.목차 = ⅰ 제1장 서론 = 1 제2장 연구의 배경 = 3 2.1 요구사항 관리 및 품질 속성 = 3 2.1.1 요구사항의 정의 = 3

정보의 폭발로 표현되고 있는 문헌생산량의 증가로 인하여 문헌정보의 축적과 검색은 정보처리 및 관리 환경에서 매우 중요한 기능이 되었다. 비록 정보공학의 발전이 신속히 이루어지고 있지만 기존의 자동색인 기법은 원시문헌의 내용을 정확하게 색인하기에는 충분하지 못한 것이 사실이며, 자연어 색인의 한계인 개념의 분산과 개념간의 다양한 관계를 제시하지 못함으로 인해 문헌의 주제분석이 어려운 실정이다.
본 논문은 자동색인기법에서 취약점으로 지적되어온 어휘개념의 분산문제와 개념간의 다양한 관계를 색인상에서 표현할 수 있도록 어휘동시출현의 퍼지 그래프기반 분석을 통한 자동색인기법을 제안하였다.
이 기법은 한 문장내에 나타난 임의의 두 어휘가 그 문장을 표현하기 위한 의미 있는 집합이며 문헌내의 각 문장에서 추출된 어휘쌍 집합에서 두개 이상의 문장에 동시출현한 어휘쌍은 문헌이 표현하고자 하는 내용에 밀접한 관계가 있다는 가설하에 이들의 관계를 정보 결집력과 퍼지그래프 이론을 응용하여 동시출현 어휘쌍 그래프를에 나타난 어휘관계에서 각 어휘의 전체 어휘쌍 그래프에 대한 결집력을 분석하여 그 결집력을 색인어 선정의 우선순위로 하였다.
그리고 결집력이 높은 핵심어휘군으로 부터 주변어휘들간에 퍼지적관계를 설정하여, 가장 관계가 높은 집단을 퍼지그래프 이론에 따라 그 문헌 전체의 뜻을 전달할 수 있는 확장된 색인어 집단을 추출함으로써 경제적인 색인어 선정을 할 수 있는 가능성을 제시하였다.Recently as information has been increasing rapidly in various forms of text, saving and searching of information has played an important role in handling and managing of information. Although there has been a good deal of advancement in the area of information engineering, however, the existing Automatic Indexing Technique(AIT) is not good enough to accurately index the information of a text. The AIT fails to identify the variance of concepts and their relationships in a natural language, and therefore it is very difficult to analyze the contents of texts.
In consideration of this, and as a way to identify term variance and the relationships between concepts, the present dissertation is concerned with suggesting a new AIT which is based on fuzzy graph analysis of simultaneously appearing pair terms.
The new technique provided in this dissertation is dependent on this assumption: a given pair of terms in a sentence is a meaningful collection for the sentence: and the pairs which appear simultaneously in two or more sentences are closely related to the contents of the text from which the sentences are derived. The suggested technique intends to identify the relationship between the pairs of terms and the contents of a text, while best making use of synthetic system of information and the theory of fuzzy graph. Thus it intends to analyze the synthetic system of information (the relationships between concepts in a text) and use it for indexing.
In addition, the technique establishes a fuzzy relationship between the collection of nuclear terms and the collection of related terms, and derives an extended set of terms for indexing which are likely to represent the contents of a text according to the fuzzy graph theory. It seems that the proposed technique points to the possibility of an economic indexing procedure.그림차례 = iii 표차례 = iv 국문초록 = v 1. 서론 = 1 1.1 연구의 목적 및 필요성 = 1

Studies on the sentence patterns in the Korean language have a relatively short history and are not at as an advanced level in comparison with America and Japan. Also the emphasis of the studies are in the Korean language itself. Therefore, to be useful studies should to be done in the area of computer science for the purpose of machine translation.
In my study I examined and analyzed the existing sentence patterns in the Korean language and made a corpus.
A corpus is a collection of varied sentence patterns that contain knowledge and information on the Korean language. One characteristic feature of the Korean language is that there are indispensable and optional elements in a sentence to clarify the meaning of the pridicates, which are verbs, adjectives and copula.
Based on this feature I selected sentence patterns through two steps. Then, I expanded the sentence patterns according to variants of the endings that append to the predicates.
In the first step I sampled 32 sentence patterns, 19 according to verb patterns, 8according to adjective patterns and 5 according to the copula patterns. Then, in the second step, to get more complete forms of patterns for machine translation, I applied nominal sementic marker to those 32 patterns.
The result was 153 verb patterns, 41 adjective patterns and 19 copula patterns.
The study is useful for even learning the Korean language in addition to machine translation since these selected sentence patterns show well the basic structures of Korean sentences.
First, it helps solve ambiguous sentences in analysis.
Secondly, it makes it possible to translate homonyms.
Thirdly, it helps judge whether different elements of a sentence are correctly. connected in terms of meaning.
In order to make any proper machine dictionary or to judge its quality, it is absolutely necessary to depend on language-statistical information such as a 'Word and Sentence Patterns Frequency Dictionary'.
Therefore the constribution of this study, I consider, is to make a preliminary stage for building a 'sentence pattern frequency dictionary'.
Desired studies, which we need in the future, will be studies on sentence patterns of the Korean language with a bigger corpus and to buield a 'sentence patterns frequency dictionary'.목차 = 0 Ⅰ. 서론 = 1 A. 연구 배경 = 1 B. 연구 내용 = 2 Ⅱ. 한국어의 문장패턴 = 4

This study proposes to design an educational model that utilizes a programming essay to facilitate learning of the general-purpose programming language by non-programming majors in programming education. We developed a program based on the ADDIE model for instructional design equipped with a systematic procedure. This study has two main purposes: (1) to develop a teaching method based on the instructional model that improves programming language learning, and (2) a grading system that evaluates the efficacy of learning. Based on these purposes, our specific research objectives are as follows: First, what is the effect of the“ instructional method for programming essay” based on the ADDIE model for instructional design as observed from the grades of course attendees? Second, what is the effect of the“ instructional method for programming essay” based on the ADDIE model for instructional design according to the level of satisfaction seen in course attendees? Third, how satisfied are the course attendees with the programming essay grading system? We based our research on the formative research method to solve these problems. According to the ADDIE research procedure of analysis, design, development, implementation, and evaluation, we developed a programming essay course system based on a review of previous studies and related literature, analysis of existing data, and interviews with experts. The process of developing the programming essay course system is described in further detail as follows: First, in the analysis stage, we conducted interviews to identify the needs of experts and learners based on existing studies and related literature, and the analysis of existing data. The needs were identified according to the analysis based on the interview transcription and literature review of previous studies and data. The module was designed considering the process of program operation based on the data received through the interviews in the analysis stage. Essential factors were incorporated into the module design. Second, the draft design was developed based on the review of literature and existing data in reference to the essential factors for the 3 designed modules. In Module 1, the guidelines for the programming course using programming essay were suggested; In Module 2, a course system which helped to understand general programming using the essay was developed; In Module 3, a system for programming essay task (problem) development and grading learners’ answers was developed. The study results offer the following implications: First, ADDIE-based programming essay modeling improves the performance of non-major students in a programming course compared to the general programming teaching method. Thus, this approach is more effective for a programming course for non-majors. Second, this ADDIE-based programming essay modeling is a teaching-learning method that enhances learners’ satisfaction with the programming course for non-majors as compared to the general programming teaching method. We found that the current programming education for non-majors did not capture learners’ interest or help them to improve their problem-solving skills when learning programming. This finding supports the results of the interviews and existing studies mentioned in the research background. Third, the grading system effectively increases the academic achievement in and course satisfaction with the ADDIE-based programming essay course. We observed that this approach addressed the difficulties of the complicated evaluation and grading methods seen in the current programming education by replacing it with the new grading system. Further, it allowed an effective performance of programming tasks using the programming essay. This not only enhances learners’ academic achievement, but also increases their course satisfaction. Therefore, it is applicable to various courses of programming education. This study is valuable in that it helps overcome the limitations of the current programming education, and suggests a new model of natural learning toward programming education at the university, which is stressed by many experts.Ⅰ. 서 론 1 1. 연구의 필요성 및 목적 1 2. 연구 문제 9 Ⅱ. 이론적 배경 10 1. 프로그래밍 학습에 대한 요구 10

음성인식 기술은 사람과 기계간의 인터페이스를 편리하고 자연스럽게 만들 수 있기 때문에 음성인식 기술의 연구 개발이 국내외에서 꾸준히 진행 되어 왔으며, 그 결과 단순한 단어 인식을 넘어 자연스럽게 발성한 음성도 인식할 수 있는 수준으로 발전해 왔다. 특히 근래 애플이 발표한 시리는 아이폰에 탑재되는 자연어 음성인식 처리 소프트웨어로 많은 이들의 관심을 지속적으로 받고 있다. 이러한 연속 음성 인식 소프트웨어들은 대부분 확률 통계 이론에 기반을 둔 HMM 기법을 활용해 개발된 고가의 수입 소프트웨어 들이며 그 구현 비용이 너무 비싸 음성인식의 높은 효용성에도 불구하고 많이 보급되지 못 하고 있는 실정이다.
실제 국내에서 음성 인식을 도입한 통신사 및 몇몇 카드사의 사례들도 미국의 Nuance 엔진을 이용 구현 되었다. 일부 국산 음성 인식 소프트웨어를 활용해 구현한 사례들도 있으나 그 대부분은 단어 위주의 인식 서비스로 구성 되어 있으며, 이 또한 잡음처리 및 음성 에너지 조절 기능들이 부족해 만족할 만한 인식률을 보이지 못하고 있으며 또한 요구된 발성 규칙을 따르지 못한 음성 질의들은 아예 처리하지 못하고 있는 실정이다. 본 논문에서는 이러한 현실적 어려움을 개선할 수 있는 방안으로 음성인식 엔진에 독립적이며, 또한 단순히 잡음 제거 기능만 수행하는 전처리 엔진이 아니라 규칙에 따르지 않은 음성 질의도 처리 할 수 있는 핵심어 추출 기능, 더 나아가 그 핵심어를 수식하는 전 술부 및 그 해당 음성 질의로부터 수행하기를 원하는 후 술부 까지도 추출할 수 있는 다목적 전처리 프로세서 설계 방법을 제안 하고자 한다. 더불어 스마트 폰 등에서 사용 가능한 음성 dictation 기능으로부터 발성자의 음성 질의 의도도 파악할 수 있는 후 처리 설계 방법도 같이 제안 하고자 한다. 이러한 후 처리기는 음소 별로 독립적으로 부여된 고유 값을 이용 인식 단어들 전체의 동질성 값을 계산하는 기능을 갖추어 dictation 결과가 잘못되었을 시 이를 보정할 수 있는 기능을 갖추고 있다. 이렇게 설계된 전 처리기를 기존 음성 인식 솔루션에 병행 사용하여 그 인식 기능을 제고할 수 있음을 G사의 직원 자동 연결 시스템과의 연동에서 확인 할 수 있었으며, 국내 통신 회사의 실제 고객들 음성 질의도 처리 할 수 있음을 확인 하였다. 후 처리기 또한 국내 카드사의 스마트 폰에 음성 처리 능력을 지원하는 데 사용되어 그 상업적 유용성을 입증하였다. 본 논문을 통해 제안된 처리기는 소프트웨어적으로 구현되었고 음성 인식 소프트웨어에 독립적으로 설계되었기에 인식 단어 또는 문장의 증가 시 그 처리 능력이 떨어 질 수 있으나 더욱 빨라진 하드웨어 처리 능력으로 이를 극복할 수 있을 것으로 생각되며 앞으로 이와 같은 처리기 관련 연구를 촉발하는 데 기여할 수 있기를 기대한다.제Ⅰ장 서 론 1 제1절 연구의 배경 및 목적 1 제2절 연구의 방법 및 논문의 구성 3 제Ⅱ장 이론적 배경 4

현재 대부분의 정보검색시스템에서 사용되고 있는 키워드 매칭기법은 기하급수적으로 늘어나는 정보를 효과적으로 처리하기에는 부적합하다. 이러한 문제는 의미정보를 활용하여 해결할 수 있으며, 본 논문에서는 의미정보를 효과적으로 활용할 수 있는 한 방법을 제시한다. 본 논문에서 제안한 기법은 의미정보를 개념그래프로 이용하여 표현하고, 이를 정보검색에 활용한다.
구현된 시스템은 완전매칭과 부분매칭이 가능한 시스템이다. 완전매칭은 기존의 키워드매칭과 비슷한 개념이며, 부분매칭은 구문적 부분매칭과 의미적 부분매칭 두 가지가 있다. 의미적 유사도는 온톨로지내의 서브클래스 관계로 계산된다. 이러한 기법은 정보검색 뿐만 아니라 동적 하이퍼링크의 구현등 다양한 분야에서 적용될 수 있다.Keyword matching technique which is used in most information retrieval systems is unfit for efficient processing of geometrically increasing information. The problem can be solved by using semantic information and an efficient method of semantic processing is introduced in this paper. The technique uses conceptual graph to represent the semantic information and apply it for information retrieval. The implemented system can perform exact matching and partial matching. Partial matching has two different types. One is syntactic partial matching and the other is semantic partial matching. The semantic similarities are measured by the subclass relations in the ontology. The proposed technique can be used not only for information retrieval but also in various applications including an implementation of dynamic hyperlinks.목차 Abstract = ⅲ Ⅰ. 서론 = 1 Ⅱ. 관련연구 = 2 1. 정보검색시스템 = 2

본 연구에서는 상태 다이어그램을 활용한 게임 프로그래밍 학습이 초등학생의 컴퓨팅 사고력에 미치는 영향에 대해 알아보고자 하였다. 이를 위하여 초등학교 5학년 8학급 학생 171명을 4학급씩 통제집단과 실험집단으로 구분하고, 사전검사를 통하여 동질성을 검증하였다. 이후 통제집단에서는 자연어 알고리즘을 활용한 게임 프로그래밍 학습을, 실험집단에는 상태 다이어그램을 활용한 게임 프로그래밍 학습을 각각 10차시 실시하였다. 수업 처치 후 사후검사로 컴퓨팅 사고력 검사를 실시하였으며, 두 집단 간 검사 결과를 비교하였다. 그 결과 실험집단이 통제집단에 비해 컴퓨팅 사고력이 향상되는 효과가 나타났다. 따라서 상태 다이어그램을 활용한 게임 프로그래밍 학습이 초등학생의 컴퓨팅 사고력에 긍정적인 영향을 미친다는 것이 확인되었다.I 서론 1 1. 연구의 필요성 및 목적 1 2. 연구의 내용 및 방법 4 3. 연구의 제한점 5 4. 용어의 정의 6

인공지능에 대한 연구는 주로 인간이 정의한 지능에 초점을 맞추어 진행되었다. 그것은 지능활동이 갖는 특징들-기억, 추론, 예측 등-을 정의했고 여러 가지 접근법으로 연구되었다. 대부분의 경우 기계가 특정 행동을 하도록 규칙을 정하고 강제하는 경우가 많았는데 이런 방법은 학습 능력이 없는 기계가 예외 상황이 발생 했을 때 적절히 대응하기 힘들었다. 자연어 처리를 예로 들어보면, 기계에게 인간의 언어를 이해시키려는 노력은 주로 인간의 언어를 분석하고 이들이 갖는 규칙을 정의하여 기계에 적용하려 했다. 그러나 언어는 일정한 규칙을 가지면서도 수많은 예외 사항이 존재한다. 특히, 문어가 아닌 구어에서는 규칙을 따르지 않는 경우가 많다. 뿐만 아니라 모든 예외를 처리할 수 있다고 해도 정해진 규칙에 따라 동작하는 것일 뿐, 언어를 이해했다고 보기 힘들다. 때문에 기계에게 인간의 언어를 이해시키려는 노력은 비록 많은 발전이 있긴 했으나 목표를 온전히 이루지는 못했다.
이에 대해 저자는 근본적 원인이 그 접근법이 생물학 적이지 않기 때문이라고 가정하고 뇌가 정보를 어떻게 처리하는지 알아보고 그것을 모방해 기계가 인간의 언어를 이해하고 사고할 수 있게 하는 방법에 대해 논한다. 따라서 인간의 의사소통을 관찰하고 그것이 어떤 메커니즘으로 일어나는지 아는 것은 의미 있는 일이다.
본 논문에서는 기존의 인공 신경망을 구성하는 뉴런 하나에 하나의 의미를 부여하고 기존의 인공 신경망이 공간적인 결합만이 가능했던 것에 비해 시간적인 결합도 가능하게 하는 새로운 형태의 인공 신경망을 제안했다. 그리고 그 인공 신경망을 검증하기 위해 간단한 덧셈을 수행하는 실험을 했다. 거기에 덧붙여 제안한 인공 신경망과 유사한 연결 형태를 가지는 Hopefield 신경망을 비교하여 공간적 학습과 시간적 학습을 실험했다.제 1 장 서론 …………………………………………………… 1 제 2 장 관련연구 및 배경 …………………………………… 3 제 3 장 이론 및 가설 ………………………………………· 8

본 논문에서는 위치 기반 광고의 한계점을 극복하기 위해 실시간으로 사용자의 관심사를 고려하는 맞춤형 광고 시스템을 개발한다. 이를 위해 사용자의 상태와 감정이 실시간으로 잘 드러나는 사용자의 모바일 채팅 메시지를 분석하여 사용자의 관심사를 알아낸다. 관심사를 알아내기 위해서는 모바일 채팅 메시지와 같이 ‘자연어’로 이루어진 상품 설명 정보로 학습시킨 ‘상품 정보 자동 분류기’를 개발하여 활용한다. 학습을 위해 doc2vec-based document embedding 기법을 활용한 새로운 feature 추출 방법을 고안하고, 이를 바탕으로 feature를 추출하여 학습시킨다. 관심사를 분류한 후에는 광고 타이밍을 판단하기 위해 분류기의 ‘분류 확률’을 대상으로 threshold를 설정하여 광고 타이밍을 조정한다.
성능평가를 위해서는 관심사 추출에 사용되는 상품 정보 자동 분류기와 맞춤형 광고 시스템의 성능을 각각 평가한다. 상품 정보 자동 분류기의 성능을 평가하기 위해 feature를 여러 방식으로 조정하여 정확도를 비교한다. 실험 결과 최대 90%의 정확도를 보였고, bag-of-words modeling과 word-level embedding을 이용한 기존 방법과 비교했을 때 상당한 정확도 향상을 보였다. 광고 시스템의 성능을 평가하기 위해서는 window 크기, 품사 조합의 종류, 임계치 값을 조정하여 각 분류 level 별 parameter의 최적값을 찾는다. 결과적으로 parameter 조정에 따라 최대 62%의 평균 R-score를 보였다.In this paper, we develop a customized advertisement system that considers the user's interest in real-time to overcome limitations of location-based advertising(LBA). To do this, we analyze the user's mobile instant message in which the user's status and feelings are revealed in real time. We use the 'automatic product description classifier' which is learned using product descriptions composed of 'natural language' like mobile instant message to find out the user's interests. We design a new feature extraction method using the doc2vec-based document embedding technique for learning, and extract feature based on this. After classifying the interest, the advertisement timing is adjusted by setting a threshold for the 'classification probability' of the classifier.
For the performance evaluation, we evaluate the performance of the automatic product description classifier and the customized advertisement system, respectively. In order to evaluate the performance of the automatic product description classifier, the features are adjusted in various ways to compare the accuracy. Experimental results showed a maximum accuracy of 90% and showed a significant improvement in accuracy compared with the existing methods using bag-of-word modeling and word-level embedding. In order to evaluate the performance of the customized advertisement system, the optimal values of parameters for each classification level are searched by adjusting the window size, combination types of part-of-speechs, and threshold value. As a result, we were able to achieve the maximum average R-score of 62%.1. 서론 1 2. 관련 연구 5 2.1. 모바일 채팅 메시지 분석 5 2.2. 관심사 추출 5 2.3. 맞춤형 광고 시스템 8

본 연구는 ‘국립중앙도서관’을 대상으로 웹페이지 검색어 로그 분석을 통해 이용자의 검색 행태를 분석하였다.
국립중앙도서관에서 2015년 일 년 간 검색된 상위권 검색어를 시계열, 내용별, 형태별로 분석하여 이용자의 검색 행태를 분석하였다. 또한 인기 검색어와 실제 이용되는 인기 자료의 목록과 베스트셀러 목록을 비교하여 검색과 이용의 상관성을 연구하였다.
그 결과 검색어 입력 빈도가 낮은 연초에 비해 연말의 검색어 입력 비율이 4~7배 높았다. 또한 상위 10위까지의 검색어는 월별이나 연간 전체 검색 빈도의 약 17%를 차지하였다. 검색어 내용으로 보면 고전서명, 한국작가, 대학교명의 검색 빈도가 높았다. 검색어 형태면에서 언어로는 한국어, 한자, 영어 순으로 입력 비율이 높았으며, 숫자는 한자리와 두 자리 수, 네 자리수가 가장 많이 검색되었다. 오타 검색어로는 주로 한글과 알파벳 자음의 비율이 높았다.
검색어와 이용도서의 상관관계를 분석한 결과는 ‘역사 · 여행’, ‘철학 · 종교’ 분야 자료의 검색어 일치가 높게 나타났다. 주제별자료실에서는 ‘사회과학실’ 자료와 검색어의 일치가 가장 높게 나타났다. 또한 국내자료가 국외자료(번역서)보다 약 15% 검색어 일치율이 더 높았다. 끝으로 2015년 베스트셀러 목록과 국립중앙도서관 검색어의 일치율은 약 37%로 나타났다.
이 결과는 국립중앙도서관 이용자들의 실제 정보 검색 행태가 어떻게 이루어지는지, 또한 검색과 자료 이용의 현황을 보여주었다. 본 연구는 현재 뿐만 아니라 향후 이용자의 검색 경향과 정보 요구를 예측하고 부응하여 자료제공의 효율을 제고할 것이다.This study has analyzed the user's search trend through webpage query log analysis for' National Library of Korea '.
I have analyzed the search behavior of the top searchers in the National Library of Korea by using time series, contents, and type collected during 2015. Also I investigated the correlation between search terms and actual utilizations by comparing popular searches with a list of best sellers.
As a result, the input rate of the search terms was 4 ~ 7 times higher during the end of the years compared to the beginning of the year. In addition, the top 10 search terms accounted for about 17% of the monthly or annual search frequency in total. In terms of contents of the search terms, there have been a significant number of searches related to classical books, the books written by Korean authors and name of universities. When it comes to language, it has been seen that the largest input was typed in Korean, followed by Kanji and English. Also, the single digit, two digits, and four digits tend to be found most frequently when searched by figures. Korean alphabets and its consonants were likely to be the largest part of typing errors of search terms.
As a result of analyzing the correlation between search terms and circulated books, the search terms of 'history · travel' and 'philosophy · religion' match each other. The archive by subjects showed 'social science room' data substantially corresponds to its search words. Furthermore, domestic data have a 15% higher matching rate than overseas data (translated version). Lastly, the best-seller list in 2015 was 37% identical to the National Library of Korea search terms.
This research is worthwhile to understand how efficient the actual information retrieval system is for the users of National Library of Korea, as well as the status of retrieval and utilization of materials. This study would be useful to improve the efficiency of data provision by responding to the current trends of users and information requests and forecasting them for the future.1. 서론 1 1.1 연구필요성 및 목적 1 1.2 연구내용 및 방법 2 1.3 선행연구 3 1.3.1 국내 선행연구 4

신문 또는 e-mail을 통해서 접하게 되는 "이미지" 형태의 광고들은 단순한 "그림"이 아닌 "정보"를 포함한 개체이다. 본 논문에서는 '이미지' 형태 광고의 광고문구를 이용하여 해당 광고의 목적과 내용을 알아내기 위한 정보추출 시스템의 개발 과정 및 결과를 기술한다.
"이미지"형태 광고의 광고 문구는 문구의 배열이 상하로 이루어지거나 테이블로 이루어진 경우가 대부분이기 때문에, 자연어 처리 기법을 활용한 정보 추출 방법 이 아닌, 도메인 지식을 활용하여 미리 구축한 온톨로지를 이용한 정보추출 기법을 사용 한다.
정보추출 시스템의 개발 단계는 광고를 분류하기 위한 지식 정보를 관리하고, 정의된 개념을 명세화 시키기 위한 온톨로지의 구축하는 과정과, 온톨로지를 활용하여 광고문구로부터 정보를 추출하기 위한 시스템을 개발하는 과정으로 진행되고, 실험 단계는 광고문구를 token단위로 분리, 각 token 분류, 정보화의 3단계를 거쳐서 진행된다. 검증 단계는 사람이 작업한 정보추출 결과와 시스템의 정보추출 결과를 비교하여 시스템의 성능을 평가한다.
실험결과, 본 논문에서 제안하는 정보추출 시스템은 72.6%의 재현도와 50%의 정확도, 59.2%의 f1-score를 보인다.Advertisements in the form of "image" which we often see in newspaper or email, are objects containing "information" rather than mere "picture" This study described the process and results of developing an information extraction system, which is to find the purposes and contents of the advertisements from advertising phrases in image-type advertisements.
In most cases, advertising phrases in image-type advertisements are presented from top to bottom or in table. Thus, instead of information extraction using natural language processing techniques, we used an information extraction method based on ontology built in advance using domain knowledge.
The information extraction system was developed through the step of building ontology for managing knowledge information, itemizing defined concepts, and classifying advertisements, and the step of implementing the system for extracting information from advertising phrases. In the step of experiment, we divided advertising phrases into tokens, classified the tokens, and converted them into information. In the step of verification, the performance of the system was evaluated by comparing the results of manual information extraction with the results of information extraction by the system.
According to the results of the experiment, the proposed information extraction system showed reproducibility of 72.6%, accuracy of 50%, and f1-score of 59.2%.1. 서 론 2. 관련연구 2.1. 정보추출 분야 2.2. 사전

본 논문은 심층학습(Deep Learning)을 이용한 화음 진행 추천에 관한 연구이다. 화음 진행은 음악에서 그 양식과 정서, 구조 등에 결정적인 영향을 미치는 중요한 요소로써 작곡가에게는 다양한 화음 진행을 구사할 수 있는 능력이 요구된다. 하지만 기존의 음악 이론이나 제한적인 경험에만 의존하면 작곡가의 화음 진행은 일정한 유형이나 전형성에 갇히기 쉽다. 이에 본 연구는 심층 신경망으로 화음 진행을 학습함으로써 사용자가 제시한 화음 진행에 대한 다양한 변주 가능성을 추천해 줄 수 있는 모델을 제안한다.
자연어 처리의 대표적인 임베딩 기술인 word2vec을 활용하여 화음을 벡터 형태로 표현하고, 순환 신경망(RNN)으로 다양한 양식의 화음 진행 순서를 학습하여 사용자의 화음 진행을 대체할 수 있는 새로운 진행들을 생성한다.
제안된 방법을 이용하여 기존의 화음 추천, 혹은 화음 진행 생성 모델들에 비해 길이와 양식 측면에서 보다 확장된 결과를 얻었다.1장 서론 2장 관련 연구 1절 word2vec을 이용한 화음의 벡터 표현 2절 RNN을 통한 화음 진행 순서 학습 3장 제안 방법

RFID와 같은 전자태그가 본격적으로 도입되는 유비쿼터스 환경에서는 리더기에 인식되는 태그의 양이 기하급수적으로 증가하고 다양한 애플리케이션의 정보 요구로 인해 이벤트 브로커의 개입은 필수적인 사항이 된다. 지역적으로 분산되어 있는 이벤트 브로커와 애플리케이션의 이기종 통신을 가능하게 하기 위해 등록/발표(subscribe/publish) 방식의 구조를 차용하여 이벤트 브로커를 구성하는데 이때 애플리케이션이 이벤트 브로커에 등록할 필터를 어떻게 정의하고 기술하느냐에 따라 이벤트 브로커의 질이 판가름될 수 있다.
본 논문에서는 이벤트 브로커에 애플리케이션이 필터(관심있는 정보)를 등록할 때 최대한 자연어 형태의 텍스트로 기술할 수 있게 하는 방안에 대해서 다루고 있다. 텍스트를 바탕으로 등록 관리자 모듈에서 온톨로지를 참조하여 실제 물리적인 이벤트 브로커로의 등록을 대행한다. 이때 온톨로지는 내부적인 이벤트 브로커 네트웍 및 물류 도메인을 고려하여 작성되어 있다.
등록 관리자와 물류 도메인에 적합한 온톨로지를 만들고 몇가지 시나리오를 바탕으로 제안한 고수준 이벤트가 어느 정도의 표현력을 가지는지 평가한다. 다양한 분야를 만족하기에는 부족하고 제한된 도메인에 적용 가능하다는 한계성을 가지지만 저수준 이벤트 등록을 대치할만한 표현력을 가진다는 점에서 의의를 가진다.In ubiquitous environment that electrically tags such as RFID are introduced seriously, amount of tags are very increase and various applications required that information. So event broker’s intervention is essential. We borrow subscribe/publish architecture to communicate with locally distributed event brokers and applications. At this time how to definite and describe filter that application register on event broker is very serious problem because event broker’s quality will depend on that point.
In this paper the method to subscribe as maximum natural text in application’s subscribe filter(interested information). Subscription helper act for register process to real physical event brokers. And ontology is made with consideration about internally event brokers and physical distribution.
The body treats ontology suitably subscription helper and physical distribution and evaluate the expressiveness of subscription method. Although the limitation of the scope of ontology, this paper have meaning that suggested high level event subscription is suitable to replace low level event subscription.목차 제 1 장 서론 = 1 제 2 장 관련 연구 = 3 제 1 절 유비쿼터스 컴퓨팅 (Ubiquitous Computing) = 3 제 1 항 MIT의 Oxygen 프로젝트 = 4

최근 몇 년 간, Web 2.0의 급속한 발전과 모바일 기기의 급격한 확산에 따라 트위터 (Twitter), 페이스북 (Facebook), 인스타그램 (Instagram) 등의 다양한 소셜 미디어의 사용자가 급증하고 있다. 또한 개방과 참여 및 공유의 특성을 가지는 Web 2.0의 특성에 따라 사용자들은 소셜 미디어 환경에서 특정 관심분야에 대해 생각, 의견을 자유롭게 표현할 수 있고, 이것은 사용자 개개인의 특성을 가지고 있는 데이터를 제공한다. 이에 따라, 이러한 데이터를 이용하여 사용자 개개인의 감성 분석이나 또는 이를 이용한 사용자 맞춤형 시스템에 대한 연구가 활발히 진행되고 있다.
그러나 글, 이모티콘, 사진 등 소셜 미디어에서 표현할 수 있는 데이터의 종류와 양은 제한적이기 때문에 사용자의 현재 감정을 정확하게 분석하기 어렵다. 또한 소셜 미디어의 특성상 사용자들이 표현하는 데이터가 항상 빈번하거나 주기적이지 않기 때문에 실제 사용자들의 감성 분석에 대한 연구나 이를 이용한 개인 맞춤 컨텐츠 추천 연구에 대한 많은 어려움이 따른다.
이에 따라 본 논문에서는 좀 더 정확하고 효율적인 감성 분석을 위해 ACO (Ant Colony Optimization) 알고리즘을 이용하여 소셜 미디어 사용자의 감성 분석에 기반한 컨텐츠 추천 방법을 제안한다. 우선, 타당성 있는 실험을 위해 트위터에서 사용자 별 데이터를 추출하여 페로몬 값을 구하는 실험을 하고 이를 이용하여 소셜 미디어에서 추출한 사용자 개개인의 감성 분석을 수행하며, 이에따라 사용자 키워드의 감성 목록을 만든다. 그리고 마지막으로 영화전문 소셜 미디어 MovieLens에서 제공하는 영화관련 데이터와의 비교를 통해 시간에 따른 감성 변화에 따라 사용자 별 맞춤 영화를 추천한다.1. 서론 6 2. 배경 및 관련연구 9 2.1. 소셜 미디어 9 2.2. 감성 분석 12 2.3. ACO 알고리즘 15

도메인 온톨로지는 개체들간의 개념적, 용어적 불합의 최소화를 통해 특정 도메인에 존재하는 개체들간의 의미적 불일치 (semantic gap) 문제를 해소하기 위하여 사용된다. 그러나 텍스트 문서로부터 지식 체계를 추출하고 도메인 온톨로지를 수동으로 구축하는 것은 도메인 전문가들에 의한 장시간의 협업이 필요한 매우 어려운 과정이다. 또한, 지식 체계 획득 및 온톨로지 구축을 자동화하고자 하는 연구 역시 구조화된 지식 표현 체계의 결핍 및 자연어 처리 기술의 한계와 같은 문제점으로 인하여 많은 어려움을 겪고 있다. 따라서, 도메인 온톨로지가 실제 환경에서 효과적으로 활용 가능한 도구로서 작용하기 위해서는 몇 가지 중요한 문제들이 반드시 극복되어야만 한다. 첫 번째 중요 문제는 텍스트 문서로부터 도메인 후보 용어를 획득하는 과정 및 이렇게 추출된 용어들 중 도메인 온톨로지 획득을 위한 주요 도메인 개념들을 효과적으로 획득하는 과정이다. 두 번째 중요 문제는 도메인에 존재하는 실제 지식 체계와 일치하는 도메인 개념들간의 분류적, 비분류적 의미 관계를 추출하는 과정이다.
최근, 도메인 온톨로지를 자동으로 혹은 반자동으로 구축하기 위한 다양한 연구들이 진행되고 있다. 그러나, 이러한 연구들은 특정 도메인에 대한 말뭉치 (corpus)에 극히 의존적이라는 점과 특정 문헌들로부터 도메인 말뭉치를 구성하기 위하여 전문가의 수작업이 필요하다는 점과 같은 여러 가지 한계점들을 가지고 있다. 또한, 기존의 연구들은 텍스트 문서에 존재할 수 있는 주요 명제들을 누락시킬 수 있는 대명사의 대용 해소 문제를 고려하지 않고 오직 명사 구만을 도메인 온톨로지 구축에 활용하고 있다는 문제점이 있다.
본 학위논문에서는 텍스트 문서로부터 지식체계를 추출하기 위한 새로운 방식의 도메인 온톨로지 구축 기법을 제안한다. 본 학위논문은 언어적 패턴 및 클러스터링 기법을 활용한 반자동적, 도메인 독립적, 비감독 기법을 제안함으로써 기존연구들과의 차별성을 보인다. 본 학위논문에서는 도메인 온톨로지 구축 과정을 위하여 다음과 같은 기법들을 개발하였다: 1)의미적 언어 패턴을 이용한 후보 용어의 추출 기법 및 의미적으로 연관성이 있는 개념들의 클러스터링을 통한 도메인 개념의 선정 기법, 2) Hearst 패턴을 활용한 도메인 개념간의 분류적 관계성 추출 기법, 3) 언어 패턴을 활용한 도메인 개념간의 비분류적 관계성의 추출, 명명 및 방향성 할당 기법, 4) 중간 단계 지식 체계 표현을 위한 도메인 개념도의 구축 기법, 5) 도메인 개념도로부터 도메인 온톨로지를 구축하기 위한 기법.
먼저, 언어적 타입 의존 규칙을 활용하여 텍스트 문서로부터 후보 용어들을 추출한다. 그 후, 추출된 후보 용어 쌍 간의 의미적, 구조적 유사도를 바탕으로 용어쌍 간의 최종 유사도를 계산한다. 이렇게 계산된 용어쌍 간의 유사도 값들은 친근도 전파 알고리즘의 입력값으로 활용된다. 친근도 전파 알고리즘은 고품질의 데이터 견본 (exemplar)가 발견되어질 때까지 데이터 포인트 간의 메시지 교환을 반복적으로 수행하는 데이터 클러스터링 알고리즘이다. 친근도 전파 알고리즘을 활용하여 추출된 모든 견본 데이터 포인트 (즉, 도메인 용어)들은 도메인 온톨로지를 학습하기 위한 도메인 개념으로서 활용된다. 그 후, 개념도 구축을 위하여 각각의 클러스터 내의 후보 용어들 간에 분류적/비분류적 관계성이 할당된다. 최종적으로, 구축된 개념도 개체들을 온톨로지 개체들로 변환하는 과정을 통하여 도메인 온톨로지를 획득한다.
마지막으로, 다양한 실험을 통하여 제안하는 시스템의 성능을 검증한다. 실험을 통하여, 본 학위논문에서 제안하는 기법을 통하여 구축된 도메인 온톨로지는 도메인 전문가의 수작업을 통하여 생성된 온톨로지와 일치함을 볼 수 있었으며, 도메인 전문가에 의한 평가를 통하여 구축된 도메인 온톨로지가 정보 시스템 도메인 및 학계 도메인의 지식 체계와 높은 수준으로 일치한다는 것을 확인할 수 있었다.Domain ontology can be used to bridge the semantic gap among the members of a domain through minimization of conceptual and terminological incompatibilities. Extracting knowledge from text documents and learning domain ontology manually is, however, a difficult, controversial, lengthy, and time consuming task that involves domain experts. Automatic or semi-automatic knowledge acquisition and ontology learning is also a non-trivial task due to the lack of structured knowledge representation and most of the data in documents are available in a free text format. Therefore several barriers must be overcome before domain ontology becomes a practical and useful tool. First important issue is the acquisition of candidate terms from text documents and then selection of domain concepts from these extracted terms for domain ontology learning. Second important issue is the extraction of taxonomic and non-taxonomic relationships between domain concepts which contain the actual context of a domain. Recently, various approaches for automatic or semi automatic construction of domain ontology have been proposed. However, these approaches suffer from several limitations such as heavy dependency on domain specific corpora and manual effort of experts required to populate the domain corpus from selected literature. Also, these approaches consider only the noun phrases in the text documents without resolving the anaphora resolution problems for pronouns which leads to miss the important propositions available in the text documents caused to decrease the recall.
The proposed system presents a novel approach for domain ontology learning, defining new techniques for knowledge extraction from text documents. The utilization of linguistic patterns and clustering technique to the free text documents composing a semi-automatic, domain independent and unsupervised approach distinguishes the proposed system from the previous systems.
We have been developed the following methods for the domain ontology construction process: 1) extraction of candidate terms using semantic linguistic patterns and selection of domain concepts by clustering semantically related concepts, 2) extraction of taxonomic relationships between domain concepts using Hearst’s patterns, 3) extraction, labeling, and assignment of direction of non-taxonomic relationships between domain concepts using linguistic patterns, 4) construction of domain concept map form extracted knowledge used as intermediate level knowledge representation, 5) finally, construction of domain ontology from constructed concept map. First, we extract candidate terms from documents using typed dependency linguistic rules. Second, Diset similarities are calculated based on semantic and structural similarity between pairs of extracted candidate terms. We then exploit affinity propagation algorithm, which takes as input Diset similarities between pairs of extracted candidate terms called data points. Real-valued messages are passed between candidate terms until a high quality set of exemplars iteratively emerges. All exemplars will be considered as domain concepts for learning domain ontologies. Then, extracted relationships are assigned between candidate terms in each cluster to complete the concept map. Finally, domain ontology is obtained from the constructed concept map by transforming concept map entities into domain ontology entities. The whole methodology has been implemented using different programming tools, providing a scalable solution.
Finally, we verify the appropriateness of the proposed system by experimental results. Our empirical results show that the semi automatically constructed domain ontology conform to the outputs generated manually by domain experts, since the degree of difference between them is proportionally small. Also, domain experts have verified that the constructed domain ontologies are highly accordance with their knowledge and perception about information system domain and academia domain.Abstract 1 1 Introduction 4 1.1 Motivation 4 1.2 Ontology Basics 6 1.3 Overview of our Solution 9

최근 지속적인 경기침체와 저금리 기조로 금융투자에 대한 관심이 증가하고 있다. 이에 따라 인공지능 기술을 금융 분야에 적용하려는 시도가 꾸준히 이 루어지고 있다. 금융 시장 예측은 통계학, 경제학 등의 분야에서 꾸준히 연구 되어 왔으나 잡음이 극심하고 비선형적인 특성으로 인해 아직까지 예측이 매 우 어려운 영역으로 인식되어 있다. 이에 본 연구에서는 데이터 간의 복잡한 상관관계를 파악하고 학습하는데 뛰어난 성과를 보이는 딥러닝 기법을 금융 예측에 적용한다.
딥러닝 알고리즘은 이미지, 자연어 등 기존의 인공 신경망 모형으로 해결하 기 어려운 정보를 분석하는데 탁월한 성과를 보이고 있으며, 네트워크 구조가 개선되고 새로운 알고리즘이 개발되면서 다양한 분야로 활용 범위를 넓혀가 고 있다. 딥러닝 내에는 주어진 데이터를 효과적으로 처리하기 위한 다양한 파라미터가 존재한다. 입출력변수에 대한 네트워크 구조(network topology), 학습 조건(learning parameter) 등의 선정이 바로 이에 해당한다. 특히, 딥러닝 모형은 내부 계층이 매우 많아, 최적의 네트워크를 구성하기 위해 사전에 연구자가 조정해야 하는 하이퍼 파라미터(hyper-parameter)의 수는 기하급수적으로 증가했다. 하지만 기존 연구들은 수 많은 실험을 바탕으로 한 시행 착오법이나 전문가 노하우에 의존하고 있으며, 이에 대한 체계적인 연구는 부족한 실정이다. 이러한 한계점을 극복하고자, 본 연구에서는 대표적인 서치 알고리즘인 유전자 알고리즘을 이용하여 딥러닝 모형의 구조를 최적화하고 이 를 주가 지수, 부도 위험과 같은 금융데이터에 적용해 예측 성과를 높일 수 있는 방법론을 제시한다.
우선 본 연구에서는 유전자 알고리즘으로 최적화된 LSTM네트워크(long short-term memory network)를 통한주가지수 예측 방법론을 제시하였다. LSTM네트워크는 과거의 정보를 학습에 반영할 수 있는 딥러닝 네트워크 중 하나로, 시계열 데이터에 적용되어 뛰어난 성과를 보인다. LSTM네트워크를 학습시킬 때에는, 과거의 정보를 어느 정도까지 포함시킬 것인지, 즉 타임윈 도우의 크기를 어떻게 설정할지 결정하는 것이 중요하다. 만약 타임윈도우의 크기가 너무 작다면 중요한 정보를 학습에 포함시키지 못할 수 있고, 타임윈 도우의 크기가 너무 크다면 과다한 정보가 학습에 포함되어 잡음이 될 수 있
다. 이에 본 연구의 첫 번째 모형에서는 유전자 알고리즘을 이용해 LSTM네트워크의 타임윈도우의 크기와 은닉층을 구성하는 LSTM유닛의 개수를 최적 화한다. 제안 방법론을 검증하기 위해 한국 주가지수 데이터를 사용하였으며, 실험 결과 유전자 알고리즘을 통해 최적화를 수행한 모형이 비교 방법론에 비 해 뛰어난 성과를 보이는 것으로 나타났다.
두 번째로는 멀티-채널 합성곱 신경망 모형을 이용해 주식 시장의 방향성 을 예측하였다. 유전자 알고리즘을 이용해 합성곱 신경망의 특성 추출 과정을 담당하는 합성곱 계층(convolutional layer)과 풀링 계층(pooling layer)에 대한 최적화를 수행했다. 합성곱 계층에서는 커널을 통해 주어진 입력에 대해 패턴을 파악하며 풀링 계층에서는 합성곱 단계에서 탐색된 특징들 중 가장 대 표적인 값을 추출하는 작업을 수행한다. 따라서 커널의 크기는 입력 데이터로부터 주요 특징을 추출하는 데 가장 중요한 역할을 한다. 커널의 크기가 너무 큰 경우 입력 데이터의 세부적인 특성을 고려하지 못하게 되고, 너무 작으면 지나치게 많은 정보를 학습해 혼란을 야기할 수 있다. 따라서 합성곱 신경망 모형의 성과 향상을 위해서는 해결해야 하는 문제가 속한 도메인과 데이터의 특성을 가장 잘 반영할 수 있는 크기의 커널을 사용해야 한다. 하지만 커널 크기를 설정하는 과업은 아직까지 실험을 통해 적합한 값을 결정하는 기술 (art)적인 문제로 남아있으며 이에 대한 체계적인 연구는 부족한 실정이다. 이에 본 연구에서는 유전자 알고리즘을 이용하여 최적의 네트워크 구조를 도 출하고 이를 합성곱 신경망에 적용하여 한국 코스피 지수의 방향성을 예측하 는 통합 방법론을 제시하였다. 제안한 모형의 효과성 검증을 위해 기존 주식 시장 예측 연구에서 보편적으로 사용되었던 인공신경망 모형과 함께 최적화를 수행하지 않은 기본 합성곱신경망 모형의 성능을 비교한 결과 제안 방법론의 성과가 가장 뛰어난 것으로 나타났다.
마지막으로는 유전자 알고리즘으로 네트워크 구조를 최적화한 심층신뢰망을 이용하여 기업의 부도 여부를 탐색하였다. 심층신뢰망은 최초로 연구된 딥러 닝 모형으로 다수의 제약 볼츠만 머신으로 이루어져 있다. 본 연구에서는 심 층신뢰망의 핵심 구성요소라고 할 수 있는 제약 볼츠만 머신의 구조를 최적화 하였다. 이를 통해 잡음이 심하고 비선형적인 특성을 가진 부도예측데이터에 대한 학습 효율성과 예측 성과를 향상시켰다.
본 연구는 유전자 알고리즘으로 딥러닝 네트워크 구조를 최적화함으로써 경영 분야의 중요한 문제 중 하나인 금융 예측의 성과를 높였다. 이를 통해 실제 투자에 필요한 의사결정체계를 효율적으로 개선하는데 기여할 수 있을 것 이라고 기대한다.Recently, artificial intelligence (AI) technologies have attracted critical attention because of their practical applications in various fields. In the history of AI, deep learning techniques such as deep belief networks (DBN), convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been considered a key factor in this prosperity. Deep learning is a type of artificial neural networks (ANNs) that has many hidden layers between the input and the output layers. Deep learning not only increases the complexity of the model but also assigns an appropriate role to each neuron for the purpose of analysis; therefore, it can effectively solve the target problems that could not be solved using traditional neural network models.
This study applies deep learning techniques to the financial prediction problems. The financial market is known to be very difficult to predict and analyze because of the nature of its noisy and nonlinear environment. Traditionally, researchers usually adopted a variety of statistical and machine learning approaches including ANN and support vector machine (SVM). However, these methods had limitations in achieving good performance, because the financial market has numerous factors affecting its volatility and it is not easy
to capture the nonlinearity of these factors. In the financial field, there are great opportunities to derive valuable insights by using deep learning techniques with its great ability in learning features. Accordingly, this dissertation suggests various novel financial forecasting models using deep learning techniques which have improved prediction performance in various research fields.
Furthermore, we aim to optimize the deep learning architecture to improve the model performance. Many studies have demonstrated the efficacy of deep learning techniques, but there are several shortcomings in building and using these techniques. First of all, it is not easy to determine a suitable model which can reflect nature of problem and learn data patterns, because there are numerous controlling parameters. However, most of the existing studies tend to depend on trial-and-error-based methods, and such methods are more of an art than a science. Therefore, here, we propose a method to systematically optimize the parameters for the deep learning models by using genetic algorithm (GA). GA is exploited in the search of optimal hyper-parameters for the deep neural network. Many studies have applied GA in conjunction with other AI and machine learning techniques, but few studies have attempted to integrate the GA and deep learning models despite a great potential for effective applications in this area. For this reason, this dissertation suggests GA-based deep learning techniques to automatically discover the most appropriate set of components for the deep neural networks and the proposed methods are developed in order to solve financial problems.Introduction 1 A. Research background 1 B. Research objective 4 C. Organization of the dissertation 8 Review of Related Research 9

일상 생활 속에서 발생하는 비정형 데이터의 생산량이 폭발적으로 증가하고 있다. 이러한 비정형 데이터는 자연적 상황에서 발생하기 때문에 기술 및 사회 현상을 정성(定性)적으로 읽어내는데 있어서의 가치를 인정받고 있다. 그러나 아직까지 질적 분석 방법론들은 수동 분석에 의존하는 경우가 많으며, 빠르고 실용적인 분석 방법이 부재한다. 데이터 마이닝 기법들이 존재하기는 하나, 특정 용도로 설계되어 있거나 코딩을 해야하므로 비전공자에게 접근성이 떨어지는 실정이다.

하지만 최근에 기성품 인공지능 서비스가 등장함에 따라 새로운 가능성을 엿볼 수 있게 되었다. 자연어 처리 기술의 고도화로 기계가 텍스트를 의미론적으로 이해할 수 있게 되었으며, 또한 지식 베이스를 기반으로 한 추가적인 해석이 가능해졌기 때문이다. 마이크로소프트사의 엑셀 소프트웨어의 보급으로 추가적인 코딩 없이도 기초적인 통계 분석이 가능해졌듯이, 이러한 인공지능 서비스의 보급화 또한 자료 분석에 - 특히나 정성적인 해석에 - 영향을 미칠 것으로 예상한다. 따라서 본 연구는 인공지능 서비스 보급화의 징후가 엿보이는 시점에서 인공지능 서비스를 해석적 작업에 실증적으로 적용해보고, 나아가서는 인간-인공지능의 질적 분석 협업의 사용자 경험을 고찰하고자한다.

첫번째로 인공지능 소프트웨어의 질적 분석 적용안을 제안하기 위해서 12개의 KDDM(Knowledge Discovery Data Mining) 모델을 살펴본 후 공통된 요소를 도출하고, 그 중 현재 출시된 인공지능 서비스들이 대체할 수 있는 영역을 발굴했다. 구체적으로는 IBM Alchemy 서비스를 활용하여 키워드 자동 추출을 활용한 질적 분석 방법을 제시한다. 두번째로는 인간-인공지능 질적 분석 협업의 사용자 경험을 살펴보기 위하여 31명의 피실험자를 모집하여 User Study를 진행하였다. 마지막으로 인간-인공지능 해석적 작업의 과정을 더 면밀히 살펴보기 위해 12명(14개 세션)을 대상으로 Think Aloud를 진행하여 인공지능의 자동 키워드 추출에 대한 사용성과 수용성을 평가하였다.

그 결과, 사용자들은 컴퓨팅 파워를 기반으로 한 빠른 분석만을 원할 뿐 아니라 의미적으로 중요도를 구분해주기를 바란다는 것을 밝혀낼 수 있었다. 인간과 인공지능의 키워드 추출은 23%의 유사도 (similarity)를 보였다. 수용성 측면에서 인간은 인공지능이 추출한 키워드 중 약 28%를 수용했으며 이 중 약 40%는 인간이 홀로 해석할 때에는 추출해내지 못한 키워드였으며 내용적으로는 대부분 구체적인 고유명사에 해당하였다. 나아가서 Think Aloud 세션을 통해 이러한 수용과 기각에 영향을 주는 변수는 사전지식, 보철지식, 그리고 반복성이라는 것을 밝혀낼 수 있었다.

본 연구는 인간-인공지능 질적 분석 협업 모델을 실질적으로 제안하고 적용해보았다는데 의의가 있다. 기 출시된 인공지능 서비스를 활용하기 때문에 효율적이고 접근성이 높은 적용안이라는 점에서 산업적 의의를 가진다. 또한 학술적으로는 인간-인공지능 질적 분석 협업의 사용자 경험을 과정과 결과 측면에서 모두 고찰하였다는데 의의가 있다. 나아가서 현재 인공지능이 점차 일상 생활에 파고들고 있는 시점에서 인공지능 사용자 경험에 대한 논의와 고민을 확장하는데 기여하고자 한다.제 1 장 서 론 1 제 1 절 연구의 배경 1 제 2 절 연구의 대상 및 목적 4 제 3 절 본 논문의 구성 5

지역 관광객 수의 예측은 해당 지역의 생산유발, 고용유발 등 경제적 효과에 큰 영향을 미친다. 특히, 관광산업이 전체 산업의 70%를 차지하는 제주도의 관광객 수를 정확하게 예측하는 것은 제주도 관광산업의 발전에 좋은 기초자료로 사용될 수 있는 중요한 연구라고 할 수 있다. 하지만 관광수요의 예측에 주로 사용되어 온 시계열 모형을 이용할 시 여러 정상성 등 여러 가정을 만족해야 한다는 한계점이 있다.
이에 본 논문에서는 최근 이미지 인식, 음성 인식, 자연어 처리 등 다양한 분야에서 우수한 성능을 보여주고 있는 딥러닝(Deep learning)을 사용하여 제주도를 방문하는 관광객 수를 여행 목적별로 예측한다. 또한 최근 많은 분야에서 연구되고 있는 인터넷 검색어 자료를 제주도 관광객 수의 예측변수로 사용하며 국내 최대 검색 점유율을 가지는 네이버의 검색어의 검색량을 사용한다. 딥러닝 알고리즘 중 심층신경망(Deep Neural Network; DNN)을 이용하고 심층신경망의 여러 단점들을 해결해주기 위해 Dropout정규화와 활성화 함수로는 ReLu함수를 심층신경망에 적용한다. 또한 딥러닝 알고리즘의 예측력을 확인하기 위하여 기존에 사용되던 계절형 ARIMA 모형과 비교한다. 분석에 사용한 자료는 2007년 1월부터 2016년 7월까지의 월별 자료이며 분석 결과 인터넷 검색어 변수를 이용한 딥러닝 모형이 계절형 ARIMA모형과 좋거나 비슷하다는 것을 알 수 있었다.1. 서론 1 1.1 연구 배경 및 목적 1 1.2 연구의 구성 2 2. 선행연구의 고찰 4

온라인 상에서 얻을 수 있는 텍스트 정보의 양이 급증함에 따라 이들의 효율적 이용이 요구된다. 문서의 자동 분류는 미리 정의된 범주에 문서를 할당하는 작업으로 방대한 양의 수작업을 감소시켜 효율적인 문서 이용에 도움을 줄 수 있다. 또한 정보 추출의 사전 단계로 추출의 정확성을 높이는데 중요한 역할을 한다.
문서의 자동 분류를 위해 문서상에 나타난 모든 단어를 이용하는 방법, 자연어 처리 기법을 사용하여 의미를 파악해 분류하는 방법 등 다양한 분류 방법이 연구 되어왔다. 그러나 이런 기법들은 분류의 성능이 좋지 못하거나 추가적인 언어학적 지식을 필요로 하는 등의 문제가 있다.
이에 본 논문에서는 문서에서 메타 데이터를 추출하고 이를 이용해 기계 학습 알고리즘을 사용하는 분류 방법을 제시한다. 메타 데이터는 데이터에 관한 데이터로서 속성을 기술하는 데이터를 의미하는데, 키워드만을 이용하는 것 보다는 그 문서가 가진 속성을 이용하므로 문서 분류의 정확도를 높일 수 있을 것이라 생각한다. 또한 XML의 표준화 작업이 진행 중 이므로 앞으로 메타 데이터의 추출은 더욱 수월해질 것으로 예상되어 효율적으로 분류에 이용 할 수 있다.
문서가 갖는 메타 데이터는 다양한데, 본 연구에서는 제목, 저자, 초록, 참고문헌을 이용해 분류를 수행하였고, 문서 내의 모든 키워드를 이용한 분류와 메타데이터를 이용한 분류 실험과 결과를 보여준다. 문서 내의 모든 키워드를 이용한 분류와 비교했을 때 ‘제목, 저자, 초록’을 이용한 분류가 가장 높은 정확도를 보여주었고, ‘초록’을 이용한 분류가 가장 높은 F-measure를 보여주어 메타 데이터를 이용한 분류가 더 좋은 성능을 보여주는 것을 확인할 수 있었다. 또한 정보 획득량을 이용한 자질 선택 기법을 수행하여 분류의 성능을 향상 시킬 수 있었고 ‘제목 저자, 초록’을 이용해 나이브 베이즈 알고리즘을 사용한 분류와 참고문헌의 유사도를 이용한 분류를 결합해 89.4%의 정확도를 얻을 수 있었다.As the amount of online text information increases rapidly, we need to use it more efficiently. We could save a great amount of handwork and make good use of information through automatic text classification. Furthermore, automatic text classification plays an important role as the pre-processing of information extraction in improving performance of extraction.
Many researches on automatic classification, for example, using keywords in documents or natural language techniques have been carried out. However, these showed low performance of classification and required additional linguistic knowledge.
This thesis suggests an approach to text classification using meta data from text documents. Meta data is data about data, which describe the properties of documents. We can classify documents more accurately with the meta data than with the keywords only, since we are able to reflect the properties of documents. In addition, with XML standardization on the way, it is expected to extract meta data more easily resulting in more efficient classification.
Documents could have various types of meta data, among which this thesis considered the title, author, abstract and references. This thesis includes results of a series of experiments for text classification. Compared with the classification using the keywords in documents, classification using the title, author and abstract produced the highest accuracy. When the abstract was used only, it showed the highest F-measure, which verifies the meta data improves the performance of classification. In addition, classification with feature subsets generated by information gain showed improved performance. Furthermore, a combined approach of Naive Bayes on the meta data and similarity vectors on the references produced 89.4% of classification accuracy.

추천 시스템은 사용자의 취향이나 관심사를 고려하여 사용자들의 요구에 가장 적합한 아이템을 추천해주며 사용자의 의사결정을 도와주는 시스템이다. 과거에는 상품 추천을 할 때 사용자가 어떤 사람인지 상관없이 모든 이용자에게 똑같은 정보를 제공하여 사용자는 기호와 선호도와는 무관한 정보를 얻는 경우가 많았다. 이러한 문제를 해결하기 위해 검색, 클릭 수 및 평점 등과 같이 사용자와 아이템 간의 상호 정보를 기반으로 정확한 추천을 제시하는 협업 필터링 추천 기법이 등장하였다.

자연어 처리에서 문장 내부의 단어 간의 연관성을 파악하여 벡터로 변환하는 방법인 Word2Vec을 텍스트 분석 분야뿐만이 아닌 상품 추천 시스템에서 활용하여 사용자 및 아이템 간의 유사도를 개선한 협업 필터링 추천 연구가 제안되었다. 더 나아가 전역 문맥뿐만 아니라 단어의 지역 문맥을 함께 고려한 하이브리드 방식의 모델인 GloVe를 활용한 아이템 기반 협업 필터링 추천 방법과 또한 각각의 문서를 하나의 벡터로 표현하여 비슷한 의미를 가진 각각의 문서들을 벡터 공간상 근거리에 위치하도록 벡터를 생성하는 기법인 Doc2Vec 모델을 사용하여 Paragraph Vector를 통해 사용자 간 유사도를 개선한 사용자 기법 협업 필터링 추천이 제안되었다.

본 논문에서는 하나의 아이템에 평가한 모든 사용자를 평점순으로 나열하여 한 문장으로 구성하는 방법을 제안하여 기존 논문에서 사용한 문장 구성 방법들과 성능을 비교하고, 다양한 사용자 벡터화 기법을 활용하여 유사도를 개선하고 사용자 기반 협업 필터링에 적용시켜 추천 시스템의 성능을 높이는 방안을 제시한다. 특히 Doc2Vec에서 PV-DM 모델을 사용하여 한 문장의 문서 벡터를 하나의 아이템으로, 단어 벡터들을 한 아이템을 평가한 모든 사용자들로 치환한 후 같은 공간 안에서 평균 계산된 사용자 벡터를 활용하여 사용자 간의 유사도를 개선한 사용자 기반 협업 필터링 추천 방법과 문장 내에 있는 전체 단어의 통계정보를 제대로 활용하지 못하는 Word2Vec의 단점을 보완한 알고리즘인 GloVe를 이용한 사용자 협업 필터링 추천 방법을 함께 제시한다.

실험은 다양한 사용자 벡터화 기법을 통해 얻은 벡터들로 사용자간 유사도를 개선한 사용자 기반 협업 필터링 추천에 적용하였고 모델마다 성능을 비교하였다. 실험 결과로 본 논문에서 제안한 문장 구성 방법을 사용하였을 때 모든 기법에서 성능이 더 좋은 것을 확인하였고, 제안했던 기법인 Doc2Vec PV-DM 모델을 통해 사용자에 해당하는 Word Vector를 구하여 사용자간 유사도를 개선한 방법과 GloVe를 통해 사용자 벡터를 구하여 사용자 간 유사도를 개선시킨 모델이 더 좋은 성능을 보인 것을 확인하였다.The recommendation system is a system that helps the user to make decisions by recommending the item that suits the user’s needs. In the past, product recommendations often provided the same information to all users regardless of who they were, resulting in information that was unrelated to their preferences. To solve these problems, collaborative filtering recommendation have emerged that present accurate recommendations based on mutual information between users and items.

Collaborative filtering recommendation study was proposed to improve item-to-item similarity and user-to-user similarity using Word2Vec. In addition, item-based collaborative filtering method was proposed to improve item-to-item similarity using GloVe, a hybrid model that considered the local context of words as well as global contexts. Also, user-based collaborative filtering recommendation was proposed using the Doc2Vec model to improve the similarity between users through Paragraph Vector.

This study proposes a method to improve the performance of user-based collaborative filtering recommendation system by improving user similarity using various user vectorization methods. In particular, this study proposes user-based collaborative filtering recommendation method that improves the similarity among users by using Doc2Vec PV-DM and GloVe.

The experiment was applied to user-based collaborative filtering recommendation that improved user-to-user similarity with vectors obtained through various user vectorization methods and compared performance across models. Experimental results show that the performance is better when the sentence composition method proposed in this paper. In addition, Doc2Vec PV-DM and GloVe that improved the similarity between users showed better performance.국문 요약 vi 제1장 서론 1 1.1 연구 목적 1 1.2 연구 내용 3

As increasing of needs of embedded system, it is emphasized that embedded system software must have safety. To verify the safety of embedded system software, valid embedded software testing is required. To test embedded system software, requirement based software testing methods are frequently used. In this paper, to apply requirement based software testing methods, we propose a tool for requirement description, and requirement management. We also design, and implement the proposed tool.내장 시스템의 필요성이 증가하고 있는 가운데, 내장 시스템 소프트웨어에 대한 안정성의 중요성이 강조되고 있다. 내장 시스템 소프트웨어의 안정성을 검증하기 위해서는 소프트웨어에 대한 올바른 테스트가 이루어져야 한다. 내장 시스템 소프트웨어를 테스트 하기 위해 널리 사용되는 기법이 요구사항 기반 소프트웨어 테스트 기법이다. 본 논문에서는 요구사항 기반 소프트웨어 테스팅을 위한 요구사항 기술, 관리 도구를 제안하고, 이를 설계 구현한다.그림 차례 (LIST OF FIGURE) 7 표 차례 (LIST OF TABLE) 8 제 1 장 서 론 9 제 2 장 요구사항 기반 소프트웨어 테스팅 12 제 1 절 요구사항의 작성 13

최근 사용자의 편의를 고려한 사용자 인터페이스 연구가 활기를 띄고 있으며 특히 인터넷의 보편화로 인하여 범람하는 인터넷 정보와 인터넷 상점들 속에서 사용자 개개인의 취향과 특성을 파악하고 거기에 맞게 잘 대처할 수 있는 사용자 인터페이스의 필요성을 실감하고 있다.
사용자 개개인의 특성을 파악하기 위하여 인간의 감정상태를 이용한 인터페이스 에이전트의 중요성을 인식하고 있으나, 아직까지 대부분의 연구는 자연어 처리를 기반으로 한 것으로 인간의 얼굴 표정이나 행동에서 나타나는 감정상태나 의사 전달은 고려하지 못하고 있다.
본 논문에서는 사용자 인터페이스 시스템 환경에서 사용자 개개인에게 적합한 환경을 제공하기 위하여 얼굴 표정과 머리 움직임의 정보를 활용하여 사용자 프로파일을 학습하는 시스템을 제안한다. 얼굴 표정이나 머리 움직임을 보고 사용자의 감정상태를 파악하는 일은 불확실하고 모호한 정보를 이용하는 것으로써 퍼지 추론기를 적용하여 사용자의 만족상태를 모델링한다. 퍼지 추론기를 통하여 얻어진 사용자 만족도를 사용자 프로파일 학습피드백으로 사용함으로써 사용자의 암시적 정보를 포함하는 프로파일을 구성한다.Recently the user interface has been the focus of many researchers who seek for improvements of the communications between users and systems. Since increasingly many people and institutions regard the internet as their means of search for information and place for marketing, research for user interface will be more important field focusing on grasping each user's characteristics and interests.
Although researchers aware of the importance of user interface agent which uses the emotional conditions for catching each user's personal characteristics, they have been focused only on the natural language handling, rather than the facial expression or the head movement recognition.
In this paper, the user interface system, considering each user's emotional conditions, is proposed. This system learns the user profile by using the facial expression and the head movement. Because it is very ambiguous and uncertain to read user's emotions by getting the facial expression and the head movement, the fuzzy controller, suitable for solving the ambiguous and uncertain problem is being used to find out user's emotional condition, called user's satisfaction status. As it is added to user's feedback, it helps learning the user profile, reflecting the user's implicit information.감사의 글 차례 = ⅰ Abstract = ⅲ 요약 = ⅳ Ⅰ. 서론 = 1

Korea is affected by the yellow dust, every year and the days and duration of yellow dust are getting lengthened. This yellow dust worsens the visibility, causing malfunctions of machinery in the industrial field and problems with airplane operation, as well as health hazards such as respiratory diseases and eye diseases of citizens. In addition, the damage, caused by yellow dust through SNS, according to the recent increase in the distribution rate of smart phones, is easily seen.
In this regard, we collected tweet data for 11 days from 18th to 28th, centering on the dustiness occurred on February 23, 2015, recorded as the highest concentration of yellow dust in Seoul since 2009, and conducted the analysis of issue words after natural language processing.
As a result, an interest in 'mask,’ and ‘fine dust' in relation to yellow dust was higher and people were highly concerned about ‘going out’ along with a rising worry of health problems such as ‘cold’ and ‘respiratory diseases’ before February 23, 2015, when the yellow dust occurred, while an interest in the advertisements of the articles such as 'masks,’ and ‘ air cleaners ' in accordance with the interest in health sharply increased after the occurrence of yellow dust. In addition, this study classified 10 groups by using the Visual Basic 2015 in view of the relevance of issue words to develop a program based on the Visual Basic language which can classify the tweet data. In particular, an analysis of association rule was conducted at a 5% and 10% support level in terms of life, emotions, food, health group among the groups to find the correlation between the worry for going out in real life and wearing of mask as a way to protect health and the correlation with the diseases such as cold and rhinitis. In addition, it identified the relation of caution in emotions with fine dust and cold as well as the relation of foods, good for yellow dust. Furthermore, we analyzed association rules focusing on the diseases of the medical departments among the health groups. It was revealed that asthma was highly associated with 'bronchitis, coughing' and rhinitis were highly associated with 'cold’ and ‘mask,' and conjunctivitis were highly associated with 'eye disease’ and ‘ophthalmology’. In addition, it acquired data from health insurance review & the assessment service for the number of medical treatments, made in February 2015, such as the occurrence of tweet data for three diseases; conjunctivitis, rhinitis and asthma, which are suspected of being affected by yellow dust, along with high concern of citizens. With the exception of the third week, which gets out of the general pattern due to the occurrence of straight holidays, it conducted a significance test for the number of treatments targeting the fourth week when the yellow dust occurred and the first week and second week when yellow dust did not occur. The study results of conducting the significance test at a 5% significance level by dividing areas into 16 according to the similar concentration of yellow dust showed a similar concentration of conjunctivitis in all areas except for three areas among the 16 cities and provinces, while similar results in rhinitis and asthma were found in six areas and three areas respectively. In particular, the number of all cases on the day when the yellow dust occurred increased by 18~34% in conjunctivitis, 9~10% in rhinitis and 18~19% in asthma.
In this study, we analyzed the concerns, anxieties, and thoughts of citizens in accordance with the occurrence of yellow dust, and confirmed the health damage to the citizens’ diseases. Through this study, we could confirm that the utilization possibility of SNS data including tweet data is very high. In addition, it is judged that SNS data will be appropriately used for providing the measures of local governments if this study is utilized for disasters and accidents as well as health care.I. 서론 1 1. 연구배경 및 목적 1 2. 연구방법 3 II. 연구이론 4 1. 황사와 시민건강 4

Ⅰ. 서론 1 1. 연구의 필요성과 목적 1 2. 연구 방법 및 구성 3 Ⅱ. 문헌연구 4

Voice recognition, electronic medical record, input system, artificial intelligence, system test목 차 국문초록 ⅰ Abstract ⅳ

제 4차 산업혁명의 도래로 컴퓨터의 연산속도가 증진되고 적용 가능분야가 확대되면서 인간의 업무를 일부 대체 가능한 로보틱스의 등장이 두드러졌다. 특히 1990년대 단순 매크로로 대변되는 SDI/SDT 기술에서 더 나아가 특정 규칙성 및 Rule-Based 업무처리 가 가능한 현 RPA(Robotic Process Automation) 단계에 이르기까지 인간의 단순반복업무는 상당 부분 로봇에 의해 대체되는 추세를 보여왔으며, 인공지능 및 머신러닝과 같은 학습 기반의 자연어처리가 가능한 지능형 로봇이 앞으로 발전됨에 따라 그 적용범위는 보다 확장될 것으로 예측된다.
다만 현재 비금융권 대기업 위주로 확산되고 있는 RPA 도입은 주로 오퍼레이션 위주의 Back-Office 업무에 적용되어 왔으며, 인건비 감축 및 프로세스 개선을 통한 비용절감이 주 효과로 인식되어왔다. 금융 산업에서도 콜센터 챗봇(Chat-Bot) 및 자동 여신심사 부문 등에서 RPA가 활용되는 모습을 보여왔으며, 이로써 비용절감을 수행해내는 금융사들이 다수 등장하였지만 실질적으로 대고객 서비스 분야에 RPA를 도입하였던 사례는 현재까지 소수였다. 이에 따라 현재 S 은행에서 실제로 운영 중인 자동화 기반 대고객 셀프 서비스에 대한 고객들의 인식을 조사하여 어떠한 심리적 도입장벽들이 유효하고 우리나라 고객들이 이와 같은 서비스를 수용할 준비가 충분히 되어있는지 검증하였다.
또한 실제 S은행이 RPA 도입을 통해 비용절감을 도모하였던 분야에 대해 현업자 인터뷰를 기반으로 비용절감 효과를 추정하여 실제로 몇 퍼센트 가량의 효율성 증진 결과가 도출되었는지 보임으로써 금융사 입장에서 RPA를 대고객서비스 분야에 적극 도입하는 것이 적합하다는 것을 밝혀내었다. 그리고 이를 바탕으로 앞서 언급하였던 지능형 로보틱스의 확대적용이 이루어진다면, 포화상태에 도달했다 평가되는 현 금융시장에서 효율 증진을 통해 경쟁사 대비 비교우위를 점할 수 있으리라 판단된다.

최근에는 4차 산업 혁명과 함께 인공지능에 대한 관심도가 높아지고 있다. 인공지능은 지금까지 다양한 종류의 기계학습 방법을 활용하여 연구되었는데 인공신경망의 발전형인 ‘딥러닝’ 등장 이후, 그 성능이 다른 인공지능 기법들에 비해 우수하기 때문에 최근에는 딥러닝을 중심으로 인공지능 연구가 이루어지고 있으며 종래의 기법으로 해결하지 못했던 다양한 문제들을 풀고 있다.
딥러닝은 이미지 인식에서부터 이미지 생성, 자연어 처리, 게임 인공지능까지 폭 넓은 분야에서 연구되고 있고 지금도 딥러닝을 활용해 새로운 분야에 적용하기 위한 연구 또한 진행되고 있다. 그 중에서도 최근에 각광을 받고 있는 연구 분양 중 하나는 오토인코더(Autoencoder) 기법의 접근법과 같은 잠재 공간(Latent space) 기반의 딥러닝 기법들이다.
잠재 공간 기반의 기법들은 고차원 공간의 복잡한 문제들을 저차원으로 매핑하여 처리하는 기법이라고 볼 수 있다. 종래에도 PCA같이 차원축소를 통해 해결하는 방법도 있었지만, 많은 연구를 통해 딥러닝은 그보다 더 고차원에서도 동작하는 것으로 알려져 있다. 예를 들어 단어를 잠재 벡터로 재표현하기 위한 워드 임베딩(Word Embedding)부터 잠재 공간을 활용하여 이미지를 분류 하는 Siamese Network, 이미지 생성을 위한 Variational Autoencoder 그리고 이미지의 생성 및 분류를 위한 Generative Adversarial Network 등 다양한 분야에 걸쳐서 연구되고 있으며 그 성능또한 인정받고 있다.
본 연구에서는 이러한 잠재 공간을 활용하는 기법 중 하나인 오토인코더를 활용하여 워드 임베딩과 같이 이미지를 잠재 공간에 매핑시키고 학습 데이터에서 이미지의 클래스를 의미하는 콘텐츠 벡터와 클래스와는 관계없는 스타일 벡터를 구분하도록 학습시키는 방법에 대해 제안한다.
실험에는 MNIST와 SVHN 데이터가 사용되었으며 실험 결과, 원본 이미지와 재생성된 이미지가 잠재공간을 통해 매핑되어 있는 것을 이미지로 비교를 통해 알 수 있었으며 잠재벡터간의 연산시 콘텐츠 벡터만의 연산을 통해 원본 이미지의 스타일이 유지되면서 이미지의 클래스가 변경되는 것을 확인할 수 있었다.Deep Learning is a state-of-arts in artificial intelligence topics. Nowadays it solving many problems which cannot solve with typical methods in many domains. Especially in the image domain, it trying to not only recognize but also generate images.
These type of methods, such as Variational Autoencoder and Generative Adversarial Network, are working based on latent space. Unlike word embedding which is a latent-space based method in word domain, the methods can do vector arithmetic with only the mean vector of some images for a single meaning. Because an image can contain many of meaning.
In this study, I propose a latent-space based learning method to separate the content vector which means the class of image and the style vector which is not related to the class of image from training data. I used MNIST and SVHN data in the experiment and it showed that the original image and the reconstruction image mapped through the latent space. Furthermore, it also showed that the style of the image could transfer with another image.제1장 이미지와 오토인코더 1 제2장 관련연구 8 제1절 Deep Learning 8 제2절 Convolutional Neural Network 9

대부분의 중·대규모 프로젝트에서는 체계적인 프로젝트 진행을 위하여 소프트웨어 개발 방법론을 활용하여 소프트웨어 개발의 생산성을 높이고 산출물의 품질을 높인다. 이러한 방법론에서는 소프트웨어를 분석·설계 하기 위하여 보통 UML(Unified Modeling Language)을 사용하여 모델링을 하며, 이 모델을 기반으로 소프트웨어를 구축하기 때문에 모델의 품질은 목표 소프트웨어의 품질과도 직결된다. 그러나 보통 방법론에서는 모델링에 대한 가이드라인을 자연어로 기술하고 있기 때문에 검증이 어려우며, 여러 분야의 전문가가 필요하다는 현실적인 문제점을 가지고 있다.
본 논문에서는 실시간 내장형 시스템(Real-time Embedded System)을 개발하기 위한 방법론인 COMET방법론을 대상으로 하여 전문가의 지식인 방법론의 가이드라인을 기술하고, 이를 통하여 프로젝트의 산출물인 모델을 평가할 수 있는 룰 기반 COMET방법론 가이드라인 평가 프레임워크를 제안한다.
본 프레임워크는 다음과 같은 특징을 갖는다. 전문가가 IF-THEN-ELSE-GUIDELINE 형식의 룰 구조로 모델에 대한 가이드라인을 기술하고, 모델링의 결과물을 XMI(Xml Metadata Interchange)를 기반으로 검색하여 가이드라인의 위배여부를 결정한다. 모델러는 프레임워크 수행의 결과물인 가이드라인 위배 정보를 보고 모델을 수정하여 프로젝트 산출물의 품질을 높일 수 있다.
본 논문에서는 프레임워크의 유용성을 검증하기 위하여 지능형 로봇의 주행 시스템을 비전문가가 모델링한 결과에 프레임워크를 적용하여 보았고, 그 결과를 정리하였다.In order to lead some project systematically, a software development methodology which is suitable for the projects is needed. And by using the methodology, the artifacts of the project can have good quality. In these projects, the UML (Unified Modeling Language) is generally used to analyze and design target software systems. And the target software systems are implemented based on the UML model artifacts, so that the quality of UML model artifacts is directly associated with the quality of target software systems. However, since the guidelines for the UML modeling in the methodologies are generally described with natural language, it’s difficult to validate its feasibility; so many fields of experts are also needs for that.
In this paper, we focused the guidelines of the COMET methodology for the real-time embedded system, and proposed the guideline evaluation framework for COMET methodology which can describe those guidelines, and evaluate the model based on the pre-described guidelines.
The characteristics of this framework are as follows: the methodology experts describe the guidelines for the model with the rule syntax (IF-THEN-ELSE-GUIDELINE). In this framework the model can be accessed with the XMI (Xml Metadata Interchange) which was exported in the modeling tool. The framework evaluates the XMI model by using the pre-described rule and finally generates a guideline violation report for the model. Then, the modeler can improve the quality of the project’ artifacts with the guideline violation report which was generated in our framework.
The case study (Intelligent Robot Navigation System) was used to validate the framework in this paper.

서비스 지향 아키텍처(SOA, Service Oriented Architecture)와 클라우드 컴퓨팅(Cloud Computing)은 정보기술 분야의 새로운 패러다임으로써 정보기술 환경 변화의 혁신적인 이슈이다. 이러한 변화에 따라 SLA(Service Level Agreement, 서비스 수준 협약)가 많은 사람들의 주목을 받고 있다. SLA 관리시스템이 IT 서비스의 품질을 보장하면서 안정된 서비스를 고객에게 제공하기 위해서는 SLA에 기초한 IT 서비스 관리(ITSM)를 통하여 서비스 가용성 및 장애관리, IT 기반자원관리, IT 서비스 변경 및 구성 관리를 수행해야한다. 서비스 관리 자동화는 이러한 서비스 기반 기술의 핵심요소이다.

최근 상업적인 SLA 관리 도구는 어플리케이션 코드 또는 데이터베이스 계층에 계약 로직을 감추고 있다. 즉 SLA 규칙들이 암시적으로 표현되고 때로는 수정이 힘든 경우도 있다. 이것은 분산된 계약 조항 공유, 광범위한 재구현의 노력 없이 새로운 요구사항 반영, 자동적으로 유지관리와 모니터링, 그리고 다량의 SLA 실행 등의 작업들을 어렵게 한다.

본 연구의 목표는 서식과 지식을 함께 내장하고 있는 능동문서인 ASLA를 만들어, 현재의 SLA가 가지는 문제점을 해결하는 것이다. 본 논문에서 제안하고 있는 ASLA 아키텍처는 제한된 범위 내에서 자동화가 가능하고 실제 ASLM 시스템에 적용이 가능하다. Prova와 Prolog로 표현된 ASLA을 이용한 실험을 통해 현재 SLA의 문제점들은 ASLA에 의해 해결될 수 있음을 확인하였다.

본 논문에서 제안하는 ASLA는 기업의 업무프로세스나 업무문서 등이 이질적이어서 계약문서를 교환하거나 공유하기가 쉽지 않은 영역에 계약문서 교환의 표준으로 사용할 수 있다. 또한 기업간의 서비스 수준 계약 관리 업무를 자동화시켜 업무를 효율적으로 수행하는 데 활용할 수도 있다. 아울러 ASLA는 기업 고유의 업무를 외부에 위탁하는 아웃소싱 시장 확산에 기여할 수 있다.[목 차] [국문 요약] ⅲ [목 차] ⅴ [표 목 차] ⅷ

최근 모바일 환경의 발달로 인해 SNS(Social Network Service) 등으로 부터 생산되는 데이터의 양이 급격하게 늘어나면서 비정형(unstructured) 혹은 반정형(semi-structured) 문서로부터 의미 있는 정보를 찾기 위한 정보 추출(Information Extraction) 기술에 대한 연구가 진행되고 있다. 정보 추출은 자연 언어(Natural Language)에 대한 이해가 요구되며 자연 언어 처리(Natural Language Processing)를 위한 대한 많은 방법들이 연구되었다. 그러나 현재 자연 언어 처리 기술로는 자연언어를 완벽하게 이해할 수 없기 때문에 모든 문서로부터 원하는 정보를 찾기에는 다소 무리가 따른다. 하지만 신문 기사와 같이 제한된 도메인의 문서로부터 사건, 사고 등의 정보를 추출하는 것은 해당 응용에서 필요한 정보만 분석하면 된다. 이와 같이 완벽한 자연어의 이해가 요구되지 않는 수준의 정보 추출 방법들이 연구되어 왔다.

본 논문에서는 최근 자연 언어 처리 분야에서 핵심적인 영역으로 인식되고 있는 정보 추출 문제를 위하여 기계 학습 기반의 의미역 결정 및 관계 추출 연구를 수행한다. 먼저 의미역 결정 문제를 위하여 structural SVM을 이용한 전이 기반(transition-based)의 한국어 의미역 결정 시스템을 구축한다. 또한 전이 기반 파싱(Parsing) 알고리즘을 의미역 결정 문제에 알맞게 수정된 알고리즘을 사용하였으며, 의미역 결정 문제를 다중 분류(multi-classification) 문제로 바꾸어 학습 및 실험을 진행한다. 두 번째로, 한국어 관계 추출 문제를 위하여 딥러닝(Deep Learning)을 이용한 임베딩 기반(embedding-based)의 한국어 관계 추출 시스템을 구축하고, 학습 데이터의 부족 문제를 해결하기 위하여 distant supervision 방법을 기반으로 자동으로 구축한 학습 데이터를 말뭉치로 이용한다.

실험을 통하여 한국어 의미역 결정의 성능은 논항 인식/분류(AIC)에서 68.32%(F1)의 성능을 보였으며, 한국어 관계 추출의 성능은 84.15%(F1)의 성능을 보였다.Information extraction (IE) is an important part of natural language processing (NLP) finding meaningful information from unstructured or semi-structured machine-readable documents. There are three typical IE subtasks: named entity recognition (NER), coreference resolution (CR), relation extraction (RE). In spite of continual efforts to understand the natural language texts, however, it is still difficult to find the information what you want from all documents. Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. For example, Extracting accidents or events from the news articles and prices on the web pages are comparatively easy problem to solve with current technology.

This paper focuses on the problem of learning to perform relation extraction (RE) and semantic role labeling (SRL) under the machine learning to treat information extraction problem. RE is sub-task of IE to assign relations between entities such as PERSON born in LOCATION(e.g. John was born in United States). RE also considers that entities have already been detected by a different process, such as a named-entity recognizer. SRL is another IE task to detect predicates in text, choose their correct senses, identify their associated arguments and predict the semantic roles of the arguments. First, We apply transition-based parsing algorithm for Korean semantic role labeling. For its learning, we use a structural SVM. Second, We adopt the embedding method for Korean relation extraction with deep learning. To solve the lack of learning data, we collect data under the distant supervision method. The experiments of SRL achieves 68.32%(F1) on argument identification/classification(AIC) and RE achieves 84.15%(F1).1. 서 론 1 2. 관련 연구 4 2.1. 의미역 결정 관련 연구 4 2.2. 관계 추출 관련 연구 5 3. Structural SVM을 이용한 전이 기반 한국어 의미역 결정 8

인류의 미래를 주도할 첨단 산업으로 주목받고 있는 생명 과학 기술의 급격한 발전으로 인해 생물학 분야의 문헌들이 빠른 속도로 증가하고 있는 추세이다. 이러한 문서들로부터 의미 있는 정보를 추출하는 작업은 많은 시간과 노력을 필요로 하기 때문에, 효율적인 정보 관리 및 정보 추출이 요구된다.
단백질 상호 작용에 대한 연구는 생명 현상의 기본적인 원리를 규명하는데 있어 가장 중요하고 필수적인 작업 과정이다. 생물학 문헌으로부터 단백질-단백질 간의 상호작용 정보 추출 작업은 단순 문자열 비교를 통한 자동 정보 검색으로는 그 한계가 있으므로, 전통적으로 사용되는 자연어 처리 기법과 최근 활발히 연구되고 있는 기계 학습 기법을 이용하여 보다 더 정확한 정보 추출 작업을 수행하고 있다.
본 논문에서는 문장의 문법 구조, 품사 정보 등을 이용한 텍스트 마이닝 (text mining) 기법을 통해 유전자나 단백질의 이름을 자동으로 추출하는 시스템을 설계, 구현하고 이것으로부터 유전자 및 단백질간의 상호 작용 정보를 자동으로 추출하는 시스템을 제안한다.
우선 정확한 유전자, 단백질의 이름을 추출하기 위한 시스템으로 문자나 숫자, 특수 기호를 포함한 단일명의 유전자와 단백질 이름의 품사 처리에 대한 기존 방법들을 개선하고 여러 단어로 이루어진 이름에 대해서도 조건부 확률을 이용하여 처리하는 방식들을 사용하였다. 이러한 방법을 통해 구현한 시스템에서 유전자와 단백질 개체의 추출은 다른 프로그램에 비해 약 10 - 20% 의 성능 향상을 보이고 있다.
다음으로 유전자, 단백질의 상호 작용 추출을 위한 방법으로 베이즈(Bayes) 학습을 사용하였다. 간단한 베이즈(Bayes) 정리를 이용하여 단백질 상호 작용을 나타내는 동사들을 자동으로 추출하고 추출되어진 동사들을 판별 동사 (Discriminating Verb)라 정의한다. 이 동사들은 생명 공학 관련 문서로부터 유전자, 단백질의 상호 작용을 더 잘 나타내는 동사의 집합(set)이다. 이를 이용하여 단백질 사이의 상호작용을 추출하였으며 실험을 통해 84%의 정확도를 확인하였다.Due to the rapid growth of bio-technology which is a promising lead to benefitting humanity, huge amounts of biological literature are expanding everyday. As we have to devote much effort and spend tremendous time to extract useful information from the literature, the development of an efficient, automated information extraction system is required in order to manage the information effectively.
Research on protein-protein interactions is one of the most important and inevitable processes to manifest the basic principle of biological phenomena. Considering the limitations of the extraction systems for protein-protein interactions based on simple methods such as matching sequence of characters, natural language processing and machine learning-based methods have been studied vigorously to obtain more accurate results.
In this paper, two methods have been proposed for protein name extraction. First, we have made improvements on existing methods for handling single word gene/protein names consisting of characters, special symbols, and numbers. Second, compound word gene/protein names are also extracted accurately using conditional probabilities of the occurrences of neighboring words. Our system improved by 10% ~ 20% compared to other methods for extracting protein names.
To extract the protein interactions, the Bayesian learning has been applied. Based on the Bayes theorem, the verb that represents the interaction is extracted automatically, which is defined as the ''Discriminating Verb''. The set of these verbs represents the interaction of proteins in the biological literature. This approach produced 84% of accuracy in extracting such information.

전자전 분야에서의 레이다 신호 분류는 전자전 시스템을 통해 수신된 항공기, 지상 레이다, 미사일 등의 적 위협 레이다 신호의 주파수 또는 펄스반복주기 변조 특성을 분석하여 해당 위협의 종류를 판별하는 문제이다. 최근의 레이다는 대 전자전 능력 향상을 위해 저출력 신호를 사용하여 탐지를 어렵게 하거나 신호 추적을 회피하기 위해 전통적으로 사용되던 방식과 다른 다양한 형태의 변조 방식을 채택하고 있어 기존의 알고리즘으로는 분석하기 어려운 경우가 많다. 또한 실 환경에서 수신되는 신호는 환경 영향에 의한 누락 및 하드웨어의 특성에 따른 측정 오차가 발생하는 경우가 빈번하여 이에 강건한 알고리즘의 설계가 요구된다. 최근에 각광받고 있는 방법인 딥 러닝 기법은 심층신경망을 통해 대량의 데이터를 학습시켜 도메인 전문가의 개입 없이도 컴퓨터가 스스로 데이터의 패턴을 인식할 수 있도록 하는 기법으로서 컴퓨터비전, 자연어 처리, 음성인식 등의 분야에 활발히 활용되고 있다. 본 논문에서는 대표적인 딥 러닝 기법으로 분류되는 Convolutional Neural Network, Recurrent Neural Network 기법을 적용하여 레이다 신호의 변조 특성 및 노이즈 발생에 영향 받지 않는 레이다 신호 분류 방법을 제안한다.제 1 장 서 론 1 1.1. 연구의 배경 1 1.2. 연구의 목적 2 1.3. 논문의 구성 3 제 2 장 관련 연구 4

발전 연료인 유연탄의 가격을 사전에 예측하고 발전 원가를 절감함으로써 공공재로서의 전력가격 안정에 기여하고자 한다. 이를 위해 관리되기 어려운 수많은 종류의 정보를 시기적 효용에 맞도록 관리하는 예측모델을 설계하여, 가격변동성을 적절히 대응 하고자 한다.

다양한 정보를 정량적으로 사용하기 위해 NLP기반 뉴스지표를 생성한다. 사용 알고리즘은 word2vec과 doc2vec을 활용한다.
유연탄 가격과 관련성이 높을 것으로 예상되는 경제인덱스를 수집하고, 상관분석을 통해 경제지표로 관리한다.
가격의 예측은 데이터의 시계열적 특징을 유지하되, 장기 예측이라는 특성을 고려하여 LSTM알고리즘을 활용한다.

현재 시점으로부터 6개월 이후의 유연탄 가격을 예측한 결과 MAPE 20% 이하의 예측결과를 얻을 수 있었다.

본 연구에서 제시된 방법에 의한 가격예측은 예측모델방법은 Loss의 검토결과 유효한 것으로 판단될 수 있으나, 자연어 처리 및 안정적 모델을 위한 후속 연구가 필요한 것으로 판단된다.We intend to contribute to stabilizing electric power prices as a public goods by predicting the price of bituminous coal, which is a generation fuel, and reducing the cost of electricity generation. To do this, we design a forecasting model that manages many types of information that are difficult to manage to meet timely utility, and respond appropriately to price volatility.

Generate NLP-based news indicators to use various information quantitatively. Usage algorithms utilize word2vec and doc2vec.
Economic indexes that are expected to be highly related to the price of bituminous coal are collected and managed as economic indicators through correlation analysis.
The prediction of price maintains the time-series characteristics of the data, but utilizes the LSTM algorithm considering the characteristic of long-term prediction.

As a result of estimating the bituminous coal price after 6 months from the present point of view, the prediction result of MAPE was less than 20%.

The price prediction by the method proposed in this study can be judged to be valid as a result of the review of the forecasting model method, but it is considered that further study for natural language processing and stable model is needed.제 1 장 서론 1 1.1 연구배경 및 목적 1 1.2 사전 연구조사 1 1.3 제안하는 방법의 개요 3

Research shows there are two different kinds of errors that the foreign student makes when he/she, whose mother tongue is English, learns Korean. One is called 'transfer errors' that are caused by the transfer from English to Korean. The other is called 'over-generalization errors' that comes from the student's misunderstanding of Korean grammar.
The goal of this research is to build an intelligent tutoring system. KOTOR, that tutors Korean grammar to American students. This paper describes the design and implementation of KOTOR that identifies the student's errors and remediates them. The characteristic of KOTOR is the explicit representation of grammatical knowledge required to transfer English sentences to Korean sentences in the form of rule. This explicit representation makes it possible to explain the causes of errors and provides a basis for student modeling.
This paper also presents a mixed grammar representing the transformational knowledge and combining English grammar and Korean grammar to diagnose the transfer errors.차례 그림 제1장 서론 = 1 제2장 'ITS (Intelligent Tutoring System) '란? = 3 2.1. ITS의 특성 = 3

오늘날 무선 랜 기술의 성능이 증가함에 따라 널리 사용되는 통신 매체가 되었고, 많은 사용자들이 무선 랜 기술을 통해 인터넷에 접근한다. 이러한 무선 네트워크 사용의 확장으로 인해 보안 이슈도 나타나고 있다. 다양한 분야에서 무선 기술들이 사용되기 때문에 방대한 양의 트래픽에서 특정 트래픽이 정상 트래픽인지 비정상 트래픽인지 판단하기가 어려워졌다.
이러한 보안 문제를 해결하기 위해 무선 네트워크 환경에서 이상 트래픽을 검출하는 침입 탐지 시스템과 관련된 연구들을 찾아보았다. 그 중 한 침입 탐지 시스템은 트레이닝 단계에서 만들어진 정상 모델을 기반으로 이상 트래픽을 검출한다. 하지만 정상 모델을 한 번 생성한 후 더 이상 학습을 하지 않기 때문에, 잠재적인 정상 트래픽을 비정상 트래픽으로 잘 못 판단하는 비율이 높아지게 된다. 따라서 본 논문에서는 자연어 처리 분야에서 사용되는 N-gram 모델을 적용해서 IEEE 802.11 프레임으로 구성된 패턴에 대해 확률을 계산하여 정상 트래픽인지 아닌지를 판단하는 침입 탐지 시스템을 제안한다. 제안한 시스템의 주된 목적은 기존 침입 탐지 시스템보다 정상 트래픽을 비정상 트래픽으로 잘 못 판단하는 비율을 낮추는 것 이다. 이 시스템은 N-gram Smoothing 알고리즘을 이용하여 관찰된 트래픽의 패턴에 대한 확률을 계산하여, 확률 값이 특정 임계치를 넘어서면 정상 패턴으로 간주하고, 정상 모델에 해당 패턴을 추가하여 계속 학습해 나가는 시스템이다.
제안한 침입 탐지 시스템의 성능을 평가하기 위해 두 개의 실험을 진행하였다. 첫 번째 실험은 관찰된 패턴이 정상인지 비정상인지를 판단하는 임계치 값을 정하기 위한 실험이고, 두 번째 실험은 기존에 존재하는 침입 탐지 시스템과 정상을 비정상으로 판단하는 비율과 비정상을 정상으로 판단하는 비율에 대해서 비교하는 실험을 진행하였다.
본 논문에서는 와이파이(Wi-Fi) 환경에서 조건부 확률 모델인 N-gram 모델을 적용하여 기존 시스템보다 정상 트래픽을 비정상 트래픽으로 판단하는 비율을 감소시켰고, 수많은 트래픽 중에서 새로운 패턴의 트래픽이 정상인지 아닌지를 판단하는 시스템을 제안하였다.Today, as improving the performance of wireless LAN technologies, the wireless LAN has become a communication medium that widely used. Currently a number of people access the Internet via wireless LAN technologies. This expansion of wireless network availability has resulted in security issues. It is difficult to decide whether new traffic among enormous traffic is normal or anomaly traffic because wireless technologies are used in the large scope.
In order to solve these issues, we need original researches related with anomaly detection in the wireless network. An existing anomaly detection system cannot detect potential normal traffic because it detects traffic based on normal model that has not updated since it is created during the training phase. So, it increases the false positive rate. In this paper, we suppose the anomaly detection system that is applied the N-gram model that used in speech and language processing in order to detect normal traffic by assigning probabilities to patterns that consist of sequences of wireless LAN (IEEE 802.11) frames. The main purpose of the proposed system is to decrease the false positive rate than the existing anomaly detection system. The proposed system determines whether a normal pattern or an anomaly pattern by calculating probability of a pattern using the N-gram Smoothing algorithm. If the probability value of the pattern exceeds a specific threshold, the normal model is updated by adding this pattern.
To assess the performance of this system, we executed two types of tests. In the first experiment, we decided a threshold that is the criteria that judges whether a normal pattern or an anomaly pattern. In the second experiment, we compared this system that is proposed in this paper and an existing system in terms of the false positive rate (FPR) and the false negative rate (FNR). Finally, the contribution of this paper is that it proposes an intrusion detection system that can detect whether new traffic among enormous traffic is normal or anomaly traffic and decreases the false positive rate than the existing system by applying the N-gram model that is conditional probability model in the Wi-Fi environment.ABSTRACT IV I. Introduction 1 II. Related Works 3

하루가 다르게 과학 기술이 발전하고 있지만 지금까지 나온 홈 관리 시스템들은 단순한 기기들의 작동으로 이루어진 것들이 대부분이어서 여러 가지 발생할 수 있는 상황에 대해 유연하게 대처할 수 있는 능력을 갖춘 시스템들은 아직까지 제대로 개발되고 있지 못한 실정이다.
따라서 본 논문에서는 지식기반 시스템을 이용한 지능형 홈 관리 시스템을 구성하고 구현한다. Intelligent and Integrated Security and Safety ((IS)2) System이라고 명명한 이 시스템의 구성은 홈 네트워크의 편리한 확장성을 위해 X10프로토콜을 기반으로 하였고, 시스템간의 연동을 고려해 COM(Component Object Model) 기반의 전문가 시스템 개발 도구인 CLIPS를 사용하였다. Knowledge Base를 구성하기 위해서 자연어에 가깝도록 Semantic Primitive를 이용하여 RULE을 작성하였으며 Temporal Reasoning으로 발생 시간까지도 고려한 유연한 추론을 함으로써 정확한 상황 판단 및 조치를 취할 수 있고, RESCUE(Remote Supervisory Customer Extension) System Server를 통해 원격지에서도 감시 및 제어를 가능토록 하였다.
지식기반 시스템을 이용한 (IS)2 System의 구현으로 보다 안전하고 편안하며 지능을 갖춘 smart home의 실현 가능성을 알아볼 수 있었다.제 1 장 서 론 8 제 1 절 연구의 배경과 목적 8 제 2 절 논문의 구성 9 제 2 장 홈 지능관리 시스템의 구성 10 제 1 절 (IS)2 System의 구성 목적 10

The purpose of this paper is to make the aggregate of corruption-related legal knowledge into interchangeable information by utilizing computer systems. This is based on the view that legal knowledge about corruption is a sort of data or legal contents. This book focuses on building a legal ontology as a methodology structuralizing these legal datum. A legal ontology is a part of ontology, that is defined as ‘explicit conceptualisation of domain’. It makes legal knowledge to be sharable and reusable. Furthermore, it can be used to establish knowledge base systems or create expert systems.
A research on corruption has accumulated various fields of study, such as law, public administration, and economics. There are also many legal systems related to curb corruption. These researches and laws related to corruption play a role as a fundamental data to establish legal ontology that has a domain of corruption. The legal ontology presents and structuralizes legal information of corruption. This is because the legal ontology is a tool of a presentation of knowledge. The ontology shows a process of knowledge engineering that corruption-related researches transform into legal information in computer systems. A legal knowledge about corruption can be said to be an area chosen for concrete presentation. In other words, the legal ontology about corruption aims to structure the contents of scattered acts, statutes related to the domain. The purpose here is to explore a little further into making a platform of knowledge that allows computers and researchers to interact with each other through the ontology. We may consider the subject under three headings: (1) Searching for a New Methodology for corruption research; (2) Structuralization of corruption-related data; (3) Building a legal ontology about corruption.
First of all, we need to look back on previous studies about corruption. To put it more precisely, legal studies related to corruption have not been organically structured individual achievements. It means that no historical theorem or accumulation of discourse on issues was made in the study of corruption-related legal systems. In the operation of the legal system similarly, the accumulation of relevant knowledge was not effectively achieved due to the limitations that it was difficult to ensure the continuity of the work.
The lack of accumulating knowledge would arise from not understanding the importance of a formality which is able to systematize results of studies and operation about corruption-related legal systems. In other words, there is little attempt to apply new methodologies to the research environment. There was no awareness of the development of information and communication technology(ICT) and the connection of academics. In particular, the problem of corruption effects on the entire of social members, and also is necessary to participate in watching and reporting related to corruption. Therefore, it is worth to pursue the expansion of access to relevant knowledge or systematization of disclosure through the use of computer systems.
In the second chapter, we will begin by considering references of specialized research groups, such as the Korea Association for Corruption Studies. This step defines the pattern of the study and points out the limits. Then this paper observes how to utilize ICT for academic research to find out a new methodology. Especially we took note of Legal Tech and AI & Law in the field of law. Above all, a legal ontology can play an important role in the translation of existing legal knowedge. In this chapter we explain a role (legal) ontologies and their methodology. Many existing ontologies are analyzed because they are a viable resource for construction in the form of predefined concepts and relations.
In the third Chapter, we describe to structuralize the data related to corruption in order to refine the underlying contents as a previous process to build a ontology. Firstly, we inquire into the concept of corruption, the causes and the characteristics of corruption. In particular, the characteristics of corruption unique to our country were examined according to the fields such as political, systemical, social-cutural, and private sector, and the hierarchy and types of the codified corruption-related legal system were defined. In addition, from a comparative legal perspective, other countries' corruption-related legal systems were reviewed.
The forth chapter describes the process of actually building a legal ontology about corruption. The domain specific ontology represents the contexts of the corruption-related legal systems which are structuralized into obligated, prohibited(disallowed) and allowed norms, and this classification is the normative status of a behaviour with respect to a certain norm. The built ontology provides the basic knowledge on which specific rules to curb corruption and punish corruption crimes like a bribery. The methodology of the ontology involves transforming the legal knowledge derived from the previous stucturalization into classes and properties by using a protégé the tool to make an ontology. The ontology can be used to make a knowledge-based system, a platform for academic research and a legal expert system about corruption.
The work presented in this paper contains only codified legal contexts but it is expected to be used by many ways. If it is upgraded and expanded according to specific uses, it can be shared by various users, such as public officers, task processors, and the general public. Though the ontology, sharing and accumulating legal knowledge of corruption on the Web could lead to the creation of an integrated database on corruption, and the path to creating a corruption AI could be closer. This paper describes the role of a knowledge-engineering that explains the processes of knowledge acquisition, presentation and stucturalization. There can be more discussion and new directions addressed as a way to understand and exchange corruption-related specialized research areas with computer systems. Moreover, we hope that this paper serves a momentum to make a new research environment to be able to organize and accumulate knowledge by using various methods of knowledge expression, including a ontology, in various fields of law and even across disciplines.이 연구는 부패에 관한 법제도 및 관련 연구성과 등을 법률콘텐츠 또는 데이터라는 측면에서 바라보고 이를 종합하여 컴퓨터시스템에 탑재하여 공유할 수 있는 방안을 모색하는 것을 목적으로 하였다. 즉, 부패 관련 법지식의 총체를 컴퓨터에 축적하여 상호 교환할 수 있는 정보 및 지식으로 만들기 위한 연구를 추구하였다. 그 수단으로서 법률 온톨로지를 구축하는 방법을 취하였다. 법률 온톨로지는 법 분야의 일정한 개념에 관한 형식적이고 명시적인 명세를 의미하는 것으로서 특정한 개념에 관한 공통 요소를 형식화하여 지식의 공유와 재사용을 가능하게 하고, 지식베이스 구축이나 전문가시스템 생성의 기반이 된다.
부패라는 주제와 관련해서는 법학을 비롯한 행정학 등 여러 사회과학적 분야에서 다양한 논점들이 다루어졌고 이를 규율하는 법률도 다양하다. 기존에 이루어진 부패연구와 법률의 구체적인 내용과 해석을 기초 데이터로 전제하여 우리나라 부패 관련 법제도를 대상으로 부패 법률 온톨로지를 구축하였는바, 이는 부패 관련 법률 데이터의 구조화를 표방한다. 즉, 특정한 주제인 부패 관련 연구를 정보화하여 컴퓨터시스템을 활용할 수 있도록 지식화(지식 엔지니어링)하는 모습을 보여주고자 한 것이다.
연구의 방향은 기존의 부패 관련 연구에 대한 반성적 고려에서 출발한다. 다른 분야와 마찬가지로 기존의 부패 관련 법제도에 대한 연구들은 문제의 산발적 지적과 해결책 제시에 그치고 연구자들 상호간 유기적인 소통이 이루어지지 아니하였다. 즉, 부패 법제도에 관한 연구에 있어서 여러 쟁점이 계속 논의되고 법령의 개정에 일부 반영될 뿐 쟁점의 논의에 대한 역사적 정리나 담론의 축적이 이루어지지 못하였다.
이러한 문제점은 연구가 대부분 비정형문서의 형태로 이루어져 정형화된 지식 기반이 확립되지 않았다는 현실에 기인한다. 연구의 성과나 제도운영에 관한 데이터들을 체계화하거나 메타데이터를 구축하는 등의 접근에 대한 공감대가 형성되지 못한 것을 원인으로 볼 수 있다. 즉, 기존의 부패 법제도의 운영이나 연구 환경을 정보통신기술의 이용으로 개선할 수 있는지에 대한 고민이 부족하다고 보았다. 기술의 발달은 학문연구나 제도운영과 상호 무관할 수 없다는 인식을 바탕으로 한 것이다. 특히, 부패에 관한 법제도의 연구나 운영의 경우 부패의 다양한 양상에 대한 이해와 대처, 부패문제에 대한 높은 사회적 관심이나 부패 관련 신고 활성화 등 일반 국민의 의식 전환 및 참여가 중요한 분야이기 때문에 컴퓨터시스템의 활용을 통하여 관련 지식의 접근가능성 확대나 담론의 공개화 등을 추구할 필요가 있는 것이다.
본 연구의 흐름은 다음과 같다. 먼저, 종래 부패와 관련한 연구의 양상과 제도 운영의 현황을 살펴보고 그 한계점을 도출하고자 한국부패학회 등 부패 관련 전문연구단체의 문헌들을 종합하고 연구패턴을 분석하였다. 한편, 새로운 방법론을 모색하기 위하여 정보통신기술의 발달로 인하여 학문연구의 방법이나 주제에 변화가 생기는 모습에 주목하였고, 특히 법학 분야에서 리걸테크나 AI & Law 분야의 발달을 살펴보았다. 그리고 법률 온톨로지의 개념과 역할, 기존의 구축 사례에 대한 분석을 통하여 부패 관련 법률 온톨로지의 설계 방향 및 구체적 방법론을 정립하였다.
실제 온톨로지의 구축에 앞서 그 바탕이 되는 내용을 정제하기 위하여 부패 관련 데이터를 구조화하고자 하였다. 먼저, 부패에 관한 이론적 기초를 정리하여 부패의 개념, 원인과 우리나라에 고유한 부패의 특성을 분야에 따라 살펴보고, 성문화된 부패 관련 법제도를 대상으로 체계와 유형을 파악하고 종래 논의된 법적 쟁점을 정리하였다. 이와 함께 비교법적 시각에서 다른 나라의 부패 관련 법제도를 검토하였다. 이러한 배경지식을 바탕으로 부패 법제도의 범위와 구조를 파악하고, 개별구체적 법제도를 의무규범, 금지규범 및 허용규범으로 항목화하였다. 항목화된 법제도의 내용을 근거로 부패 법률 온톨로지를 구축하였다. 개별·구체적 법제도가 구조적으로 어디에 위치하고 각 제도가 어떤 관계를 갖는지에 대하여 규명하고, 클래스와 프로퍼티로 구성되는 온톨로지 요소를 정의하여 프로티지라는 도구를 활용하였다. 이렇게 구축한 부패 관련 법률 온톨로지는 부패 관련 지식베이스나 전문가시스템을 개발하기 위한 기반 지식에 해당한다. 위 온톨로지의 효용을 구체적으로 설명하기 위하여 부패 법률 온톨로지를 기초로 부패 연구의 플랫폼이나 부패 법지식기반 데이터베이스에 사용되는 지식 표현의 양상을 설명하였다.
본 연구에서 구축한 부패 법률 온톨로지는 관련 법령의 내용만을 반영한 것이지만, 향후 이를 고도화하고 구체적 용도에 따른 확장을 거듭하게 된다면 공무원, 업무처리자나 일반국민 등 다양한 사용자에 의하여 공유될 것으로 본다. 이러한 부패 법률 온톨로지를 바탕으로 부패 지식을 웹상에서 공유하고 축적한다면 부패에 관한 지식기반 데이터베이스를 구축할 수 있고, 부패 인공지능을 만드는 길도 가까워질 수 있을 것이다. 필자는 지식 엔지니어링의 입장에서 부패 관련 전문연구의 영역을 이해하고 지식화하여 컴퓨터시스템과 상호교환할 수 있는 방법으로 법률 온톨로지의 구축을 시도하였으나 앞으로는 온톨로지에 국한하지 않은 더 많은 지식 표현의 방법에 대한 논의가 펼쳐질 것이다. 이를 통하여 새로운 연구의 장이 펼쳐지기를 기대한다.제1장 서론 1 Ⅰ. 연구의 목적 1 Ⅱ. 연구의 방법과 범위 5 제2장 새로운 부패 연구 방법론의 모색 9 Ⅰ. 부패 법제도 연구의 현황과 한계 9

노동시장의 불안정은 산업과 직업 간 노동이동을 증가시키고 있다. 이러한 산업‧직업 간 직업이동성 증가에 따라, 범생애에 걸친 ‘수평적 다양성’과 통생애에 걸친 경력이동에 의해 체화되는 ‘수직적 융합성’이 동시에 촉진되고 있다. 직업‧경력의 다양성과 융합성은, 국가차원에서 인적자본축적과 개인차원의 숙련형성 및 경력개발 과정에 지체와 손실을 주며, 특히 국가인적자본축적에도 비효율과 소모적 비용을 확대시키고 있다. 따라서 국가차원에서 ‘산업인적자본축적’을 위한 방향성과, 해당 산업분야에 종사하는 전문 인력들의 ‘개인 차원에서 직업 비전’이라는 두 가지 관점을 아우르는 연구가 필요하다.
이 연구는 기계산업에서 요구하는 직업능력의 지식‧기술을 파악하여 색인화하고, 직업능력 간 지식‧기술의 유사성을 분석하며, 이를 통해 직업 내 또는 직업 간 공유하고 있는 지식‧기술(Uks)의 정도에 따라 기술인력이 단계적‧점증적으로 숙련을 형성하는 과정을 선형적 경로로 탐색하였다.
분석단계에서는 한국기계산업진흥회에서 개발한 기계산업 분야 국가직무능력표준을 활용하였다. 기계산업 분야 국가직무능력표준에서 제시된 132개 능력단위에서 자연어로 기술된 1,386개의 지식‧기술(Uks) Text를 추출하였고, 이를 Text mining을 통해 366개 지식‧기술(KSx)로 표준화‧색인화 하였다. 색인화된 366개의 지식‧기술(KSx)을 설명변수(독립변수)로, 1,386개의 지식‧기술(Uks)로 설명되어 지는 7개 직업군(Oj), 21개 직업(Jj), 132개 능력단위(Ux)를 각각 종속변수로 하여 대응일치분석과 계층적 군집분석을 하였다.
능력단위의 계층적 군집분석을 통해 지식‧기술(Uks) 유사도에 따른 위계적‧계층적 군집유형을 덴드로그램을 통해 확인하였다. 또한 기계산업 분야의 132개 능력단위(Ux)의 계층적 군집분석 결과를, 직업과 직업능력수준의 2차원 행렬 형태로 도식화한 능력단위 맵으로 변환하여, 단계별 능력단위의 군집유형을 능력단위 간의 연결로 나타냈다.
이로써 국가직무능력표준의 능력단위를 기준으로, 지식‧기술(Uks)의 공유정도에 따라 이동 가능한 직업 내 숙련형성 경로와 직업 간 숙련형성 경로를 위계적 선형으로 분석하였다. 이 결과는 기계산업 분야 기술인력이 입직단계에서 부터 전문가로 성장하기 까지 발전하는 과정에서 나타나는 숙련형성과정의 선형적 경로인 것이다.
이와 같이 직업과 직업을 구성하고 있는 지식‧기술, 즉 숙련형성에 요구되는 구체적인 정보는 직업적 숙련형성과 경력개발에 대한 정보뿐만 아니라 교육훈련‧인사관리‧자격 등의 분야에서도 유용하게 활용될 수 있다. 즉 직업적 숙련에서 요구되는 지식‧기술과 이에 대한 발전단계에서 나타나는 선형성은, 기업 내 직무교육 또는 산업기술인력 양성을 위한 교육훈련 프로그램을 체계화하는데 구체적인 정보를 제공할 뿐만 아니라, 기업 내 채용‧승진‧배치 등 인사관리에도 활용 가능한 정보를 제공하고 있다. 또한 국가기술자격체계에 산업현장의 실제적인 직무를 반영하고 이의 발전단계에 따라 구조화하는데 유용하다.
이번 연구의 방법과 결과로서, 전문 직업인으로서 숙련형성을 계획하는 단계, 이에 따라 직업능력을 개발‧육성하는 실천단계, 직업능력을 중심으로 기업 및 국가 차원에서 인적자본을 전략적 관리‧운영하고 이를 피드백하는 환류단계까지, 개인‧조직‧국가 모두에게 구체적이고 효용성이 높은 정보를 제공할 수 있는 산업별‧직업별 연구의 활성화를 기대해 본다.The uncertainty of labor market is increasing the movement of labor force between industries and occupations. Following the increase in job mobility between industries and occupations, the life-wide 'horizontal diversity' and the 'vertical convergence' internalized by lifelong career movement are being facilitated simultaneously. The diversity and convergence of occupation and career give delay and loss to the accumulation of human capital at national level and the process of skill formation and career development at personal level, and in particular, is expanding the inefficiency and exhaustive cost of the national human capital accumulation. Therefore, a study that encompasses the two viewpoints of the directionality for 'accumulation of industrial human capital' at national level and the 'occupational vision at personal level' of professional manpower working for the pertinent industrial field is needed.
This study identified and indexed the knowledge and skill of occupational ability demanded by the machinery industry, analyzed the similarity in knowledge and skill among occupational abilities, and explored the skill formation path of HRST(Human Resources devoted to Science and Technology) by stage and gradually following the degree of knowledge and skill shared within or among occupations through a linear path.
In the stage of analysis, this study utilized the National Competency Standards(NCS) of the machinery industry field developed by the Korea Association of Machinery Industry(KOAMI). This study extracted the 1,386 knowledge and skill texts(Uks) described in natural language from the 132 competency units presented in the NCS of the machinery industry, and standardized and indexed them into 366 knowledge and skill(KSx) through text mining. This study conducted the correspondence analysis and hierarchical cluster analysis with the indexed 366 knowledge and skill(KSx) as the explanatory(independent) variable and the 7 occupational groups(Oj), 21 skill type(Jj), and 132 competency units(Ux), which are explained with 1,386 knowledge and skill(Uks), as the dependent variable, respectively.
This study identified the hierarchical cluster types following the similarity of knowledge and skill(Uks) through the dendrogram derived from the hierarchical cluster analysis of competency units. In addition, this study converted the dendrogram of hierarchical cluster analysis of 132 competency units(Ux) of the machinery industry field into the competency map which is schematized into the shape of 2-dimensional matrix of skill type and level, and indicated the cluster types of competency units by stage into the connection(similarity) among competency units.
This study analyzed the path of skill formation within skill type between which one can move following the degree of sharing of knowledge and skill(Uks) and the path of skill formation within or among skill type through hierarchical linear with the competency units of NCS. This result is the skill/career formation path that appears in the process of the HRST in the field of machinery industry to develop from the stage of beginning into the growth as an expert.
Like this, the concrete information demanded for occupations and the knowledge and skill composing the occupations, that is, the skill formation, can be usefully utilized for not only the information on the occupational skill formation and career development but also the fields of education and training, personnel management, qualifications, and so on. That is, knowledge and skill demanded by the occupational skills and the linearity appearing in the stage of development of that not only provides concrete information to the systematization of education and training programs for job training within enterprises or nurture of HRST but also provides the information that can be utilized to the personnel management within enterprises including employment, promotion, placement, etc. In addition, they are useful to reflect the actual job in the industrial settings to the national technical qualification system and to structuralize following the development stage of that.
It is expected that the studies by industry and by occupation that can provide the concrete information with high utility to individuals, organizations, and countries will be revitalized with the methods and results of this study from the stage of planning the skill formation as professional men to the stage of practice to develop and foster the occupational ability and to the stage of reflux to strategically manage and operate the human capital at business and national level focusing on occupational ability and to make a feedback of that.Ⅰ. 서 론 1 A. 연구배경 및 목적 1 B. 연구문제 및 범위 3 C. 용어의 정의 4 Ⅱ. 이론적 배경 6

As the numbers of articles that are published in biological domain are increasing at a considerable rate, many researches in the areas of information retrieval, information extraction and text mining have been performed to obtain knowledge automatically. Information retrieval approaches are good for specific topics that the number of related articles is small.
As the number is bigger, searching skill and knowledge acquisition ability are useless, therefore there are many efforts to extract information from literature automatically. But, many approaches have concentrated on specific entities, such as proteins, genes and their interactions. Although many efforts have been made to create databases that store verified information in a structured form, much information still remains in unstructured text.
Recently, natural language processing (NLP) methods have been applied for extracting information on gene regulation, protein phosphorylation and tissue specificity. Because of the inherent complexity of this task, only a few systems have been designed to extract multiple types of relation. Furthermore, because of the complexity of biological sentences, those sentences cannot be easily analyzed with existing NLP tools those are not optimized in biological domain. So, I have developed a system that extracts relations between various categories of biomedical entities without limits in types of relation.
Many existing full parsers that are not tuned to the biomedical domain frequently fail to parse, or their parsing results are often incorrect. This result is caused by complexity of biological sentences. Most sentences in the biological literature are syntactically complex and named entities in them are usually not simple and consist of many words that their morphological tags are various. This makes it difficult to parse biomedical texts.
The basic idea of the proposed method in this dissertation is that sentences in the biological literature are simplified after multi-word substitutions and an existing non-biological full parser can parse these simplified sentences even if the parser is not tuned to biomedical sentences. Then relation information is extracted by analyzing the results of the parser. A relation between two named entities is extracted from the traversing of the sentence structure tree. This is similar to finding a path between two leaf nodes in a tree. To select relation information from paths, I have defined the syntactic structures that contain biological information.
After sentence simplification, the parser parses additional 12% among given 532 sentences and 96% overall. The correctness of parsing results is improved to 96% from 83%. Using the proposed method, protein-protein interactions can be found at a recall rate of 47.45% and a precision rate of 84.61% from sentences.
In an experiment to find the evidence for protein-protein interactions, queries are generated from two given proteins automatically and then abstracts are collected from PubMed. Following these, target proteins and their synonyms are recognized and their interaction information is extracted from the collection. The evidence for 46.32% of the given interactions can be found when the given abstracts are used, the evidence for 68.95% can be found when collected PubMed abstracts are used and the evidence for 87.37% can be found when the given full-text papers are used. If collected abstracts with proper co-occurrence, the evidence for 77% of interactions can be found by the proposed method.
By the method proposed in this dissertation, biological relation information is extracted from biological literature precisely without a biological syntactic parser. The proposed method is very effective in extracting relation information and finding the evidence of relation from biological literature. This makes it possible to obtain relation information from the huge number of literature.1. 서론 1 1.1 연구의 배경과 목적 1 1.2 연구의 내용과 범위 5 1.3 논문의 구성 10 2. 관련 연구 11

최근 다양한 정보채널들의 등장과 소셜 미디어의 사용이 기하급수적으로 증가함에 따라 빅데이터에 대한 관심이 높아지고 있다. 이 같은 현상의 일어나는 가장 큰 원인 중 하나는, 스마트 기기들의 활용 이 대중화됨에 따라 유저가 생성하는 사진, 동영상, 텍스트 과 같은 비정형 데이터의 양이 기하급수 적으로 증가하는 점에서 찾을 수 있다. 특히 비정형 데이터 중에서 텍스트 데이터의 경우, 유저들의 생각, 의견 및 많은 정보를 보다 정확하게 표현하는 특징이 있다. 따라서 텍스트 데이터에 대한 분석을 이용하여 새로운 가치를 창출하고자 하는 시도가 활발히 이루어지고 있다. 텍스트 분석에서 사용되는 대표적인 기술로 오피니언 마이닝이 있다. 오피니언 마이닝은 자연어 처리기술들을 사용하는데, 텍스트 데이터를 입력으로 사용하여 파싱, 필터링 등을 수행한다. 문장의 분류 및 추론에 있어서 문서가 긍정이나 부정의 감성을 가지는 경우에 오피니언 마이닝으로 감성분류를 수행할 수 있다. 최근 많은 연구에서는 감성분류의 정확도를 높이기 위한 감성사전의 필요성을 연구하고 그 중요성을 입증하고 있다.
본 연구에서는 Word2vec을 이용한 새로운 그래프 기반 감성사전 구축 방법을 제안한다. 그래프에서의 가중치를 계산하기 위한 기존의 유사도 계산 방법인 PMI(Point-wise Mutual Information)는 단어들이 문장의 맥락이나 문맥 상 의미를 통한 단어 간의 관계를 전혀 고려하지 않은 유사도 계산식이다. 이를 개선하고자 단어의 의미와 문장에서의 맥락을 고려하여 단어를 벡터로 표현한 Word2vec 방법을 이용하여 새로운 그래프의 가중치 계산식을 제안하였다. 또한, 세 가지 LP(Label Propagation) Algorithm에 새로운 Word2vec 가중치를 적용하여 감성사전의 정확도를 향상시켰고, 각 Algorithm들의 비교를 통하여 감성사전을 최적화하였으며, 실험을 통해서 성능 향상을 확인하였다. 감성사전과 새로운 텍스트에 대한 감성분류의 정확도를 비교하기 위해 영화평 5,000건과 상품평(mp3) 1,500건에 대한 실험을 수행하였다. 영화평 데이터를 기반으로 수행한 실험에서 Word2vec 가중치 계산식을 사용했을 때, 감성사전의 정확도가 약 8% 향상되었고, 구축된 감성사전을 이용하여 새로운 문장에 대한 감성분석에서는 최대 3%의 정확도가 향상되었다.

소프트웨어개발 프로젝트에 적용되는 방법론이나 품질인증제도들은 프로젝트의 성공과 프로세스의 개선을 위해 다양한 활동과 각 단계에 따른 산출물의 작성을 제안한다. 하지만, 규모가 작은 프로젝트나 조직에서 복잡한 단계와 많은 산출물을 감당하기는 쉽지 않다. 이에 따라, 최근에는 작동하는 프로그램을 만드는 것을 최대 가치로 하는 애자일(Agile)방법론 같은 경량화된 방법론이 제안되었다. 애자일 방법론은 짧고 반복되는 개발주기로 인해 요구사항의 변경에는 기민하게 대처할 수 있지만, 문서화의 부재라는 문제점도 안고 있다.
본 논문에서는 추가적인 문서작업 없이 원시코드에 작성되어 있는 주석을 이용하거나 첨삭하여, 원시코드에 구현되어 있는 비즈니스 로직과 고객의 요구사항을 추출해 낼 수 있는 주석 작성에 대한 가이드라인을 제시한다. 이것은 자연어로 작성된 유저스토리 이외에 다른 요구사항 문서를 작성하지 않는 에자일 방법론에서 고객과의 의사소통, 프로그램 테스트나 오류수정, 유지보수 시에 시스템을 분석하기 위한 근거로 사용될 수 있다.
또한 가이드라인에 맞게 작성된 원시코드를 도식화된 HTML문서로 출력 해 주는 분석기 프로그램을 구현하여 제시된 가이드라인이 실제 요구사항을 얼만큼 포함해 낼 수 있는지 검증하였다.Software development project methodology or Process improvement model such as CMMI suggest variety activities and writing documentation based on each steps for success of projects and improvement of processes. Therefore, in these days, right-weight methodology such as Agile Method which emphasizes developing working software is the most important value is suggested. Because of the small and iterated release, Agile Method can deal with requirement changes quickly, but non-existence of documentation is one the problems.
This thesis suggests the Comment Guide Line can extract requirements from source codes without another documentation working by adding and removing comments at source codes. To communicate with customers, test or debug program code and maintain system, this can be used to basic materials at Agile Method which don’t have any documentation except user story written by natural languages.
Also, through implementing parsing software to create HTML from source code include well-formed comments, this thesis prove how many real requirements can be extracted.

고도의 기술력과 인력을 요구하는 첨단 산업은 제조 환경이 글로벌화 되어있을 뿐만 아니라 제조 단가가 매우 높다. 또한 대규모의 기술 투자 및 대규모 생산시설을 구축하기 위해 매우 큰 투자가 요구되어 산업 특성상 실패 했을 경우 큰 손실을 입게 되므로 정확한 가격 예측이 중요하다 할 수 있다. 수출과 수입의 비중이 높은 산업으로 환율에 따른 원자재 가격의 변화는 생산 단가의 변동으로 이어지게 된다. 따라서 그동안 수많은 선행 연구에서 환율 예측 모형을 구축하였으나 기존의 통계적 또는 계량경제 모형이 주식가격, 환율 등과 같이 예측하기 어려운 비선형적인 특성을 가지는 시계열 자료의 분석에 일정한 한계를 보임에 따라 시계열 자료의 대안적 분석 모형으로 인공신경망이 제시되었다. 더 나아가 최근에는 인공신경망의 문제점을 보안하기 위해 딥러닝 기법이 다양하게 발달하고 있는 바 이에 본 논문은 최근 이미지 인식, 음성 인식, 자연어 처리 등 다양한 분야에서 우수한 성능을 보여주고 있는 합성곱 신경망(Convolutional Neural Network)을 환율 예측에 적용하여 시계열 데이터 분석에서의 성능을 평가하는 것을 목적으로 한다. 딥러닝 알고리즘 중 신경망의 여러 단점들을 해결해주기 위해 합성곱 신경망을 이용하였으며 환율에 영향을 줄 수 있는 변수 6개를 선택하여 환율의 예측을 시도하였다. 분석에 사용한 데이터는 2000 년 1 월 3 일 부터 2016 년 12 월 31 일까지의 일별 데이터이다. 모형 구축을 위하여 학습용 데이터와 구축된 모형의 검증을 위한 검증용 데이터의 두 부분으로 분할하였다. 검증용 데이터를 학습된 신경망 모형에 입력하여 환율을 예측한 결과, 실제 값과 어느 정도의 차이를 보여주었지만 환율의 흐름을 파악하는 것이 가능하였고 인공신경망으로 모델링한 선행 연구와 비교했을 때 좋은 성능을 보였다.

추천 시스템(Recommendation System)이란 사용자들의 관심사나 취향을 고려하여 성향에 부합하는 아이템을 추천함으로써 사용자의 의사결정을 돕는 시스템이다. 추천 시스템에서 가장 널리 이용되는 기법은 비슷한 관심사를 갖는 이웃을 찾고, 이들이 아이템에 대해 평가한 정보를 기반으로 아이템을 추천하는 협업 필터링(Collaborative Filtering)이다. 협업 필터링 기법은 추천을 위해 사용자별 아이템 구매나 평점 등의 선호도 정보를 이용하는데 선호도 정보가 많은 사용자의 경우 정확도 높은 성능을 보이지만 아이템에 대한 선호도 정보가 부족한 신규 사용자에게는 정확한 아이템을 추천할 수 없는 신규 사용자 문제(New user problem)를 갖는다. 이러한 신규 사용자 문제는 목표 사용자가 무엇을 좋아하는지에 대한 정보가 미흡한 상태에서 특정 아이템의 추천 여부를 결정해야하기 때문에 대부분의 연구들은 새로운 사용자 문제를 해결하기 위해 사용자의 인구통계학적 정보나 성향 등 부가적인 정보를 사용한다. 그러나 온라인 쇼핑몰이나 디지털 콘텐츠 서비스를 이용하려는 사용자에게 이와 같은 부가정보를 입력하도록 요구하는 것은 사용자의 만족도를 저하시키는 요인이 될 수 있다.
본 논문에서는 이러한 부가정보의 요구 없이 새로운 사용자 문제를 개선하는 방법으로 말뭉치(Corpus) 내에서의 단어(Word) 관계를 비지도 학습(Unsupervised learning) 방식의 분석을 통해 벡터로 표현하는 자연어 처리 분야의 워드 임베딩(Word Embedding) 기법을 활용하여 아이템을 벡터화하는 새로운 추천 기법을 제안한다.
아이템 벡터화 기법은 관계 분석 대상이 되는 모든 아이템 객체를 제한된 벡터 공간에 임베딩하여 유사도 계산에 의해 인접한 객체를 선별할 수 있도록 함으로써 신규 사용자 문제를 해결한다.
본 논문에서 제안하는 추천 기법은 중심 단어로부터 주위 단어를 유추할 수 있는 모델인 Skip-gram 알고리즘을 추천 시스템에 적합하도록 두 가지 측면에서 개선한 새로운 아이템 벡터화 학습 모델을 통해 사용자 선호도 정보를 기반으로 아이템 관계를 분석한다. Skip-gram에 사용자별 선호도 학습을 위한 새로운 의미단위를 구분 개념을 적용하며, Negative Sampling 알고리즘이 가지고 있는 한계인 긍정적인 객체가 Noise 객체로 선택될 수 있는 문제를 극복할 수 있도록 새롭게 제안하는 Noise Sampling 방식을 사용한다. 아울러 제안한 추천 기법은 학습된 아이템 벡터화 결과에 인기도 가중치를 반영한 추천 목록 생성 알고리즘을 통해 수치화가 가능한 다양한 아이템 속성이 쉽게 Plug-in 될 수 있도록 한다.
본 논문에서는 다양한 평가지표를 이용하여 아이템 벡터화 기반 추천 기법의 성능을 분석하였으며, 새로운 기법이 사용자의 선호도 정보가 부족한 신규 사용자 문제 상황에서 기존 기법에 비해 월등히 우수한 성능을 보임을 확인하였다.A recommendation system refers to a system which helps users’ decision-making by recommending the items which meet their interest and preference. The technique most commonly used in this system is ‘collaborative filtering’ which finds neighbors with a similar interest and recommends items based on their assessment on such items. The collaborative filtering technique analyzes preference information such as item purchase or rating by use for recommendation. In case of the users which a lot of preference information including purchase and rating, high recommendation performances are displayed. In case of new users having no item preference information, on the contrary, it cannot recommend accurate items. This disadvantage of the inability to make a recommendation to new users is called ‘New User Problem.’ Regarding the new user problem, the system has to make a decision to recommend certain items with little information on users’ preferences. To solve the new user problem, therefore, most studies refer to additional information such as demographic information and personal propensity. However, asking online shopping mall or digital contents service users to enter this kind of information can reduce user satisfaction. This study proposes a new recommendation technique which vectorizes items using word embedding as a way to solve the new user problem without additional information. Word embedding, a language model in natural language processing, is a technique which analyzes word relationship in a corpus in an unsupervised learning manner and vectorizes the words. This study learns users’ preference information and analyzes item relationship after finding out that Skip-gram algorithm is a model which can derive words from the key words among the word embedding techniques. In the proposed item relationship analysis technique, the Skip-gram algorithm was altered to classify learning units suitable to preference learning by user. For the improvement of activation functions, the proposed negative sampling technique is adjusted and used in recommendation. Negative sampling is a model which changes item preference attributes and designates them as noise objects. In addition, this study suggests a model which creates a recommendation list by adding popularity weight to the result of the vectorization. The proposed model allows the quantifiable attributes to be easily plugged in. The item vectorization-based recommendation technique proposed in this study revealed upgraded performances under the new user problem in which there was a lack of preference information on target users through diverse experiments, compared to other conventional recommendation techniques.1. 서론 1 1.1 연구 목적 및 필요성 1 1.2 연구내용 요약 3 1.3 논문의 구성 5 2. 이론적 배경 및 관련연구 6

이 논문은 한국어 문장 분석에 필요한 사전 구축과 형태소 분석에 관한 연구 논문이다. 본 논문의 가장 큰 특징은 체계적으로 구성된 사전에 있다. 전통적 사전이 인간을 위한 것이든 기계를 위한 것이든 그 내용이 임의성이 많은데 비하여 여기의 사전은 단순어에 기반을 가지고 체계적이며, 중복성이 없고, 항목 선택의 명확한 기준이 있으며, 또한 구성적으로 구성되었다. 단순어란 의미의 최소 단어로서 통사적 독립성을 유지하고 있다. 한국어에 있어서 컴마나 빈칸등에 의해서 만들어지는 글자들은 여러개의 이들 단순어로 구성되어 있다. 이 단순어들은 레고를 가지고 임의의 인조물을 만드는 것과 같이 문장의 합성과 분해에서 어휘의 확장성을 가지고 있으며 어휘의 특징을 추론할 수 있게 한다. 그들은 특별한 변화없이 문장에서 활용어 형태와 일치된다. 그러므로 활용어 사전의 구축은 형태소 분석기의 대부분을 차지하게 된다. 문장을 분해(해석)하는 데 있어서 모호성이 없는 시스템은 물론 없다. 모호성에 대하여 많은 시스템은 통계나 규칙을 가지고 문제에 대하여 접근하고 있다. 그것은 닫힌 세계에서의 지식의 완전성과 건전성 (Completeness and Soundness)을 자연어에서는 보장하지 않기 때문에 생기는 문제이다. 다시 말하면 언어직 지식은 자동적으로 축적될 수 없으며 그러므로 인간에 의해서 행해져야 한다. 우리의 연구에 있어서, 언어적 지식은 통계나 휴리스틱한 방법이 아닌 오직 언어적 전문가에 의해서 축적을 한다. 다시말하면 이런 언어적 방법이 비록 많은 시간이 걸리지만 가장 정확성을 보장해준다. 언어적 방법에 의해서 구축된 모호성 해결 정보는 우리의 형태소 분석기에서 사용될 것이고, 그 정보의 구축을 위하여 어떤 모호성 구조가 있는가를 분석하는 것이 물론 전제 조건임은 언급할 필요가 없다. 현재 구축한 단순어 사전으로 단순 명사, 명사 구성 접두사, 명사 구성 접미사, 명사 후치사, 동사, 동사 활용어, 형용사, 형용사 활용어 사전등이 제공되고 있다. 이 사전들을 바탕으로, 문장 분석에 필요한 두개의 활용형 사전인 명사화 유형 사전, 술어-활용어 사전, 그리고 명사를 위한 문법적 규칙을 이 논문에서 구축하였다. 이 사전정보와 모호성 해결 정보가 본 논문의 형태소 해석기인 품사 표시기(EPL)에서 활용되고 있다.

이 연구는 동시를 올바로 이해하고 동시를 보다 친숙하게 느끼고 접할 수 있게 하여 폭 넓은 시작품의 수용 자세를 마련해 주기 위해, 읽기 교과서에 실린 동시들에 제시된 시어와 이미지를 분석하여 그것들이 어떤 기능을 하고 있는지, 그리고 그러한 동시들이 아동들의 흥미와 관심사에 부합하는지를 분석하였다. 또한 그러한 동시들을 수업 상황에서 효과적으로 지도하기 위한 방법들을 모색하였다.
Ⅰ장에서는 연구의 필요성과 선행 연구의 문제점을 살펴보고, 연구 대상으로 삼을 텍스트를 제한하여 제시하고 연구의 방법 및 범위를 설정하였다.
Ⅱ장에서는 읽기 교과서에 나타난 동시의 시어와 이미지의 표현에 관한 논의 준거로 삼고자 시어의 의미와 시적 기능에 관한 이론들과 이미지의 의미와 이미지의 기능에 관하여 살펴보았고, 동시의 시어와 이미지를 활용한 동시교육의 본질과 의의를 살펴보았다.
Ⅲ장에서는 전 학년에 수록된 동시 중에서 읽기 교과서에 수록된 동시만으로 국한하여 총 84편을 분석하였다. 시어 분석은 시어의 의미를 유형화하여 물질, 자연어, 추상어, 인물어, 식물어, 동물어, 색채어, 외래어 등 8가지 범주로 나누고, 그것을 다시 학년별로 나누어 아동들의 언어 발달 양상과 비교하는 방법으로 분석하였다. 또한 이미지 분석은 감각적 이미지, 비유적 이미지를 학년별로 나누어 주제 형상화 양상을 분석하였다.
Ⅳ장에서는 시어와 이미지 분석한 결과와 그에 따른 동시 지도 방법에 대해 살펴보았다.
읽기 교과서에 실린 동시를 시어 측면에서 분석한 결과는 다음과 같다.
첫째, 눈에 보이는 사물에 얽매이지 않고 아동의 상상력을 불러일으킬 수 있는 추상어의 쓰임이 다양한 형태로 나타나야 한다. 또한 사람의 감정을 표현하는 시어들이 많이 사용되어야 한다.
둘째, 색채어와 식물어의 쓰임이 저학년에서 적게 나타나고 있으므로 다양성이 요구된다.
셋째, 동식물과 관련된 시어의 다양성이 요구된다. 또한 식물을 나타내는 시어의 경우 ‘~꽃, ~나무’의 형식으로 복합형식이 많았으며 대부분 고학년에 나타나고 있으므로 보다 변화 있는 식물에 관련된 시어들이 저학년과 중학년에도 필요하다.
넷째, 동시의 시어는 다양하므로 인간과 삶과 자연, 우주가 모두 동시의 시어가 될 수 있다. 그런데 읽기 교과서 수록 동시작품들에는 자연에 관한 시어와 주변 사물 위주의 친숙한 시어가 대부분이다. 따라서 우주에 관련된 시어뿐 아니라 다양하고 풍부한 시어의 쓰임이 필요하다.
다섯째, 감각적이며 아름답고 귀여운 시어를 선택하는 것도 중요하지만 아동의 세계를 아동이 실감할 수 있는 아동의 시어로 표현하는 동시가 요구된다.
이러한 결과를 바탕으로 시어의 기능 이해에 초점을 둔 동시교육 방법을 교과서 수록 동시를 통해 생각해 보았다.
첫째, 시어의 기능 이해에 초점을 두어 시어와 일상어의 차이점을 알도록 지도한다. 일상어는 일의(一意)적, 외연(外延)적 의미를 지니고 있는 반면 시어는 다의(多義)적, 내포(內包)적 의미를 지니고 사용됨을 가르쳐 주기 위해 실제 작품의 예를 통해 바람직한 시어관을 갖도록 한다.
둘째, 시를 되풀이 하여 읽으면 읽을수록 마음속에 새로운 의미를 생성하게 되기 때문에 읽기 지도가 잘 이루어져야 한다.
셋째, 동시 작품을 읽은 후, 브레인스토밍이나 마인드 맵 기법을 통해 시어가 환기 시키는 내용을 자유롭게 적어보거나 발표해 보도록 한다.
넷째, 자신의 경험을 떠올리며 시를 바꾸어 써보게 하거나 동일 시어가 쓰인 시를 소개하거나 시어의 의미를 파악할 수 있는 참고자료를 제공한다.
국어 읽기 교과서에 실린 동시의 이미지를 분석한 결과 다음과 같다.
첫째, 감각적 이미지와 관련하여 모든 학년에 시각적 이미지가 많이 사용되는 반면 미각과 후각에 관련된 이미지는 사용이 많지 않다. 따라서 학년별 발달 특징을 고려하여 다양한 이미지의 사용이 나타나는 시 선정이 요청된다.
둘째, 청각적 이미지는 전 학년에 고른 분포를 보이고 있지만 시에 대한 흥미유발을 위해 더욱 많이 쓰이길 바란다.
셋째, 저학년과 중학년에 비해 고학년이 이미지를 형상화하기 위한 어휘 수가 늘어났고 촉각적 이미지와 공감각적 이미지도 많이 나타나고 있고 둘 이상의 이미지가 함께 있는 동시가 전체의 41.6%나 되었다.
넷째, 의인적 표현과 직유적 표현은 전 학년에 걸쳐 나타나고 있으나 은유적 표현이 중학년에 한 번도 나타나지 않았다. 따라서 아동의 발달 특징을 고려하여 생활 주변에서 볼 수 있는 소재를 중심으로 중학년부터 점차 은유의 표현이 증가해야 하며 아동들에게 상상력을 자극하고 시적 체험을 풍부하게 하기 위하여 발달 수준을 고려하여 균형 있게 선정되어야 한다.
이러한 결과를 바탕으로 이미지의 종류와 기능 이해에 초점을 둔 동시교육 방법을 교과서 수록 동시를 통해 생각해 보았다.
첫째, 교사 중심의 작품 해설보다는 동시를 읽고 아동들의 머릿속에 떠오르는 여러 가지 이미지를 그림으로 자유롭게 표현하도록 하고 그 그림을 자신의 말로 설명한다.
둘째, 아동들이 공동체 의식을 갖고 협동적으로 참여할 수 있는 시적 분위기와 이미지, 인물의 동작, 표정, 배경에 맞는 상황극을 만들거나 교과 통합 활동으로 손가락 인형, 인형, 가면을 만들어 손가락 인형놀이, 인형극, 가면극으로 꾸며 본다.
셋째, 시어를 하나 선택하여 오감으로 표현해 본다.
넷째, 시의 이미지를 구체화하고 감각적, 정서적 반응을 체험할 수 있도록 시에 나타나는 움직임과 분위기를 몸짓과 소리로 표현한다.
이 연구는 몇 가지 제한점이 있지만 초등학교 문학 교육의 한 부분으로써 동시교육의 본질을 찾아 주고자 2007 개정 교육과정 읽기 교과서 수록 동시 작품들의 시어와 이미지를 체계적으로 분석하여 동시 작품들에 시어와 이미지의 다양화와 아동의 발달단계에 맞는 동시교육의 필요성을 재인식할 수 있는 계기가 되었고, 읽고 지나치는 국어 수업에서 더 나아가 아동들이 재미있게 참여하는 가운데 상상력과 창의력을 키워나갈 수 있는 지도방법을 연구해 보았다는 점에서 의의를 찾을 수 있다.Ⅰ. 서 론 1 1. 연구의 필요성 및 목적 1 2. 선행 연구의 검토 2 3. 연구의 방법 및 범위 6

A dialogue system is a computer system intended to interact with a human using natural language to achieve certain goal of the user. Recently, along with the increasing use of mobile equipment like smart phones, a diversity of dialogue systems have been developed and operated for various domains and platforms.

But, the design and implementation of a dialogue system needs plenty of time and efforts on account of its complexity, and often require changes of many part of the system like rewriting program codes when additional application domains are introduced or some existing domains are extended.

This study proposes a ontology model for integrating domain knowledge and dialogue strategies of dialogue systems as knowledge integration platform to adapt to the change and extension of applicaiton domains. The proposed model consists of task models, dialogue flow models, dialogue strategy models, and domain concept models that provides the mechanism that enable dialogue systems to afford changes of application environment without modifications of the systems' internel structures.대화시스템은 인간과 기계가 자연어를 사용하여 상호작용을 함으로써 사용자가 원하는 목적을 성취하도록 하는 컴퓨터 시스템을 말한다. 최근 스마트폰과 같은 모바일 기기의 보급이 확산되면서 다양한 응용 영역과 플랫폼을 위한 다양한 상용 대화시스템이 개발, 운용되고 있다.

그러나, 대화시스템의 설계와 구현은 그 복잡성 때문에 많은 시간과 노력을 요구하며, 특히 기존 대화시스템에 응용 영역이 수정되거나 확장되었을 때 프로그램 코드의 재작성과 같은 시스템의 많은 부분의 수정을 필요로 하는 경우가 많다. 이는 관심 영역이 지속적으로 확장되고 변화하는 환경에서 대화시스템을 개발하고 유지하는 데 큰 장애요소로 작용할 수 있다.

본 논문에서는 응용 영역의 수정과 확장에 쉽게 대응하기 위한 지식 통합 플랫폼으로서 대화시스템의 영역 지식과 대화 전략 통합을 위한 온톨로지 모델을 제안한다. 제안 모델은 작업 모델, 대화 흐름 모델, 대화 전략 모델, 영역 개념 모델로 구성되며, 대화시스템의 응용 영역이 수정 또는 확장되었을 때 시스템 내부구조의 변경 없이 온톨로지의 수정만으로 대화시스템의 동작이 응용 환경의 변화를 수용할 수 있도록 하는 방법을 제시한다.

In NLP, a mapping from text data to Euclidean space is crucial in modeling process. We implement tuple embedding model by using subject, verb, object tuple data extracted from Reuters news headlines. Our experiments show clear syntactic relationships among various headline vectors by using triplet loss function. Headline vectors tend to gather around similar verbs and make different clusters according to similarities of subjects and objects. Interestingly, if we increase the size of the model, existing clusters are divided again to more various actors or actions. Major weakness is that headline vectors are sensitive to order, shape and the number of words in their subject-verb-object because of model architecture and the use of average vectors. Especially, using average vector produces good results only when the number of words in subject-verb-object are short. However, when headline vectors have many words, it produces arbitrary results. Hence, tackling the problem of average vector can be the key to generate more sophisticated performance.자연어 처리(NLP) 분야에서, 텍스트 데이터를 유클리디안 공간으로 맵핑하는 작업은 모델링 과정에서 핵심적인 역할을 한다. 이 논문에서는 로이터 기사 헤드라인에서 추출된 주어-동사-목적어의 튜플 데이터를 임베딩하는 모형을 적용해보았다. 특히 삼중항 손실 함수를 활용한 결과, 기존 손실함수에 비해 구문론적으로 더욱 분명한 관계를 보였다. 헤드라인 벡터들이 먼저 유사한 동사(행동)를 기준으로 큰 클러스터를 형성하였고, 또한 주어와 목적어의 유사성에 따라 세부적으로 다른 클러스터들로 구분되는 결과를 보였다. 흥미로운 점은 모형의 복잡성을 높이게 되면 기존의 클러스터들이 주체에 따라 세분화된다는 점이다. 모형의 큰 단점은 주어-동사-목적어의 순서와 이를 구성하는 단어의 개수에 민감하다는 점에 있다. 이는 모형의 구조와 평균 단어벡터의 활용 때문이다. 특히, 평균 벡터의 활용은 주어-동사-목적어를 구성하는 단어의 개수가 적은 경우에 좋은 성능을 보인다. 반면, 단어의 개수가 많아지면 비슷한 의미의 단어일지라도 그 관계가 임의적으로 변하는 단점이 있다. 따라서, 평균 벡터를 활용하는 문제점을 극복하는 것이 모형의 성능을 높이는 핵심이라고 할 수 있다.1. Introduction 1.1 Introduction 1.2 Related Studies 2. Subject-Verb-Object Tuple Embedding Models

소프트웨어 개발 시 요구사항 분석은 가장 먼저 수행된다. 따라서 요구사항 분석의 결과는 개발된 소프트웨어의 기능이나 품질에 영향을 미친다. 요구사항 분석의 최종 산출물은 요구사항 명세서이다. 요구사항 명세서는 다양한 품질요소를 갖는데 그 중 완전성은 이해관계자의 요구사항을 빠짐없이 기술해야 한다는 의미로써 가장 중요한 품질 요소 중 하나이다. 불완전한 요구사항은 소프트웨어 개발 시 자주 발생하며 발견하기 어렵다. 결국, 이는 소프트웨어 프로젝트 실패의 주요한 이유가 되기도 한다.
요구사항 완전성을 개선하기 위한 다양한 연구가 소개되고 있다. 이 연구들은 요구사항의 누락을 발견하기 위한 참조모델을 기반으로 요구명세서의 완전성을 파악하고 이를 개선한다. 그러나 이러한 참조 모델들은 대다수의 연구에서 연구 수행자의 경험이나 지식에 의존하고 있어서 타당성이나 객관성이 결여된다. 그리고 기법의 적용에 필요한 사전 정보가 많고, 그 사전 정보에 따라 성능이 결정 나는 문제가 있다. 게다가 여러 연구가 특정 도메인에만 적용할 수 있어 범용적으로 사용되기 어렵고, 개념적 절차만을 제공하고 있어서, 사용자가 자신의 프로젝트에서 적용하기 어렵다.
이 논문에서는 유스케이스 시나리오에서 누락된 행위를 찾아내고 가능한 행위를 추천해 주어 요구사항의 완전성을 개선하는 완전 자동화된 기법을 제안한다. 먼저 다양한 도메인의 프로젝트의 요구사항을 수집하고 자연어 처리와 기계 학습 기법을 이용하여 공통 유스케이스 시나리오 패턴을 추출한다. 이 시나리오 패턴을 바탕으로 사용자가 입력한 불완전한 유스케이스 시나리오를 찾아주는 패턴 식별 알고리즘을 개발하고, 알고리즘을 통해 식별된 패턴으로 적합한 대안 시나리오를 자동으로 추천해 준다.
본 기법의 타당성을 검증하기 위해 열두 개의 프로젝트에서 231개의 유스케이스를 수집하였다. 그리고 이 유스케이스를 통해 204개의 유스케이스 시나리오 패턴을 추출하였다. 본 논문에서 제안한 기법의 성능을 측정하기 위해 프로젝트별로 교차분석을 수행하였다. 실험 결과 76%의 정확율와 80%의 재현율을 기록하였고 이는 5년 이상의 경력자가 기록한 재현율인 52.5 % 보다 27.5% 높은 수치이다. 더욱이 전문가가 기법을 이용하여 요구사항을 검증한다면 재현율이 89.1%로 상승하는 것으로 분석되었다. 따라서 제안된 기법은 전문가의 시나리오 검증에 매우 유용하다는 것을 알 수 있다.
본 논문에서는 시나리오 학습을 통해 객관성과 타당성 있는 요구사항 패턴을 추출하였다. 이는 요구사항 작성이나 검증 시 좋은 참조모델이 될 수 있어 재사용이 가능하다. 그리고 요구사항 시나리오의 누락 행위를 추천해 주는 기법은 제안하고 이를 기반으로 한 공개 소프트웨어 도구를 제공하였다. 이를 통해 경험이 부족한 작성자에게 완전성 높은 요구사항을 작성하는 데 도움을 줄 수 있을 뿐 아니라, 요구사항 시나리오의 검증에 필요한 시간을 현저하게 줄여줄 수 있다.Analyzing software requirements is the first activity of software development because it broadly affects to the scope and quality of all artifacts throughout the entire software lifecycle. At the end of the software requirements analysis, a software requirement specification is generally specified. A good requirements specification exhibits diverse characteristics such as completeness, traceability, correctness and so on. Among the characteristics, completeness is the one of the key qualities, indicating that the specification has to describe all requirements of stakeholders thoroughly. Incomplete requirements frequently occur in the requirements specification, but they are hardly discovered so that it often turn out to be the one of the major causes of software project failure.

There has been some research for escalating the extent of completeness of software requirements specification. Most of the previous research build a reference model in advance, and apply it to examine the requirements specification. However, as the reference model is originated from their past experience, they are not validated in an appropriate way. In addition, the reference model is built based on a specific application domain knowledge, so that it can be hardly applicable to different application domains.

This doctoral dissertation suggests an automatic method for detecting omitted actions of use case scenario and recommending possible actions in order to improve the completeness of a requirement specification. Use case specifications are collected from various domains at first, and then common use case scenario patterns are automatically extracted by applying natural language process and machine learning methods. Based on the use case scenario patterns, incomplete use case scenarios from a user’s project are detected by the suggested pattern matching algorithm, and an alternative action is automatically proposed from the patterns.

In order to show the feasibility of this research, I collected 231 use case specifications from twelve projects of different application domains for the experiment. The 204 use case scenario patterns were extracted from the use case specifications. For the performance measure of the approach, cross-validation was carried out. The experiment shows that the approach achieved 76% of precision and 80\% of recall, and it has better performance compared to experts that have over five-year experience in terms of recall. Furthermore, it additionally shows that the expert’s recall supported by the approach is dramatically increased from 52.5% to 89.1%, which indicates that the approach can successfully support the expert’s validation of the requirements scenario.

교수학습 방법 중 서술형 평가는 학습자의 창의력 및 문제해결 능력 증진을 위한 방법으로 자주 활용되고 있으며, 교육현장에서도 그 비중이 높아지고 있다. 하지만 채점량 증가에 따라 교수자의 업무 부담과 신뢰도 높은 채점을 위해서 많은 시간과 노력이 필요하다. 이러한 문제점을 해결하기 위한 방안으로 서술형 답안에 대한 다양한 자동평가 기술이 연구되고 있다.
서술형 답안의 자동 채점 방법에 대한 관심의 증가로 영어권에서는 관련 연구들이 많이 진행되고 있지만 한국어 서술형 자동 채점에 대한 연구는 아직까지 미흡한 실정이다. 본 논문에서는 형태론적으로 풍부한 한국어를 처리하는데 적절한 어휘 의미 패턴(Lexico-Semantic Pattern: LSP) 기반의 자동 채점 방법을 제안한다. LSP는 기존의 어휘 구문 패턴(Lexico-Syntactic Pattern)이 갖고 있던 단순 어휘 표현과 문법 정보에 개념과 같은 의미 정보를 결합시킨 것이다. 그리고 어휘 표현이 갖고 있는 모호성을 제거하여 자연어 질의에 대한 사용자의 의도 파악과 방대한 정보를 사용자가 정확하게 찾을 수 있게 하였다.
또한 LSP는 자유로운 어순을 갖고 있는 한국어의 특성을 반영하는데 적절하다. 품사 분석을 통해 학생 답안의 어절과 문장의 분리를 정확하게 처리할 수 있기 때문에 어절의 순서가 다른 언어에 비해 상대적으로 자유로운 한국어 문장을 표현하는데 효율적이다. 그래서 본 연구에서는 단문 문장뿐만 아니라 두 개 이상의 절이 연결된 복문에서의 가능성을 확인하기 위한 연구를 진행하여 복문에서도 자동채점결과가 좀 더 높은 결과가 나옴을 증명하였다.
본 연구에서는 LSP를 활용하여 한국어의 단어의 기능, 형태와 의미를 표현하는 품사 정보를 자동채점에 적용함으로서 문장의 문맥을 고려함과 동시에 의미 사이의 문법적인 관계와 의미적 조합을 동시에 나타내었다.
뿐만 아니라 LSP에서는 제한적으로 사용되었던 의미를 확장해서 적용하기위하여 온톨로지 와 Word2vec 기술을 적용하여 개념적 유사도를 넓은 범위에서 적용하였다. 이를 통하여 미리 준비되었던 답안 이외의 정답을 채점의 재현율을 향상시키는데 도움이 됨을 알게 되었다. 성능 평가에 대한 실험 결과에서는 제안 시스템이 기존의 명사 위주의 자동 채점 시스템들의 성능보다 0.137 뛰어나다는 결과를 보여주었다. 또한 온톨로지와 Word2vec 기술을 사용한 LSP 자동채점방법은 어휘를 확장하지 않은 LSP 자동 채점방법에 비해 정확도가 0.16 높은 최상의 성능을 얻을 수 있다는 것을 보였다.제 1 장 서 론 1 제 1 절 연구의 배경 1 제 2 절 연구의 목적 3 제 2 장 관련연구 7

하나의 형태에 여러개의 뜻을 가지는 동형이의어의 의미를 파악하는 과정을 동형이의어 중의성 해소라고 하며, 한국어는 전체 단어 대비 동형이의어의 비중이 매우 크므로, 한국어 텍스트의 의미를 정확하게 파악하기 위해 정확한 동형이의어 중의성 해소가 필요하다. 최근에 제안된 동형이의어 중의성 해소 연구들은 신경망으로 단어 임베딩 벡터를 계산하고 이를 의미 판정 기준으로 활용하는 신경망 언어 모델(Neural Network Language Model: NNLM)을 활용하는 방법을 많이 이용하고 있다. NNLM을 활용한 동형이의어 중의성 해소 연구는 문장을 표현하는 벡터인 맥락 벡터를 계산하는 방법에 따라 CBOW(Continuous Bag-of-Words) 모델과 순환 신경망(Recurrent Neural Network: RNN) 모델로 구분 할 수 있다. 이들 중 단어의 출현 정보만을 고려한 CBOW 모델에 비해 단어의 출현 순서 정보까지 반영한 순환 신경망 모델이 좀 더 좋은 성능을 나타내는 것으로 알려져 있으나, 순환 신경망 모델도 단어순서의 순방향 정보만 사용하기 때문에 뒤의 단어가 앞의 단어의 해석에 영향을 미칠 수 있는 자연어의 특징을 제대로 반영하지 못한다는 문제가 있다. 본 논문에서는 순방향 정보 뿐 아니라 역방향 정보도 반영하는 신경망인 양방향 순환 신경망을 사용한 한국어 동형이의어 중의성 해소 모델을 제안한다. 제안한 모델을 검증하기 위한 실험 결과 양방향 순환 신경망을 사용한 동형이의어 중의성 해소 모델의 중의성 해소 정확도가 기존의 CBOW 모델, 순환 신경망 기반 모델들의 정확도보다 더 높아, 단어 출현 순서의 양방향 정보를 반영하는 것이 중의성 해소에도 유효함을 확인 할 수 있었다.국문 요약 ⅰ 영문 요약 ⅱ 목 차 ⅲ 그림 목차 ⅳ 표 목차 ⅳ

21세기 지식정보화사회를 살아가는 사람들에게 있어서 가장 중요시 되고 있는 것은 문제 상황에 직면했을 때 좀 더 창의적으로 문제를 해결하고 창의적인 아이디어를 창안해 내므로써 주어진 문제나 과제를 효과적으로 해결할 수 있는 능력이며 교육에서도 창의적인 문제 해결 능력을 갖출 수 있는 방안을 교육의 핵심으로 두고 있다.
본 연구에서는 컴퓨터 과학 분야 중에서도 가장 핵심적인 부분인 알고리즘의 개념을 활용하여 초등학생들 대상으로 하는 창의적, 논리적 사고력을 향상 시킬 수 있는 웹 기반 학습 시스템을 설계하고자 한다. 알고리즘의 정의에서 나타나는 “문제를 해결하는 기본적인 절차”에 중점을 두고 각 단계별로 문제를 해결 과정을 도입하여 ① 문제 인식(문제보기) ② 상황 분석(생각해 보기) ③ 해결 방안 모색(그림으로 표현하기) ④ 선택 방안의 실천(두리틀 프로그래밍) ⑤ 반성(두리틀프로그램 코드 작성, 실행)의 각 단계를 거치면서 문제해결력, 사고력과 논리력을 향상시킬 수 있을 뿐만 아니라 논리적 사고력 향상에 도움이 되는 프로그래밍 과정을 포함하여 컴퓨터 프로그래밍에 대한 개념과 내용을 쉽게 경험하고 학습할 수 있도록 제시하여 준다.
본 연구의 웹 기반 학습 시스템의 특징을 살펴보면 다음과 같다.
첫째, 논리적 사고력, 문제 해결력 향상을 목적으로 하는 알고리즘 개념 활용 웹 기반 학습 시스템이다.
둘째, 창의적이며 논리적인 문제 해결력 향상을 위해 문제 해결과정을 자연어 표현, 순서도 표현, 두리틀 프로그램등 다양하게 방법을 통해 기술함으로써 반복적으로 문제해결과정이 맞는지 확인할 수 있다.
셋째, 자기주도형 학습이 가능하도록 문제를 난이도별로 제시하여 연령 또는 학습자의 수준에 맞는 학습 내용 선택이 가능하며 각 단계별로도 순차적으로 모든 단계를 진행하지 않고 선택하여 학습을 할 수 있도록 설계하였다.
넷째, 대표적인 EPL 프로그램인 두리틀 프로그램을 활용하여 초등학생들에게 컴퓨터 프로그래밍에 쉽게 접근할 수 있는 기회를 제공하고 프로그래밍 학습이 가능하도록 제시하여 컴퓨터 과학 교육에 질적 향상을 가져왔다Design and Implementation of the Web-based Learning
System for Improving the Problem Solving Ability

Hye-Young, Yoon
Major in Computer Education
The Graduate School of Education
Hankuk University of Foreign Studies

A problem solving ability is considered to be the most important thing for the people living in the twenty-first century. Today's society is called a knowledge and information-based society. It is one that solves the problems effectively by the application of creative ideas when they are facing problems.
This study is to design the web-based learning system for elementary school students. The idea is to improve students' logical thinking power and promote their creative ability by learning the idea of Algorithm, which is the core concept of the modern computer science field.
As its definition tells us, it focuses on "problem solving procedure"and imports it into each of the learning steps. The system consists of ① looking at problems ② thinking ③ expressing with Flowchart ④ Dolittle Programming ⑤ producing codes for Dolittle Program. Students can learn about programming easily and improve their thinking power and logicality as they go through each of the steps.
The characteristics of the web-based learning system in this study are as follows,
First, it is the web-based learning system applied Algorithm concept to improve students' logical thinking power.
Second, students can check if the solving procedures were correct, as they go through diverse learning ways such as natural language, flowchart and the Dolittle program .
Third, it is designed to present problems individually according to levels of difficulty so that students learn by themselves. They can also choose and skip each learning step.
Fourth, it brings improvements in the quality of computer science education by providing the learning with Dolittle program. It is one of the most famous EPL ones and it gives elementary school students the opportunity to easily access computer programming.제 1 장 서 론 1 1.1 연구 배경 및 목적 1 1.2 논문구성 4 제 2 장 이론적 배경 및 선행 연구 분석 5

아바타는 가상환경에서 사용자를 대신하여 표현하는 주체로서 국내외 많은 분야에서 연구되어왔다. 아바타는 사람이나 동물과 같이 사용자에게 익숙한 형태로 표현되며, 기존의 텍스트나 이미지로 구성되던 컴퓨터 소프트웨어의 새로운 사용자 인터페이스 개념으로 자리잡고 있다. 아바타는 기존의 텍스트 위주의 정보 전달 방식에서 벗어난 애니메이션, 사운드 및 텍스트 등을 이용하는 멀티모달(Multi-modal) 인터페이스를 제공하며, 생동감 있는 다양한 동작을 통하여 사용자의 관심과 흥미를 유발시켜 정보전달의 효과를 증진시키는 ‘Person Effect’와 같은 효과를 가져오기도 한다. 최근 아바타와 관련하여 다양한 연구들이 진행되어 왔는데 이중 아바타 행위 표현 및 제어에 관한 연구가 중요한 주제로 부각되고 있다. 아바타의 행위 표현 및 제어 기법은 행위 표현의 수준에 따라 저수준 방식과 상위수준 방식으로 구분할 수 있다. 저수준 방식은 아바타의 신체 각 관절을 제어하는 방식으로 세밀하고 자유로운 애니메이션 데이타를 생성하기 위한 기본적인 제어 방식이다. 상위 수준은 아바타의 행위를 자연어에 가까운 문법으로 표현하는 것으로 주로 텍스트 기반의 행위 명령어로 아바타를 제어한다. 최근에는 웹 환경에서 아바타의 활용이 증가하면서 상위수준의 행위 표현 및 제어 방식을 적용한 스크립트 언어 기법에 대한 연구가 활성화 되고 있다. 스크립트를 이용한 제어 기법은 사용자의 자연언어와 유사한 상위수준으로 아바타의 행위를 표현하기 때문에 일반 사용자도 작성이 비교적 용이하며, 작성된 시나리오 스크립트의 용량이 작고, 또한 특정 플랫폼에 독립적으로 표현될 수 있는 장점이 있다. 즉, 스크립트 언어는 사용자의 행위 명령어와 구현 환경의 아바타 애니메이션 엔진을 연결하는 중요한 인터페이스 역할을 한다.최근에 많은 스크립트 언어 연구가 진행되었으나, 활용분야 환경에서 다양한 가상 객체와의 행위 상호작용을 고려한 아바타 스크립트 언어 연구가 부족하고, 또한 스크립트 작성을 위한 직관적인 인터페이스 연구는 이제 시작 단계이다. 본 논문에서는 추상적인 행위 명령을 제공하는 객체 기반의 행위 인터페이스 기법을 제안하였다. 제안된 객체 모델은 가상환경에서 상위수준의 아바타-객체 행위 인터페이스를 제공하도록 설계되었다. 이를 통해 사용자가 행위 메뉴 인터페이스를 통하여 아바타가 다양한 객체와 상위수준의 행위를 수행할 수 있도록 하고, 사용자의 조작이 실시간으로 작업레벨 행위 스크립트로 생성되도록 하였다.사용자는 제안 객체 모델에 따라 아바타를 제어하게 되는데 실제로 제어한 결과를 시나리오로 저장하기 위해서 스크립트 언어가 요구된다. 이를 위해 제안 행위 스크립트는 계층적으로 정의되었는데 최상위 사용자 인터페이스 계층부터 구현환경에서 구동되는 스크립트까지 단계적으로 아바타의 행위 및 동작을 표현한다. 최상위 수준인 작업레벨 행위 스크립트에서는 제안된 객체 인터페이스를 통한 사용자 의 행위 입력을 표현하며, 중간 수준인 상위레벨 동작 스크립트에서는 객체모델의 행위 및 동작 시퀀스 요소를 이용하여 추상적 행위 명령을 동작 시퀀스로 표현한다. 마지막으로 가장 하위수준인 기본 동작 스크립트에서는 구현 환경의 기하정보 등을 표현하여 실제 아바타 애니메이션을 호출 및 제어한다.구현 결과에서는 사이버강의 및 일기예보 환경에서 제안 객체 모델과 스크립트를 적용하여 가상환경에서 객체들과 상호작용하는 아바타 제어 시스템을 구현하였다. 실험 결과 사용자는 본 시스템을 이용하여 행위 시나리오 스크립트를 상대적으로 적은 시간과 노력으로 생성하였으며 높은 만족도를 보였다. 따라서 제안 기법은 시나리오 형태의 아바타 애니메이션 활용이 가능한 교육, 투어 가이드(Tour guide), 방송과 같은 활용 환경에 적용이 가능할 것으로 판단된다.Avatar, also known as embodied agent, human character or synthetic actor, is a primary element that can be seen in typical cyberspace nowadays. One of the most important aspects of avatar application is animation, which is also called motion or behavior. Through its behavior, an avatar becomes friendly and believable while it conveys various types of information in storytelling-style contents. It provides a multi-modal interface which uses animation, sounds and text media unlike the typical text-based information delivery methods. Moreover, the animated life-like behavior of avatar makes a user feel interests and draw attention. In other words, it is more effective to describe information through various behaviors through the use of human-like characters than merely using simple text or voice narrations. It is called persona effect that an animated character can deliver information more effectively. Many avatar related studies have been active recently. Especially, representation and control of avatar behaviors becomes important issues. Behavior representation and control techniques can be divided into two ways: low-level and high-level techniques. The low-level technique is that a user manipulates each part of avatar body joint and segment for detailed and free animation to create animation data. Therefore, it requires good skill of animator and much time and costs. The high-level technique represents avatar behavior as a natural language style grammar and it controls the avatar in text-based behavior commands. Recently, there have been a number of studies on script language to produce, for general users, the animation scenario of the avatar at the high level. Because the text-based script language represents avatar behavior in the close natural language style, the end user can create script relatively easy and the amount of written script is very small, also it is independent from specific platforms. Most important role of scripting language is to bridge between language concept of end users and avatar animation engines of software.There have been many studies about the scripts, however avatar script language for behavior interaction of various virtual object and intuitive scripting interface in a domain environment is still lacking. Especially, when providing the high-level behavior interface in application environments, abstraction problems are occurred. In the proposed method, the spatial and semantic abstraction problems are defined, when an abstract behavior command is translated into actual motion sequence and animation. To resolve the problem, the context information is defined. The context is consisted of object state variables, behavior division and motion sequence elements. The proposed context-based object model is designed to provide end user with the high-level avatar-object behavior interaction interface. Thus, the user can let the avatar interacts with various object easily in a application environment.A script language is required to write a scenario script which is the result of user’s avatar behavior manipulation process by the proposed model. To do this, the proposed avatar behavior script language is consisted of layered structure which represents user interface, motion sequence, and implement environment information at the each level. A task-level behavior script at the uppermost level represents user input information through object interface, a high-level motion script at the middle level represents motion sequence and at the low-level, a primitive motion script contains geometric information of implementation software to control low-level animation data.In the result, the proposed methods were applied to an avatar control system. A group of users tested the system, and with less cost and time, the users created a scenario script with a high satisfaction score. Therefore, it is expected that the proposed methods can be applied to elearnng, tour guide, and digital broadcasting domains.

질의 응답(Question answering, QA) 시스템은 사용자가 질문을 하면 그에 대한 답변을 찾아 대답해 주는 시스템으로, 주로 스마트폰이나 스마트 스피커에 탑재된 가상 비서의 핵심 기능으로써 채용되고 있다. 기존의 검색 엔진들은 사용자의 질의에 대해 관련된 웹 문서들을 나열할 뿐 직접적으로 정답을 찾아 주지는 못했고, 사용자들은 검색 엔진이 찾아낸 방대한 양의 웹 문서들 속에서 자신이 원하는 정보를 시간과 노력을 들여 스스로 찾아야만 했다. 질의 응답 시스템은 검색 엔진의 이러한 단점을 개선하여 사용자를 대신해 사용자의 질문에 대한 답변을 찾아 주는 시스템이다.

질의 응답 시스템은 정보를 정형화된 지식 베이스로 미리 구축해 놓고 사용하는 knowledge base(KB) 기반 시스템과, 웹 상에 존재하는 수많은 비정형 웹 데이터를 이용하는 information retrieval(IR) 기반의 시스템으로 구분할 수 있다. Knowledge base 기반의 질의 응답 시스템은 상대적으로 정확한 답변을 내놓을 수 있다는 장점이 있지만, 초기 구축에 많은 비용과 노력이 들어가고 유지 관리가 힘들다. 반면 information retrieval 기반의 질의 응답 시스템은 knowledge base를 구축하고 유지할 필요가 없으며, 인터넷의 방대한 웹 데이터를 사용할 수 있고, 최신의 정보를 얻기가 용이하다는 장점이 있지만 knowledge base 기반 시스템에 비해 구현이 힘들다는 단점이 있다.

제대로 구현할 수만 있다면 knowledge base 기반의 시스템보다 information retrieval 기반의 시스템이 여러모로 이상적인 시스템이라고 할 수 있으며, 이러한 information retrieval 기반 질의 응답 시스템의 기반 기술로 기계 독해가 있다. 기계 독해(machine reading comprehension, MRC)란, 사용자의 질문을 받은 컴퓨터가 내용이나 분량에 관계 없이 관련된 문서를 즉시 읽고 이해하여 문서 내에서 질문에 대한 정확한 답변을 찾아내는 기술을 말한다.

본 논문에서는 기계 독해 데이터셋 중 하나인 SQuAD를 이용한 기계 독해 시스템의 성능 향상에 초점을 맞춘다. SQuAD를 이용한 기존의 기계 독해 시스템들은 최신 머신 러닝 기법들은 적극적으로 도입한 반면, 전통적인 자연어 처리 기법들을 충분히 활용하지 않았다. 따라서 본 논문에서는 형태소 분석 결과와 구문 구조 정보, 개체명 정보 등의 언어 자질과 wh-word를 이용한 질문의 유형 정보를 활용하고, 정답의 형태소와 개체명을 예측하여 정답 추론에 활용한다.

먼저, 언어 자질을 추가한 기계 독해를 수행한 결과, baseline인 R-NET 모델 대비 EM 0.78 pp, f1 0.52 pp의 성능 향상을 이뤄낼 수 있었다. 그 다음 언어 자질을 사용한 모델에 wh-word를 이용한 자질을 추가했을 때, EM 0.38 pp, f1 0.50 pp의 추가 성능 향상이 있었다. 언어 자질을 사용한 모델에 예측한 답변 언어 자질을 추가했을 때는 baseline 대비 성능이 EM 0.08 pp, f1 0.43 pp 상승하였고, 언어 자질을 사용한 모델 대비 성능이 EM 0.70 pp, f1 0.09 pp 하락하였다.Question answering (QA) systems are systems which tries to directly answer users’ questions instead of returning web search results. They are most commonly found on smart phones and smart speakers as an integral part of virtual assistants. Question answering systems are an natural improvement on current search engines which return web search results to users’ queries and make them read through vast amounts of web documents to find the information that they want.

QA systems are divided into two categories, a knowledge base (KB) based system which stores knowledge in a structured form and looks it up for answers, and an information retrieval (IR) based system which takes advantage of countless documents on the web. KB based systems offer precise answers, but is costly, and requires a lot of effort to construct and update. IR based system on the other hand, does not require a knowledge base and can utilize the web’s extensive data and has access to more up-to-date information, but is considered harder to develop.

If created properly, IR based QA systems are likely more ideal than KB based ones, and machine reading comprehension (MRC) systems could be a possible foundation for this particular technology. MRC is a technology where you ask the computer a question about a document, and the machine fully comprehends the article and answers your question.

This paper aims to improve MRC systems based on SQuAD (Stanford QUestion Answering Dataset), an open MRC dataset. While existing SQuAD models were quick on adopting the latest machine learning techniques, they seem to lack interest in utilizing existing natural language processing techniques. This paper uses linguistic features like part-of-speech tags, dependency parsing, and named entities to improve the results. It also tries to use question type informations based on wh-words and tries to predict the POS tags and named entity tags of the start and the end of the answer span, and use them to figure out the answer.Ⅰ. 서 론 1 Ⅱ. 관련 연구 7 1. 기계 독해 데이터셋 7 2. 기계 독해 모델 8 Ⅲ. 제안 방법 9

정보 통신 기술이 발전하면서 정보의 양이 폭발적으로 증가해왔고, 정보의 홍수 속에서 대용량의 비정형 문서로부터 주어진 질의에 대하여 구체적인 정답을 제공해주는 질의응답 시스템에 대한 요구가 증가했다.
질의응답 시스템은 크게 사용자의 질의를 분석하는 방법인 질의 분석과 문서 내에서 적합한 정답을 추출하는 방법인 정답 추출로 이루어지며, 두 방법에 대한 다양한 연구들이 진행되고 있다. 본 연구에서는 문장의 의존 구문 분석 결과를 이용하여 질의응답 시스템 내 정답 추출의 성능 향상을 위한 연구를 진행한다. 정답 추출의 성능을 높이기 위해서는 문장의 문법적인 정보를 정확하게 반영할 필요가 있다. 한국어의 경우 어순 구조가 자유롭고 문장의 구성 성분 생략이 빈번하기 때문에 의존 문법에 기반한 의존 구문 분석이 적합하다. 기존에 의존 구문 분석을 질의응답 시스템에 반영했던 연구들은 구문 관계 정보나 구문 형식의 유사도를 정의하는 메트릭을 사전에 정의해야 한다는 한계점이 있었다. 또 문장의 의존 구문 분석 결과를 트리 형태로 표현한 후 트리 편집 거리를 계산하여 문장의 유사도를 계산한 연구도 있었는데 이는 알고리즘의 연산량이 크다는 한계점이 존재한다. 본 연구에서는 구문 패턴에 대한 정보를 사전에 정의하지 않고 정답 후보 문장을 그래프로 나타낸 후 그래프 정보를 효과적으로 반영할 수 있는 Graph2Vec을 활용하여 입력 자질을 생성하였고, 이를 정답
추출 모델의 입력에 추가하여 정답 추출 성능 개선을 시도하였다. 의존 그래프를 생성하는 단계에서 의존 관계의 방향성 고려 여부와 노드 간 최대 경로의 길이를 다양하게 설정하며 자질을 생성하였고, 각각의 경우에 따른 정답 추출 성능을 비교하였다. 본 연구에서는 정답 후보 문장들의 신뢰성을 위하여 웹 검색 소스를 한국어 위키백과, 네이버 지식백과, 네이버 뉴스로 제한하여 해당 문서에서 기존의 정답 추출 모델보다 성능이 향상함을 입증하였다. 본 연구의 실험을 통하여 의존 구문 분석 결과로 생성한 자질이 정답 추출 시스템 성능 향상에 기여한다는 것을 확인하였고 해당 자질을 정답 추출 시스템뿐만 아니라 감성 분석이나 개체명 인식과 같은 다양한 자연어 처리 분야에 활용 될 수 있을 것으로 기대한다.In this paper, we study the performance improvement of the answer extraction
in Question-Answering system by using sentence dependency parsing result.
The Question-Answering (QA) system consists of query analysis, which is a
method of analyzing the user's query, and answer extraction, which is a method
to extract appropriate answers in the document. And various studies have been
conducted on two methods. In order to improve the performance of answer
extraction, it is necessary to accurately reflect the grammatical information
of sentences. In Korean, because word order structure is free and omission of
sentence components is frequent, dependency parsing is a good way to analyze
Korean syntax. Therefore, in this study, we improved the performance of the
answer extraction by adding the features generated by dependency parsing
analysis to the inputs of the answer extraction model (Bidirectional LSTM-CRF).
The process of generating the dependency graph embedding consists of the steps
of generating the dependency graph from the dependency parsing result and
learning the embedding of the graph. In this study, we compared the performance
of the answer extraction model when inputting basic word features generated
without the dependency parsing and the performance of the model when inputting
the addition of the Eojeol tag feature and dependency graph embedding feature.
Since dependency parsing is performed on a basic unit of an Eojeol, which is a
component of sentences separated by a space, the tag information of the Eojeol
can be obtained as a result of the dependency parsing. The Eojeol tag feature
means the tag information of the Eojeol.
The process of generating the dependency graph embedding consists of the steps
of generating the dependency graph from the dependency parsing result and
learning the embedding of the graph. From the dependency parsing result, a
graph is generated from the Eojeol to the node, the dependency between the
Eojeol to the edge, and the Eojeol tag to the node label. In this process, an
undirected graph is generated or a directed graph is generated according to
whether or not the dependency relation direction is considered. To obtain the
embedding of the graph, we used Graph2Vec, which is a method of finding the
embedding of the graph by the subgraphs constituting a graph. We can specify
the maximum path length between nodes in the process of finding subgraphs of a
graph. If the maximum path length between nodes is 1, graph embedding is
generated only by direct dependency between Eojeol, and graph embedding is
generated including indirect dependencies as the maximum path length between
nodes becomes larger.
In the experiment, the maximum path length between nodes is adjusted
differently from 1 to 3 depending on whether direction of dependency is
considered or not, and the performance of answer extraction is measured.
Experimental results show that both Eojeol tag feature and dependency graph
embedding feature improve the performance of answer extraction. In particular,
considering the direction of the dependency relation and extracting the
dependency graph generated with the maximum path length of 1 in the subgraph
extraction process in Graph2Vec as the input of the model, the highest answer
extraction performance was shown. As a result of these experiments, we
concluded that it is better to take into account the direction of dependence
and to consider only the direct connection rather than the indirect dependence.
The significance of this study is as follows. First, we improved the
performance of answer extraction by adding features using dependency parsing
results, taking into account the characteristics of Korean, which is free of
word order structure and omission of sentence components. Second, we generated
feature of dependency parsing result by learning - based graph embedding method
without defining the pattern of dependency between Eojeol. Future research
directions are as follows. In this study, the features generated as a result of
the dependency parsing are applied only to the answer extraction model in order
to grasp the meaning. However, in the future, if the performance is confirmed
by applying the features to various natural language processing models such as
sentiment analysis or name entity recognition, the validity of the features can
be verified more accurately.

단어나 문구의 의미적 관련성을 수치적으로 연산하는 것은 기계 학습이나 정보 추출을 포함한 다양한 자연언어처리 기술의 근간이 되는 기반 기술이다. 이 논문은 공기 네트웍에 기반한 새로운 의미 연관성 척도를 제안한다. 코퍼스에서 관측되는 공기 관계는 네트웍으로 표현될 수 있다. 이 공기 관계의 네트웍을 본 연구에서는 기본적인 의미 표현 형태로 삼는다.
공기 벡터 대신 공기 네트웍을 기본적인 의미 표현으로 삼기 위해서는 두 가지 주요한 이슈를 해결해야 한다. 첫번째는 하나 이상의 단어로 이루어진 문구의 의미 표현이다. 벡터는 잘 정의된 벡터 합 연산이 존재해서, 단어 벡터의 합으로부터 벡터 중앙(centroid)을 구할 수가 있다. 따라서 문구를 위한 벡터 표현은 단어의 벡터 표현을 합함으로서 구할 수 있다. 본 연구에서는 네트웍의 합집합 연산과 교집합 연산을 정의하여 네트웍을 처리한다. 이들을 통해 네트웍은 벡터처럼 합하거나 뺌으로서 구의 표현으로 쓰일 수 있다.
두번째 이슈는 네트웍을 위한 비교 연산이다. 벡터는 잘 정의된 거리 연산과 유사도 연산이 존재하기 때문에 벡터의 형태로 표현된 공기 정보는 쉽게 거리나 유사도를 계산할 수 있다. 네트웍의 경우에도 벡터처럼 수학적으로 타당한 원칙적인 비교 척도가 필요하다. 본 연구에서는 그래프 커널을 도입함으로서 네트웍을 비교한다. 그래프 커널은, 두 네트웍이 공유하고 있는 서브구조를 비교함으로서 주어진 네트웍에 대해 높은 차원의 공간에서 연산한 내적(inner product) 값을 돌려준다. 본 연구에서는 이 내적값을 표준화 한 값을 공기 네트웍을 위한 기본적인 비교 연산으로 사용한다.
본 논문은 세 가지 자연어 처리 응용을 통해 제안한 의미 척도를 평가하고 있다. 동의어 찾기, 단어 모호성 해소 및 태그 번역이 그 세가지 응용 분야이다. 제안된 척도는 이 세 가지 응용에서 공기 벡터 기반의 방법에 대해 꾸준히 더 좋은 성능을 보였으며, 그 결과는 학계에 알려진 최고 수준의 시스템들과 비교할 만한 성능이었다. 제안된 척도는 이들 응용 뿐 아니라 단어나 문맥의 의미를 비교해야 하는 어떤 응용에도 사용될 수 있으며, 이 척도는 또한 워드넷(WordNet)같은 외부 리소스에 의존하고 있지 않다. 보통의 레이블 되지 않은 코퍼스 만으로 학습이 가능하기에, 워드넷 같은 상세한 언어 리소스가 부재한 한국어 등의 언어에서도 쉽게 적용될 수 있는 방법이다.1 Introduction 1 1.1 Background 1 1.2 Approach of the Dissertation 5 1.3 Contribution Summary and Organization of the Dissertation 10 2 Related Work 12

Machine translation has been rapidly developed by deep learning. There are three kinds of machine translation which are RBMT, SMT and NMT. Currently, NMT is used and well known that NMT has better performance than RBMT and SMT. Especially, NMT has excellent performance in context understanding, which means that deep learning is used to understand context and translate. However, NMT algorithm is in black box, so it is hard to know exactly which conditions are good to optimize performance in. In addition, there are many things to consider about performance optimization, even in GNMT.
In the beginning, we discussed the purpose and background of this study. And we studied the definition of machine translation, kinds of machine translation and related works to NMT through Google Translate. Then, when using GRU, we evaluated its performance, the least learning time and loss value to figure out optimal condition.
As a result, the more embedding size and batch size we got, the less learning time and loss value we got. It got pretty closer to optimal condition by fine-tuning, but we cannot expect optimal performance as much as we want. That is, there are limitations on using only fine-tuning to optimize performance. Also, it is necessary to consider many kinds of approaches such as using pytorch-nlp or TensorFlow2.0 libraries etc.자연어 처리(NLP) 분야 중 한 분야인 기계 번역(MT)은 딥 러닝(DL)이라는 기술에 힘입어 비약적인 발전을 이룬 분야 중 하나이다. 기계 번역(MT)은 크게 규칙기반 기계 번역(RBMT), 통계기반 기계 번역(SMT), 그리고 신경망 기계 번역(NMT) 등 세 가지 종류로 구분할 수 있다. 현재, 사용되고 있는 방식은 신경망 기계 번역(NMT)으로 기존 통계기반 기계 번역(SMT)이나 규칙기반 기계 번역(RBMT)보다 성능이 뛰어난 것으로 알려져 있다. 특히, 신경망 기계 번역(NMT)은 딥 러닝(DL) 방식을 이용하여 문맥을 파악하기 때문에 기존 방식들보다 우수한 번역 결과를 얻을 수 있다.
그러나 블랙박스 형태로 되어 있어 신경망 기계 번역(NMT)의 성능이 어떠한 조건에서 최적화를 이루는지 구체적으로 알기 어려운 상황이다. 여기에 성능 최적화와 관련하여 고려해야 할 부분이 많다. 가장 많이 사용되고 있는 구글 번역기에서도 아직 개선해야 할 부분이 많다고 할 수 있다.
서론에서는 연구 목적, 배경 및 의의에 대하여 논하고. 본론에서는 기계 번역(MT)의 정의와 종류, 신경망 기계 번역(NMT) 관련 연구 등에 대하여 살펴보았다. 이어 순환 신경망(RNN) 계열인 GRU를 사용했을 때 성능을 측정하고, 이 때 최소 학습시간, loss 값 등을 측정하여 이와 관련한 최적화 조건을 찾고자 하였다.
결론에서는 측정 결과를 바탕으로 각 수치들이 보이는 양상을 제시하였는데, 전체적으로 embedding size와 배치 사이즈(batch size)를 단계적으로 늘릴수록 학습 시간과 loss 값이 줄어드는 것으로 나타났다. fine-tuning을 통하여 최적화 조건에 어느 정도 근접하였으나, fine-tuning만으로는 우리가 원하는 만큼의 최적화된 성능을 기대하기 어렵다는 한계를 보였으며, pytorch-nlp와 향후 정식 공개될 TensorFlow2.0 라이브러리를 이용하는 등 다양한 관점에서 접근할 필요가 있다.

Extraction and Analysis of Research
Based on NLP Technologies
Abstract
This thesis reveals the methods how efficiently extract keywords and
materials from articles by using natural language. In order to build a
database and extract the keywords, we use PAT-tree which is a
deformation of Trie, ( Trie is the data structure of expressing words in
natural language; and from database. We can extract meaning keywords
through average frequency of words. )
In this thesis, average frequency is used as a basic statistics method,
which can be used to delete inevitable meaningless keywords and
connect those keywords in each statement as units, and then extract more
significant keywords, thus reduce the amount of keywords and reduce
burden for application of keywords clustering in next stage. Therefore,
based on existing researches, we add two new methods:
Method 1: Using complexity to extract String
In a specific period of time, on the basis of Strings, calculating
probabilities of character combination and extracting more accurate
keywords.
v
Method 2: Using Heuristic Rules to delete meaningless words
In the previous stage of the output in the system, method 2 is used to
remove the higher frequency but meaningless words.
The system which is introduced in my thesis can be realized to deal with
English and Chinese simultaneously. The experimental subject focus on
thesis in the field of computer language, using ROLING’s paper which
was published at the China Institute of computer languages.
The conclusions of this thesis: clustering keywords for analysis can be
used to know what fields of study the thesis belongs to; and can also be
used for searching information in all areas ultimately.초록
본 논문에서는 자연언어로 기술된 문서로부터 그 문서의 소재와
관련있는 키워드를 효율적으로 추출하는 방법을 제시하였다. 키워
드를 추출하기 위한 데이터베이스를 구축하기 위해 자연어 단어를
표현하는 일반적인 자료구조인 Trie의 변형인 PAT-tree를 이용
하였으며, 구축된 데이터베이스로부터 평균 출현 빈도수를 이용하
여 의미있는 키워드를 추출하였다. 본 논문에서는 평균 출현 빈도
수를 이용하는 기본적인 통계적 방법에서 피할 수 없는 무의미한
키워드들을 제거하고, 각 어절 단위의 키워드들을 연결하여 보다
큰 덩어리의 키워드로 추출해 냄으로써 키워드의 수를 현저히 줄
여 다음 단계의 응용인 키워드 클러스터링의 부담을 줄일 수 있게
하였다. 이를 위하여 기존의 연구에 다음과 같은 두 가지의 새로운
방법을 추가하였다.
방법 1: 복잡도를 이용한 스트링 추출
특정 시간에 스트링을 기준으로 앞 뒤의 문자와의 결합 확률을 계
산 하여 보다 정확한 키워드를 추출할 수 있도록 하는 방법
방법 2: 단어 Heuristic 을 이용한 무의미어 제거
vii
시스템이 결과를 출력하기 전 단계에서 출현 빈도는 높지만 의미
가 없는 단어를 제거하는 방법.
본 논문에서 제시한 시스템은 중국어와 영어를 동시에 처리할 수
있도록 구현하였다. 실험 도메인은 컴퓨터언어 영역의 논문을 대
상으로 하였으며, 중국의 컴퓨터 언어학회인 ROCLING 에 발표된
논문들을 이용하였다.
본 논문의 결과는 키워드들을 클러스트링 하고 분석함으로써 해
당 논문이 어느 분야에 속하는 지 구분하는 등의 연구에 사용할 수
있으며 궁극적으로는 정보검색의 전 분야에 사용할 수 있다.Table of Contents Abstract iv 초록 vi I. Introduction 1 II. Related Work 4

Recently most hospitals have adopted some form of electric medical record system that computerizes existing medical records which have been written on a paper without any loss of process structure, scope and content of information. The electric medical record system plays a key role in providing information for connection between all of systems managed in a hospital as well as gathering information for clinical research or strategic business. In this thesis, some meaningful patterns of certain disease treatment are extracted from discharge summary that describes a course of inpatients cure procedures in electronic medical record system. The patterns are extracted with present illness by a prefix-span algorithm which is one of the most efficient sequential pattern mining algorithms. In order to use the mining algorithm, a set of discharge summary data has been selected and transformed into an equivalent set of simple sentence in temporal order by applying natural language processing techniques. Our experiment shows some meaningful sequential patterns of treatment procedure among lung cancer patients could be found when a minimum support is 35 and the length of pattern is greater than 4.최근 많은 병원들은 전자의무기록 시스템을 도입하고 있다. 전자의무기록은 종이매체에 기록하던 의료기록을 업무처리 구조나 정보의 범위, 정보내용에 변형 없이 전산화시킨 형태를 말한다. 전자의무기록 시스템은 병원에서 관리되는 시스템과의 연계에 있어 정보를 제공하고 임상연구 및 전략 경영을 위한 정보 수집에 중요한 역할을 한다. 본 논문에서는 순차 패턴 마이닝을 이용하여 전자의무기록 시스템의 데이터 중 퇴원 요약지 정보를 사용하여 질병 치료 패턴을 추출하였다. 퇴원 요약지 데이터 중 현 병력을 대상으로 데이터의 전처리과정을 거쳐 순차패턴 마이닝의 한 방법인 prefixspan 알고리즘을 이용하여 패턴을 추출하였다. 알고리즘을 적용하기 위해 특정 데이터 집합을 선택하여 이것을 자연어 처리 기법들을 사용하여 간단한 문장들로 변환하여 시간 순서로 나타내었다. 본 논문의 실험 결과 폐암환자들의 경우 최소 지지도 35와 패턴 길이 4 이상일 때 치료과정에서 의미 있는 순차패턴들을 발견할 수 있었다.1. 서론 = 1 2. 관련 연구 = 3 2.1 데이터 마이닝 = 3 2.1.1 데이터 마이닝의 개요 = 3 2.1.2 데이터 마이닝의 수행과정과 고려사항 = 3

It has become more difficult to manage software requirements as software has become more huge and complex. Moreover, many stakeholders included in a project have their own requirements and it causes conflicts between requirements, which may cause a failure of functions or even whole project. Currently, most of researches have focused on identifying requirements and some research has tried to resolve requirement conflicts but it was only based on requirement priority. These researches which are using formal language or ontology method need too much cost at the environment of natural language requirement specification. This thesis proposes the Creative Resolution for Requirements Conflict (CRRC) to resolve requirement conflicts in a creative way using TRIZ methodology. TRIZ, which means the theory of solving inventor’s problems, is known as helpful for developing a creative solution to resolve conflicts.
CRRC has four kinds of characteristics to resolve requirement conflicts. First, it classifies requirements cases. Second, it suggests components for resolving requirement conflicts. Third, it shows how to find solution according to each case of requirement conflicts. At last, it applies specific TRIZ theory to get a creative idea for each requirement conflicts. Therefore, CRRC provides solution patterns for each case of requirement conflicts.
The result of experiment, it proves that CRRC helps to get various kinds of creative solution for requirement conflicts.소프트웨어의 규모가 커지고 복잡해 짐에 따라 요구사항의 관리가 점점 어려워지고 있다. 특히 다양한 이해당사자들의 요구사항을 만족시키기 위한 요구사항 변경으로 인해 발생하는 요구사항의 충돌은 시스템의 기능의 오작동이나 프로젝트 전체의 실패를 가져올 수 있다. 현재 충돌을 해결하는 연구는 식별에 치중되어 있고 해결에 관한 연구는 우선순위에 의하여 선택을 하는 것이 대부분이다. 이런 정형언어나 온톨로지를 사용하여 해결하는 연구는 현재 자연어를 기반으로 요구사항을 작성하는 환경에서는 많은 비용과 노력이 소요된다. 따라서 본 논문에서는 요구사항 단계에서 TRIZ의 이론을 적용하여 창의적인 아이디어를 제시할 수 있도록 돕는 요구사항 충돌 해결 원리 (CRRC)를 제안한다. TRIZ는 충돌을 해결하여 아이디어를 내는 데 특화된 이론이다.
요구사항 충돌 해결을 위하여 본 논문이 제안한 CRRC 은 4가지의 특징을 가지고 있다. 첫째, 요구사항 충돌의 유형을 분류하였다. 둘째, 요구사항 충돌 해결 원리의 구성요소를 제안하였다. 셋째, 요구사항 충돌 유형에 적합한 해결원리를 도출하는 과정을 정리하였다. 넷째, 요구사항 충돌 유형에 적합한 TRIZ이론을 소프트웨어에 맞게 적용하였다. CRRC 은 충돌 유형별 해결 방안을 제시함으로써 요구사항 충돌의 패턴역할을 한다고 할 수 있다.
CRRC 을 대조 실험 적용 결과 아이디어의 수가 증가하였고, CRRC 을 제공하면 다양한 종류의 창의적인 요구사항 충돌 해결 방안을 제시할 수 있었다.

가변 길이의 텍스트를 텍스트의 맥락 정보를 반영한 벡터로 변환하는 방법인 문장 혹은 문단 임베딩은 다양한 기계학습 시스템에서 감성 분석 등의 텍스트 분류 문제, 텍스트의 유사도 측정, 클러스터링, 시각화 등 고정된 차원의 벡터를 입력으로 요구하는 각종 과제를 수행하기 위한 기본적인 특징 추출 방법으로 사용되고 있다.
기존의 텍스트의 벡터 표현을 위해 널리 사용되고 있었던 단어 자루(Bag-of-Words) 모형은 단순하고 효과적이지만 차원의 크기가 단어의 숫자에 비례해서 증가하며 레이블 없는 텍스트 데이터를 활용하기 어렵다는 단점이 있다. 이러한 한계를 극복하기 위해 제안된 문단 벡터(Paragraph Vector)는 새로운 데이터에 대한 벡터 표현을 학습하고 생성하는데 적용하기 위해서는 기존 모델을 새로운 데이터에 대해 추가적으로 학습시키는 추정 과정을 필요로 한다는 한계가 있다. 시퀀스 투 시퀀스(Sequence to Sequence) 모형을 기반으로 선후 문장과의 문장 간 관계를 활용해 문장의 벡터 표현을 생성하는 인코더를 학습하는 생각 생략 벡터(Skip-Thought Vectors) 모형은 학습을 위해 문장 간 선후 관계를 활용하기에 여러 문장이 포함되며 선후 관계를 설정하기 어려운 문단 수준에는 바로 적용되기 어려우며 텍스트 임베딩의 생성에 많은 연산 자원을 필요로 한다는 단점을 갖고 있다.
본 연구에서는 전이 학습의 문제와 임베딩 생성에 필요한 연산량 및 가변 길이 텍스트 처리의 용이성의 문제 등을 고려하여 단어 임베딩의 합을 통해 텍스트 임베딩을 생성하는 모형을 제안한다. 기존의 단어 임베딩이 유니그램(Unigram)을 사용하는 것과는 달리 본 연구의 모형에서는 바이그램(Bigram) 및 트라이그램(Trigram) 등의 n-그램(n-gram)을 활용해 텍스트 임베딩을 개선하였다. 또한 이를 컨볼루션 신경망을 사용한 모형과 비교하여 컨볼루션 신경망과 같은 추가적인 구조의 도입 없이도 좋은 성능적 특성을 보일 수 있음을 검토하였다.
본 연구는 문장 혹은 문단의 벡터 표현을 생성하기 위한 단순한 방법을 제시하는 동시에 부수적으로 학습한 n-그램 단어 임베딩의 특성을 분석하였다. 단어 임베딩의 특성에 대한 분석과 단어 임베딩의 단순한 결합이 보여주는 효과성을 통해 자연어 처리 과제에 요구되는 텍스트의 특성을 포착하기 위해 필요한 조건을 이해하는 것과 함께 텍스트의 고속 처리가 필요한 실용적인 상황에서 사용할 수 있는 도구로서 기여할 수 있을 것으로 기대한다.제 1 장 서 론 6 제 1 절 연구의 배경 6 제 2 절 연구의 목표 9 제 2 장 선행 연구 11 제 1 절 문장 및 문단 임베딩 11



It is not too much to say that the main object of the Internet spread so far and wide in the daily life nowadays is to find rapidly and exactly information users try to get.
It is search engine that makes it possible. Users of the Internet utilize information searched through various engines such as Yahoo, Empas, Naver, etc.
These search engines provide some specific information, enhancing their search function, but it requires lots of time and efforts to find and use detailed information and data of specific field.
The object of this thesis is to design and implement search system for French education materials, saving time for information retrieval and focusing on quick search for detailed information users need.
To achieve this goal, French education materials was created as database files and two search modules of web page and bulletin board data were composed to make it possible to search words users want to. Mysql was used as DBMS(Database Management System) with PHP(Hypertext Preprocessor) by a script language for this thesis. For better results, it was designed and implemented so that users could search what they were looking for through the expansion search of Query Language, selecting one of a options such as 'Contents search' and 'Title search', 'Including and Excluding Words search', 'Natural Language search', 'And oprator search' or 'Or oprator search'.목차 = ⅰ 제1장 서론 = 1 제1절 연구의 필요성 및 목적 = 1 제2절 연구의 제한점 및 방법 = 3 제2장 이론적 배경 = 4

소프트웨어가 현 사회에 끼치는 영향은 점점 커지고 있다. 이러한 현상은 교육계에도 큰 영향을 주고 있으며 미국을 필두로 여러 선진국들에서는 소프트웨어를 가르치는 코딩교육을 공교육에 편입시키는 등 적극적으로 코딩교육을 시행하려는 움직임을 보이고 있다. 이러한 전 세계적인 현상은 코딩이 외국어를 배우는 것처럼 모든 산업 분야에서 쓸 수 있는 실용적인 능력이라는 것을 반증하고 있다.
그러나 전통적으로 소프트웨어 교육은 학습자 측면에선 배우기 어렵고 교수자 측면에선 가르치기 어렵다는 평판과 이미지를 가지고 있다. 본질적으로 프로그래밍은 고차원적 사고 및 문제해결전략을 수반하며, 컴퓨터 과학의 기본 원리들은 극히 추상적이기에, 가르치고 배우는 모든 사람들이 가장 어려워하고 싫어하는 과목 중의 하나로 인식되고 있다. 이는 코딩교육을 준비해야 하는 교사들에게 큰 부담으로 작용하고 있다.
이런 상황에서 코딩교육 이러닝은 교사들을 지원하는 요긴한 도구이자 가이드가 될 수 있으며, 학생에겐 매력적인 학습 도구를 제공할 수 있다. 그런데 교사가 부담을 느끼는 한 가지 부면은 코딩을 가르치는 이러닝의 수는 많지만, 이들 중에서 어떤 이러닝을 선택하여 수업을 준비할 수 있는지에 대한 정보가 부족하다는 점이다. 코딩교육 이러닝을 선택하기 위해 교사가 직접 많은 이러닝들을 경험해본다는 것은 교사에게 시간적, 비용적 부담을 지울 수 있다. 따라서 어떤 코딩교육 이러닝이 교사들에게 적합한 것인지 코딩교육 이러닝에 대한 기본적이고 종합적인 정보를 제공하는 것이 필요하다.
이에 이 연구에서는 코딩교육 이러닝에 대한 분석 연구를 수행하였다. 구체적인 연구문제는 첫째로 코딩교육 이러닝은 컴퓨터적 사고력을 어떤 내용과 방법으로 교육하고 있는지, 둘째로 코딩교육 이러닝을 수업에서 활용해 본 경험자들은 코딩교육 이러닝에 대해 어떻게 생각하며 수업에서 어떻게 활용하고 있는지, 셋째로 코딩교육 이러닝의 문제점은 무엇이냐는 것이었다.
연구방법으로는 오피니언 마이닝 기법을 활용하였다. 오피니언 마이닝은 텍스트로 되어 있는 다양한 데이터로부터 자연어 처리 과정을 통해 중요한 정보를 추출, 분류하고 유의미한 정보를 분석하여, 개인이나 조직이 주관적인 생각보단 객관적인 데이터에 근거해 합리적인 결정을 내리도록 도와주는 인텔리전스 지원 기법이다. 이 연구는 코딩교육 이러닝 15개를 선정하고 이들 이러닝을 수업 맥락에서 활용해본 경험이 있는 279명의 전문가, 교사 및 기타 사람들의 리뷰 데이터를 341개를 수집하여, 코딩교육 이러닝의 학습내용과 교육방법, 코딩교육 이러닝 활용 경험자들의 의견, 코딩교육 이러닝의 문제점을 분석하였다.
이를 토대로 분석된 결과는 다음과 같다. 첫째, 코딩교육 이러닝은 학습내용 측면에서 컴퓨터적 사고력의 요소들을 부분적으로 포함하고 있었는데 컴퓨터적 사고력 개념 요소들은 일반적으로 모두 학습내용으로 포함하고 있었지만 컴퓨터적 사고력 연습의 리믹스해서 재사용하기, 모듈화해서 추상화하기는 각각 53%, 40%만 포함하고 있었다. 학습방법 측면에선 동기부여를 위해 고무적 동영상, 배지 및 포인트, 캐릭터 사용, 온라인 수료장이 사용되었고, 지식이해를 위해선 튜토리얼, 라이브 프리뷰, 게임하기가 사용되었다. 지식구성을 위해 퀴즈와 과제, 창작 작업이, 정보교환을 위해선 공유하기와 리믹스하기가 사용되고 있었다. 둘째, 코딩교육 이러닝 활용 경험자들의 의견은 대체로 긍정적이었다. 이들이 어떤 부면에서 긍정적으로 느끼는지에 대해 오피니언 극성 빈도 비율, 오피니언 오리엔테이션 점수, 워드 클라우드, 트리맵을 통하여 추출된 유의미한 정보를 시각화하여 제시하였다.
이 연구의 시사점은 다음 세 가지이다.
첫째, 코딩교육 이러닝은 기존 소프트웨어 교육이 가지고 있는 문제점을 다양한 부면에서 극복하고 있었다. 둘째, 이 연구는 교사들이 코딩교육 수업에 적합한 코딩교육 이러닝을 선정하는 면에 있어 경험자들의 리뷰를 근거로 유의미한 정보를 제공하였다. 셋째, 코딩교육 이러닝이 어떤 부면에서 개선되어야 하는지 확인하였다.The effect that SW influences on the society is getting bigger. This trend is also influencing the education field, and many leading countries are trying to attempt to bring in SW education in the context of public education. It can be said that this proves coding is a practical skill that can be used in all kinds of industry like a language skill.
However, SW education has a negative reputation that it’s difficult to learn and teach programming from the both perspectives of teachers and students. Essentially programming accompanies high dimensional thinking, problem solving strategies, and many computer science theories are extremely abstract to understand thus it is being recognized as one of the subjects that people teaching and learning it find very difficult. This is giving some burden on the teachers who should prepare the lesson.
In such a context, e-learning for SW could be a helpful and supportive tool for the teachers and provide an attractive learning tool But what the teachers feel burdensome is that there are many e-learnings for SW education, but they are not sure about which e-learning they should choose for the class. They feel burdensome about that they should spend much time to learn about the e-learnings just to choose what they think is good. Thus it’s needed to provide the teachers with overall information about the e-learnings for SW education.
An analysis study was conducted. Research questions are as follows: First, with what contents and methods do e-learnings for SW education educate students computational thinking. Second, what do those who have used the e-learnings for SW education think about and how do they use them in the class?
As a research method, opinion mining was used to analyze the e-learnings. Opinion mining is an intelligence support technology that helps an individual or an organization to come to a rational decision based on rather objective information by analysing the relevant information through extracting important information among various sources of text data. This study was conducted by collecting 341 reviews from 279 persons including experts, teachers and analyzed the contents, the methods, the opinions of those who used the e-learnings and the problems that they mentioned..
The study results are as follows: First, the e-learnings for SW education partly included some components of computational thinking. All the computational thinking concepts were included, but just 53% of Remix and Reuse was included and 40% as Modularization and Abstraction of computational thinking practices. Inspirational videos, badges, points, characters and online certificates were used to motivate the students. Tutorial, live preview, games, quiz, assignment and creation were used for knowledge construction while sharing and remixing were used for knowledge understanding. Second, opinions of those who used the e-learnings for SW education in the class were generally positive. The meaningful information through opinion orientation rate, opinion orientation score, word cloud and tree map were presented.
Based on the findings of the study, it was possible to derive the three discussion points. First, the problems that the traditional SW education had were being overcome in various aspects. Second, useful information for deciding a proper e-learning for SW education was provided for teachers based on the review data of those who used the e-learnings in the class. Third, what should be improved in the e-learning was identified.I. 서론 1 1. 연구의 필요성 및 목적 1 2. 연구문제 6 3. 용어의 정의 7

디지털 음악 시장의 규모는 스마트폰, mp3 플레이어와 같은 디지털 미디어 기기들의 등장과 함께 빠르게 증가하여 2016년 처음으로 물리적 시장의 수익을 넘어섰다. 현재 디지털 음악 시장은 전세계적으로 6.7백만 US달러의 수익을 가진 시장이 되었으며 스트리밍 분야의 45%에 육박하는 높은 성장률에 따라 매해 그 규모가 증가하고 있다.
디지털 음악 시장의 규모가 확대됨에 따라 물리적 시장을 기반으로 했던 이전의 음악 시장에 비해 월등히 많은 수의 음악들이 사용자에게 배포되었다. 하지만 접근 가능한 디지털 음악의 수가 늘어남에 따라 이를 검색하는데 있어 사용자의 부담이 증대되었고 효율적인 음악 정보 검색의 필요성이 대두되었다.
음악 대표 구간 추출은 음악 정보 검색의 중요한 연구문제 중 하나로 음악의 일부만을 추출해 들려줌으로써 전체 음악의 특징을 쉽게 파악할 수 있도록 하는 방법이다. 사용자가 전체 음악을 파악하는데 걸리는 시간을 단축함으로써 음악 정보 검색의 효율성을 높일 수 있다. 음악의 대표 구간을 검출하는 연구로는 음악 요약 연구가 있다. 기존의 음악 대표 구간 추출 연구들은 모두 음악 신호를 기반으로 대표 구간 추출 알고리즘을 제안하였다. 하지만 음악 신호를 이용하는 것은 음악 구조의 경계가 뚜렷하고 구간의 변주가 적은 경우 대해 높은 성능을 보이지만 반대의 경우에는 그 성능을 보장할 수 없다. 또한 음악 구간에 대한 사람들의 선호도를 반영할 수 없다는 한계점을 가지고 있다.
본 연구에서는 대표적인 소셜 음악 스트리밍 서비스인 사운드 클라우드(Soundcloud)의 시간 주석 코멘트 데이터를 활용하여 사용자의 선호를 반영할 수 있는 사용자 기반의 음악 대표 구간 추출 방법을 제안하였다. 코멘트 데이터를 자연어 처리 기법 중 하나인 감성 분석을 통해 코멘트의 감성 극성 값을 구한 뒤 시간에 따른 코멘트 감정 극성 평균을 구해 최고점을 찾고 대표 구간을 추출했다. 또한 코멘트 분포를 시스템에 반영하기 위해 시계열 분석법인 특이 스펙트럼 분석(SSA)을 통해 시간에 따른 코멘트 분포의 비조화 성분들을 제거해 평활화 한 뒤 감성 분석의 결과와 곱해 그 최고점에서 대표 구간을 추출하였다.
추출된 대표 구간들은 사용자 평가를 통해 성능 평가하였다. 5개의 장르에 대해 각각 20곡을 수집하여 45명에게 사용자 평가하였고 이때 피험자는 교내에서 30명을, 교외에서 15명을 모집하였다. 데이터는 모두 해당 장르의 사운드 클라우드 Top 50 차트 에서 수집하였다. 제안 시스템 3가지(감성 분석을 통한 추출, 감성 분석과 단일 스펙트럼 분석을 활용한 추출 방법 2가지)와 미리 듣기(초반 30초), 기존 시스템(음악 썸네일링 알고리즘)에 대해 명료성, 간결성, 전반적인 평가 3항목을 5점 척도 평가하였다.
사용자 테스트 결과 제안 시스템들은 미리 듣기 보다 우수하지만 기존 시스템보다는 성능이 떨어지는 것으로 분석되었다. 또한 제안 시스템간 성능이 통계적으로 유의미한 차이를 보이지 않았고 따라서 추가로 단일 스펙트럼 분석을 활용한 추출 방법이 통계적으로 유의미한 성능 향상을 보이지는 못했음을 확인하였다. 제안 시스템은 모두 사용자평가에서 3점 이상을 보이며 대표 구간으로서 받아들일 수 있다고 평가되었다.
본 연구를 통해 소셜 데이터를 이용한 음악 정보 검색의 새로운 방법론을 제시하며 시간 주석 코멘트 데이터의 음악 정보 검색 활용가능성을 확인하였다. 음악 이외에도 시간 주석 코멘트를 가진 다양한 소셜 멀티미디어 서비스로의 적용을 기대한다.1.서론 1 1.1 연구의 배경 1 1.2 연구의 목적 4 2.관련 연구 5



以上의 結語Ⅰ을 더욱 要略하면 다음과 갗이 結論을 맺을 수 있다.
1.隱喩는 言語表現의 限界線을 擴張하는 方法이다. 즉 意味領域의 擴張方法이다.
2.隱喩의 發生은 七個의 見解를 가질 수 있으나 大別하면 四個로 할 수 있다.
㉠神과 事物과 言語의 未分離
㉡欲求充足과 昇化의 脫出口
㉢意味論的 律動의 原理
㉣言語經濟說
3.隱喩는 言語의 內包性(Connotation)을 重視하고 考察하여야 한다. 隱喩는 文脈(Context)과 作者나 讀者의 位置(Situation)에 다라 같은 材料材(Vehicle)가 相異한 意味材(Tenor)로 解釋될 수 있다.
Tenor Vehicle
Tenor₄- Vehicle - Tenor₂ Vehicle₄- Tenor - Vehicle
Tenor₃ Vehicle₃
異質的인 隱喩 -denotation(外廷)의 觀點으로 보아서를 結合시켜 豫期치 못했던 第三의 意味場을 形成할 수 있다. ex) 本文에 있어 H₂O를 參考
5. 隱喩는 그 意味變化 轉移에 있어서 內部를 演繹하기 위해 그와 類似한 外部世界의 것을 牽引하여 의미를 상기 한다. 一般的으로 抽象의 意味를 傳達시키기 위해 具體的 意味가 있는 것으로 代置한다.
6.隱喩에는 그럿을 使用하는 社會의 모습이 잘 反映되어 있다.
7.共感覺的 隱喩에는 視覺的 隱喩가 壓倒的으로 많다.
8.隱喩의 構造形態는 마치 天體構造的인 모습을 連想 시킨다.
◁그림삽입▷(원문참조하세요)
本論文題 : 國語隱喩의 意味論的 考察
作成者 : 尹弘老目次 序論 = 1 (一) 隱喩의 意味限界 = 5 (二) 隱喩의 發生起源考 = 10 1. 未分□狀態에서의 同類意識 = 10

라이헨바흐의 시점이론에서 가장 구별적인 특징이 되는 기준시점(point of reference)은 화자가 사건의 상적 특성을 평가하는 시간, 즉 사건의 흐름을 바라보는 관찰자의 시간적 위치이다. 러시아어에서 이러한 관찰자의 시간적 위치는 상황이 사건 진행의 결과에 따라 동시적 혹은 회상적 시점이 되는 판단기준으로 사용된다. 발화 영역인 경우 현재시제가 아닌 과거시제에서 동시적 시점과 회상적 시점이라는 두 가지 대립적인 시간적 위치가 동시에 나타날 수 있다. 따라서 시제와 상을 통합범주로 한 러시아어의 과거시제 형태에서 과거와 현재와의 관계에 따른 관찰자의 시간적 위치가 평가된다. 과거와 현재와의 관계에 따른 두 가지 대립적인 시점의 발현은 시점상 상호 연관성을 갖는 동사의 시상형태와 시간적 상황부사와의 결합에서 분명하게 나타난다. 그 전형적 예로 러시아어 동사의 시상형태와 시간적 단절성 여부로 두 가지 의미로 분류되는 시간부사 ''давно''의 결합은 그 대립적 시점의 발현을 더욱 극명하게 드러내 준다. 두 가지 의미로 분류된 시간부사 ''давно''는 각각 상응하는 동사의 시상의미와 상호관련되고, 이는 결국 문장 전체에서 동사의 시상형태와 ''давно''간의 유일한 시점이 확립되는 결과를 낳는다. 시간부사 ''давно''를 두 가지 의미로 분류하는 시간적 단절 성분은 또한 ''давно''와 관련된 의사소통적 구조를 설명하는 도구가 된다. 시간적 단절 성분에 따라 과거에서 현재까지 상황의 정상적인 진행이 유지되는지 여부는 이른바 ''알려진 것''이 현재순간에 존재하는지 확인할 수 있는 바로미터가 된다. 그러므로 이러한 시간적 단절 성분은 테마 혹은 레마로서 시간부사 ''давно''의 의사소통적 역할을 부여한다. 결국 라이헨바흐의 시점이론에 기반하여 러시아어 동사의 여러 시상형태와 시간부사 ''давно''가 결합되는 패턴을 의미-화용적으로 분석해 보고, 이를 통하여 자연어에서 화자가 의도하는 시간체계가 어떻게 확립되는지 그 메커니즘을 규명하기 위한 시도가 본 연구의 목적이 된다.

The analysis and searching of software stored in a repository is getting more attention because enormous amount of software are gradually stored and managed in the repository. Existing methods to describe the software in a natural language or using a restrictive mechanical tool do not provide enough flexibility to query and analysis the software. In this thesis, we selected UML(Unified Modeling Language) as the documentation in order to convert it automatically into KIF(Knowledge Interchange Format) format to recognize the semantics directly in a computer system. It means that smaller cost enables us to have much flexible and sophisticated analysis than existing methods. To show this, we developed an UML-to-KIF converter and applied it to the actually developed UML diagrams at a commercial company. The result reveals that a Blob AntiPattern exists in the software. It shows that it is possible to perform a higher level of analysis than basic software searches. Since it is expected that the XML(eXtensible Markup Language)-based UML notation will be taken as a standard by OMG(Object Management Group), there should be more study on the development of translation methods from XML-based UML into KIF. In order to provide efficient searches in the repository, we could convert a specific portion of the modeling information into the form of knowledge in order to reduce the reasoning cost.저장소에 저장된 소프트웨어에 대한 검색과 분석은 점차 대량의 소프트웨어가 저장소에서 관리되게 됨에 따라 중요한 문제가 되고 있다. 기존의 방법은 자연어에 의한 기술이나 제한적인 기계적 툴을 사용하여 검색에 이용하지만 이는 실제 소프트웨어 검색과 분석에 충분한 유연성과 깊이를 제공하지 못한다. 본 논문은 객체지향 소프트웨어 개발과정에서 필수적인 모델링 수단인 UML을 지식표현의 표준형식인 KIF 형식으로 자동변환하여 지식처리시스템에서 그 의미를 직접 인식할 수 있도록 하여 기존의 수단에 비해 적은 비용으로 훨씬 유연하고 섬세한 분석이 가능함을 보였다. 이를 위해 본 논문에서는 UML-to-KIF 변환기를 작성하였으며, 이를 이용해 실제 업체에서 설계한 UML 다이어그램 자료를 분석하여 AntiPattern의 일종인 Blob 패턴이 현재 개발중인 소프트웨어에 존재함을 확인했다. 따라서 응용하기에 따라 기본적인 소프트웨어 검색 뿐만 아니라 보다 고차원적인 분석이 가능함을 보였다. 향후에는 XML 기반의 UML 표현이 표준으로 자리잡을것으로 전망되므로 XML 기반의 UML 표현을 KIF로 변환하는 연구가 진행되어야 할 것이며, 저장소의 검색목적에 따라 소프트웨어 설계 모델 정보의 일부만을 지식표현방식으로 변환하여 추론에 따르는 비용을 줄일 수 있는 것에 관한 연구도 진행되어야 한다.

본 연구는 근대산업유산의 대표적 사례“당인리 발전소＂의 10개년 (2013년(도시재생특별법 제정년도) 이전 5개년(선행 연구 논문 및 학술지 데이터) 및 이후 5개년(문화체육관광부 보고서 데이터)) 빅데이터에서 나타나는 계획요소(주요 키워드)를 텍스트마이닝 연구방법(분석도구:RStudio)을 활용하여 분석함으로써, 주요 시기별(구간별) 변화에 따른 계획요소들의 변화 패턴(각 구간별 빈도수와 분포도)을 살펴보고, 네트워크 분석을 통해 요소들 간의 연결망을 시각화하여 각각의 계획요소들 간의 관계를 보다 계량적으로 규명하였다는데 의의를 둘 수 있다.

시기(구간)별로 계획요소를 도출하는 과정은 공통적으로 다음과 같이 진행했다. 첫 번째, 분석 준비 단계에서는 데이터를 수집하였다. 2013년 이전 5개년(2008년~2013년)은 국내 최대의 학술연구정보서비스(RISS)에서“당인리 발전소”로 검색해서 선정된 국내 학술지(15개 학술지, 2,223개 단어) 및 연구 논문 자료(13개 논문, 6,807개 단어)를 수집하였고, 2013년 이후 5개년(2013년~2018년)은 문화체육관광부에서 주관하여 전문가 집단이 발간한 “당인리 발전소 보고서”관련 자료를 2013년 기본구상 보고서(344페이지, 42,428개 단어), 2014년 기본계획 보고서(357페이지, 48,122개 단어), 2015년 당인리 문화창작발전소 발전방안 연구 보고서(276페이지, 11,108개 단어), 2018년 문화공간 조성 및 운영계획 보고서(235페이지, 32,195 개 단어) 및 공간 활용계획 보고서(70페이지, 11,394개 단어)를 수집해서 PDF파일을 모두 텍스트(text) 파일로 변환하고, RStudio 프로그램을 사용하여 자연어 처리 및 명사(키워드)를 추출하였다. 두 번째, 빅데이터 분석 단계에서는 빈도수가 높은 키워드 순서로 Matrix data를 시각화 하고, 주요 키워드 및 연관어 들을 Wordcloud를 사용해서 군집화 하였으며, networkD3를 사용해서 네트워크 연결망을 시각화 하였다. 세 번째로 해석 및 평가 단계에서는 주요 키워드들의 변화 과정을 연결성, 근접성, 매개 중심성으로 분석했고, 최종적으로 머신러닝 방법인 nnet 패키지를 이용해서 인공 신경망 알고리즘을 통해 최초 31개 키워드(계획요소) 중에서 최종적으로 빈도수가 높은 8개의 상위 키워드(계획요소)를 도출해냈다. (제4장 분석 결과 참조)

향후 도시계획분야에서도 연구방법론 중에 하나로써, 빅데이터 방법론을 활용한 많은 시도가 있기를 바라며, (범죄/재난/기후변화 예측, 관광이미지(트렌드)분석, 스마트시티 전략 도출 등에서는 이미 사용되고 있다.) 갈수록 증가하고 있는 근대산업유산의 슬럼화(유휴공간의 하나로 전락)를 막고, 많은 잠재성(장소성, 다층성, 활용성, 재생성)을 가진 근대산업유산을 활용한 계획요소에 대한 방향을 설정하는데 본 연구가 도움이 되기를 바란다.This study is a representative example of modern industrial heritage: the ten-year period of the "Dangin-ri Power Plant" (the year of enactment of the Special Act on Urban Regeneration in 2013) five years prior to the first five years (the data of leading research papers and academic journals) and five years thereafter (the report data of the Culture, Sports and Tourism Ministry) By analyzing the planning elements (major keywords) appearing in big data using the text-mapping research method (analysis tool:RStudio), the patterns of changes in planning elements (frequency and distribution plots for each section) could be examined and the network analysis could help to more quantify the relationship between components.

The analysis results show the following changes in the planning factors (major keywords) as they occur by time (separation). The process of deriving the planning element over time (interval) was generally carried out as follows: First, the data were collected in the analysis preparation stage. Previously, the nation's largest academic research information service (RISS) collected data on local academic journals (15 academic journals, 2,223 words) and research papers (13 papers, 6,807 words), and the 2013 report was published by the Ministry of Culture, Sports and Tourism and published by the Ministry of Culture, Sports and Tourism.To convert Rts and all PDF files in the tourism industry into text files, the Operational Plan Report (235 pages, 32,195 words), the Space Utilization Plan Report (70 pages, 11,394 words) was collected and natural language processing and nouns (keywords) were extracted using the RStudio program. Second, in the big data analysis phase, matrix data was visualized in the order of frequent keywords, key keywords and connections were clustered using the word cloud, and network connections were visualized using networkD3. In the third analysis and evaluation phase, the process of changing the key word with connectivity, proximity, and return-centricity was analyzed, and eight higher-key words (planning elements) were finally derived from the highest frequency of the first 31 keywords (planning elements) using artificial neural network algorithms. Machine Learning nnet Package (see Chapter 4 Analysis Results)

As one of the future research methodologies in urban planning, I hope that there will be many attempts to utilize big data methodologies (it is already used in predicting crime/disaster/climate change, analyzing tourist images (trends) and developing strategies for smart cities). I hope that this research will help prevent the increasing slumming of modern industrial heritage and establish direction for planning elements utilizing modern industrial heritage with many potential (materiality, multilayeredness, usability, and regenerative).

건설 현장은 다양한 공종, 업체들이 함께 작업한다. 발주자, 원도급업체, 협력업체, CM, 감리자 등 다수의 이해관계자가 참여한다. 그러기에 성공적인 프로젝트 진행을 위해서는 각 이해관계자들 간의 효율적인 의사소통과 명확한 정보전달이 매우 중요하다. 프로젝트 참여자들은 다음과 같은 행위들을 통해 소통하고 정보를 전달한다. 크고 작은 회의, 직접 대면을 통한 대화, 전화/무전기 등을 통한 음성 전달, 오프라인 서류 등을 활용한 서면자료 전달, 이메일 또는 컴퓨터 파일(엑셀, 워드 파일 등)을 활용한 소통, 인터넷 기반의 회사 PMIS(Project Management Information System) 활용 등이 있다. BIM(Building Information Modeling) 기술의 발전으로 인해, BIM 소프트웨어를 활용한 의사소통도 점차 늘어나고 있다.
또한 모바일 기술의 발전으로 인해 모바일 기기를 활용한 정보 전달도 증가하는 추세다. 특히 대부분의 스마트폰 사용자들이 일상생활에서 활용하는 채팅 어플리케이션이 전문 건설관리 소프트웨어보다 건설 현장 의사소통에 더 많이 활용되고 있었다. 관리자들은 사용법이 복잡하고 추가적으로 설치해야하는 현장 관리 소프트웨어보다 사용하기 쉽고 추가적으로 설치가 필요 없는 채팅 어플리케이션을 선호하였다. 대표적인 채팅 어플리케이션인 카카오톡, 네이버 밴드 등을 활용하여 텍스트 메시지 기반으로 소통하고 현장 정보를 주고받았다. 하지만 채팅 어플리케이션을 통해 주고 받는 정보는 일회성 정보로 소모되며 체계적으로 저장되거나 활용되지 못하였다.
본 연구는 현장 관리자들 간의 채팅 어플리케이션을 활용한 의사소통에서 어떠한 정보들이 교환되며, 현장 관리에 유용한 정보들이 있는지 확인한다. 또한 현장 관리자들간의 텍스트 메시지에서 수집된 정보들로 현장 공사일보를 작성할 수 있는지 확인하고 그 방법을 제시 하고자 한다.
본 연구는 다음과 같은 방법으로 진행되었다. 현재 공사일보 작성의 문제점과 현장에서 건설관리 시스템, 채팅 어플리케이션의 활용 정도를 파악하기 위하여, 현장관리 경험이 있는 관리자들 대상으로 설문조사를 진행하였다. 현장관리자들이 생각하는 현재 공사일보 작성의 문제점, 개선방향, 채팅어플리케이션의 활용 현황 등에 대한 설문 결과를 얻을 수 있었다. 현장관리자들이 생각하는 공사일보 작성방식의 문제점은공사일보 작성 시 걸리는 시간과 취합의 비효율성이었다. 또한 90% 이상의 관리자가 스마트폰 채팅 어플리케이션을 활용하여 현장 정보를 공유하고 있었다.
또한 현장에서 활용되고 있는 공사일보 16개를 수집, 분석하여 공사일보에 어떠한 정보들이 담겨져 있는지 파악하고 정보의 종류들을 분류 하였다. 정보의 종류는 총 64가지였으며, 6개의 성격으로 이 정보들을 나눌 수 있었다. 6가지 성격의 정보는 현장기본정보, 인력정보, 장비정보, 자재정보, 작업량정보, 작업내용 정보였다.
이렇게 분류된 공사일보 정보들을 현장관리자간 채팅 어플리케이션 내 텍스트 메시지로부터 수집할 수 있는지 확인하고자, 7개 실제 현장에서 활용되는 채팅 어플리케이션의 단체 대화방의 텍스트를 추출하여 분석을 진행하였다. 이러한 채팅어플리케이션 대화 내에는 인력정보, 장비정보, 작업내용 정보 등 현장 관리에 유용한 정보들이 전달되었다. 하지만 대화들은 주로 자연어로 구성되어 있고, 데이터들이 정형화 되어있지 않으며 체계적으로 저장되지 않았다.
본 연구를 통해 현장 관리자들 간 채팅 텍스트 데이터 내에 공사일보 정보를 포함한 대화문을 분류할 수 있는 방법을 제시하였다. Google Cloud NLP(Natural Language Process)를 활용하였으며 현장별로 공사일보 정보를 포함한 대화문 100개를 수집하여 객체리스트를 도출하였다. 이를 활용하여 수집된 데이터와 전체 채팅 대화문 사이의 유사도를 구할 수 있었다. 공사일보 정보를 포함한 대화문과 포함하지 않은 대화문의 코사인 유사도를 비교결과 공사일보 정보를 포함한 대화문과 포함하지 않은 대화문간의 유사도 차이가 있었다. 이를 통해 특정 유사도 이상의 값을 가진 말풍선을 수집하면 공사일보 정보를 포함한 대화문을 수집할 수 있었다. 또한 공사일보 정보를 포함한 대화문에서 상세정보를 분류하기 위한 규칙을 제시하였다. 이 규칙을 위하여 인원, 장비, 작업내용을 지칭하는 단어 라이브러리를 구성하였으며, 이 규칙을 활용하여 인원 정보 및 장비 정보의 출력 정보 수집이 가능하였다.
본 연구에서 제시한 방법을 통해 현장관리자 간 채팅 텍스트 데이터 내에서 공사일보 정보를 수집할 수 있으며, 이는 현장에서 현장관리를 위해 작성되는 일일 공사일보 작성에 활용될 수 있다. 이를 활용하여 공사일보 작성의 시간 단축 및 현장관리자의 생산성을 향상 시키고, 효과적인 현장관리가 가능할 것이다.

기업의 부도는 경제 전반에 충격적인 연쇄효과를 가져와 경제가 불안정해지고 그로 인해 기업의 부도가 지수 분포적으로 증가하는 등 심각한 사회문제가 될 수 있는 부정적인 요소를 지니고 있다. 특히 국내의 경우, 1997년 IMF 구제 금융을 겪었으며 BIS(Bank of International Settlement)가 세계 우량 금융 기관들에 대해 내부 신용평가모형을 장려하는 등의 금융 시장 환경의 변화를 맞고 있다. 따라서 국내 금융 기관들은 여신기업에 대한 사전심사 강화와 도산 가능성을 사전에 파악하고 도산에 대비하기 위한 부도예측시스템의 개발에 상당한 노력을 기울이고 있는 추세이다.
근래 들어 이러한 부도예측시스템을 개발하기 위한 모델링 기법으로서 다양한 인공지능기법(Artificial Intelligence Method)들이 도입되어 활발히 연구 또는 활용되어 지고 있다. 특히 최근에는 이들 개별 인공지능 기법들의 결합(Hybrid)을 통해 각 기법이 지닌 장점들을 포함하고 단점들을 보완함으로써 보다 안정적이고 지능적인 부도예측시스템을 개발하고자 하는 시도가 늘어나고 있다.
본 연구는 여러 가지 인공지능 기법들 중에서도, 서로 상호 보완적인 특성을 지니고 있는 인공 신경망 기법(Neural Network)과 퍼지 시스템(Fuzzy System)을 결합하는 방법 중 하나인 Fuzzy-Neural System을 기반으로 한 부도예측시스템의 개발에 초점을 두고 있다.
퍼지 논리(Fuzzy Logic)는 경험지식을 통한 논리적인 기반 하에 만들어지는 IF-THEN 규칙 기반의 논리체계로서 인간이 사용하는 언어적 변수의 애매모호함을 시스템 안에서 처리할 수 있도록 해주는 장점을 지니고 있다. 따라서 퍼지 논리를 기반으로 한 퍼지 시스템은 언어적인 추론능력을 바탕으로 애매한 정보를 처리, 추론하는데 적합하며 지식 표현 능력이 뛰어나다. 반면 신경회로망은 자동 학습 능력을 가지고 있어서 데이터 구동형(Data-driven) 패턴처리에 우수하며, 시스템 구성에 있어 유연성이 크다.
그러나, 퍼지 시스템이 적절한 성능을 나타내기 위해서는 전문가가 많은 시행착오를 통해 적절한 제어 규칙을 결정해야 하며, 특히 비전문가의 경우는 최적의 제어 규칙과 소속함수를 결정하는데 있어 많은 어려움이 있다. 이는 학습 능력의 부재에서 기인하는 것으로 인공 신경망을 퍼지 시스템에 도입함으로써 이러한 단점을 보완할 수 있다.
반면 인공 신경망은 지식을 구조화하는 것에서는 우수한 성능을 보이지만 그 지식을 직접적으로 표현할 수 없다. 즉 신경망은 가공되지 않은 정보에 쉽게 적용할 수 있기 때문에 정보처리 과정 중 하위 처리에는 우수하지만 자연어와 같은 상위 정보처리에는 약하다. 이와 반대로 퍼지 시스템은 상위 정보처리에는 우수성을 보이지만 하위 정보처리에는 미약하므로 이 두 가지 기법의 결합을 통해 각각의 장단점을 상호 보완함으로써 오는 시너지 효과를 기대해 볼 수 있다.
본 연구에서는 Fuzzy-Neural System을 기반으로 한 부도예측모형의 실증적 연구 결과를 기존의 순수 인공 신경망 모델과 다변량 분석기법 기반 부도예측모형들의 성과와 비교 분석하여 그 응용 가능성과 장단점에 대해 검증하고 있다.
실험 결과 Fuzzy-Neural System 즉 ANFIS를 기반으로 한 부도예측모형인 Neuro-fuzzy model의 분류 예측력이 다른 두 비교 모델에 비해 더 우수하게 나타난다는 것을 알 수 있었다. 특히 다변량판별분석 모델의 경우에는 부도기업 그룹에서의 예측력이 매우 떨어진다는 것을 볼 수 있다. 반면 Neuro-fuzzy model과 인공 신경망 모델은 부도기업/건전기업 그룹 모두에서 정확도가 근사하게 나타나거나 오히려 부도기업 그룹에서의 예측력이 더 높게 나타나는 등 보다 나은 분류 정확도를 보이고 있다. Neuro-fuzzy model과 인공 신경망 모델간의 비교에서는 상당히 유의한 차이가 나는 것은 아니지만 Neuro-fuzzy model의 예측력이 부도/건전 예측 부문과 총체적인 부문 모두에서 약간 우세하게 나타난다는 것을 볼 수 있다.
따라서 본 연구는 처음 시도된 연구였음에도 Neuro-fuzzy model과 인공신경망모델, 다변량판별분석 모델과의 차이와 다양성을 바탕으로 Neuro-fuzzy system의 부도예측분야에서의 이용 가능성을 성공적으로 검증한 주요한 초기 연구가 될 수 있다고 생각한다. 물론 Neuro-fuzzy model의 부도예측분야에의 응용 가능성이 보다 일반화 되기 위해서는 앞으로 많은 연구들이 뒷받침 되어야 하겠으나, 본 연구의 결과를 통하여 Neuro-fuzzy model의 주요 장점 및 한계점 등을 살펴볼 수 있다.In 1997, under the International Monetary Fund (IMF) system, many of Korean companies faced on the bankruptcy situations. Many companies' financial distress can influence on overall economic situations of the country. And it's closely related with company loan profit of financial institutions as well. So many financial institutions and governments are considerably interested in distinguishing between good companies and bad companies, before and after.
Many loan officers of financial institutions are using many techniques such as statistical models, credit scoring model or other various models for developing bankruptcy prediction system. Specially, in these days, artificial intelligence modeling methods like artificial neural network, decision tree, case-based reasoning, fuzzy logic are studied and applicated to various modeling areas. And the hybrid methods between theses artificial intelligence methods are taking attention on the both of academic and application areas.
This paper is focused on the hybrid modeling method of artificial neural network and fuzzy logic in the corporate bankruptcy prediction area. Artificial neural network and Fuzzy logic have the interactive features each other in hybrid system developing. The hybrid methods of artificial neural network and fuzzy logic can be divided into three categories: fuzzy neural networks, neural fuzzy systems, fuzzy-neural hybrid systems. In this paper, we used ANFIS(Adaptive Network-based Fuzzy Inference System) algorithm, one of the fuzzy neural networks method among them, for building bankruptcy prediction model in financial area. And, for generalization, we compared the classification capability of neuro-fuzzy model with those of disciminant analysis model and general artificial neural networks model.
Overall, this paper's empirical test result indicated that neuro-fuzzy models have higher classification capability as compared to the other two models and has many remarkable features in bankruptcy prediction modeling.목차 1. 서론 = 1 2. 퍼지 논리에 관한 연구 = 4 2.1 퍼지 논리의 개념 = 4 2.2 퍼지 논리의 구조 = 5

본고에서는 한국시의 중요한 내적 동인이 되고 있는 향토성을 소월과 백석의 시세계를 중심으로 살펴보았다.
소월과 백석은 활동 시기가 20년대와 30년대라는 시간적 차이는 있지만, 식민지 당대의 험난한 삶을 산 동시대인으로서 민족적 삶의 바탕과 전통적 정서의 기틀에 밀접한 시세계를 보여주어, 그 시적 기반이 동일해 보인다. 당대 문단의 주류와 서구사조에 함몰되지 않고, 독자적인 시세계를 구축했다는 점과 그들의 시에 나타나는 토속적 세계의 배경이 되고 있는 고향이 같다는 점에서 비교의 대상이 되었다.
Ⅱ장에서는 근대시에서의 향토성의 의의와 전개를 살펴 보았다. 향토성은 식민지 현실에 대응하는 정신적 자세로 민족의 근원적 정서를 시화하여, 일제에 대한 저항과 민족 문화의 확립에 기여했다. 또한 향토성은 전통지향적 시인의 계보를 형성하여 현대시사의 한 주류를 이루었다고 할 수 있다. 향토적 경향을 띤 시의 흐름은 자연친근성과 토속적 경향으로 나누어 전개되는데, 자연친근성이 강한 시인으로는 신석정, 김동명, 김상용 등이고, 토속적 경향이 강한 시인으로는 김영랑, 노천명, 서정주, 박목월 등을 들 수 있다.
Ⅲ장에서는 소월과 백석의 향토성을 구체적으로 고찰한 바 고향의식의 형상화, 소재면에서 향토성의 추구, 시어의 토착어 사용으로 나누어 살펴보았다.
식민지 시대의 기본 정조가 되어 흐르는 고향상실감은 조국의 상실과 연결되어 있다. 소월의 고향은 님이 거주하는 곳이며, 현실에서 잃어버리고 꿈속에서나 만날 수 있는 님 때문에 고향상실로 이어진다. 소월이 추구하는 고향은 현실을 넘어서 있는 초월적 세계, 인간의 근원적 고향인 것이다.
백석의 고향은 유년의 시각으로 재구한 산골마을 사람들과 풍속, 인정, 말이 함께 있는 민속적 고향이다. 실제의 고향을 재현하여 민족의 보편적 고향을 꿈꾸고, 식민지 현실의 부당함을 고발한다.
민족·무속·설화·지명·자연 등 향토적 소재는 민족의 삶의 바탕이 되며, 이를 공유하는 공동체의 유대감을 강화한다. 소월은 향토적 소재를 사용하여 토속미를 강화하고 민족의 보편적 정서를 수용했다. 특히 지명의 시화는 고향상실의식과 맥을 같이하며, 시대의식을 반영하여 민족의 호출을 통한 민족의 구체성을 형성하여 공감을 자아낸다.
백석의 풍속적 소재는 그가 재현하는 토속적 세계에 접근할 수 있는 통로의 역할을 한다. 특히 그가 집착한 음식물 관계어는 한국인의 인정과 구수한 삶의 모습과 연결되어 궁핍한 시대의 굶주림을 상기시키고 있다.
소월과 백석은 개인어와 방언을 사용하여 각기 개성있는 목소리로 시어의 폭을 확대했다고 할 수 있다. 소월은 서울말을 표층으로 하고 평북 방언을 기층으로 하여, 단순하고 평이한 방언의 활용과 개인언어도 사용하여, 민중적 공감을 유발한다.
백석은 표준어를 바탕으로 명사와 감각적인 방언을 사용했다. 있는 그대로의 자연어를 사용하여, 구체적 경험의 세계를 생생하게 그리고 있다.
Ⅳ장에서는 소월과 백석의 시세계를 대비하여 특징과 영향관계를 살펴보았다. 소월과 백석의 한과 서러움의 정조, 토속적 세계에 그 시적 기반을 둔 점, 과거지향성, 향토적 소재에 대한 관심 등 전통지향적 경향은 그들의 정신세계를 공유하는 흐름을 짐작하게 한다.
이상에서 살펴본 바와 같이 향토성을 기반으로 하는 소월과 백석의 전통지향적 성격은 일제 식민지체제의 서구지향성에 대응하는 문화적 저항의 자세로 한국시사의 중요한 위치에 서 있다. 소월의 전통지향적 성격과 그 영향을 입은 백석 등 전통주의 시인들의 계보가 현대시사 흐름의 주류를 형성하고 있는 것이다.목차 Ⅰ. 서론 = 1 1. 연구의 목적 = 1 2. 선행연구 검토 = 3 3. 연구방법 및 범위 = 5

지금까지 효석의 문학세계는 많이 논의되어 왔지만 문학사적으로 肯定的인 면과 否定的인 면에서 많은 논의가 이루어져 왔다.
본고는 효석 문학의 특질을 파악하기 위해 전기와 후기작품에 나타딘 특성을 연구하여 보기로 한다.
효석의 전기작품을 同件者 修家的 경향으로 평가한 기존의 많은 연구를 검토, 그와 다른 면을 찾기 위하여 노력했다. 그동안 한국의 同律者 作家는 대체로 이효석과 유진오로 대표되고 있으나 실증적으로 작품을 검토하여 볼 때, 효석의 작품에는 그련 성향이 미약한 듯하다. 그래서 본고에서는 효석 작품의 근본 특질인 고향에 대한 鄕愁와 異國的 情緖, 에로티시즘 Eroticism, 詩約 律情性을 살펴 보기로 한다.
그의 작품세계를 2기로 나누는 것이 일반적인데, 그것은 1927년부터 1932년까지 전기, 1933년부터 1942년 그의 사망까지를 후기로 구분 짓는다. 그까닭은 전기에 나타난 동반자 작가의 작품적 성향이 강하고, 후기로 다양한 경향이 나타난 것이 그 구분의 원인이 되었다 그러나 이것은 너무 인위적인 구분이기 때문에 이해의 平易를 위해 전후기의 작품으로 분리하여 특질의 연속성을 따져 보았고, 될 수 있는 한 모든 작품을 연구 대상으로 했다.
전기작품과 후기작품에 일관성 있는 큰 흐름은 세 가지로 요약할 수 있다.
첫째, 鄕愁와 異國憧憬의 표현이다. 전기작품에 나타난 동경은 그의 문학에 나타난 고향에 대한 그리움과 異國에 대한 막연한 그리움으로 구분된다. 누구나 자기가 태어날 고향을 잊지 못하듯 효석도 그의 고향을 꿈으로서 그리고 있다는 점이며, 異國에 대한 憧憬은 그의 본성이 그러하듯 서구적이었다는 점이다. 전기의 작품에서는 아주 가까운 異國에서 후기로 갈수록 먼 구라파로 향하는 갈망의 표현이 主流를 이룬다.
전기작품의 「行進曲」, 「露領返海」, 「奇遇」의 둥장인물의 일부가 國外로 나가고 있고,「上陸」, 「北國私信」이 이미 나가 있거나, 「北國點景」의 일부가 異國에 대해
강렬한 관심을 가지고 있다. 후기작품에 나타난 고향에 대한 그리움은 「개살구」, 「山崍」, 「메밀꽃 필 무렵」, 「장미 병들다」, 「鄕愁」등의 작품에 여실히 나타나 있다. 또, 이국적인 것은 「花粉」의 주인공 영훈을 통하여 잘 그려내고 있다.
둘째, 인간의 본능인 性의 탐구라고 볼 수 있다. 인간의 性이나 동식물의 性은 근본적으로 차이가 없다는데서 출발하였다. 효석의 작품에 표현된 性은 ① 沒道德的이다. 즉, 도덕이라는 굴레를 전혀 의식하지 않았다는 점 ② 觀淫症的 性愛를 통해 쾌감을 느끼는 것이다. ③ 遠親相姦으로서의 性 ④ 사람과 동식물을 等價的 위치로 놓았다는 점이다. 때로는 난삽한 듯도 하지만 原初的, 本能的 性을 통해 인간 본연의 모습을 形象化해서 인간의 성의식을 전환시켰다.
전기작품 「奇遇」의 "찬호"와 "계순"의 만남이나 마굴에서의 사랑도 성의 순수성에 뿌리를 두고 있는 것이지 그들의 만남이 사랑을 매개로한 사상성을 목적으로 한 것은 아니라는 점이다. 「오리온과 능금」에서도 인간의 성욕을 동물적 행위로 나타내어 강렬하고 자극적으로 그려가고 있다.
셋째, 詩的 抒情性을 작가의 특유한 언어와 기질로 아름답게 승화시켰다. 그의 抒情的 特質은 ① 話素 자체가 抒情的이다. ② 사건이 지극히 압축적으로 짜여져 있어서 散文的 현실이 詩的으로 축약되어 있다. ③ 동식물과 인간의 삶을 입체적으로 구성하여 자연의 신비를 드러냄으로써 情緖的이게 하고 있다 ④ 세련된 언어와 詩的 분위기의 형성이 抒情性을 드러내는 큰 요소이다. 즉, 문체와 내적인 여러 요소가 감각적이다. 특히 전기작품에서는 자연어를 잘 활용하였고, 후기의 작품 「메밀꽃 필 무렵」은 다양한 비유나 세련된 언어를 통하여 시적 서정성을 잘 살려 나가고 있다.
상기한 특질들을 대부분의 논자들이 전기와 후기로 나누어 다른 평가를 내리는 데서 오는 전후기작품 이해의 거리감을 줄이기 위해 본고에서는 대체로 같은 경향을 가진 작품을 그 연구대상으로 하였다.Even though we have talked about the literature worms of Hyo Sok.
We have got literary the difference valuation between the affirmative aspect and negative aspect until now
In this paper, we have found what is called the special quality of the side of the same quality in order to understand in detail the literature characteristic of Hyo Sok.
We have studied the aspects of the difference from the former works of Hyo Soy who had an effect on the other writers. We have thought that he had the inclination of accompany writer who throw am image on works into a phenomenon in the poor life regarding the atmosphere of an age.
So far we have studied, exoticism, eroticism, poetic lyric gender of the fundamental characteristic through Hyo Sok's worn. Generally the world of literature divided two period, the former period from 1927-to 1932, the latter period from 1933 to 1942 until his death. In this paper we excluded such a artificiality because it divided artificially so much
We have divided into the works of the former and latter and studied the continuation of characteristic in order to understand his works.
It was divided three kinds of the former and latter worms in the follow. First, nostalgia is expression. We divided into the attachment of a foreign country and his literature showing the former works. As we could not forget our home town. he pined for home with dreams he was western European and had conceding nostalgia of a foreign country. In the former worms, he expressed western European style from a foreign country to the latter.
The former works such as 「Haeng Jin Gok」, 「No Ryong Keum Hea」, Kr Woos they come out into a foreign country partially, including 「Sang Ryuk」,「Book Gook Sa Shin」. Some parts of 「Book Gook Jum Kyong」 was interested in a foreign country.
It was revealed the attachment regarding home town in the latter works such as 「Kae Sal Koo」, 「San Hyub」, 「Mae Mil GGoch Pil Moo Ryeob」, 「Jang Mi Byang Deul Da」, 「Hyang Soo」and we could understand through "Yong Hoon" who is character of 「Hwa Boon」.
Second, It looks like the study of sex of human nature, we can say that it has no difference meaning eccektimitty between a sex men mind animals or plants, A sex which expressed in the latter work of hypo Soy is immoral. Namely it is not conscious of the scope of the moray. It is better felling that they study the other's face on the sly with the morbid sexual love, a sex of incest, miccociatikg on min equal footing bedeck a man mind animals or plants. Sometimes it has no difference meaning from these things. We diverted from a sexual conscious which had a appearence of the human nature through a fundamented and instinctive sex.
In 「Orion & Nung Keum」, we have drawn the feature of appearing an animals conduction into human sexual desire.
Third, It can say that we sublimated beautiful in distinctive language and character of writer far the poetic lyric gender. His lyric character us following.
① A subject matter of conversation us lyric itself.
② The development of contents is composed in suppressive and the reality of the prosy is implicated emotionally.
③ It is emotional that they appeared the mysteries of nature of nature of composing in three dimensions the life of human and animal or plants.
④ It is a factor that they appeared lyrically the development of poetic attention and excellent language.
As a matter of fact, sentence and the internal elements is appreciated. Especially in the farmer works. He used the ward very well with the language of nature.
「Mae Mil GGoch Phil Moo Ryeob」 in the latter work used very well without any difficulties the poetic lyrical thoughts throughout the variety metaphor and the refunded language.
Even though most of writers estimated dividing unto two part : the farmer and latter period about the mentioned character. We studied about the same tendency of the works to make out the contents of the farmer and latter works.國文抄錄 = ⅰ 目次 = 1 Ⅰ. 序論 = 1 1. 硏究史 = 1 2. 硏究目的 = 8

Token-based source code clones detection provides a promising way to detect the source code duplication and redundancy. While preprocessing of clone detection plays an important role in KDD for further processing. Just as the old saying goes: well begun is half done.
However, processing unstructured source code files of large software systems is really challenging and time or space consuming. This Thesis introduces a novel way to clean, tokenize and transform the source code into the appropriate form for mining. A tool called OPP (One-pass Preprocessor) has been developed to preprocess the source code files (according to the configuration table (as in appendix A)) effectively, efficiently and flexibly. It preprocesses the source code file only once, provides numerical form output file besides string form for advanced user, and users have the great freedom to choose the relevant data to preprocess. The thesis experimented on three large open source projects like Wildfly1.02, Linux core-3.6, VTK of different host languages, and the result shows that our tool has great power and flexibility to preprocess the source code files and product high quality output.
Tool OOP has been implemented in Java and can be executed under Window, Mac or Linux. It preprocesses source code file from a number of different types such as “.java”, “.cpp”, “.c” etc. This tool can also be used for other purposes, such as nature language, text mining preprocessing.토큰 기반 소스 코드클론 탐지는 소스코드 중복과 잉여를 탐지하는데 유용한 방법을 제공한다. 한편 클론 탐지의 전 처리는 ‘시작이 반이다’라는 격언처럼 KDD에서 이후 단계에서 중요한 역할을 한다.
그러나 대형 소프트웨어 시스템의 비 구조화된 소스코드 파일의 처리는 매우 어렵고 시간 또는 공간이 소요된다. 이 논문에서는 마이닝을 위해 적합한 형태로 변환되고 토큰화된 소스코드를 정제하는 새로운 방법을 소개한다. OPP (One-Pass Preprocessor)라고 명명한 도구는 효과적으로 구성표(configuration table; 부록 A 참조)를 따라 소스코드파일을 능률적이고 유연하게 전처리하기 위해 개발되었다. 이것은 오직 한번 소스코드파일(고급 사용자를 위한 문자열 형식 이외의 수치 형태로 출력파일을 제공)을 전처리하고 사용자들은 전처리하기 위해 관련된 데이터를 선택하는데 있어서 매우 자유롭다. 이 논문은 다른 호스트언어인 Wildfly1.02, Linux core-3.6, VTK와 같은 3개의 큰 오픈소스프로젝트를 실험했고 그 결과 우리 도구는 고품질의 결과를 생산하고 소스코드를 전 처리하기 위한 유연성과 큰 힘을 지니는 것을 보였다.
이 도구는 또한 자연어나 텍스트마이닝 전처리등의 다른목적에도 사용할 수 있다.Chapter 1 Introduction 1 1.1 Thesis Statement 1 1.2 Thesis Contributions and Advantages 3 1.3 Research Overview 4 Chapter 2 Related Work 6

In this study, genetically modified organism (GMO)-related media coverage by The Chosun Ilbo and The Hankyoreh was analyzed using semantic network analysis. The purpose of the study was to identify linkages between the words used in their media coverage by applying the new research methodology “K-CORE” in the semantic network analysis to analyze the details of media coverage related to GMO issues. As the target data, GMO-related articles published between January 1, 2008 and December 31, 2017, were extracted and analyzed. The Korean and natural language processing program TEXTOM and the visualization program NETDRAW developed by UCINET were employed as analysis programs for this study. The results of the analysis showed that GMOs have scientific facts, but the GMO-related news lacked the provision of scientific information. The Chosun Ilbo and The Hankyoreh had ideological differences in their typical political orientations, and this was also reflected in their GMO-related reports. Regarding the coverage of GMOs, The Chosun Ilbo focused on the existence of economic, scientific, and medical benefits from introducing GMOs by using expert groups and overseas government agencies as informants, and frequently dealt with economic and industrial topics as well as technical research and development. On the other hand, The Hankyoreh focused on the potential risks of the import and cultivation of GMOs on agriculture and the environment. It also reported the opinions from all walks of life using important informants. Both The Chosun Ilbo and The Hankyoreh emphasized anxieties about the GMO technology, but rarely touched on the political benefits and risks of GMOs. In other words, the examination of the two media companies’ articles related to GMOs, an emerging scientific technology, showed different reporting focuses according to their political orientations.본 연구는 조선일보와 한겨레의 GMO에 관련한 언론보도를 의미연결망분석방법을 통해 분석하였다. GMO 이슈에 관련한 언론 보도를 내용 분석하는 데 의미연결망분석에 K-CORE라는 새로운 연구방법을 적용해 보도에 사용한 단어와 단어 간에 연결 관계를 파악하는데 목적을 두었다. 분석 시기는 2008년01월01일부터 2017년12월31일까지 최근 10년간에 GMO에 관련한 기사를 추출하여 분석을 하였다. 본 논문의 분석에 이용한 프로그램은 국어 자연어 처리 프로그램 TEXTOM과 시각화 기능을 갖고 있는 UCINET의 NETDRAW이였다. 분석 결과, GMO가 과학적 사실(Fact)을 갖고 있지만 뉴스에서 과학 정보 제공이 부족하므로 알 수 있다. 조선일보와 한겨레는 대표적 정치적 성향에 이념적 차이가 있음으로 불구하고 GMO에 관련한 보도에서도 반영되었다. 조선일보는 전문가 집단과 외국 국가들의 정부기관을 정보원으로 많이 사용하여 GMO을 도입하는 데 경제적 혜택과 과학/의료적 혜택이 있다고 보도의 중심을 두었다. 그리고 조선일보는 경제 산업적 주제와 기술 연구 개발하는데 많이 다루었다. 반면, 한겨레는 GMO에 관련된 보도에서 GMO을 수입과 재배를 하는 것이 농업과 환경에 발생 가능한 위험에 중심을 두었다. 그리고 사회 각 계층에 의견을 중요한 정보원을 사용하여 보도를 하였다. 조선일보와 한겨레는 GMO 기술에 대해 불안감을 강조하였고 정치적 혜택과 위험에 대해 거의 다루지 않았다. 즉, 조선일보와 한겨레가 새로운 과학기술인 GMO에 관련한 보도 내용을 살펴보면 정치적 성향에 따라 보도 중점이 다르게 나타났다.

Words have more than one meaning in natural language. For example, The word ‘다리' means that part of the body is attached under the body of a human or animal and a structure built to span physical obstacles without closing the way underneath such as a body of water, valley, or road, for the purpose of providing passage over the obstacle. People can easily determine whether this word is used in a sense in the sentence. But computers are not. So the word has more than one sense work is needed to understand what it means in context. This process is called Word Sense Disambiguation.

Word Sense Disambiguation can be categorized as Supervised and Unsupervised. The Supervised Word Sense Disambiguation is method of extracting the statistical information from the sense tagged corpus and determining. However, in order to obtain a good performance, it requires a large amount of tagged corpus, and the range is also limited to a few words. In this paper, we propose an Unsupervised Word Sense Disambiguation Approach using the sub-graph based algorithm to complement these issues.

However, Graph-based Word Sense Disambiguation has some problem. if any ambiguity word in sentence or context build sense graph, unnecessary node and edge are added and have the disadvantage of increasing the error. In this paper, we used the method of iterative approach to graph-based Word Sense Disambiguation. This method is to match ambiguity words in sentence or context by any criterion and to rebuild iterative approach to graph. In this paper, we have matched the most similar words in sentence or context using the Word2Vec and have constructed the graph with a high order similarity value. As a result, this paper has a higher F1-Measure value than the previous methods by using Word2Vec.자연어에서는 하나의 단어가 둘 이상의 의미를 가지기도 한다. ‘다리’라는 단어는 사람이나 동물의 몸통 아래 붙어있는 신체의 부분을 의미하기도 하고, 물을 건너거나 또는 한편의 높은 곳에서 다른 편의 높은 곳으로 건너다닐 수 있도록 만든 시설물을 의미하기도 한다. 인간은 이러한 단어가 문장에 사용되었을 때 어떤 의미로 사용되고 있는지 쉽게 판단할 수 있지만 컴퓨터는 그렇지 않다. 따라서 둘 이상의 의미로 사용되는 중의성 단어가 문맥에서 어떤 의미로 사용되는 지를 정확하게 파악 하는 작업이 필요하다. 이 작업을 단어 중의성 해소라고 한다.

단어 중의성 해소 방법은 지도 방식 및 비지도 방식으로 분류할 수 있다. 지도 방식 단어 중의성 해소 접근 방법은 단어들에 대한 의미 태그가 붙은 말뭉치를 사용하여 통계적인 정보를 추출하고 이를 판단하는 방식이다. 하지만, 신뢰할 수 있는 결과를 산출하기 위하여 많은 양의 태그가 붙은 말뭉치를 필요로 하고, 그 범위 또한 몇몇 단어들로 제한되어 있다. 그러나 언어의 모든 단어와 의미를 포함하고 적합한 학습데이터를 작성하는 것은 매우 어렵고 비용이 많이 드는 작업이다. 본 논문에서는 이런 문제점들 때문에 대규모의 의미 태그가 된 말뭉치를 필요로 하지 않는 그래프 기반 알고리즘을 사용한 비지도 방식 단어 중의성 해소 접근방식을 제시한다.

그래프 기반 단어 중의성 해소는 모든 중의성 단어를 가지고 의미 그래프를 구축하게 되면 불필요한 간선과 노드 정보가 추가되어 오류를 증가시킨다는 단점이 있었다. 본 논문에서는 이러한 문제를 해결하고자 반복적 접근 방식의 그래프 기반 단어 중의성 해소 방식을 사용했다. 이 방식은 모든 중의성 단어들을 특정 기준에 의해서 단어 매칭을 하고 매칭 된 단어들을 반복적으로 그래프를 재구축하여 단어중의성을 해소하는 방식이다. 본 논문에서는 Word2Vec을 이용해 문맥이나 문장 내에 중의성 단어와 가장 의미적으로 유사한 단어끼리 매칭하고, 매칭 된 단어들의 유사도가 높은 순서대로 그래프를 재구축하여 연결 관계를 보고 중의성 단어의 의미를 결정하였다. 결과적으로 Word2Vec의 유사도를 이용함으로써 이전에 연구 되었던 그래프 기반 방법과 반복적 접근 방식의 그래프 기반 방법보다 더 높은 F1-Measure값을 얻을 수 있었다.

오늘날, 지적이거나 정책적인 이유에 있어서, 과학분야나 사회과학분야에서의 개념, 아이디어, 그리고 문제들 간의 연계성을 도식화할 수 있도록 하는 것은 매우 중요하다. 이러한 도식화를 위해 몇 가지 방법이 시도되어졌다. 과학연구와 과학정책에서 사용되어진 전통적인 방법은, 상대적으로 소수 전문가들의 견해를 구하는 방법이었다(peer review)(Law & Whittaker, 1992). 서지적 연구(본 연구에서는 이를 bibliometrics 이라고 정의함)는 정량적 측면에서 이러한 작업을 수행하기 위한 또 다른 방법으로 제시되고 있다.
Bibliometrics는 비교적 생소한 분야로, 과학문헌에 계량서지학을 적용시킴으로써 일정한 패턴을 찾아내고 설명하는데 중점을 두고 있으며 소스로는 서지학 데이터베이스를 주로 사용하였다. 풍부한 서지학 자료들은 bibliometrics의 몇몇 관점을 지식검색과정과 데이터 마이닝(data-mining)으로 변모시켰다. 도메인 시각화(domain visualization)는 아직까지 거의 발전이 이루어지지 않은 연구분야이다. 이는 전체 지식영역을 하나의 분석단위로 취급하던 전통적인 영역분석에서 파생된 분야이다. 도메인 시각화에서는 지식영역의 구조와 원동력을 탐색하여 연구하고 개발하는 과정에서 정보 시각화(information visualization)의 역할을 매우 강조하고 있다. 장래성 있고 주목을 끌고 있는 트랜드는 철학, 사회학, bibliometrics, 정보시각화, 영역분석 등의 여러 전문분야에 걸친 시너지를 통해 그 형태를 잡아가기 시작했다.
Bibliometrics은 문헌의 양적연구에 의해 이루어진다. 이것은 Viz, 인용 및 동시인용분석과 같은 다양한 경험적 방법을 포함하고 있다. 동시인용분석은 서지적인 분석에서 작고 중요한 부분이다. Small(1973)이 “이전 연구들이 이후연구들에 의해 함께 인용되어지는 경우의 빈도”라는 개념과 정의를 소개하면서 동시인용분석기법은 많은 학문분야의 지적구조를 측정하는데 성공적으로 적용되어졌다. 이 표준들은 일반적으로 특정한 서표(Marker)가 발생 또는 동시발생 빈도수를 포함하거나, 저자 동시인용, 저널 동시인용, 키워드 동시인용 등과 같은 증가되어진 정보를 제공한다. 특히 저자, 저널, 원문상의 내용과 같이 각기 다른 부분으로부터의 정형화 된 학문적 기록에 적용이 가능하다. Ding et al.(1999, 2000)은 최근 연구를 통해 서지적 연구들이 저자 동시인용을 이용한 주제의 맵핑과 정보검색분야에서의 저널 동시인용분석의 발전을 추적하는데 어떻게 이용될 수 있는지를 보여주었다. 또한, 서지적연구들은 사회적이고 정책적인 질문들에서 과학과 기술을 개발하고 지식의 발전을 목표로 하고 있다(van Raan, 1997). 저자나 저널의 동시인용분석과 같은 전통적인 서지분석기술들은 과학논문에 내포되어있는 인용자료들에 대한 분석을 근간으로 이루어진다. 이러한 종류의 분석들은 흥미로운 결과를 가져다주는 반면, 논문을 이용한 연구주제의 실제적인 내용에 대한 즉각적인 그림을 공급해 주지는 못한다. 주어진 주제 상에서 출간되어진 논문 내에서 키워드의 동시출현을 측정하거나 분석하는 동시단어(Co-word) 분석은 이런 류의 분석적 문제를 간결하게 처리할 수 있는 잠재력을 가지고 있다(Callon et al., 1991) 동시단어 분석방법은 데이터에 포함되어져 있는 기본적인 정보를 유지하면서 데이터를 독특한 시각적 방법으로 단순화시켜서 보여준다. 이것은 과학적인 개념이나 아이디어 그리고 지식 등의 중요한 제공자로서 자연어를 근간으로 한다(van Raan and Tijssen, 1993). 많은 연구자들이 다양한 분야에서 개념의 연결관계를 찾아내기 위한 중요한 방법으로 동시단어 분석기법을 사용하고 있는데, 예를 들면, 소프트웨어 엔지니어링(Coulter et al., 1998), 폴리머 화학(Callong et al., 1991), scientometrics(Courtial, 1994), 신경망연구(Noyons and van Raan, 1998; van Raan and Tijssen, 1993), 생물학적 안전성(Cambrosio et al., 1993), 산성화 연구(Law and Whittaker, 1992), 특허(Courtial et al., 1993), 광학공학(Noyons and van Raan, 1994), Bioelectronics(Hinze, 1994), 약학(Rikken et al., 1995), 생물학(Rip and Courtial, 1984; Looze and Lemarie, 1997), 농축물질물리학(Bhattacharya and Basu, 1998) 등이 있다.
과학기술의 특징은 복잡하고 서로 다른 학문분야의 서로 다른 지식 도메인으로 구성되어 있고, 상호 관련된 측면이 존재한다는 것이다. 또한 오늘날 과학기술과 관련된 많은 양의 정보가 출판물과 특허에 체화(embedded)되어 끊임없이 증가하고 있다. 그러한 대규모 데이터로부터 구조화된(well-structured) 패턴의 정보를 추출하기 위한 기법을 개발하는 것은 상당한 도전이다. 현재까지 그러한 패턴은 관련성을 인식하는데 영향을 주는 숨어있는 특성이며 근원적인 것으로 드러났다.
진보된 bibliometric 방법론, 특별히 Knowledge Map은 많은 가능성을 제공하고 있다. Knowledge Map은 출판물, 특별히 키워드의 동시 출현(co-occurrence)에 숨어있는 데이터의 통계적 특성을 가시적으로 보여준다. 그러한 지도제작법 표현은 몇 가지 중요한 장점을 가지고 있으며, 거대하고 복잡한 데이터를 가시화(visualization)하는 것은 보다 짧은 시간 안에 좀 더 완성된 개관을 제공한다. 게다가, 그러한 가시화된 표현은 좀 더 쉽게 기억된다.
Knowledge Map의 시계열적 분석은 과학기술의 구조적 개발에 대한 역동성을 드러내준다. 예를 들어, 새로운 활동의 출현, 과학적 도구의 중요성 증가, 합성과 분열과 같은 과학기술분야의 시간흐름에 따른 중요한 변화를 확인할 수 있다.제 1 장 서론 1. 연구의 목적 및 필요성 2. 연구의 방법 및 구성 제 2 장 지식맵(Knowledge Map)의 연구동향 및 이슈



서문 : 본 논문의 당면성과 학술적 새로운 면이 성격 규정되어 있고 연구의 방법적 기초와 목적, 과제가 나와 있음
제 1장 "아끄메이즘의 극복의 길" : 아흐마뜨바는 아끄메이즘의 원칙을 자기 것으로 하면서 그 원칙들이 자기실현을 허용하는 범위 한에서만 아끄메이즘에 남아 있었다. 최초의 시집 "저녁(1912)에서 일상적인 것과 존재론적인 것을 미학적으로 통일시키며 아흐마뜨바는 아끄메이즘의 특징중의 하나인 세상에 대한 낙천주의적 경향과 "물적 자연"의 표현을 터득하고 있었다. 그러나 그녀의 시들에는 다른 동료들의 시에서 보다 훨씬 더 자주 더 높은 수준에서 서정적 주관성이 나타난다.
다른 아끄메이스트들과 마찬가지로 아흐마뜨바도 서정적 장르를 편애했다. 아흐마뜨바는 외면세계와 내면세계의 표현에서, 시에서의 여러 가지 문제의 해결에서 한가지 방법만 사용한 것이 아니었다. 그녀는 여러 가지 방법을 동시에 선택했으며 이것은 그녀 시의 지평을 넓히는 구실을 하였다. 여기서 주목할 사실은 러시아 전통의 구비 문학으로 이끌리는 방법이 그녀의 시를 풍부화시키고 있다는 것이다.
제 2장 "아흐마뜨바의 시에서 물적 디테일의 체계" : 아흐마뜨바의 시에서 외면세계의 표현은 시각, 청각, 촉각, 미각, 후각의 여러 가지 성격을 얻고 있으며 그 결과, 색조에서, 무게와 부피, 조형적 형태에서, 물질의 표현에서, 세계의 입체 영화적 표현의 효과를 얻게 된다.
시에서 나타나는 대상들은 내면세계의 표현에 봉사하게 되는데, 여기에 아흐마뜨바 시학의 새로운 면이 나타나고 있다. 아흐마뜨바의 시학체계에서는 외면세계와 내면세계의 상호작용이 특징적으로 나타나며 전자가 후자의 표현에 봉사하게 된다.
제 3장 "아흐마뜨바 서정시의 구성" : 먼저 시의 구성에는 높은 수준에서의 날카로움, 전환과 이미지 변화의 뜻하지 않음이 특징적으로 나타난다. 시의 근본에 놓여 있는 것은 긴장되기도, 일시적이기도 하다. 그리고 이것은 시에서 내용의 흐름을 규정하며, 알려진바 대로, 시공의 범위에서 일어난 체험의 특별한 이야기" 라는 형태로 나타나고 있다.
아흐마뜨바 서정시 구성의 특징은 직접적으로가 아니라 외면세계의 디테일의 도움으로 심적 체험을 표현하고자 하는 데 있다. 이러한 풍부한 표현력에도 이 디테일들의 독자성은 사실 크지 않고 그들은 서정적 주인공의 마음속에 일어난 것의 표현에 봉사하고 있다.
제 4장 "아흐마뜨바 서정시에서 말에 나타난 문체의 특성" : 이 장의 분석의 중심에는 단어가 나타나지만 단어는 예술 텍스트의 모든 면과 연관되어 있다. 아흐마뜨바가 단어의 의미를 확장시키기는 했지만 단어들의 의미상의 상호작용이 완전히 새로운 의미를 낳지는 않고 그녀 시에서 단어들은 단지 새로운 뉘앙스만을 얻게된다. 그녀가 은유법을 사용하기보다는 직유법을 더 선호하는 것이 이것을 설명해주고 있다.
아흐마뜨바는 은유법보다는 비유적 형용어나 비교를 더 선호하는데 이런 이유로 여기에서는 비교되는 부분들 사이의 관계가 느껴진다.
아흐마뜨바 서정시의 문체의 특징은 우선 그녀 시의 주요한 특징으로써 심리주의에 의해 설명되어진다. 표현의 과장을 극도로 피하고 자연어에 근거하면서 시인은 내면세계의 창조에 집중하고 있다.
결론 : 이상에서 바로 문체의 수준에서(물적 디테일, 구성과 마지막으로 단어의 문체에서) 아흐마뜨바의 서정시는 자기 속에 갇혀 있는 것이 아니라 세계로 향하고 있다. 아흐마뜨바의 회상에 따르면 오시쁘 만델쉬땀은 언젠가 그녀에게 말했다 "당신의 시를 읽으면 가끔 나에게 비상의 기분이 느껴진다... 항상 그렇게 쓰기를..."
우리의 연구 과제는 아흐마뜨바 시에서 무엇이 이러한 비상을 느끼게 하는가를 설명하는 것이다.

언어학습의 목표는 의사소통이며, 충분한 어휘 학습은 의사소통 능력 향상의 전제 조건이 된다. 어휘 지식은 연어적 지식이며(Nation, 2001), 연어 능력의 부족은 다양한 문맥이나 단어들이 실제로 어떻게 사용되는지에 대한 어휘 학습 활동의 부족에 기인한다(Hill, 2000). EFL(English as a Foreign Language) 환경에서 연어 습득은 인위적으로 직접적인 교육을 통해서 습득할 수 있다(Lewis, 1993). 여기서 실제 원어민들이 쓰는 말뭉치의 개념인 코퍼스 자료는 언어의 실제 사용에 대해 더 많은 정보를 얻을 수 있고, 귀납적 방법으로 학습자의 발견 학습과 자율성을 보장 할 수 있는 자료로서 효용성이 매우 높다. 따라서 본 연구는 영어의 어휘적 연어(lexical collocation)의 학습에 있어서 코퍼스를 활용한 방식의 효과를 검증하고, 그 결과가 학습자의 영어 능숙도에 따라 차이가 있는지에 대해 알아보고자 한다.
본 연구는 서울 소재 인문계 고등학교 2학년 4개 학급 160명을 대상으로 4주간 실시되었고, 실험 전후에 사전·사후 검사를 실시하였다. 연구 문제는 첫째, 콩코던스 자료와 전통적인 어휘 학습 자료를 활용한 집단 간의 어휘적 연어 학습에 차이가 있는가와 둘째, 집단 학습자들 간의 영어 능숙도에 따라 어휘적 연어 학습에 차이가 있는가이다. 실험 집단에는 학습자의 수준을 고려한 콩코던스 어휘 활동자료, 통제 집단에는 의미와 사용중심의 전통적인 어휘 활동자료가 각각 제공되어 모둠활동을 통한 발견학습이 진행 되었다. 어휘지식의 사용, 형태, 의미 (Nation,2001) 중 형태와 사용에 관한 수용적 지식에 대해 사전․사후 평가를 실시하였고, 실험 효과는 두 집단별 및 수준별 그리고 전반적인 어휘능력과 세부적인 어휘능력 향상 도를 비교 분석 하였는데, 그 결과는 다음과 같다.
첫째, 실험 집단에서는 어휘적 연어의 형태 및 사용 인지에서 통계적으로 유의미한 향상을 보인 반면에 통제집단에서는 유의미한 향상이 없는 것으로 나타났다. 둘째, 학습자의 능숙도에 따라 실험 집단 상위·하위 학습자 모두 통계적으로 유의미한 향상을 보인 반면 통제 집단에서는 상위․하위 학습자 모두 통계적으로는 유의한 향상이 없는 것으로 나타났다.
또한 세부적인 어휘 능력(사용, 형태)에 따른 학습자의 능숙도별 어휘적 연어에 대한 인지도에는 상․하위 집단 에서 모두 유의미한 향상이 있었는데, 특히 사용 및 형태 모두 사전 대비 사후의 평균값 증가가 하위 집단보다 상위집단이 더 높게 나타났다. 따라서 콩코던스 자료를 활용한 어휘 학습이 전통적인 자료의 어휘학습에 비해 어휘적 언어 학습에 상위․하위 학습자 모두에게 긍정적인 영향을 미친다는 사실을 알 수 있다.
이상의 연구 결과를 통해 언어 교수 및 학습에서 코퍼스를 활용하는 방안에 대해 다음과 같은 시사점을 제공한다. 첫째, 교사는 콩코던스 용례 문장을 어휘적 연어 학습에 보다 적극적으로 활용하도록 한다. 영어의 입력이 부족한 EFL 환경에서는 실제 그 언어가 사용되는 자료를 풍부하게 접하는 것이 중요하다. 따라서 자연어의 총체인 콩코던스 예문은 원어민 화자들이 느낌으로 터득하는 어휘적 연어 학습에 실증적인 자료로 효과적이다. 둘째, 교사는 콩코던스 자료를 학습 여건과 학습자의 수준을 고려하여 재구성 하도록 한다. 단순히 코퍼스에서 추출한 원자료(raw corpus)를 학습자에게 제공하기 보다는 학습자의 영어 능숙도에 맞게 해당 학년 교과과정에 제시된 어휘를 기준으로 그 어휘 수준과 그 양을 설정하고 이에 준하여 콩코던스 자료에서 선별작업을 하여 학습활동자료를 제공 한다면, 하위 학습자들의 이해도와 참여도가 증대될 것이다. 셋째, 어휘 교수와 코퍼스 활용을 접목 시키려는 노력 이전에 코퍼스와 관련한 교사 교육이 선행되어야 한다. 교사에게 코퍼스 접근 방법의 장점과 이를 사용할 여러 가지 동기를 부여해 주는 것이 우선되어야 한다. 교사 대상의 각종 자격 연수 등의 과정에서 코퍼스를 활용한 교수 방법에 대한 과정이 증대 되어야 할 필요성이 있다.The purpose of this study is to examine the effects of corpus-based English lexical collocation teaching according to learners' English proficiency level and also to discuss and propose an efficient corpus-based English lexical collocation teaching method for Korean high school students. To meet this purpose, this study investigates any significant difference between the experimental group treated by corpus-based vocabulary tasks and the control group treated with traditional vocabulary tasks in English lexical collocation teaching. In addition, the students completed the pre-and post-treatment tests measuring the word form and use aspects of target vocabulary items in order to assess learning gains and measure the interaction between corpus-based learning and the learner's English proficiency. The subjects, four classes, a total of 160 Korean boys' high school students, participated in the study through either corpus-based learning or traditional instruction. All of the groups focused on twenty items of lexical collocation during a four-week session and each group consisted of one upper-level class and one lower-level class. The English proficiency levels of each class were almost the same.
The results of this study are as follows. First, in the experimental group the corpus-based vocabulary task was helpful for the learning of word forms and use. In particular, corpus-based benefits were greatest for the collocation aspects of the use of low-proficiency EFL learners. On the contrary , in the control group, the traditional vocabulary task had no effect on the learning forms and use. These findings suggest that the corpus-based vocabulary task helps students raise language awareness of lexical collocation by exploring corpus data. Second, the corpus-based vocabulary task was more effective than the traditional one for both high and lower-level students. Especially, lower-level students of the experimental group show higher achievement in their post-test. That is because teacher offered background knowledge about how to analyze the corpus data which was given in the present session, before starting cooperative learning of the corpus tasks. That is, low-level students of the experimental group as well as high-level students could get used to corpus data in advance. Moreover, depending on the level of students, the corpus data was edited with suitable words, which helped lower-level students facilitate the activity.
In conclusion, corpus-based learning leads to the enhancement of students in lexical collocation and there is no significant correlation between learners' proficiency level and corpus-based learning. In order to lead the successful implementation of the corpus approach, teachers have to meet the practical needs of students who have various proficiency levels. For this to be real, teachers should make an effort to develop various corpus materials based on the level of students and use various group activities, which facilitate language learning.논문개요 ⅶ Ⅰ. 서론1 1.1 연구의 필요성 및 목적1

Predicting the accurate travel speed is vital, since the travel speed is the basis of traffic information and is necessary for controlling or managing the dense highway network. However, the methodologies developed so far were not sufficient to ensure a satisfactory level of accuracy. Recently, with the increased utilization of big-data and the development of deep-learning technology, deep-learning based on big-data is widely used in the field of speech/image recognition and natural language processing. Also, deep learning is continuously applied in the field of transportation engineering, and has shown good applicability. Thus, this paper used the deep-learning technology to predict the accurate travel speed of the dense highway network.
The existing models, such as Time-series model and Kalman filter method, are limited for their spatial and temporal range; thus, the researcher himself/herself should define the range of analysis. However, this paper used the RNN model, one of deep learning technologies, and expanded the limitation of spatial and temporal range.
The test-bed of the analysis was set to the upward section of Youngdongdae-ro and the hyper parameters were optimized to boost the performance. To verify the performance of the developed model, the result derived by Kalman filter, the naive forecasting method, the RNN model for a single link, and the RNN model for multi links were compared according to the %RMSE value and pattern of the scatter diagram
It is concluded that the accuracy of the travel speed prediction was placed in the following order: the RNN model for multi links, Kalman filter, the RNN model for a single link, and the naive forecasting method. In the future, the removement of abnormal links or upper limit of the travel speed is required to improve the performance of the developed model.통행속도는 교통 제어 및 교통 관리에 기본이 되는 요소이며 교통 정보 서비스의 근간을 이루는 핵심 정보로, 통행속도 추정의 정확도를 확보하는 것은 그 중요도가 매우 높다. 정확도 제고를 위한 다양한 노력이 전개돼 왔지만 현재까지 발표된 기술로는 만족할 만한 수준의 정확도를 확보하지 못했다. 최근에 빅데이터의 가용성이 확대되고 딥러닝 기술이 발달하면서 빅데이터와 딥러닝은 음성․이미지 인식, 자연어 처리 분야 등에서 새로운 표준으로 빠르게 자리 잡고 있다. 교통 분야에서도 두 가지 시대적 흐름에 부응하려는 연구들이 속속 등장하여 접목 가능성을 타진한 바 있다. 본 연구에서는 빅데이터와 딥러닝을 통행속도 추정에 적용하여 기존 연구의 한계점을 극복하고, 추정된 통행속도의 정확도를 제고시키고자 하였다.
시계열 모형이나 칼만 필터와 같은 기존의 통행속도 추정 모형은 연구자가 시․공간적으로 연구의 범위를 한정해야 하는 한계를 가지고 있었다. 하지만 본 연구에서는 딥러닝 기술 중 하나인 RNN 모형을 사용함으로써 시․공간적인 연구의 범위를 확장할 수 있도록 하였다.
영동대로 상행구간을 테스트베드로 설정하여 다중 링크의 통행속도 추정을 위한 RNN 모형을 최적화시켰다. 개발된 예측 모델의 성능을 검증하기 위해 기존 모형인 칼만 필터, 단순 예측 방법인 나이브 예측, 단일 링크만을 독립 추정할 수 있는 RNN 모형을 함께 구현하여 %RMSE와 산포도를 비교하였다.
%RMSE, 산포도 분석 결과, 예측력의 순위는 다중링크를 상관 추정할 수 있는 RNN 모형, 칼만 필터, 단일 링크에 대해 독립 추정할 수 있는 RNN 모형, 나이브 예측 순으로 드러났다. 향후 통행속도 예측의 상한을 제거할 수 있는 방법론이 마련된다면 획기적인 모델 성능의 향상을 가져올 것으로 판단된다.Ⅰ. 연구의 개요 1 가. 연구의 배경 1 나. 연구의 필요성 2 다. 연구의 목적 및 범위 4 Ⅱ. 선행 연구 고찰 5

본 연구의 목적은 증가하는 민원에 따른 행정 소요를 감소하기 위해, 민원 분류 절차에 문서 분류 모형을 적용하여 자동화하고자 하는 것에 있다. 4차 산업혁명에 따라 사람 간의 의견을 공유하기 위한 다양한 의사소통 수단이 생겨났다. 이에 따라 시민들의 민원청구 채널도 증가하였다. 민원 청구가 간편해지면서, 국내 지자체 민원량은 최근 3년간 급격히 증가하고 있다. 민원 증가에 비해 지자체의 민원 처리 담당 인력은 한정되어 있어, 처리 부담이 증가하고 있다. 서울시의 민원 처리 프로세스에는 민원을 카테고리별로 분류하는 절차가 포함된다. 이 절차를 자동화할 수 있다면 민원 행정의 효율성이 증가하고 민원 이슈 상황의 빠른 해결이 가능할 것이다. 따라서 본 연구에서는 서울시 응답소의 민원 텍스트 데이터에 분류 모형을 적용하였을 때, 정확한 분류가 가능한지 확인하고자 하였다. 응답소의 민원 본문 텍스트와 카테고리 데이터를 대상으로 전통적 분류기 및 딥러닝 분류기를 통해 분류하여 분류 예측 정확도를 비교 측정하였다. 문서 분류 관련 최근 연구에서 전통적인 분류기에 비해 딥러닝 분류기인 계층적 주의 네트워크(Hierarchical Attention Network, HAN)가 높은 분류 성능을 보였다. 그래서 본 연구에서는 HAN분류기를 중심으로 전통적인 알고리즘과 민원 분류 정확도비교 분석을 실시하였다. 먼저 수집된 민원 텍스트 데이터에 대해 토픽 모델링 분석을 시행하였다. 15개, 20개의 토픽을 추출하여 민원 분류 카테고리와 비교한 결과, 모든 토픽들이 기존 민원 분류 카테고리에 포함되었다. 이를 통해 기존 민원 카테고리 분류가 미분류 되는 민원이 없이 분류할 수 있음을 확인하였다. 다음으로 수집된 민원 본문과 카테고리 텍스트 데이터로 구성된 데이터셋을 구성하였다. 데이터셋을 대상으로 분류 모형을 적용한 각 분류모형의 정확도는 Naïve Bayes 25.813%, Logistic Regression 32.436%, Support Vector Machine 32.212%, Random Forest 81.246%, XGBoost 94.647%로 전통적인 분류 모형에서는 XGBoost가 가장 높은 분류 성능을 보였다. 딥러닝 모형인 HAN 모형의 경우 99.042%의 정확도를 보여 전체 실험한 모형 중 가장 높은 분류 성능을 보였다. 추후 본 연구의 결과를 활용한다면 HAN 모형을 활용해 민원 처리 프로세스를 단축함으로써 지자체의 업무 효율성 및 시민 만족도를 향상시킬 수 있으며, 향후 타 지자체를 대상으로 적용 범위를 확장시킬 수 있을 것이다.
핵심 키워드: 민원, 딥러닝, 카테고리 분류, 계층적 주의 네트워크, 자연어 처리The purpose of this study is to automate the application of a document
classification model to civil service classification procedures in order
to reduce administrative disturbances due to increasing civil complaints.
The fourth industrial revolution resulted in various means of
communication to share opinions between people. As a result, the number
of citizens' channels for civil petitions has increased. As civil
petitions have been simplified, the amount of complaints filed by local
governments in Korea has been increasing rapidly over the past three years.
The number of people in charge of handling civil petitions is limited
compared to the increase in civil petitions, which is increasing the
burden of handling them. The Seoul Metropolitan Government's process of
handling civil petitions includes procedures to categorize complaints by
category. If this procedure can be automated, the efficiency of the civil
service administration will be increased and a quick resolution of the
civil service issue situation will be possible. Therefore, when the
classification model was applied to the civil service text data of the
Seoul Metropolitan Response Center, the purpose of this study was to
ensure that the correct classification was possible. The accuracy of the
classification prediction was compared and measured by classifying the
text and category data of the response station through the traditional
classifier and deep learning classifier. Recent studies on document
classification showed higher classification performance by the deep
learning classifier (HAN) compared to the traditional classifier. Thus,
in this study, traditional algorithm and civil service classification
accuracy comparison analysis were conducted with the focus of HAN
classifier. A Topic Modeling analysis was performed on the collected civil
text data first. After extracting 15 and 20 topics and comparing them
with the civil service classification category, all topics were included
in the existing civil service classification category. It was confirmed
that existing classifications of civil service categories could be
classified without unclassified complaints. Next, a dataset consisting of
the collected body of civil petitions and category text data was organized.
The accuracy of each classification model applied to datasets was Na.8ve
Bayes 25.813%, Logistic Recession 32.436%, Support Vector Machine 32.212%,
Random Forest 81.246%, and XGBoost 94.67%, showing the highest
classification performance in traditional classification models. The HAN
model, a deep-learning model, showed 99.02 percent accuracy, the highest
classification performance among all tested models. If the results of
this study are utilized in the future, by using the HAN model to shorten
the process of handling civil petitions, local governments will be able
to improve their work efficiency and citizen satisfaction and expand their
coverage to other local governments in the future.

이 論文은 空間을 二次모델 形成體系(secondary modelling system)의 한 言語로 보고 그것이 구체적으로 詩의 텍스트 안에서 어떻게 作用하고 있는지를 分析함으로써 文化記號論의 텍스트 理論을 定立하고자 한 것이다.
그리고 그것의 實□性과 有效性을 檢證하기 위해서 작품량이 많아 코르프스(資料體)로 그 적용 範圍가 넓고, 하늘 땅과 같은 宇宙論的인 성격을 지니고 있는 靑馬의 誇를 대상으로 한 것이다. 記號論의 基本을 이루고 있는 것은 差異이고 그 차이는 二項對立的 關係에서 생겨나는 것이다. 그러므로 音韻體系와 같은 方法으로 空間의 離散的 單位를 추출하면 垂直, 水平으로 분절되고 그것은 다시 上/中/下와 內/境界/外의 下位分節이 가능해진다. 그리고 그러한 공간체계는 聖俗이나 生死와 같은 兩極化된 觀念, 그리고 긍정 부정과 같은 價値의 대립체계와 연계된다. 이같은 空閣의 離散性과 그 意味作用을 檢證하기 위해 서 靑馬의 誇的 言述(poetic discourse)을 분석하고 그 空間이 意味하는 것을 神話的인 그리고 字宙論的인 原型的 空間의 象徵性과 比較하면 그 構造의 相同性이 드러나게 된다. 終局에는 이같은 방법에 의해 그의 全詩를 精造化하고 一定한 略號를 사용해서 그 變異項 들을 記述해 낼 수 있게 된다. 뿐만 아니라 一次言語의 意味가 二次體系의 言語로 번역되는 코드 轉換을 밝혀냄으로써 그가 많이 사용하고 있는 키워드(旗, 山, 나무, 하늘, 거리, 그리고 侮恨, 孤絶, 애련 등)의 內包的인 意味를 解讀할 수 있게 된다. 그것을 要約 하면 다음과 같다.
① 垂直的인 空間의 텍스트는 理念的인 價値의 記號로서 作用하여 上方的인 空間은 하늘, 下方的인 空間은 땅(거리)으로 分割되어 肯定과 否定의 二元的 價値를 나타내는 世界像의 메타言語가 된다.
② 그러므로 下空間에서(땅/거리) 上方으로(하늘) 上昇하고 있는 대상물들은 모두 垂直的 超越의 意味作用을 나타내는 記號의 役□을 한다. 우러러러, 발돋음, 솟아나다 등등의 述語가 그렇다.
③ 上下의 二元構造에 動的인 變換體系를 일으키는 것이 그 媒介項의 機能을 나타내는 山, 樹木, 장대, 旗와 같은 中間空間이다. 上下의 兩義性을 떤 그 中間的 空間은 새, 나비, 잠자리와 같은 생물과 위로 올라가는 담배 煙氣, 불꽃, 향내 등 감각적인 自然現象에 이르기까지 多義約 記號(polysemic)의 意味를 生成하게 된다. 그리고 이러한 媒介空閣은 무수한 텍스트의 變異態를 生成하여 日常的 言語의 脫自動化와 같은 變則, 逸脫의 誇的 言述을 가능케 한다.
④ 垂直空間이 精神的인 理念體系를 나타내는 記號形式이라고 한다면 水平空間은 世俗的인 人間關係를 나타내는 物貿的 體系를 나타내는 空間言語라고 할 수 있다. 內/外로 分節되는 水平空間에서 內空間은 주로 居佐空間인 집과 故鄕 등이며 그와 대립되는 外空間 이 북만주의 廣野나 아라비아 사막, 바다, 그리고 섬들이다. 北이 外空間을 나타내는 記號로서 作用하고 있는 데 비하여 南은 故鄕의 內空間을 表示하고 있는 작용을 한다.
⑤ 水平空間은 垂直空間과 달리 人物의 空間移動이 可能하므로 자연히 그 텍스트는 動態的인 것이 되고 敍事的 性格을 띠게 된다. 그러므로 그 動態的인 텍스트는 主人公=話者가 內空間에서 外空間으로 향해 나가는 텍스트와 外空間에서 內空聞으로 들어오는 텍스트로 對立되고 그에 파라 서로 相違한 意味構造가 생겨나게 된다. 垂直體系에서는 宇宙樹와 같은 意味作用을 나타내던 나무가 水平的 텍스트에서는 한 곳에 뿌리박고 있는 그 不動性 때문에 폐쇄공간의 否定性을 나타내게 된다.
⑥ 水平空間에서의 境界領域이나 □壁/窓 같은 것들은 垂直空間에서의 媒介項과 同一한 機能을 갖게 된다. 內와 外의 交換이 이루어지는 港口, 驛, 우체국, 그리고 길 같은 곳은 靑馬의 텍스트에 있어 多義的인 認號를 生成하고 있는 空間이 된다.
水平垂直의 이같은 離散的 單位들이 結合하여 以上과 같은 여러 形態의 텍스트를 形威 하고 그것들의 記號作用을 통해서 自然語로서는 記述할 수 없는 論的 意味를 産出하는 空間의 誇學을 定立하게 된다.1. Poetic linguistics offers highly effective method for the scientific interpretation and description of literary works without destruction of their autonomy. Since it takes natural language as the object of its analysis, however, it is bound to have limitations when it comes to the interpretation of the extra-liguistic world of meaning. It Is In order to deal with this problem that the Semiotics of Culture has been developed, studying literary language within a "secondary modelling structure" with the concept of space as one of its fundamental models. As a result, semiotics scholars such as Lotman. V.N.Torpor of the Tartu school, the French critics Roland Barthes and Ge´rard Genette, have investigated the signification of Space in its function as signifier.
Nonetheless, they have not arrived at a systematic result applicable in practical criticism. It is the task of the present study to offer a systematization of the Semiotics of Space, such as may be applicable in practical criticism, developing the hitherto embryonic theories of semiotics. In order to do this, the attempt is made to fuse semiotic theory with the phenomenological theories of space advocated by Bachelard and Mircea Elliade, with the theories expounded by Frances Yates in his exploration of the theories of the Topos used as an art of memory in the old rhetorics, together with Cassrer's theones of Symbolism.
2. The object of this study: at the same time to create a theory and to test its effectiveness. The Complete works of Chung Ma have been chosen for study and analysis since, compared with other poets, they offer a large number of poems and, as a corpus, provide a wide range of applications. Particularly, since much of they are paticularly apt for study as a model of spatial text. In addition, since we find here a mixture of lyric, abstract, sociological, political and even epic, it may prove possible to test and establish the meaningful activity of space in various literary areas.
3. The Contents and procedure of this research: the basis of Semiotics lies in the differences deriving from binary oppositions. If we survey the dimensions of space using this system, we see that it divides vertically and borizontally, or again into oppositions by double-b Teakdown-above, middle, below, or inside, frontier and outside.
Such a system of space can be related to pluralized concepts-sacred and propane life and death, sorrow and rejoicing in emotion, positive and negative in values-systems. In order to verity these dimensions of space and their semantic movement this stud analyses the poetic discourse of Chuag Ma's poetry, comparing its images of space with archetypal symbols of myth and cosmology in order to discover a homological structure of unexpected identities. Such a method not only structuralizes these poems, it also defines their variations using a variety of codes. By tracing He transformations of their codes from primary language to secondary, we are enabled to decipher to connotative meaning of those key words which are most frequently used (flag, mountain, or emotional terms such as regret, solitude, pathos). The semantic movement of the tex may be analysed as follows:
1) The text of verticai space represents differences of ideological value; upward space is sky, downward space is earth or distance; given positive or negative values, these function as metaphors indicating the order of the Universe.
2) Therefore, objects rising up from the downward space (earth/distance) function as codes which represent tile vertically transcendent semantic movement. Thus, Words like "looking up", "stand on tiptoe", "arise", "stand" occur very often in Chung Ma's poems. To such texts is opposed the semantic movement of an opposing text, descending from up to down.
3) A dynamic transformational system is constituted by middle space such as mountains, trees. rods and flag, which have catalytic functions in both So this middle space. with its double references. creates polysemic codes ranging from animals such as birds and butterflies or dragonflies to tangible natural phenomena such as rising smoke, flames of fume.
Such catalytic space forms numerous textual variants, making possible the poetic discourse of anomaly and deviation, as in the automatisation of common language.
4) While vertical space is a semiotic form representing spititual systems of ideas, horizontal space can be: described as an expressive space representing those material systems which profane human relationships. Horizontal space is divided into within and without, inner space being residential space, such as house or native village, contrasted with outward spaces, the wildmess of northern Manchuria the Arabian desert, sea and islands. North functions as a code representing outward space while South represents inner space.
5) Since characters can move through horizontal space, naturally the text becomes dynamic, charged with epic meaning. Dynamic text arises when the character (narrator) moves through space, from inner to outward or from outward to inner space. As a result, there arises a varying semantic structure. In the vertical system, the tree represents a positive semantic movement as it grows up like a cosmic tree towards the above whereas when it is pinpointed in a horizontal system, it represents the negative force of the closed inner space because it is immobile, rooted in one place.
6) Objects which mark the confines of horizontal space, such as walls and windows, have functions similar to the catalytic items found in vertical space. Locations such as harbors, stations, post offices and roads are spaces which create polysemic codes in the works of Chung Ma. Such spaces having a function of mutual exchange, they perform at the same level as trains, ships and letters.
7) While the falcon is the bird representative of vertical space, the swallow is the bird of horizontal space. Since the swallow migrates across the sea though it builds its nest under the eaves of human nomes, it represents escape from and return to within the house (inner space). The sparrow then functions to represent complex emotions or actions. Animal language such as that of the sparrow is the poetic discourse of horizontal space.
4. The results of this study: discrete units of vertical and horizontal space cohere to pro- duce various forms of text and the semiotic movement of these texts yields poetic meanings not definable by natural language.
The space achieved by the system of binary opposition, center and circumference, soaring and descending, opening and closing, emptiness and fulness, with sensual space, serve to compose the poet's semantic space through their fragmentation and consequent interplay. The semantic movement of deconstructing space creates a polysemic poetic message, yielding not only poetry but also other neighboring texts using epic types - art architecture, dance, sculpture, painting, movies-and other broad cultural texts with this common denominator.
In times past, when a primitive culture produced a cosmological text, the language of space created a grammar of myth and ritual. Here we are not dealing with the culture-centered, sound-oriented language which is discussed by Derrida but via a sigh close to the code of difference. In conclusion, we may say that the less space a discourse employs, the closer it comes to being a cultural text of logocentricism. It is known that as a poet of protest Chung Ma wrote many strong poems attaching social injustice, althougn he was a poet of idea in his early stage. However, this semiotic study reveals that he always found his expressions in space language of a secondary structure. so that he could stand outside the orbit of logocentricism.
Special discourse cannot exist without the concrets places and times of life. such as "I", "now" and "here", artistic text contains a point of view and this is one of the elements for the construction of a text point of view here means the localizing of space; speaking in terms of space, distinguishing the artistic text from an abstract, scientific discourse, such have been the distinctive features of this study, they are the problems which we have been investigating in our semiotic approach to literary space.목차 = ⅰ Ⅰ. 序論 : 空間硏究의 基點과 背景 = 1 1. 硏究의 方向과 目標 = 1 가) 文學硏究의 세 要求點과 言語科學 = 1 나) 그 方法論約 限界 = 5

이 논문은 1930년대에 독특한 방언 구사와 토속적 소재로 인해 모더니즘의 특수한 성취로 문학사에 기록된 백석(白石)이 북한에서 1957년 출간한 《집게네 네형제》속에 실린 동화시 <개구리네 한솥밥>에 대한 연구이다.
해금시인인 백석의 동화시집《집게네 네형제》가 남한에 소개된 1997년 이후, 아동문학 출판계에서는 백석의 동화시 출판이 잇달아 이어져 현재까지 15종 이상의 그림동화 시집이 나와 있으며, 특히 <개구리네 한솥밥>은 비록 전문(全文)이 실리지는 못했지만 2007년 개정《초등학교 국어 읽기 2-1》교과서에 실려 2009년 3월부터 아동들에게 학습되고 있다.
백석의 시세계가 우리 민족문화의 원형에 근간을 두고 있으며, 그 사상이나 시적인 형태가 민족적이면서도 현대적이라는 데에서 우리 아동문학이 나아가야 할 하나의 방향을 제시하고 있다고 생각됨에도 불구하고, 동화시 개별 작품에 대한 본격적인 아동문학적 분석이 없고 교사로서 아동들의 감상교육을 지도하기에도 참고할만한 자료가 부족하여 이 연구를 하게 되었다.
동화시 <개구리네 한솥밥>을 연구 분석한 내용은 다음과 같다.
Ⅱ장에서는 동화시의 개념과 백석이 동화시를 창작한 배경을 그의 아동문학평론과 시대적 배경을 참고하여 살펴보았다. 동화시의 개념 분석 결과에 따르면 동화시는 서정성과 서사성을 결합한 양식으로, 서사적인 내용을 시로 쓴 것이다. 따라서 이야기적인 요소가 큰 특징이긴 하나 흔히 말하는 이야기시와는 달리 아동을 대상으로 한 동화 자체가 가진 교육성과 환상성을 그대로 담고 있다.
1988년 해금이후 남한에 알려진 바에 의하면, 백석은 해방 이후 북측에서 주로 아동문학에 매진했다. 북한은 한국 전쟁 후 새로운 사회주의 건설을 위해 북한의 작가들에게 사회주의 정신을 추구하는 내용이 들어간 작품을 요구하였다. 사회주의 국가에서 시인의 순수성을 지키기가 쉽지 않았던 그 당시 북한의 문학 풍토와 격렬한 아동문학논쟁 속에서, 백석은 '사상성과 예술성의 조화'라는 자신의 문학적 이상을 실천하려 했으며, 이 노력의 결과가 동화시로 나타난 것이고, 서정과 서사를 아우르는 동화시의 구조는 해방 전 자신의 시세계를 계승하는 유일한 길이었던 것이다.
Ⅲ장에서는 백석의 대표적 동화시인 <개구리네 한솥밥>의 세계를 주로 형식적인 특성 위주로 분석하였다. <개구리네 한솥밥>의 세계를 주로 형식적인 특성 위주로 분석한 이유는, <개구리네 한솥밥>이 지닌 문학적 특성과 장점이 무엇보다도 '동화시'라는 데 있다고 보았기 때문이다.
백석의 《집게네 네형제》에는 12편의 각기 다른 이야기가 아름다운 우리말과 함께 독특한 운율을 살려 시로 표현되어 있다. <개구리네 한솥밥>의 서사 구조는 옛이야기 중에서도 민담에 가장 가까운 형태로, 이야기가 시간의 흐름을 따라 옛이야기처럼 펼쳐지는 단선적 구조이다. 서술과 구조는 반복적이고 낙관적이며 행복한 결말로 끝을 맺고 있다. 행복한 결말이란 이 작품의 마지막에 '모두 모두 둘러앉아' 한솥밥을 먹는 장면으로, 이는 동심의 특성상 아동들에게 가장 행복한 결말이다.
<개구리네 한솥밥>의 율격은 2음보나 3음보로 전체적으로 우리에게 익숙하고 안정적인 민요의 율격을 채택하고 있다. 또한 행과 연의 반복이 계속 늘어나면서 행위 주체자들이 나열적으로 구조화되는 병렬적 반복의 형태를 띈다. 백석은 이 시를 통하여 소박하고 진솔한 자신의 민족적, 향토적 이상을 그에 가장 적합한 형식인 민요의 음보율과 반복과 병렬이라는 민요의 중요한 형식적 특징을 활용하여 창작하였다
<개구리네 한솥밥>에서의 반복은 전체 서사의 구조적 원리로도 사용되었다. 즉 낱말과 문장이 연쇄적으로 반복될 뿐만 아니라, 시의 서사 전체가 반복을 그 구조적 원리로 차용하고 있다. 결과적으로 이러한 반복 기법은 이 시 전체의 언술을 구성하는 원리로서 시의 의미 구조 형성에 궁극적으로 기여하고 있다고 볼 수 있다.
<개구리네 한솥밥>의 시어는 민족 공동체의 삶의 저변에 뿌리를 내리고 있는 토착적인 일상어와 방언을 사용했다. 또한 농촌에서 주로 사용되는 어휘와 토착어들이 있음에도 불구하고, 전체적으로 볼 때 리듬감있게 잘 읽히는 이유는 농촌어나 방언이 주로 사물이나 인간을 지칭하는 명사에 집중되어 있을 뿐, 말투에 큰 영향을 미치는 동사의 서술어미는 정확히 표준어를 구사했기 때문이다. 아동들에게 다소 낯선 어휘라 하더라도 우리의 본성에 이미 그 의미가 체감되어 있는 생생한 모국어이며 자연어이기 때문에 전체 서사의 진행에 지장을 줄 정도는 아니다. 또한 의성어보다 의태어의 다양한 활용이 두드러지는데 이는 백석이 종래 뛰어난 이미지스트로서 시각적 이미지 표현을 중시했던 측면과 무관하지 않다. 즉 백석은 의성어&#8228;의태어를 사용하여 사물의 이미지 혹은 사건 자체를 더욱 생생하고 현장감있게 전달하려고 했다. 그 결과, 의성어&#8228;의태어가 시 전체에 걸쳐 거의 매 연마다 등장하여 등장물들의 구체적 행위와 감각을 생생하게 전하여 주고 장면을 극대화하는 효과를 주고 있다. 또한 시 전체에 걸친 반복에 의해 형성되는 언어 자체의 울림이 리듬을 형성하여 즐겁고 명쾌한 음악성을 획득한다.
<개구리네 한솥밥>은 작은 생물끼리의 연대에 의한 삶의 총체성 회복을 우리 민족의 원형적 삶의 모습에 비유하여 노래하고 있으며, 아동들에게 타자와 자신의 관계를 맺는 협동과 상호보은의 모습으로 건강한 교훈을 전해주고 있다. 더 나아가 인간과 살아있는 생명체 모두가 함께 평화롭게 사는 세상을 꿈꾸었던 동물(자연)친화론자인 백석의 이상이 드러나 있다고도 할 수 있다.
Ⅳ장에서는 2007년 개정《초등학교 국어 읽기 2-1》교과서에 수록된 <개구리네 한솥밥>의 현황을 살펴보고 지도서 내용을 검토하여 교육적 의의를 질적 연구 형식으로 추출하였다.
개정 교과서 수록 <개구리네 한솥밥>의 가장 큰 문제점은 무엇보다 시의 전문(全文)이 아닌 부분이 수록된 것이며, 일부 학습 활동이 부적합함을 논하였다. 효과적인 감상 교육을 위해 여유있는 차시 배정과 맥락 소통을 위한 교사의 세심한 비계설정이 필요하다.
<개구리네 한솥밥>의 교육적 의의로는 아동들이 시에 쉽게 접근할 수 있도록 하는 서사성의 효과, 자연스럽게 우리말의 아름다움과 민요적 율격을 직감하고 즐길 수 있는 것, 유머러스한 상상력의 즐거움, 작은 생물들끼리의 연대와 그로 인한 진정한 감동의 효과를 들 수 있다.
결론적으로 '한솥밥'을 먹는다는 공동체적 이상을 아름답게 노래한 <개구리네 한솥밥>은, 우리 삶의 총체성을 회복할 수 있는 것이 자연과 인간의 조화와 공존임을 암시해주어 단순한 동심 이상의, 보편적인 세계관을 보여준다.Ⅰ. 서론 1 1. 연구의 필요성 및 목적 1 2. 선행 연구 고찰 3 3. 연구 방법 및 범위 7 Ⅱ. 동화시의 개념과 백석 11

인지 술어는 주어와 주어의 판단을 연관짓는 가장 중요한 언어적 수단으로써, 어휘 의미적인 측면에서 뿐만 아니라, 논리-통사적 구조, 그리고 양상의 측면에서도 매우 중요한 의미를 갖는 어휘 범주이다. 본 논문은 러시아어 인지 술어 전반의 상호 관계의 틀을 살펴봄과 동시에 개별 술어의 세부적인 의미를 고찰함으로써, 마크로적인 측면과 미크로적 측면 모두에서 인지 술어의 의미를 연구하는 것을 목적으로 한다. 연구 대상은 러시아어 인지 술어 знать, считать, думать 및 기타 중요 인지 술어들 그리고 관련 술어들이다. 기본 술어들의 여러 가지 통사 구조 중에서 가장 기본적이라고 간주되는 «знать Acc», «считать: Р», «думать о Prep»에 의해 인지 술어 전반에 포함되는 «앎», «생각 상태», «생각 행위»를 표현할 수 있다는 것이 본 연구의 기본 논조를 이룬다.2장에서는 러시아어 인지 술어 전반의 어휘 의미적 특성을 살펴본다. 즉 인지 술어를 논리-통사적 특성에 따라 크게 ''앎''의 인지 술어와 ''생각''의 인지 술어로 나누며 각각의 인지 술어를 어휘 의미에 따라 이차적으로 세분한다. 따라서 ''앎''의 인지 술어에는, 순수한 ''앎'', ''이해'', ''기억'', ''인식적 발견''의 하위 범주가 포함되고, ''생각''의 인지 술어에는 순수한 ''생각'', ''믿음'', ''기대'', ''결정'', ''평가'', ''논리적 관계''의 하위 범주가 포함된다. 또한 이러한 인지 술어와 경계를 이루고 있는 것으로 간주되는 ''지각 술어'', ''감정 술어'', ''희구 술어'', ''발화 술어''와 ''인지 술어''의 의미적 상관 관계도 간략하게 살펴본다. 3장과 4장에서는 2장에서 분석한 러시아어 인지 술어 중에서도 특히 의미 원소로 기능하는 것으로 간주되는 앎의 술어 знать와 생각의 술어 думать, считать의 의미-통사 구조를 집중적으로 고찰한다. 이러한 의미-통사 구조는 인지 내용의 명시성에 따라 명시적 정보 구조와 비명시적 정보 구조로 나뉜다. 또한 이러한 의미-통사 구조에서는 주어진 동사의 통사적 결합 관계 뿐 아니라, 인칭이나 시제, 상, 태, 법 등도 의미를 결정하는 중요한 요소로 작용한다고 보고, 문법 범주에 따른 의미 차이를 상세하게 살펴본다. 한편 주어진 술어들의 경우는 부정문에서 독특한 의미적 함축을 포함하기 때문에, 몇몇 부정문은 따로 구분하여 심층적으로 기술하여 준다.3장에서는 앎의 인지 술어 знать의 비명시적 정보 구조 즉 대격 보어 구문, 전치사구 보어 구문, 의문사 구문과 명시적 정보 구조인 명제문, 동일시 구문, 명사구 대격 보어 구문, 명령문을 여러 가지 문법적, 통사적 기준을 적용하여 심층적으로 분석해 본다. 또한 기타 앎의 술어 구문 известно Р, 생각-앎의 술어 구문 догадываться-догадаться Р, 이해의 술어 구문 понимать-понять Р, 기억의 술어 구문 помнить Р, 인식적 발견의 술어 구문 обнаруживать-обнаружить Р, 지각 술어 구문 видеть Р, 감정 술어 구문 радсожалеть Р 을 살펴본다.4장에서는 우선 생각 동사 считать의 명시적 정보 구조인 명제문, 재귀 구문, 동일시 구문, 명령문을 살펴본다. 또한 동사 думать의 비명시적 정보 구조 즉 전치사구 보어 구문, 의문사 구문, 미정형 구문과 명시적 정보 구조 즉 명제문, 재귀 구문, 명령문이 분석된다. 이와 더불어 기타 считать, думать와 관련된 의견 술어 구문 полагать Р, казаться Р, 확신의심의 술어 구문 уверен(а) Р, сомневаться Р, подозревать Р, 믿음의 술어 구문 верить Р, 기대우려의 술어 구문 надеяться Р, бояться Р, 결정의 술어 구문решать-решить Р, 가치 평가의 술어 구문 оценивать-оценить Р, 논리적 관계의 술어 구문 заключать-заключить Р의 의미-통사적 구조도 살펴본다.5장에서는 실제적 사용에서 관찰되는 동사 знать와 думать, считать의 화용론적 특징을 살펴보았다. 우선 주어진 인지 술어의 억양 구조의 특성, 인식적 양상을 나타내는 삽입어와의 결합 관계를 살펴보고, 일인칭 인지 술어 구문이 실제 발화에서 표현하는 화용론적 의미를 살펴본다. 이러한 실제 문장에서의 사용을 통해서 이것이 어휘-의미와 맺고 있는 관련성과, 주어진 술어들의 인식적 양상 표현의 기능, 그리고 러시아어 발화 체계에서의 존재 이유를 모색해 본다. 이와 같이 본 논문은 러시아어 인지 어휘-의미 그룹에 대한 분류와 화용론적 특징에 대한 논의를 통해 러시아어 인지 동사 전반의 공통적인 의미적 특성을 살펴봄과 동시에 문법적, 의미-통사적 특징에 따른 세밀한 의미적 차이를 연구하는데, 이러한 과정에 자연어 의미 원소를 사용한 의미 기술의 방법이 사용된다.Реферируемая работа посвящена исследованию семантических и синтаксических особенностей разнообразных семантико-синтаксических структур и употроблению эпистемических предикатов в русском языке. Эпистемические предикаты не мало анализируются как в русском, так и в других языках. Дело в том, что они отражают общие основные виды деятельности человека, выражая отношение мыслящего субъекта к содержанию суждения. К тому же они тесно связаны с такими разными темами лингвистики, как проблемы пропозиционального отношения, семантическая группировка глаголов, субъективная и объективная модальности и т.п. Следовательно, эпистемические предикаты считаются основным и важным типом предиката в любом языке. Принимая во внимание все многообразие существующих мнений по данному вопросу, следует произвести многостронный и интегральный анализ данных предикатов.Цель работы – рассмотреть семантику русских эпистемических предикатов с разных точек зрения, т.е. не только их общие лексико-семантические значения и прагматические особенности, но и частные синтаксические и грамматические особенности, и представить семантико-синтаксические структуры данных предикатов с помощью семантических примитивов. Объектом работы служат русские глаголы знания, мнения и мышления, в том числе знать, считать и думать.Диссертация состоит из введения, 4 глав и заключения. Во введении говорится о цели, объекте и аналистическом методе данной работы и излагается краткая история изучений эпистемических предикатов в русском и в английском языках.Во второй главе обсуждается лексико-семантическая особенность русских эпистемических предикатов и проведена их классификация. Данные предикаты разделяются прежде всего на предикаты знания и предикаты мнения и мышления. Предикаты знания снова группируем на предикаты чистого знания, предикаты понимания, предикаты памяти и предикаты эпистемического обнаружения, а предикаты мнения и мышления – на предикаты чистого мнения и мышления, предикаты веры, предикаты ожидания, предикаты оценки, предикаты решения и предикаты логического отношения. Кроме того эпистемические предикаты тесно связаны со смежными с ними по семанитическим и синтаксическим чертам предикатами, такими как перцептивные предикаты, эмотивные предикаты, предикаты желания, предикаты говорения. Поэтому исследование соотношения указанных и эпистемических предикатов находит отражение в представляемой работе.В третьей и четвертой главах рассматриваются семантико-синтаксические структуры русских эпистемических предикатов. Мы применяем концепцию семантического примитива А. Вежбицкой к анализу и исследуем разные семантико-синтаксические структуры предикатов с помощью основных эпистемических глаголов знать, считать и думать, подробнее, основных семантико-синтаксических структур «знать Acc», «считать: P» и «думать о Prep». Мы разделяем структуры по эксплицитности информации на неэксплицитно-информационные и эксплицитно-информационные структуры. Главное различие между этими двумя структрами заключается в том, что в первых структурах говорящий информирует адресату непольную и неэксплицитную информацию, а в последних – полную и эксплицитную информацию. Поэтому в семантических описаниях семантико-синтаксических структур последнего типа добавлен семантический компонент «я говорю это» (н.п. я знаю, что Р = Р; я знаю это; я говорю это). К тому же в представлениях семантическо-синтаксических структур русских эпистемических предикатов также учитываются граматические категории лица, времени, вида, наклонения и залога, потому что они делают важные семантические различия.В третьей главе анализируются семантико-синтаксические структуры глагола знания знать. Мы разделяем структуры глагола знания знать по их эксплицитности информации на неэксплицитно-информационные структуры (знать Acc знать о Prep знать, WH Р) и эксплицитно-информационные структуры (знать, что P знать A как B знать Acc (phrase)) (см. 3.2), и рассматриваем значение каждой из групп подробно. Кроме того в предлагаемый анализ входят семантические структуры таких предикатов, которые относятся к знанию, как известно Р, догадываться-догадаться Р, понимать-понять Р, помнить Р, обнаруживать - обнаружить Р, видеть Р, радсожалеть Р.В четвертой главе мы сосредоточиваемся на семантико-синтаксических структурах глаголов мнения и мышления думать и считать. Предикат считать имеет только эксплиццтно-информационные структуры (считать, что Р считаться, что Р считать А как В) (см. 4.2), а думать – не только эксплицитно- информационные (думать, что Р думать о том, что Р думаться, что Р), но и неэксплицитно-информационные стурктуры (думать о Prep думать, WH Р думать Inf) (см. 4.3). В данной части описываются семантические структуры и других предикатов мнения и мышления с помощью семантического примитива мнения «считать: Р» и семантического примитива мышления «думать о Acc». Рассмотренные структуры предикатов мнения и мышления включают полагать Р казаться Р, уверен(а) сомневаться подозревать Р, верить Р, надеяться бояться Р, решать-решить Р, оценивать-оценить Р, заключать-заключить Р.В пятой главе прослеживаются некоторые прагматические особенности русских эпистемических предикатов и раскрывается, что лексико-семантические значения эпистемических предикатов отражены в высказывании, и данные предикаты выполняют важные функции в дискуссиях. Во-первых, что касается интонации, то предикаты знания попадут под фразовое ударение, а предикаты мнения обычно не подвергаются таковому. Во-вторых, эпистемические предикаты имеют некоторые ограничения в сочетании с вводными словами эпистемической модальности, так что данные предикаты тоже представляют собой один из самых важных способов выражения эпистемической модальности. В-третьих, избыточные по лексическому значению формы первого лица данных предикатов, то есть, я знаю, я думаю, я считаю указывают на авторитетность или вежливость, и уверенность или неуверенность говорящего и выполняют необходимые функции в дискуссиях и обычных высказываниях.В заключение мы резюмируем содержание данной работы, утверждая возможность описывать разные семантико-синтаксические структуры русских эпистемических предикатов с помощью таких семантических примитивов как «знать Acc», «считать: P» и «думать о Prep». Дело в том, что данные семанические примитивы точно передают не только лексическую семантику, но и синтаксические и прагматические особенности других более сложных семантико-синтаксических структур русских эпистемических предикатов.

A new-word is an important factor when you implement text analysis because new words are produced rapidly as times go by. The process of new words’ appearance and disappearance are as fast as the change of society. Unlike English, Hangul has many morphemes in one word, so morphological analysis is essential in natural language processing. Without using dictionary including newly produced words, you cannot analyze an appropriate morphological analysis because newly produced words might be broken down into smaller morphemes, and thus losing its meaning. When you analyze morphological analysis without including newly produced words, you cannot expect to get good results from morphological analysis.
In addition, it is not easy to construct a dictionary that includes rapidly produced new words. If the text data contains a previously unknown new word, the new word will not be extracted into one morpheme. Therefore, it is necessary to construct a dictionary that includes new word from the text data if you analyze text analysis. This dictionary should be used again for morphological analysis process.
In the study, a new-word discrimination model was proposed as a method of extracting new-word from text data. To make a model, 3 month(from July to September 2018) post titles are collected from Domestic Baseball Gallery of www.dcinside.com, the largest online community in Korea. After preprocessing the collected data, all possible partial characters in each word were generated and partial characters with frequency more than 0.01% of total number of words were selected as new-word candidates word.
The statistical characteristics of each candidate word in the whole text data were used as independent variables. And the logistic regression analysis was performed with dependent variables as the actual new-word. As a result of the analysis, the model is constructed with 7 variables and the accuracy of the model is 81.94% and the sensitivity is 81.2%.
In order to apply the model empirically, the title of the post in October 2018 from the site were collected. 127 new-words were extracted from the text by calculating 7 statistical variables for each candidate word. The extracted new-words were added into the morpheme analysis program as a user dictionary. And two word clouds were generated with most frequently mentioned nouns from 10,000 randomly selected post titles. One was with new-word dictionary, and the other was without new-word dictionary. It was confirmed that the nouns that were not appeared in word cloud without new-word dictionary are appeared in word cloud with new-word dictionary.신조어는 생성 당시의 사회상을 반영하는 단어라는 특징이 있기 때문에 텍스트 분석 시 무시할 수 없는 중요 단어라고 할 수 있다. 사회의 변화 속도가 점점 빨라짐에 따라 신조어의 생성과 소멸도 빨라지는 경향이 있어서, 신조어가 사전으로 구축되는 시점이 생성된 시점보다 늦기 마련이다. 그리고 한글은 영어와 달리 하나의 어절이 여러 형태소를 가지고 있기 때문에 자연어 처리 시 형태소 분석 과정이 필요한데, 형태소 분석 중 신조어로 인해 미등록어 처리에 대한 문제가 나타난다. 형태소 분석 결과에 신조어가 하나의 형태소로 유지되지 못 하고 더 작은 형태소로 잘 못 분해되어 신조어의 의미를 잃어버리는 경우가 발생하는 것이다.
신조어의 양이 급격하게 증가하고 있는 환경에서 신조어 사전을 미리 구축하여 텍스트 분석 시 활용하기에는 무리가 있다. 만약 분석하고자 하는 텍스트 데이터에 처음으로 등장한 신조어가 있다면 해당 신조어는 하나의 형태소로 추출되지 않을 것이다. 따라서 텍스트 분석을 진행할 때마다 분석하고자 하는 텍스트 데이터로부터 신조어를 추출하는 과정을 먼저 진행하여, 해당 텍스트 데이터만을 위한 신조어 사전을 먼저 구축하고, 이를 형태소 분석 시 다시 활용하는 방법이 텍스트 분석할 때 텍스트의 의미를 더 정확하게 파악하는 방법이 될 것이다.
본 연구에서는 텍스트 데이터로부터 신조어를 추출하는 방법으로 신조어 판별 모델을 제시하였다. 모델을 만들기 위해 국내 최대 온라인 커뮤니티인 디시인사이드의 국내 야구 갤러리의 2018년 7월부터 9월까지 3개월간의 게시물 제목을 수집하였다. 수집된 데이터를 전처리한 후 각 어절에서 조합이 가능한 모든 부분 글자들을 생성하여 빈도수가 전체 어절 수의 0.01%를 넘는 단어를 신조어 후보 단어로 선정하였다. 후보 단어가 전체 텍스트 데이터에서 가지는 통계적 특징을 독립변수로 사용하기 위하여, 각 후보 단어의 길이와 빈도수와 어절에서 시작부분에 위치하는 비율과 끝부분에 위치하는 비율을 계산하고, Python 패키지인 soynlp의 WordExtractor 클래스를 사용하여 각 후보 단어의 글자가 함께 등장하는 정도와 후보 단어의 왼쪽과 오른쪽에 등장하는 글자의 다양성을 수치화하였다. 모델의 종속변수를 확보하기 위하여 신조어 후보 단어가 실제로 신조어인지를 파악해야 하는데, 이는 본 연구자가 각 후보 단어를 인터넷에 검색해 보거나 후보 단어가 사용된 데이터 내의 문장을 검토하여 판단하였다.
확보된 분석 데이터에 로지스틱 회귀분석을 수행하여 7개의 변수로 구성된 신조어 판별 모델을 생성하였다. 한글을 형태소 분석할 때에 신조어가 더 분해되거나 빠뜨리지 않고 포함되도록 하는 것이 본 연구의 궁극적인 목적이기 때문에 신조어가 아닌 단어를 신조어라고 판단하는 경우보다, 신조어인데 신조어로 판별하지 못하는 경우를 줄이는 것이 중요하다. 따라서 모델의 정확도만큼 민감도도 중요하다고 볼 수 있다. 본 연구에서 생성한 신조어 판별 모델의 정확도는 81.94%, 민감도는 81.2%이다.
본 연구에서 생성한 신조어 판별 모델을 실증적으로 적용해보기 위해 디시인사이드 국내 야구 갤러리의 2018년 10월 게시물 제목으로부터 신조어 후보 단어를 추출하고 추출된 후보 단어마다 모델에 필요한 7개 독립변수를 계산하여 127개의 신조어를 추출하였다. 추출한 신조어를 형태소 분석 프로그램의 시스템 사전 또는 사용자 사전에 추가하여 2018년 10월 게시물 중 무작위로 뽑은 5,000건을 형태소 분석하여 데이터에서 자주 언급되는 명사 단어로 워드 클라우드를 생성하였다. 그리고 신조어를 추가하기 전의 형태소 분석 결과와 비교하여, 신조어 사전이 추가됨으로 인해 추출되지 않던 단어가 빈도수 상위 단어로 추출되고, 하나의 신조어가 나누어 추출되던 오류가 수정되는 것을 확인하였다.

In this dissertation, we considered the problems of the speech recognition in the car navigation which is the most typical product of the vehicle telematics, and performed a variety of research to improve the problems.
As we start speech recognition or control in the navigation, in other that the driver can control the navigation without touch the terminal, we performed the study on out-of-vocabulary(OOV) word rejection algorithm based on the variable vocabulary speech recognition.
The OOV rejection means that speech recognition system decides to recognize the inputted word or reject it. The rejection system can be classified into two categories by their implementation methods, keyword spotting method and utterance verification method. The utterance verification method is to determine the keyword and the OOV by comparing the likelihood of the normal phonemic model and the anti-phoneme model. In this dissertation, we add the speaker verification system before utterance verification and calculate the speaker verification likelihood. The obtained likelihood of the speaker verification is applied for determining the proposed variable-confidence threshold.
The current navigation does not use natural language based speech recognition but the isolated word based speech recognition. Therefore the navigation must use tree structure that subdivide a search phases, so the search will be complicated. For troubleshooting on complex search method of current navigations, we studied large POI speech recognition method based isolated words. At this point, the problem of the complex search method can be solved, but the number of vocabulary has increased hundreds of thousands of word to million of word.
Therefore, speech recognition algorithms are more complicated, and require a large space in the navigation. Occurring the problem of the large memory capacity and the longer processing time.
For the telematics terminal with a highly limited processing time and memory capacity, it is impossible to process more than 100,000 words in the terminal by the general speech recognition method. Therefore, we proposed phoneme recognizer using the phonetic GMM and also PDM Levenshtein distance with multi-layer architecture for the POI recognition of telematics terminal. By the proposed multi-layer recognition methods, we obtained high performance in the telematics terminal with low speed processing and small memory capacity.
When we used the proposed OOV rejection method, we made the significant performance improvement; CA(Correctly Accepted for keyword) 94.23%, CR(Correctly Rejected for out-of-vocabulary) 95.11% in office environment, and CA 91.14%, CR 92.74% in noisy environment. When we used multi-layer recognition system with phoneme recognizer and PDM Levenshtein distance method, we obtain the recognition rate of maximum 95.47% in indoor environment and of maximum 93.13% in the car navigation environment.
As a result, the methods proposed in this dissertation showed an excellent performance for the large isolated words in navigation terminal with small amount of low processing speed and memory.본 논문은 차량용 텔레매틱스의 대표적인 제품인 내비게이션 단말기에서 음성인식의 문제점을 고찰하고, 문제점들을 개선하고자 하는 다양한 연구를 수행하였다.
내비게이션 단말기을 통하여 음성인식을 시작하거나 제어할 때, 운전자가 내비게이션에 물리적인 접촉없이 내비게이션 단말기를 제어하기 위하여 가변어휘(variable vocabulary) 인식 기반의 미등록어 거절알고리즘(out-of-vocabulary word rejection algorithm)에 대한 연구를 수행하였다. 미등록어에 대한 거절 기능은 음성 인식기를 만들 때 미리 인식대상 단어를 정해 놓고, 등록되지 않은 단어가 입력되었을 때 인식할 수 없는 단어로 분류하는 것을 말한다. 거절 기능을 구현하는 방식은 핵심어 검출(keyword spotting)방식과 발화검증(utterance verification)으로 구분이 된다. 발화 검증 방식은 정상적인 음소 모델과 반음소 모델의 우도비를 비교하여 등록어와 미등록어를 결정하는 방식이다. 본 눈문에서는 발화 검증 전에 화자확인 단계를 추가하여 화자 확인 우도비을 구한다. 화자가 발성할 때마다 구해지는 화자 확인 우도비를 가변 신뢰도 문턱치를 결정할 때 적용하는 방법에 대하여 제안하였다.
기존의 내비게이션은 자연어 기반의 음성인식 방법이 아닌 고립단어 기반의 음성인식 방법을 사용하고 있다. 때문에 검색할 때 내비게이션은 검색단계를 세분화한 트리구조를 사용해야하고 검색은 복잡해진다. 기존 내비게이션의 복잡한 검색방법에 대한 문제해결을 위해 고립단어기반의 대용량의 관심지(POI, point of interest)에 대한 음성인식 방법을 연구하였다. 이때, 트리구조의 복잡한 검색 방법에 대한 문제는 해결이 되지만, 인식 대상이 되는 어휘의 수가 수십만 개에서 수백만 개로 증가하게 된다. 따라서 음성 인식 알고리즘은 더욱 복잡해지고 대규모의 탐색공간을 필요로 하게 된다. 결과적으로, 내비게이션에서 음성인식의 처리시간이 길어지고 메모리 용량이 커지게 되는 문제점이 발생한다. 제한된 연산처리 능력과 메모리를 가지는 내비게이션 단말기에서 10만 단어 이상을 일반적인 음성인식 방식으로 처리하기는 불가능하다. 따라서 본 논문에서는 내비게이션 단말기의 관심지 인식을 위하여 다단계 구조의 대용량 고립단어 인식 시스템을 제안하였다. 이 관심지 인식시스템의 성능향상을 위해 음소별 GMM(gaussian mixture model)을 사용한 음소 인식기와 PDM(phoneme distance matrix) 레빈쉬타인(levenshtein) 거리를 제안하였다.
본 논문에서 제안한 미등록어 거절 방법을 사용하였을 때, 사무실 환경에서 CA(correctly accepted for keyword)가 94.23%, CR(correctly rejected for out-of-vocabulary)이 95.11%로 나타났고, 잡음 환경에서도 CA가 91.14%, CR이 92.74%로 나타나서 성능이 향상됨을 확인할 수 있었다. GMM 음소 인식기와 PDM 레빈쉬타인 거리 방법을 가지는 다단계 인식 시스템을 사용하였을 경우 실내에서 최대 95.47%, 자동차환경에서는 최대 93.14%의 인식 성능을 얻을 수 있었다.
결과적으로 본 논문에서 제안한 방법들은 낮은 처리속도와 적은 양의 메모리를 가지는 내비게이션 단말기에서도 대용량 고립단어에 대하여 우수한 인식 성능을 나타냄을 확인할 수 있었다.제 1 장 서론 = 1 제 2 장 음성인식 알고리즘 = 6 2.1 음성 전처리 및 특징 파라미터 추출 = 6 2.1.1 끝점 추출 기법 = 7 2.1.2 선형 예측 분석 = 10

1 Chapter I Introduction 1 1.1 Background 1 1.2 APR: Autoencoder-based Personalized Ranking 4 1.3 CAAE: Collaborative Adversarial Autoencoder 6 1.4 CFGAN: A CF Framework based on GAN 7

Maps have changed in form and function according to the development of the information and communication technology, and the environment and mode of producing maps have also changed. Therefore, lots of researches have been conducted on effective information communication between map makers and map users on changing maps. In particular, the study of human cognitive aspects assists a broad understanding of the information transmission process of maps and also helps understanding the effective interaction between information communication devices and users.
While the purpose of existing map production is to make maps for map users through universal understandings, map production considering cognitive aspects takes into account the individual spatial perception and cognitive processes. It means adaptive map production could be provided customized information.
For the adaptive map production, experiment was conducted focused on map feature label. The map feature label consists of natural language and is the most recognizable map element through the human eye. The following three factors are considered to reflect the perception of the user. First, it is the scale of the map. Second, it is the cue knowledge of the individual, which is the element of the cognitive aspect. Third, it is the cluster of the gaze characteristics considering the users’ gaze.
The study area was a part of Hoegi-dong, Apgujeong-dong and Jongno areas where commercial facilities are developed. Data of forty-two participants were used for analysis. Tobii Glasses2 eye-tracker was used for the experiment. The participants had worn the eye-tracker and google map of smart-phone was used to find the destination.
An experiment was tried to find out three aspects. First, it is to figure out whether the map feature label that staring map users’ gaze to the building on corresponding to displayed on the google map. Second, it identifies what map label is easy for map users to understand. Third, in order to investigate the characteristics of adaptive map production, I clustered gaze patterns with map labels according to the industry type.
The unrecognized map feature label for the google map was about 55.17% in Hoegi-dong, about 43.75% in Apgujeong-dong, and about 44.44% in Jongno.
The eye-tracking data was used to reveal the salient map label. In order to take account of the scale factor, cluster analysis was performed and the scale step was linked to the cluster analysis result. In addition, to consider individual cue knowledge, the cluster analysis was conducted on the data with and without sign name on the sketch map. There were five clusters in Hoegi-dong and Apgujeong-dong, and four clusters in Jongno.
To examine the gaze characteristic clusters, cluster analysis was carried out using reclassified data according to ten kinds of industry categories by operational definition. Three clusters were found in Hoegi-dong and Jongno, and two clusters in Apgujeong-dong. The ratio of each characteristic cluster to industry type indicates the characteristic difference of each cluster.
The current trend of providing information is to provide individualized personalized information based on the vast amount of data that is developed according to the development of information and communication technology.
This can be an opportunity to enhance the potential of development through various approaches to maps, along with the need for the level of spatial information that can be provided in digital maps and the need for reconsideration of delivery information. Therefore, this study suggests that it is possible to contribute to the research diversity in terms of the spatial cognition approach, and it is meaningful to expand the diversity of information providing methods of digital map.지도는 정보통신기술의 발전에 따라 형태와 그 기능이 변해왔고 지도를 생산하는 환경, 양식 또한 변화해 왔다. 이에 따라 변화하는 지도에 대해 지도제작자와 지도사용자간의 효과적인 정보 전달에 관한 많은 연구가 이루어져 왔다. 특히 인간의 인지적 측면의 연구는 지도의 정보전달 과정에 대한 폭넓은 이해를 돕고 나아가 정보통신기기와 사용자 간의 효과적 상호작용에 대한 이해까지 돕고 있다.
인지적 측면을 고려한 지도 제작은 기존 지도 제작의 목적이 보편적 이해를 통한 다수의 지도 사용자들을 위한 지도 제작에 있었던 것에 반해 각 개인의 공간 지각 및 인지 과정을 고려해 맞춤형 정보제공을 위한 적응형 지도 제작을 의미한다.
적응형 지도 제작을 위해 지도 요소 중 지도 객체 주기를 대상으로 실험을 진행하였다. 지도 객체 주기는 자연어로 이루어져 안구를 통해 가장 인지하기 쉬운 지도 요소이다. 이에 사용자의 인지를 반영하기 위해 다음의 세 가지 요소가 고려되었다. 첫째, 지도학적 측면의 요소인 축척 둘째, 인지적 측면의 요소인 개인의 공간지식에 해당하는 신호지식 셋째, 사용자 측면 요소인 사용자의 시선을 고려한 응시 특성 군집이다.
실험지역은 상업시설이 발달한 회기동, 압구정동, 종로 지역 중 일부를 대상으로 하였으며, 총 42명의 실험참가자에 대한 자료가 분석에 이용되었다. 실험장비는 Tobii사의 Glasses2 시선추적기를 사용하였고 실험참가자들은 시선추척기를 장착하고 스마트 폰의 구글 지도를 이용하여 제시된 목적지를 찾아가도록 하였다.
실험을 통해 밝히고자 한 바는 첫째, 지도사용자들이 응시하는 건물에 해당하는 지도 객체의 주기가 기존 디지털 지도상에 표시되었는지 확인하고 둘째, 지도사용자들이 파악하기 쉬운 주기가 어떠한 것이며 셋째, 적응형 지도 제작을 위해 필요한 응시 특성을 살펴보기 위해 객체 주기를 업종별로 재분류하여 이에 따른 응시패턴을 살피고자 하였다.
지도사용자들이 응시하는 실세계의 객체에 대한 구글 지도상에 표시되지 않은 객체 주기는 회기동에서 약 55.56%, 압구정동에서 약 52.94%, 종로에서 약 44.44%를 나타냈다.
지도사용자들이 필요한 주기를 밝히기 위해 시선추적기를 통해 획득된 자료를 이용하였다. 이에 축척 요소를 고려하기 위해 군집분석을 실시하여 대축척에서 소축척의 단계를 군집분석결과와 연결 지었으며, 개인의 신호지식 요소를 고려하기 위해 수집된 자료를 가공하여 심상지도에 표출된 상호를 포함한 자료와 포함하지 않은 자료에 대해 각각 군집분석을 실시하였다. 회기동과 압구정동은 5개의 군집이 나타났고 종로는 4개의 군집이 나타났다.
응시 특성 군집을 살펴보기 위해 임의로 정의된 10가지 업종분류에 따라 재분류된 자료를 이용하여 군집분석을 실시하였다. 응시 특성 군집에 따른 군집 역시 신호지식 요소를 고려한 자료가 이용되었다. 이를 통해 각 신호지식의 정도에 따른 업종별 응시 특성 군집이 이루어 졌으며 회기동과 종로는 3개의 군집이 나타났고 압구정은 2개의 군집이 나타났다. 각 특성 군집에 대한 업종별 비율은 각 군집의 특성 차이를 나타낸다.
최근 정보를 제공하는 흐름은 정보통신기술의 발전에 따라 방대한 자료를 바탕으로 자료의 형태를 초월해 세분화된 개인 단위의 맞춤형 정보를 제공하는 것이다. 이는 디지털 지도에서 제공할 수 있는 공간 정보의 수준과 전달 정보에 대한 재고의 필요성 부각과 더불어 지도에 대한 다양한 접근을 통해 발전 가능성을 제고하는 계기가 될 수 있다. 이에 따라 본 연구는 공간 인식적 접근이 시도 되었다는 점에서 주기에 대한 연구 다양성에 기여 할 수 있을 것으로 판단되며 디지털 지도의 정보 제공 방법의 다양성 확대에 의의를 지닌다.I. 서 론 1 1. 연구의 배경과 목적 1 1) 연구의 배경 1 2) 연구의 목적 4 2. 연구의 방법 6

ABSTRACT

A Study on the Expert group recognition
and case for activating AI Journalism


Yoon Young Lee
Department of Theater and Film(Major in News Media)
Graduated School Cheongju University, S. Korea
Supervised by Professor Jong Mook Ahn


Klaus Schwab, founder of the Davos Forum, defined the Fourth Industrial Revolution as a destructive innovation. At the heart of this destructive innovation is artificial intelligence (AI). AI simulates human learning, reasoning, perception, and understanding of natural language, using computer programs and algorithms modelled on human judgement.
This paper begins with the work of identifying the way AI entered the journalism ecosystem. The AI journalism presented in this paper refers not only to the automatic production of articles but to all behaviour of AI technology which permeates the field of journalism. This is a new concept that affects both news reporters and news consumers. AI has already deeply penetrated the processes of producing, distributing, and consuming news; we are just not aware of it.
The task of this study was to clearly identify and systematise aspects of AI and journalism “chemical combination”. It also examines the direction of the evolution of AI journalism and explores the corresponding countermeasures. For the purposes of this study, this paper examines how AI is utilised in the field of journalism at home and abroad. In-depth interviews with an AI expert group were conducted to determine how AI journalism is activated. In-depth interviews with domestic and foreign AI journalism experts and journalists represent the first research methods in Korea, but we wanted to present the direction and possibility of AI journalism through in-depth interviews with CEOs of leading media companies such as Google, Civil, Dow Jones and Quartz as well.
In addition, we surveyed 58 Korean reporters who had worked in newspaper, telecom, and broadcasting companies for more than five years and explored the situation of domestic AI journalism and the perception of current journalists. Based on the questionnaire data, frequency analysis and t-tests between a broadcasting reporter group and newspaper reporter group were conducted, along with a correlation analysis between variables. The results showed that most reporters believe that AI journalism had a more positive effect on improving management and legal and ethical problems in the newspaper reporter group than in the broadcasting reporter group. In addition, most reporters consider the level of articles produced by AI robot reporters to be lower than those of human reporters but agree that AI robot reporters can replace human reporters in terms of simple, repetitive information gathering and article writing. It is only a matter of time before AI robot reporting is introduced, with the objective of reducing the production cost of simple, repetitive articles and allowing human reporters to focus on in-depth articles.
A total of 85% of broadcasting reporters and 64% of newspaper reporters responded as follows. Despite this situation, media executives are unaware of the importance or necessity of AI journalism and do not retain experts, in-house experts, or budget support for AI journalism. In addition, most reporters answered that there is no dedicated department or related department for AI journalism among their media. The responding reporters pointed out that the biggest obstacles preventing the activation of AI journalism are ‘lack of awareness of members, including executives’, ‘cost burden’, and ‘lack of professional manpower’. Respondents were likely to state that AI journalism would be active among their media in five years. In addition, respondents were likely to believe that journalists’ jobs would be threatened if AI journalism expanded. Specifically, job anxiety was found to be more serious among newspaper reporters than among broadcasting reporters.
AI journalism in Korea is not attracting attention from journalists, news consumers, or academia. AI journalism is still in its infancy, and news consumers are not aware of the changes it has caused. This paper is intended to be a starting point for exploring the direction of AI journalism. Furthermore, it also expresses the desire to encourage a social atmosphere in which this study does not delay the maintenance of laws and institutions. We hope that this social consensus will lead to academic interest and achievement.

Keyword: Artificial Intelligence (AI), AI Journalism, Algorithms, AI Robot Reporter, The Fourth Industrial Revolution, Robot Journalism, News Media국문초록


AI 저널리즘 활성화를 위한
전문가 집단의 인식 및 사례 연구

이윤영
연극영화학과 언론 전공
청주대학교 대학원


다보스포럼의 창립자인 클라우스 슈밥(Klaus Schwab)은 4차 산업혁명은 파괴적 혁신으로 규정했다. 이 같은 파괴적 혁신의 중심에 AI가 있다. AI는 인간의 학습, 추론, 지각, 자연어의 이해 능력을 컴퓨터 프로그램으로 실현한 기술을 말한다. 사람처럼 판단하는 알고리즘이 AI이다. 본 논문은 AI가 저널리즘 생태계에 유입된 양태를 확인하는 작업에서 시작됐다. 본 논문이 제시한 AI 저널리즘은 기사의 자동생산 뿐만 아니라 AI 기술이 저널리즘 현장에 스며든 모든 행태를 의미한다. 나아가 뉴스를 생산하는 기자뿐만 아니라 뉴스를 소비하는 소비자의 관점도 반영한 새로운 개념이다. 뉴스를 생산하고, 유통하고, 소비하는 모든 과정에 AI는 이미 깊숙이 침투해 있다. 우리는 이 같은 사실을 제대로 인지하지 못하고 있을 뿐이다. 본 논문은 저널리즘과 AI의 화학적 결합의 양태를 분명하게 인지하고 체계화하는 작업이라고 볼 수 있다. 또한 AI 저널리즘의 진화의 방향을 점검하고, 그에 상응하는 대책은 무엇인지를 탐색해 보았다. 이를 위해 현재 저널리즘 영역에서 AI가 어떻게 활용되고 있는지 국내외 사례와 함께 AI 전문가 집단에 대한 심층 인터뷰를 통해 AI 저널리즘의 활성화 방안은 무엇인지를 짚어보았다. 특히 국내외 AI 저널리즘 전문가 집단에 대한 심층 인터뷰는 국내에서는 처음으로 시도한 연구방법이다. 구글과 시빌, 다우존스와 쿼츠 등 세계 AI 저널리즘을 선도하는 언론기업의 CEO 및 AI 저널리즘 전문 언론인들과의 심층 인터뷰를 통해 AI 저널리즘의 방향성과 가능성을 제시하고자 했다. 또한 전국의 신문사와 통신사, 방송사에서 근무하는 58명의 기자들을 대상으로 설문 조사를 실시해 국내 AI 저널리즘의 현주소와 현직 언론인들의 인식에 대한 진일보한 결과를 얻을 수 있었다. 설문 조사는 방송 기자 집단과 통신·신문 기자 집단 간의 빈도분석과 t-검정, 각 변인별 상관관계 분석을 실시했다. 분석 결과, 통신·신문 기자는 방송 기자보다 AI 저널리즘이 경영 개선에 긍정적 효과가 있으며 법·윤리적 문제 해결이 필요하다고 인식하고 있는 것으로 나타났다. 또한 기자 대부분이 AI 로봇 기자가 제공하는 기사의 범주와 수준이 인간 기자보다 부족하다고 생각하고 있으며, AI 로봇 기자가 인간 기자들의 단순·반복적인 정보 수집과 기사 작성을 대체할 수 있다는데 동의했다. AI 로봇 기자의 도입은 이제 시간문제이며 단순·반복적인 기사 생산에 드는 비용을 줄이고 심층 기사에 집중하기 위해서라도 AI 저널리즘의 도입은 필요하다는 의견이다. 하지만 소속 언론사의 경영진은 AI 저널리즘의 중요성이나 필요성을 인식하지 못하고 있고, AI 저널리즘을 자체 개발할 전문가나 이를 운용해나갈 인력이나 예산지원이 전혀 없다는 응답이 방송 기자는 85%, 통신·신문 기자는 64%에 달했다. 또한 AI 저널리즘을 담당할 전담부서나 관련 부서도 없다는 응답이 압도적이었다. AI 저널리즘의 활성화를 막는 가장 큰 걸림돌로는 ‘경영진을 비롯한 구성원들의 인식부족’, ‘비용 부담’, ‘전문 인력의 부족’등을 꼽았다. 소속 언론사에서 AI 저널리즘이 활성화되는 시점은 5년 이후가 될 것이라는 응답이 가장 많았다. 또한 AI 저널리즘이 확대될 경우 기자들의 일자리가 위협을 받을 것이란 의견이 우세했다. 특히 일자리에 대한 불안감은 방송 기자보다 통신·신문 기자들이 더욱 심각하게 느끼고 있는 것으로 나타났다.
국내 AI 저널리즘은 언론 종사자와 뉴스 소비자, 나아가 학계에서도 주목받지 못하고 있다. 언론사의 AI 저널리즘은 여전히 진입 단계이고, 뉴스 소비자들도 AI 저널리즘이 초래한 변화를 인지하지 못하고 있다. 본 논문은 AI 저널리즘의 방향성을 탐색하는 단초가 되기를 희망한다. 나아가 이번 연구가 AI와 관련된 법과 제도의 정비를 더는 미룰 일이 아니라는 사회적 분위기를 환기시키는 것이 또 하나의 바람이다. 그리고 이 같은 사회적 공감대가 학계의 관심과 학술적 성과물들도 이어질 것으로 기대한다.

주제어: AI 저널리즘, 알고리즘, AI 로봇 기자, AI 인공지능, 4차 산업혁명,
로봇 저널리즘, 뉴스 미디어제1장 서 론 1 제1절 문제 제기 1 제2절 연구 목적 7 제2장 이론적 논의 12 제1절 인공지능 12

Building information modeling (BIM)은 정보 전달 및 의사 소통의 어려움 등 건설 프로젝트에서 발생하는 여러 문제들을 해결할 수 있는 기술적 대안으로 등장하였다. BIM은 벡터 기반의 정보를 3차원의 형태로 표현할 수 있는 소프트웨어 프로그램이며, 이러한 새로운 접근 방식은 건설 산업에 많은 혜택을 제공함과 동시에 새로운 요구들을 만들어 내고 있다. BIM의 사용이 전세계적으로 확산됨에 따라 이러한 BIM 도입으로 인해 발생한 새로운 요구들은 점차 BIM 사용을 가로막는 장애물로 인식되고 있다. BIM 사용을 가로막는 여러 요인들이 있지만, 본 연구는 BIM 프로젝트 참여자들로부터 가장 빈번하고 심각하게 인식되고 있는 요소인 BIM 전문가의 부족 및 교육 프로그램의 부족, 빈번한 설계 변경에 대응하기 위한 설계 자동화 검토 도구의 부족, 그리고 건설 단계로부터 시설물 유지 관리 단계로 정보를 넘겨주기 위한 요구 명세서 내용의 불명확함을 대상으로 하였다. 3가지 연구 주제 간의 상호 연관성을 정량적으로 측정하기는 어렵지만, 건설 산업이 사람에 의해 개발된 기술을, 사람에 의해, 사회적 체계 안에서 적용되는 산업이라는 점을 고려할 때 이들 3가지 연구 주제간의 상관성을 추측 하는 일은 어렵지 않다. BIM 역시 건설 산업의 한 분야로, BIM의 요구 분석에 사회과학적 접근 방법과 연구 방법을 사용하는 것은 다양한 관점에서의 문제를 진단하고 요구를 정의하는데 매우 유용하다. 복잡한 요구 내용들을 수집, 추출, 분석을 위한 연구 방법론의 한계는 연구자들로 하여금 다양한 자료들을 다양한 방법으로 분석할 수 있는 “mixed research methods”와 “mixed sources of data”에 대한 관심을 높이는 계기가 되었다. “Mixed methods”는 연구자가 양적 연구와 질적 연구의 기술, 연구 방법, 연구 접근 방법 및 연구에 사용되는 주요 언어를 특정 연구에 적용하는 것을 말하며, mixed research methods의 기본 개념은 본 연구에서 양적 연구와 질적 연구를 동시에 사용함으로 보다 복잡하고 다양한 요구를 폭넓게 이해하는 연구의 기본 개념이 되었다.
본 연구는 요구 공학의 요구 분석의 절차에 따라 mixed methods를 적용하였다. 공학이라는 단어가 의미하듯 요구 공학은 사용자의 요구, 기술적 요구, 산업적 요구를 고려하여 요구 사항을 도출하고, 구체화하고, 분석하고, 검증하는 체계를 제공한다. 요구 공학은 소프트웨어 공학의 한 영역으로 소프트웨어에 집중된 시스템의 요구를 분석을 목적으로 하는 4가지 단계로 요구 추출, 요구 분석, 요구 명세화, 그리고 요구 검증을 소개한다. 요구 공학의 프로세스는 기술적이고 사회적인 요구를 분석하는데 사용되었을 뿐 아니라 요구 분석의 결과의 논리적 일관성을 유지하는데 활용되었다.
요구 공학의 기본 절차와 mixed methods research의 개념을 기반으로, 본 연구는 BIM산업에서 새롭게 정의되어야 할 요구를 다음과 같이 구체화하였다.
- BIM 산업에서 요구하는 BIM 전문가의 역량은 무엇인가
- BIM 환경에서 설계 검토를 자동화하기 위해 필요한 것은 무엇인가
- 시설물 유지 관리 단계에서 BIM이 활용되기 위해서는 건설 단계에서 시설물 유지 관리 단계로 넘겨주어야 하는 정보는 무엇인가
첫째, BIM 프로젝트 수행 인력에 대한 요구가 높아짐에 따라, BIM 직업의 명칭, 자격은 BIM 전문가를 채용하고, 교육시키는데 매우 중요한 이슈가 되고 있다. 이러한 이슈들을 해결하기 위해, 미국, 영국, 중국에서 영어로 작성된 242개의 온라인 구인 광고를 수집하였다. 온라인 구인 광고를 분석하여, 35개의 직업 명과 역량을 설명할 수 있는 5,998개의 용어를 도출하였다. 35개의 직업 명은 Social network analysis의 role and position analysis를 사용하여 직업 간의 관계를 분석함으로 8개의 그룹으로 묶었다. O*NET 역량 분석 모델을 기반으로 5,998개의 용어는 43개 그룹으로 분류하였고, 다시 직업의 적용 범위에 따라 필수 역량, 공통 역량, 특정 직업에 요구되는 역량으로 분류하였다. 필수 역량은 모든 BIM 직업에 요구되는 역량으로“말하기,” “공학 및 기술 기반 지식,” “공학 관련 교육,” “BIM 프로젝트 관련 경험,” “컴퓨터 활용 능력,” “인력 관리 업무”가 해당되며, 공통 역량은 다수의 BIM 직업에 요구되는 역량 (8개의 그룹 중 5개 그룹 이상 적용)이며, “협력,” “디자인 관련 지식,” “대학 교육 이상 학위,” “기준 검토 및 정보 검토 업무,” “창조적 업무,” “도면 작성 업무,” “협업,” “협력 업체 관리,” “교육 지원 업무,” “조직 성과 관리”에 이에 해당된다. 특정 직업에 요구되는 역량은 유사한 직업을 구분할 수 있는 역량으로, 예를 들면, BIM 매니저는 다른 사람을 교육하거나 업무 순서를 결정하는 일은 수행하지만, BIM 코디네이터는 그렇지 않다.
BIM 역량과 관련된 기존 연구들과는 달리 본 연구는 현재 BIM 산업에 현존하는 직업 명을 소개하였고, 그들간의 관계와 분류 방법을 제안하였다. 또한 본 연구는 BIM 산업에서 활용이 가능하도록 현재 사용되고 있는 역량 분류 모델에 기반하여 BIM 직업에 따른 요구 역량 및 역량 분류 방법을 제시하였다.
둘째, BIM 환경에서 입찰 제안서에 기반한 설계 검토는 한계를 가진다. 이러한 문제를 해결하기 위해, 주거시설, 업무시설, 전시시설, 의료시설, 체육시설, 법원 프로젝트의 27개의 입찰 제안서를 분석하였다. 각 입찰 제안서는 1,800개 단어 이상으로 구성되어 있고, 이 중 컴퓨터에서 인식 가능한 문장 366개를 도출하였다. Context free grammar (CFG)를 적용하여 자연어를 명사, 동사, 조동사 및 기타의 4가지 구문으로 분류하였고, 명사는 다시 공간, 건물 구성 요소, 장비로 세부적으로 분류하고, 조동사는 다시 must, should, might, could로 세부 적으로 분류하고, 동사는 29개로 세부적으로 분류하였다. 대다수(90%)의 명사는 공간 정보였고, 29개 동사 중 6개의 동사(getNumberofArea, getNumberofSpace, isComposedOf, hasEquipment, isAccessible, getNumberofEquipment)가 전체의 95%를 차지하였다. 90%이상의 조동사는 ‘should’이며, 예외 조항이 없는‘must’보다는 다소 약한 강제력을 가진다. 본 연구의 결과는 설계 요구에는 일련의 규칙이 존재한다는 것을 보여 주었으며, 어떻게 설계 요구의 규칙을 컴퓨터가 인식 가능하게 번역할 수 있는가를 보여 주었다. 본 연구의 결과는 사용자와 컴퓨터를 모두 고려한 설계 요구 사항의 제작의 가능성을 암시하고 있다. BERA(Building Environment Rule and Analysis)나 Solibri Model Checker와 같은 기존의 규칙 검토 도구들과는 달리 본 연구는 BIM 사용자가 어떠한 프로그래밍 지식이 없다고 하더라도 규칙을 쉽게 사용하고 변환할 수 있도록 하였다.
셋째, 시설물 유지 관리 단계에 BIM 데이터를 재 사용할 수 있는가에 대한 질문은 끊이지 않고 있다. 이러한 문제를 해결하기 위해, 관찰 조사, 인터뷰, 문헌 자료로 구성된 triangulation method를 사용하여 대학 시설에서 시설물 유지 관리 단계에 요구되는 정보를 분석하였다. 정보 수집 이후 별도의 5개 대학의 사례를 분석하여 총 429개의 데이터가 대학 시설물 유지 관리 단계에서 사용되는 것을 알게 되었으며, 미국 COBie 데이터 호환 포맷을 사용하여 사용자 정보, 객체 정보, 그리고 부가 정보 그룹으로 재 분류 하였다. 429개의 데이터 중 절반의 정보 (55.9%)가 객체 정보이며, 나머지 정보 중 약 33.8%가 부가 정보, 그리고 10.2%는 사용자 정보로 구성되어 있다. Semantic set operator에 기반하여 대학에서 요구하는 정보와 COBie에서 정의하는 정보를 비교한 결과 시설물 유지 관리 단계에서 요구되는 정보의 약 절반(53.9%)이 COBie 데이터 포맷으로 지원이 가능하며, COBie에서 지원 가능한 정보의 73.1%가 객체 정보에 해당한다. 그러나 시설물 유지 관리 단계에서 요구되지만 기존의 COBie 데이터 포맷으로 지원되지 않는 약 절반 (51.5%)의 정보는 정보의 프로세스나 계약 정보와 같은 부가 정보에 해당하며, 198개의 데이터는 시설물 유지 관리자에 의해 준비되어야 하는 정보 임을 확인하였다. 중복되는 정보를 제외하며, 필수적으로 시설물 유지 관리에 사용되는 정보는 91개이고, 이들은 다시 현재 시스템에서 가져올 수 있는 정보, 새롭게 정의되어야 하는 정보, 설계자 혹은 건설 업자에게 추가로 요청해야 하는 정보, 정부에서 정한 법률 정보, 컴퓨터에서 자동 계산 가능한 정보의 5가지로 재 분류하였다. 현재 시스템에서 확보 가능한 데이터, 추가 요청 정보, 컴퓨터에서 자동 계산 가능한 정보 모두 확보 가능한 정보라고 가정할 때 429데이터 중 13개 데이터, 약 3%의 정보가 BIM 기반 시설물 유지 관리에서 추가로 요구되는 정보라는 것을 확인하였다. 본 연구의 결과는 설계 및 건설 단계에서 시설물 유지 관리 단계에 전달되어야 하는 정보가 무엇인지를 구체화하였다. 본 연구의 결과는 각 단계에서 전달되어야 하는 정보를 구체화 함으로 BIM 정보에 대한 사용자간의 기대의 차이를 줄이는데 기여할 것이며, 현재 애매하게 정의된 정보 교환에 대한 기준을 명확하게 하는데 기여할 수 있을 것으로 기대한다.
본 연구는 BIM 산업에 있어 주요한 이슈인 사람, 디자인 품질, 그리고 데이터 교환에 대한 주제를 요구 공학의 프로세스와 mixed methods research의 기본 개념을 기반으로 연구를 진행하였다. 논문 초반에 언급하였듯이, 3가지 주제의 요구 분석 결과 간의 상호 보완의 가능성의 여지는 충분하다. 예를 들면, 본 연구의 설계 요구의 패턴은 시설물 유지 관리에 요구되는 정보를 문서화할 때 활용할 수 있으며, 시설물 유지 관리에 요구되는 정보의 목록은 데이터 관리 전문가의 요구 지식 혹은 기술을 서술할 때 참조할 수 있다. BIM의 문제점을 다루고 있는 본 연구의 결과가 프로젝트 혹은 사용자들이 직면한 어려움을 해결해 줄 수 있기를 기대한다.Building information modeling (BIM) has emerged as a technical solution for numerous problems in architecture, engineering, and construction (AEC) projects, such as poor communication, information transmission, and coordination (Weippert and Kajewski 2004). BIM is a software solution that enables the creation of a vector-based representation in 3-dimensional (3D) formats for construction projects. This new approach brings many benefits to the AEC industry while simultaneously raising new requirements (Sacks and Barak 2009). As the use of BIM has expanded around the world, these requirements have been perceived as obstacles that hinder its use. There are several obstacles to active BIM use, but this dissertation focuses on the most frequent and serious issues defined by BIM project participants: lack of skilled BIM professionals and education programs for recruiting and training them, lack of computational design compliance checking tools to respond to frequent design changes, and ambiguous requirements for data handover from the construction phase to the facility management (FM) phase. It is difficult to accurately quantify the association between these three issues. However, it can be assumed that there is a mutual influence between the issues, given that the construction industry is a “social process” to be applied “by the people” of technology and developed “by the people” to achieve goals established “by the people” (Abowitz and Toole 2009).
This social science-based approaches and methods are useful to understand three requirements issues in this dissertation because BIM are also used in a social and organizational context. Social and organizational factors are not a single viewpoint but are influences on all viewpoints. Limitations of methods to collect, elicit, and analyze complex and complicated requirements encourage researchers to turn their eyes to “mixed research methods and mixed sources of data.” Mixed methods are formally defined as the class of research where the researcher mixes or combines quantitative and qualitative research techniques, methods, approaches, concepts, or language into a single study. This concept of employing mixed research methods in an effort to combine qualitative and quantitative methods is the basis for selecting and combining research methods of this dissertation.
When mixed research methods are employed for identify requirements of BIM in three areas, systematic processes are helpful to arrange research methods. As the term “engineering” implies, requirements engineering (RE) provides a disciplined and systematic approach to elicit, specify, analyze, commit, validate, and manage requirements considering user-, technical-, and business-oriented needs. RE, a branch of software engineering concerned with identifying and communicating the purpose of a software-intensive system, is defined as a set of activities comprised of four processes: requirements elicitation, requirements analysis, requirements specification, and requirements validation. The RE process is not only applicable to both technical and social requirements presented in this dissertation, but it is also useful to maintaining the logical consistency of requirements analysis results. Moreover, RE provides many cases in which combinations of research methods are useful.
Based on the process of RE and approach of mixed methods research, this dissertation specifies obstacles in the BIM industry to the following three requirements issues: what competency is required of the BIM professional; what is required to enable computational design compliance checking; and what information can be transferred from the construction phase to the FM phase.
First, as the demand for BIM project personnel has increased, questions regarding job titles, duties, and qualifications for these personnel have become critical for recruitment, promotion, and education purposes. To address this issue, 242 online job postings in English from the US, the UK, and China were collected. Through the online job posting analysis, 35 job titles and 5,998 terms containing competency were derived. The 35 job titles were classified into eight BIM job groups (BIM project manager; director; BIM coordinator; senior architect; BIM manager; BIM designer; BIM mechanical, electrical, and plumbing (MEP) coordinator; and BIM technician) by analyzing the relationships between job titles using the role and position analysis of the social network analysis (SNA). Based on the Occupational Information Network’s (O*NET) job competency classification model, 5,998 terms were categorized into 43 competency elements. These were subcategorized into the three categories of essential, common, and job-specific competencies for the eight BIM jobs depending on the frequency. The “essential” competences are required all BIM job groups and include “speaking skill,” “engineering and technology-related knowledge,” “technical vocational education,” “BIM-related work experience,” “work activity of interacting with computers,” and “work activity for establishing and maintaining interpersonal relationships.” The common competencies required by most job groups (more than five out of the eight job groups) include “work style of cooperation,” “design knowledge,” “post-secondary degree,” “work activity of evaluating information to determine compliance with standards,” “work activity of thinking creatively,” “work activity of documenting or recording,” “work activity of communicating with supervisors and peers,” “work activity of guiding, directing, and motivating subordinates,” “work activity of providing consultation and advice,” and “organizational context to monitor data on quality/costs/waste/etc.” The “job-specific” competency could distinguish similar BIM jobs. For example, the BIM manager performs “work activity of training and teaching others” in the “organizational context to determine workflow or order of tasks for others,” but not for the BIM coordinator. Compared to other existing papers regarding BIM competencies, this dissertation introduces all job titles currently used in the BIM industry—not just the representative BIM job titles—and provides their relationships and classifications. Moreover, this dissertation attempts to analyze BIM professionals’ competencies based on the competency model currently in use to improve their applicability in the BIM industry. In addition, the level of competencies for BIM professionals allows for their application for the BIM educational program development.
Second, there are limitations to check the compliance of a building design in a request for proposal (RFP), especially in BIM. To respond to these needs, a total of 27 RFPs for housing, offices, exhibitions, hospitals, sports centers, and courthouse projects were analyzed. Each RFP was composed of more than 1,800 sentences. Of these, only 366 sentences could be translated into computer-interpretable sentences. For further analysis, this study deployed context-free grammar (CFG) in natural language processing and classified morphemes into four categories: object (noun), method (verb), strictness (modal), and others. The subcategorized morphemes included three object types (space, building element, and equipment), four strictness levels (must, should, might, and could), and 29 method types. Most of the object types (90%) were spaces. Of the 29 method types, six methods (i.e., ‘getNumberofArea,’ ‘getNumberOfSpace,’ ‘isComposedOf,’ ‘hasEquipment,’ ‘isAccessible,’ ‘getNumberOfEquipment’) covered 95% of the design compliances. More than 90% of the strictness types in the RFPs were “should,” which is mandatory but is weaker than “must”; e.g., a minor exception might be allowed. The results of this dissertation show that patterns exist in the design requirements and that they can interpret computer readable rules. The results imply potentials for design requirements to be generated for both human and computer readable rules. Compared to existing rule checking tools—for example, BERA (Building Environment Rule and Analysis) or the Solibri model checker—the results of this dissertation allow BIM users to use rules easily for their BIM projects and to transform rules without any program knowledge.
Third, the requirements’ needs are ever-increasing due to questions about the possible repurposing of BIM data during the FM phase of AEC projects. To address these questions, the triangulation method—job-shadowing, interviews, and document analysis—was used to explore information required for FM in higher education facilities. The collected information was then adjusted through requirements comparisons of five additional higher education facilities. There were 429 information items required for higher education FM, which were classified into actors (users and staff in higher education facilities and third-party service providers), objects (facility, floor, space, zone, type, component, system, assembly, connection, spare, resource, and job), and additional data groups (process, inspection, contract, impact, document, attribute, coordinate, issue, and picklist) that the US construction building information exchange (COBie) had defined previously. Among the 429 information items for higher education facilities, half of the information items (55.9%) were objected data items, one-third (33.8%) was residue from additional data, and one-tenth (10.2%) was actor data. A comparative analysis of information requirements between higher education FM and COBie was conducted using the semantic set operator, which was effective in finding common information items among different data sets between systems and information sets. The comparison results showed that half of the information (53.9%) required for FM could be extracted from the construction phase in the US COBie data format. Additionally, three-quarters (73.1%) of the COBie coverable data were objected data items. However, 46.1% of the information required for FM should be prepared during the FM phase. Half of the information items (51.5%) prepared by the FM phase were additional information items, such as information of FM processes, inspections, and contracts. However, redundancies were noted in the 198 data items prepared by FM managers. With the exception of repeatedly used data items, the number of essential data items were reduced to 91 (Table 43). They could be classified into five categories: available data in the existing system, data needed to redefine, data needs to request to (sub) constructors, references provided by the government or another authority, and calculated data by computers. Given the available data from the existing system, data needs to request to (sub) constructors, and calculated data by computers, only 13 data items are required for the BIM FM about 3% of the 429 data items for FM (Table 35). The results of this dissertation specify what information could be transferred (or not) from the design/construction phase to the FM phase and who is responsible for this information. This dissertation helps to fill the gap between BIM project participants in the pre-construction phase and project owners in the post-construction phase. Additionally, it is expected that this dissertation contributes to clarifying current ambiguous requirements.
This dissertation studied three requirements regarding people, design quality, and data problems in the BIM industry based on the fundamental RE process and mixed research methods. As stated previously, it is not difficult to envisage that the results of three requirements contribute to three complementing issues. For example, derived rules patterns of RFPs in this dissertation can be used to describe documents information required for FM. Information items for BIM FM in this dissertation will be a reference that specially explains the knowledge and skills required for data management. The authors hope that the results of this dissertation on three BIM problems have the advantages of either using, transforming, or adding by demanding their fulfillment via either projects or users.

The App Store that is a kind of digital contents marketplace is showing dazzling growths based on success of Apple app store, and it is the Internet service space where contents transactions such as applications that can be used in mobile terminals called Smart phone commonly, movies, music videos, and comics etc.
The population of voices calls in the mobile market was decreased 15% vs. the year 2009, but it was brightened up that people of using all kinds of mobile medium was increased 6.6% with 40.7%. And it is a market having potentiality to the extent of expecting that mobile application market would be enlarged to Internet market scales while application numbers of the mobile are reached to 10 million in 2020.
If reviewing precedents of on-line marketplaces for setting up upgrading strategies of mobile App stores, Akiyama Ryuheyi has advocated AISAS model that contained participation and sharing through ‘holistic communication’ to AIDMA model that has been considered as general purchasing processes, indebting to expansion of market sizes and developments of search engines. As search experiences of users become mature in this way, marketing strategies of letting own products exposed to users among a great number of product information that can be approached through simple searches by existing users have been upgraded gradually, and many on-line marketplaces such as Amazon and Pinterest etc. established such social networks newly, or social curation by linking with Facebook, a representative social network service.
This study forecasted that upgraded social curation services similar to common on-line marketplace would be necessary to application marketplaces too based on such growths, and processed ethnography interviews objecting to App store users, and then analyzed obstacles and requirements through discovery experience investigations in current stores together with purchasing decision factors when buying contents such as application, music, e-book etc. at the App stores.
As results of the survey, this study grasped a fact that aimless searches of App store shown in the early stage of Smart phones were decreased because using experiences of Smart phones became mature, and respondents answered as purchasing it by entering into App store after social network services or recommendations from acquaintances became triggers.
Also, differentiated figures of discovery types by contents were appeared, and this figure could be seen as being derived from the fact that terminal environments have not been optimized yet, and service maturity was low.
Based on these results, the study designed App store focusing on social curation services, and progressed UX expert reviews in order to investigate about how much new App store having been suggested to enlarge discovery experiences of users in this way could satisfy user’s requirements that were found out from existing App store, and which problems would be occurred. This study used Thinking Aloud Protocol in which users heard new functions and situations while looking at the screen, and could express own thought and feeling on it, and the result was same as follows.
First, App store centering on social curation services need not form relations with acquaintances certainly differently from existing social network services, and it is expected that binding users who had similar tastes focusing on contents would be effective.
Second, plans have to be made so that activities of various users could be recognized more visually because contents discovery is happened centering on behaviors of surrounding users.
Thirdly, as the curation was made centering on user reviews, detailed and correct reviews are prioritized, and reorganization to easy information structures is necessary for doing so.
Fourthly, the study could know a fact that information structures of App store and interaction are also operated with evaluations bases of App store same as App selecting and purchasing standards by image qualities. Therefore, the quality of contents information has to be managed consistently when operating the App store, and separate guidelines are required for doing it well.
Fifthly, upgraded searches such as natural language search and similarity search result etc. shall be provided in case of purposive visitors, even though social curation functions are strengthened.
This study could be said as having a meaning in that it proposed information-centered store that is wanted by effective users in open-marketplaces centering on platform operators together with future sellers in addition to App stores.디지털 콘텐츠 마켓플레이스의 일종인 앱스토어는 애플 앱스토어의 성공을 바탕으로 눈부신 성장세를 보이고 있으며, 흔히 스마트폰으로 불리우는 모바일 단말에서 이용할 수 있는 애플리케이션과 영화, 뮤직비디오, 만화 등의 콘텐츠 거래가 이루어지는 인터넷 서비스 공간이다.
모바일 시장에서 음성통화만 사용하는 인구는 2009년 대비 15% 감소한데 비해 각종 모바일 미디어를 사용하는 사람은 40.7%로 6.6% 포인트 상승했다고 밝혀지고 있으며, 2020년에는 모바일 애플리케이션 수가 1천만개에 달하면서 모바일 애플리케이션 시장이 인터넷 시장규모만큼 커질 것이라는 전망이 제기될 만큼 잠재력 있는 시장이다.
모바일 앱스토어의 고도화 전략을 세우기 위해 온라인 마켓플레이스의 선례를 살펴보면, 마켓 규모의 확장과 검색엔진의 발전에 의해 아키야마 류헤이는 일반적인 구매과정으로 여겨지던 AIDMA 모델에 홀리스틱 커뮤니케이션’을 통해 참여와 공유라는 과정이 포함된 AISAS 모델을 주창한 바 있다. 이처럼 사용자의 검색 경험이 성숙해짐에 따라, 기존의 사용자가 단순 검색을 통해 접근할 수 있었던 수많은 상품 정보 중에서 자사의 상품을 사용자에게 노출시키기 위한 마케팅 전략이 점차 고도화되고 있으며, 실제로 아마존, 핀터레스트 등 많은 온라인 마켓 플레이스에서 이러한 소셜 네트워크를 새롭게 구축하거나 대표적인 소셜 네트워크 서비스인 페이스북과의 연동을 통한 소셜 큐레이션을 제공하고 있다.
본 연구는 애플리케이션 마켓플레이스 역시 이러한 성장세를 바탕으로 일반 온라인 마켓 플레이스와 유사한 고도화된 소셜 큐레이션 서비스가 필요할 것으로 전망하고, 앱스토어 사용자 12명을 대상으로 에스노그래피 인터뷰를 진행하여 앱스토어에서 애플리케이션, 음악, e-book 등의 콘텐츠를 구매할 때 구매결정 요인이 되는 항목 및 현재 스토어의 디스커버리 경험 조사를 통한 장애요인, 요구사항 등을 분석하였다.
조사 결과 스마트폰 사용 경험이 성숙함에 따라 스마트폰 이용 초기에 나타나는 앱스토어의 비목적성 탐색은 감소하고, 대신 소셜 네트워크 서비스나 대면으로 받은 지인의 추천이 트리거가 되어 앱스토어에 진입하고 이를 구매한다고 응답하여 스토어 내에서 별도의 플랫폼이 구축되지 않은 상태에서도 이미 소셜 큐레이션이 진행되고 있음을 파악하였다.
또 콘텐츠 별로 디스커버리 방식이 달라지는 양상을 보였는데 이는 아직 단말에서의 환경이 최적화 되어있지 않고 서비스 성숙도가 낮은 서비스라는 점에서 기인하는 상황으로 볼 수 있다.
이를 바탕으로 소셜 큐레이션 서비스 중심의 앱스토어를 디자인하였고, 이렇게 사용자의 디스커버리 경험을 증대하기 위해 제시된 새로운 앱스토어가 기존의 앱스토어에서 발견된 사용자의 요구사항을 얼마나 충족시킬 수 있는 지, 어떤 문제가 발생할 수 있는지 조사하기 위해 UX전문가 리뷰를 진행했다. 사용자가 화면을 보며 새로운 기능과 상황에 대한 설명을 듣고, 이에 대한 자신의 생각과 느낌을 말로 표현하도록 하는 Thinking Aloud Protocol 을 이용했고 결과는 다음과 같이 나타났다.
첫째, 소셜 큐레이션 서비스 중심의 앱스토어에는 기존의 소셜 네트워크 서비스와 달리 꼭 지인들과 관계를 형성할 필요는 없으며, 콘텐츠를 중심으로 취향이 비슷한 사용자를 묶어주는 것이 효과적일 것으로 예상된다.
둘째, 주변 사용자 행위 중심으로 콘텐츠 디스커버리가 일어나므로 다양한 사용자의 활동을 보다 시각적으로 인지할 수 있도록 계획해야 한다.
셋째, 사용자의 리뷰를 중심으로 큐레이션이 이루어지므로 기존 앱스토어보다 구체적이고 정확한 리뷰가 우선시되며, 이를 위해 작성이 용이한 정보구조로 개편이 필요하다.
넷째, 이미지의 퀄리티가 앱을 고르고 구매하는 기준이 되는 것과 마찬가지로 앱스토어의 정보구조 및 인터랙션 역시 앱스토어의 평가 기준으로 작용하는 것을 알 수 있었다. 따라서 앱스토어 운영 시 콘텐츠 정보의 퀄리티를 지속적으로 일관성 있게 관리해야 하며 이를 위한 별도의 가이드가 필요하다.
다섯째, 소셜 큐레이션 기능이 강화되더라도, 목적성 방문자의 경우 자연어 검색 및 유사검색결과 등 고도화된 검색을 제공해야 한다.
본 연구는 앱스토어 뿐만 아니라 향후 판매자와 플랫폼 사업자 중심의 오픈 마켓 플레이스에서 유효한 사용자가 원하는 정보중심의 스토어를 제안했다는 점에서 의의가 있다고 하겠다.Ⅰ. 서론 1 A. 연구의 배경 및 목적 1 B. 연구 방법 및 구성 2 Ⅱ. 디지털 콘텐츠와 앱스토어 4 A. 디지털 콘텐츠 4

Modern industrial plants contain large number of facilities interacting with thousands of sensors and control. A single failure in a facility can produce inconsistent outcomes, which can be affected to core part of industrial plant, and it becomes a critical industrial disaster. Therefore, it is crucial to find and apply the best solution for maintaining facilities and preventing industrial disasters. The early-stage solution was the regular maintenance but this approach cannot be a perfect solution to prevent most industrial disasters. This is because regular maintenance is not effective for all but only few facilities, and is spent too much time and cost to afford. The recent trend of industrial plant maintenance focuses on two main factors, alarms and human expertise.

The system collects the status of different types of facilities from the sensors, which are attached, on each facility. If there is any specific symptom detected from sensors, the alarm will be ringed. The collected alarm data are sent to the human experts in the real time. The human experts have experienced various types of industrial disasters so they have sufficient knowledge in diagnosing and treating failures.

In this dissertation, I studied how to use alarm data and expert knowledge together with these characteristics. In this study, I constructed knowledge using failure reports reflecting alarm data, expert knowledge, which are significant knowledge resources of the industrial field and proposed a method to continuously manage and use such knowledge.

This dissertation can be divided mainly into three parts of subjects of researches.

In the first study, I propose a hybrid knowledge engineering method based on machine learning- expert knowledge, which enables machine learning and domain experts to generate and update knowledge together.

First of all, after constructing a knowledge base by applying real-time alarm data and machine learning, the expert can directly update the knowledge continuously, thereby enabling knowledge creation and management in a fast and efficient manner.

After constructing a knowledge base by applying real-time alarm data and machine learning, the expert can directly update the knowledge continuously, thereby enabling knowledge creation and management in a fast and efficient manner.

Second, I propose a methodology for constructing causal knowledge as overall conditions and treatment actions for failure. Failure report includes the cause-and-effect relationship and its order of occurrence. The proposed methodology analyzes the failure reports written by domain experts using natural language processing techniques and helps to organize the cause-effect and treatments for the failures into a network form.

Finally, the knowledge constructed by the hybrid knowledge engineering method and the causal failure knowledge are fused and applied to the fault diagnosis and prediction.

As a result of the performance analysis, the proposed framework is superior to the other methodologies regarding failure diagnosis and prediction. The proposed decision support method in this dissertation can evolve the two types of knowledge required in the field gradually. Thus it was able to solve the knowledge management difficulties, and using the two knowledge together complementarily; knowledge management efficiency has been achieved. Moreover, it showed superior performance compared to existing methods based on data.현대 산업시설은 수많은 센서 및 제어장치와 연동되는 설비들로 이루어져있다. 설비에서 하나의 작은 고장이라도 발생할 경우 예측 불가능한 결과를 가져올 수 있다. 사소한 문제가 산업시설의 핵심부에 영향을 미칠 수 있으며, 중대한 산업재해를 유발하는 원인이 될 수 있다. 그러므로, 설비를 관리하고 산업 재해를 방지하기 위한 최적의 해결책을 발견하고 적용하는 것이 중요하다. 초기단계의 해결책은 설비에 대한 정기적인 유지보수이지만 이 방법은 대부분의 산업재해를 방지하기 위한 최선의 방법이 아니었다. 그 이유는 정기적인 유지보수가 특정 경우를 제외하면 전체 설비에 효과적이지 않을 뿐만 아니라, 사람의 개입이 필요하여 많은 시간과 노력이 투자되어야 하기 때문이다. 최근, 산업시설 유지보수의 트렌드는 알람데이터와 전문가 지식이라는 두가지 요소에 중점을 둔 자동화 시스템 활용에 있다.

이러한 시스템은 각 설비에 부착된 다양한 센서로부터 해당 설비의 여러 상태를 파악한다. 만일 센서로부터 특정 증상이 발견될 경우, 센서가 알람을 발생시킨다. 이를 통해, 수집된 알람데이터는 즉각적으로 전문가에게 전달된다. 전문가는 산업현장에서 다양한 경험을 가지고 있으며, 축적된 경험지식에 기반하여 고장을 진단하고 조치한다.

본 연구에서는 어떻게 알람데이터와 전문가 경험지식을 함께 이용할 수 있는지에 대해서 다룬다. 연구에서 산업현장에서 중요한 지식 자원인 전문가 지식으로 작성된 고장보고서와 이와 관련된 알람데이터를 이용하여 지식을 구축하였다. 또한, 이러한 지식을 지속적으로 관리하고 이용하는 방법에 대해서 연구를 수행하였다.

본 연구는 세 가지 주제로 나눌 수 있다.

첫째, 실시간 알람데이터를 기계학습에 적용하여 지식베이스를 구축한 후 전문가가 지식베이스를 지속적으로 직접 업데이트할 수 있도록 하여, 지식 생성 및 관리를 빠르고 효율적으로 수행 할 수 있도록 한다.

둘째, 고장에 대한 전반적인 발생조건과 조치과정에 대한 인과관계지식을 구축하는 방법을 제안하였다. 고장보고서는 고장 발생에 대한 인과관계와 그 발생 순서가 포함된다. 제안된 방법은 자연어 처리 기술을 사용하여 도메인 전문가가 작성한 고장 보고서를 시스템이 자동으로 분석하고 고장에 대한 발생원인 및 처리 방법을 네트워크 형태의 지식으로 구성할 수 있도록 한다.

마지막으로, 하이브리드 지식공학기법에 의해 구축된 지식과 인과관계 고장지식을 융합하여 고장 진단과 예측에 적용하였다.

성능 분석을 통해, 제안된 프레임워크가 고장 예측에 대해 다른 방법론보다 우수함을 보였으며, 제안된 의사결정지원방법은 현장에서 요구하는 두 가지 유형의 지식을 점진적으로 진화시킬 수 있다. 따라서 지식 관리의 어려움을 해결하고 보완적으로 두 지식을 함께 사용할 수 있다. 그 결과, 지식관리 효율성이 달성되었으며, 기존 기계학습 방법에 비해 우수한 성능을 보였다.1. Introduction 1 1.1 Overview 1 1.2 Motivation 3 1.3 Contribution 4 1.4 Thesis Organization 6

Artificial Intelligence(AI) is a technology implementing specified results by self-learning through machine learning and deep learning based on big data and computing technology. Recently, the advancement of digital technology has resulted in generation of unprecedented volumes of big data which we have not previously experienced in human history, and AI backed with such big data is not only rapidly breaching into the area of thinking and decision-making which was previously considered to be exclusively reserved by humans, but also advancing into territories beyond the reach of humans, pushing the boundaries of geographic·time limitations humans are bounded by.

However, there are number of obstacles to big data learning which is a core component in the advancement of AI. First, the biggest obstacle hindering AI from big data learning is copyright infringement. Under the Copyright Act, consent for use must be obtained from the copyright holder in order to use the copyrighted work. Through the advancement of learning algorithms such as machine learning and deep learning, AI will not be limited to learning only data input by humans or structured data generated by devices such as the computer, but adopt self-learning to recognize unstructured data such as natural language, text, photographs, voice and video clips, which will inevitably come into conflict with copyrights.

Second, personal information is also a potential obstacle for AI in big data learning. Based on the ‘Right to Informational Self-determination’ of an individual, informed consent is required from the relevant information holders in cases of using personal information, providing it to 3rd party, or transferring it overseas. It is impossible to obtain infomed consent from the relevant information holders for every case AI is trying to learn from big data.

First, in order to eliminate the legal obstacles for AI to learn from big data, the first option available for consideration would be ‘Fair Use’ enabling free use of copyrighted work which is part of the big data pool. Fair Use is limiting copyrights in case information is used for public interest. In other words, if big data learning by AI meets the qualifications to be considered Fair Use, AI can use copyrighted work without obtaining consent for use from the copyright holder or intellectual property holder.

Second, the next legal option available for consideration enabling AI to learn from big data without personal information protection issues is De-identification of Personal Information. De-identification of Personal Information is applying measures to delete·replace part or all of the personal information to make it impossible to recognize a specific individual. Such de-identified information is referred to as anonymous information or pseudonymous information. Once such de-identification of personal information is carried out, and it is not possible to carry out re-identification of the anonymous information as a result, such information should no longer be classified as personal information and be made available for free use of the data. In addition, even in case of anonymous information, the degree of sensitivity must be considered to allow unrestricted data use.

Third, obtaining informed consent from the copyright holder or information holder is mandatory in order to use copyrighted work or personal information, but this informed consent itself poses a number of issues. The information holder who gave informed consent at the time of personal information collection has no way of knowing how the data is used after informed consent is given. On the other hand, a post-consent enforces regular update of data use after personal information is collected, and allows the information holder to suspend use of personal information at any given time in case her or she deems personal information is not being used properly. In addition, a informed consent limits the use of personal information to be used for a specific purpose at the initial time of personal information collection, so in case personal information is used for a purpose aside from the initial purpose of data collection, one must go through the inconvenience of obtaining consent again. Therefore, in case a certain set of qualifications are met, rather than obtaining pre-approval of personal information use for every case, and before·after data collection, comprehensive informed consent or post-consent, obtaining approval for comprehensive use of personal information and allowing it to be used with a simple notification in case the purpose of use is similar, should be considered as an alternative.


Last, mass volume of big data is required for AI to learn big data, and if learning of copyrighted work is done through Fair Use or personal information is used through De-identification, no profit occurs for the intellectual property holder producing the data or the information holder. In order to resolve this issue, blockchain can be leveraged to provide compensation for data use based on a pre-agreed smart contract with the copyright holder or intellectual property right holder. This will not only enable easy big data learning of AI by providing legal stability, but also potentially lead to creation of new value models.

Therefore, this study focused on eliminating the legal obstacles of big data learning by AI centering around ‘Fair Use’, ‘De-identification’, and ‘Post-consent’ as legal options, and leveraging new technology such as blockchain to provide compensation to the information producers of big data which can lead to the creation of new economic value.인공지능은 빅데이터와 컴퓨팅기술을 기반으로 머신러닝과 딥러닝을 통해 스스로 학습을 통하여 특정한 결과를 구현하는 기술이다. 최근 디지털 기술의 발전으로 인류의 역사상 일찍이 없었던 많은 양의 빅데이터가 발생하고 인공지능은 이런 빅데이터를 기반으로 과거 인간의 전유물이던 정신적 사고와 판단의 영역으로 빠르게 대체하고 있을 뿐 아니라 종래에는 인간이 가진 지리적·시간적 한계를 뛰어넘어 인간이 도달할 수 없는 새로운 영역으로 진화하고 있다.

하지만, 인공지능이 발전하는 데 있어서 핵심요소인 빅데이터를 학습하는데 여러 가지 장애요소들이 존재한다. 첫 번째로 인공지능이 빅데이터를 학습하는 데 있어서 가장 큰 장애요소는 저작권 침해다. 우리나라 저작권법은 저작물을 사용하기 위해서는 저작재산권자에게 이용허락을 받아야 한다. 인공지능은 머신러닝과 딥러닝과 같은 학습 알고리즘의 발전을 통해 인간이 제공하는 데이터만 학습하거나 컴퓨터와 같은 사물이 만들어 내는 정형 데이터만을 학습하지 않고, 스스로 자연어, 텍스트, 사진, 음성, 동영상 등과 같은 비정형 데이터를 인식하고 학습하기 때문에 필연적으로 저작권과 충돌하게 된다.

두 번째로 인공지능이 빅데이터를 학습하면서 장애가 될 수 있는 부분은 개인정보이다. 개인의 ‘자기정보결정권’을 근거로 개인정보를 활용하거나 제3자에게 제공하거나 국외이전 등을 할 경우에는 반드시 정보 주체 들로부터 사전동의를 받아야 한다. 인공지능이 빅데이터 학습을 위해 일일이 정보 주체로부터 사전동의를 받는 것 자체가 불가능하다.

이러한 인공지능의 빅데이터 학습의 장애요소를 법적으로 제거하기 위해 첫 번째로 인공지능이 빅데이터에 포함된 저작물을 이용을 자유롭게 하는 데 있어서 공정이용을 고려해 볼 수 있다. 공정이용이란 공공의 목적을 위하여 사용되는 경우 저작권을 제한하는 것이다. 즉 인공지능의 빅데이터 학습이 공정이용의 요건에 해당한다면 인공지능은 저작권자 또는 저작재산권자의 이용허락 없이 저작물을 이용할 수 있게 된다.

두 번째로 인공지능이 빅데이터를 학습하는 데 있어서 개인정보보호 문제를 해결하기 위한 법적 방안은 비식별화 조치이다. 비식별화 조치란 개인정보의 일부 또는 전부를 삭제·대체하여 특정한 개인을 알아볼 수 없도록 만드는 조치이다. 이러한 비식별화된 정보를 익명정보 또는 가명정보라 부르고 있다. 일단 비식별화 조치가 완료된 후에 재식별이 불가능한 익명정보의 경우 개인정보로 분류하지 않고 자유롭게 데이터를 활용할 수 있게 하는 것이 필요하다. 또한 가명정보라 할지라도 민감도를 고려하여 자유롭게 이용할 수 있도록 해야 한다.

세 번째로 저작물이나 개인정보를 활용하기 위해서는 저작권자 또는 정보 주체로부터 사전동의를 받아야 하는데 사전동의제는 몇 가지 문제점을 가지고 있다. 개인정보의 수집 시 사전동의를 한 정보 주체는 사전동의 이후 데이터의 활용이 어떻게 이루어지는지 알 수 없다. 따라서 사전동의 이후 자신의 정보가 어떻게 활용되는지에 대한 우려가 발생한다. 반면에 사후동의제는 정보를 수집한 이후에 주기적으로 처리 상황을 보고하고 정보 주체에게 자신의 개인정보가 제대로 사용되지 않는 경우 언제든지 개인정보의 사용을 중지할 수 있게 하고 있다. 또한 사전동의제는 최초 개인정보를 수집할 시점에 특정한 목적에 한하여 개인정보를 활용할 수 있도록 제한함으로 이후에 개인정보의 수집 목적 외의 목적으로 사용하는 경우 다시 재동의를 받아야 하는 번거로움이 발생하게 된다. 따라서 일정한 요건을 갖춘 경우 일일이 개인정보의 활용과 이후 이전에 대해서 사전동의를 받기보다는 포괄적인 개인정보 사용에 대한 승인을 받고 유사한 목적의 활용에는 단순한 고지로써 이를 활용할 수 있게 하고 포괄적인 사전동의 및 사후동의제를 고민하는 것이 필요하다.

마지막으로 인공지능이 빅데이터를 학습하기 위해서는 더 많은 데이터가 필요한데 공정이용을 통해 저작물을 학습하거나 비식별화를 통해 개인정보를 학습하게 한다면 데이터를 생산하는 저작재산권자 또는 정보 주체에게는 아무런 이익이 발생하지 않는다. 이를 해결하기 위해 블록체인을 이용하여 저작권자 또는 저작재산권자와 미리 합의한 스마트계약에 따라 암호화폐 등으로 보상한다면 인공지능이 빅데이터를 학습을 용이하게 함으로 법적인 안정성을 가져올 뿐 아니라 새로운 가치 모델을 만들 수 있을 것으로 생각한다.

따라서 본 연구는 인공지능이 빅데이터를 학습하는 데 있어서 법적인 장애요소를 제거하기 위해 ‘공정이용’,‘비식별화’,‘사후동의제’를 중심으로 법적 방안을 제시하고 블록체인 등의 신기술을 통해 정보생산자에게 빅데이터에 대한 보상을 통해서 새로운 경제적 가치를 창출하는 방안을 제시하는 데 중점을 두었다.목 차 제1장 서론 1 제1절 연구의 목적 1

This research focuses on understanding the Yin-Yang and the Five Elements to explain individual personality types. To achieve this, to drawn the Five Elements characteristics by using the Delphi technique and FGI(study 1). To validate the results of study 1, the data was compared with that of OPQ (study 2).

The result is as follows.

First, reviewing study 1, we come to the conclusion that this can be categorized
into five groups.

According to the Yin-Yang and the Five Elements, the relative relation of the Yin-Yang is determined in all things. Therefore, the co-supportive relativity in time and the similarity of the Five Elements can be used to explain the principals of all matter including humans. The common nature of Yang is strong, dynamic, active, leading, open, bright and cheerful and realistic. Whereas Yin is weak, still, passive, closed, gloomy, small and idealistic.
These aspects change to form the Five Elements, Fire is the typical form of Yang, and Water is the typical form of Yin. Wood is the process of Yin turning into Yang, Metal is the process of Yang turning into Yin and earth is the balance between Yin and Yang.
Therefore, Water has the qualities of Yin and partial Yang. Fire has the qualities of Yang and partial Yin. Wood has mainly Yin qualities with a bit of Yang and catalyst like qualities that helps Yin turn into Yang. Metal has mainly Yang qualities with a bit of Yin and catalyst like qualities that helps Yang turn into Yin Yang. Earth is balanced keeping the balance among the Five Elements.
As a result, Wood personality is social, considerate, initiative, achievement oriented, outgoing, passionate, innovative, future oriented, extemporaneous, fun oriented and nervous(small issues). Fire is leading, self centered, proper, action oriented, fast conduct, intuitive, critical, realist, just, conservative and absolute. Earth is mediating role, relations, democratic, data based, according to principle, logical, responsible, reality oriented, cherishing experience, patience and not wanting to change. Metal is closed/ independent, keeping to one’s self, controlling emotions, quality oriented task oriented, complying to guide lines, avoiding competition, expressiveness, unification oriented, organization and task completion. Water is sensibility, insecure, idealistic, introverted, initiative, creative, intellectual curiosity, essential and mental.


Second, in reference to study 2, this concludes that there the differences
between the Five Elements are valid.

The OPQ character categories and the Five Element type comparative data show that the results of study 1 and 2 are almost identical. The reliability for the three categories persuasive, social confidence and worrying was insufficient to show difference. But in the rest of the 27 categories, the results of study 1 and 2 were either identical or showed high correlation.
As a result, for Initiator(Wood), caring, practical, artistic, conceptual, innovative and active/vigorous were high and independence, conservativeness, relaxed, uprightness, emotionally controlled, competitiveness and achieving were low. For Leader(Fire), controlling, forward planning, conscientious, relaxed, competitiveness and achieving was high and independence, outgoing, affiliative, modest, democratic and behavioral was low. For Key Player(Earth), data rational, behavioral, forward planning, detail conscious, conscientious, relaxed, competitive, achieving, decisive was high and change orientated seeking was low. For Accomplisher(Metal), independence, modest, artistic, behavioral, detail conscious, relaxed, though minded, emotionally controlled and optimistic was high and outgoing, critical and achieving was low. For Motivator(Water), outgoing, affiliative, democratic, data rational, traditional/ conventional, forward planning and conscientious was high and modest , innovative, detail conscious, relaxed and optimistic was low.
Never the less, this study is restricted by the researcher’s research potential, the nature of the topic, research method and etc. First, the target group for this study is adult workers so there was difficulty due to the wide age range which made this hard to generalize. So further studies for various groups are needed. Second, operational definition is needed for the Five Elements to explain human tendency. Third, trait based OPQ was used to show the validity of this study but there are limitations because OPQ doesn’t include all categories when testing.최근 HRD 영역에서 체제설계의 근간인 역량모델링 과정에서 개인수준의 핵심 역량을 구성하는 요소로서 개인의 내재적 특성에 주목함에 따라 개인의 심리적 요소가 주요 측정대상이 되고 있다. 또한 성인학습자의 학습효과를 높이기 위한 노력의 일환으로 학습장면에서 다양한 검사나 진단도구 성격검사의 사용빈도가 높아 지고 있다. 이러한 이유로 본 연구는 개인의 성격적 요소를 설명할 수 있는 새로운 가능성 이 될 수 있는 음양오행을 기반으로 그 성격속성을 도출하고 타당성을 검증 하는데 목적을 두고 수행되었다. 이를 위해 전문가그룹을 대상으로 델파이법과 FGI를 사용하여 오행유형별 성격적 속성을 도출(연구 1)하고, 이를 HRD 현장에서 빈번히 사용하고 있는 직무적성검사(OPQ) 결과와 비교하여 타당성을 검증(연구 2) 하였다.

그 결과는 다음과 같다.

첫째, 연구문제 1과 관련하여 살펴보면 오행 유형별로 구분되는 속성의 차이가
있다는 결론을 얻을 수 있었다.

음양오행론에 따르면 음양의 상대적 관계에 따라 만물의 속성이 정해지고, 음양의 속성이 변화하면서 오행의 속성이 구성된다. 음양의 시간적 양상과 동태성을 표현한 오행 또한 인간을 포함한 만물의 속성과 구성원리를 설명하는 개념으로 사용 할 수 있다. 양(陽)으로 분류된 것들의 공통적인 특성은 강하며, 동적이며, 적극적 이며, 앞장서며, 개방적이며, 능동적이며, 밝고 명랑하며, 현실적인 속성을 갖는다. 반면 음(陰)으로 분류된 것들은 약하며, 정적이며, 소극적이며, 폐쇄적이며, 수동적 이며, 침울하며, 작고, 이상적인 속성을 갖는다. 음양의 속성을 오행으로 대치한다면 화(火)는 양(陽)의 전형적인 모습이며, 수(水)는 음(陰)의 전형적인 모습이다. 목(木)은 음이 양으로 진행되는 과정의 분기점이며, 금(金)은 양이 음으로 진행되는 과정의 분기점이다. 토(土)는 음과 양이 균형을 이룬 상태라고 말할 수 있다.
그러므로 수(水)의 속성은 대부분의 음(陰)의 속성과 일부분의 양(陽)의 속성이 내재되어 있다. 화(火)는 대부분의 양(陽)의 속성과 일부분의 음(陰)의 속성이 내재 되어 있다. 목(木)은 음(陰)의 속성을 중심으로 하고 부분적으로 양(陽)의 속성과 함께 음(陰)을 양(陽)으로 전환하는 촉매적 속성이 포함되어 있다. 금(金)은 양(陽)의 속성을 중심으로 하고, 음(陰)의 속성과 양(陽)을 양(陽)으로 전환하는 촉매적 속성이 포함되어 있다. 토(土)는 음양(陰陽)이 균형을 이루면서 타 오행간의 균형을 유지하는 속성이 포함되어 있다.
그 결과 목(木)의 성격적 속성으로는 사교지향, 표현, 배려심, 추진력, 성취지향, 열정적, 혁신지향, 미래지형, 즉흥적, 풍류지향, 불안(작은 일) 등이 있었다. 화(火)의 경우에는 주도적, 자기중심적, 예의범절, 행동지향, 빠른 일 처리, 직관적, 평가적, 현실적, 정의감, 보수적, 절대적 등이 있었다. 토(土)의 경우에는 중간자 역할, 대인 관계, 민주성, 자료지향, 원칙지향, 논리적, 현실지향, 경험중시, 책임감, 끈기, 변화 거부 등이 있었다. 금(金)의 경우에는 배타적/독립적, 사생활 보호, 감정통제, 과업 지향, 원칙준수, 경쟁회피, 통합지향, 정리, 품질지향, 과업완결 등이 있었다. 수(水) 의 경우에는 대인감수성, 정서불안, 이상적, 표현력, 내성적, 직관적, 창의성, 지적 호기심, 본질추구, 정신력 등이 있었다.

둘째, 연구문제 2와 관련하여 연구문제 1의 오행별 속성의 차이는 타당하다는
결론을 얻을 수 있다.

OPQ 성격항목과 오행유형의 비교 결과를 보면, 연구1의 결과와 연구2의 결과가 대부분의 경우 일치했다. 다만 설득지향성, 사회적 자신감, 상황불안 등 3개 항목 에서는 신뢰할 수 있는 차이가 발견 되지 않았다. 그러나 나머지 27개 항목에서는 연구 1의 결과와 연구 2의 결과가 일치하거나 포함되는 것으로 나타났다.

그러나 본 연구는 연구자의 연구역량과 주제의 성격, 연구방법 등으로 인해 제한점이 있다.
첫째, 본 연구의 대상이 성인 직장인이기 때문에 전 연령으로 확대하여 일반화 하는데 어려움이 있다. 따라서 다양한 계층을 대상으로 후속연구를 진행하는 것이 필요하다.
둘째, 오행속성 도출 시 사용된 어휘들의 개념에 대한 조작적 정의가 필요하다. 인간의 경향성을 설명하는데 자연어가 편리한 출발점을 제공하지만 주관성을 배제할 수 없으므로, 추가적으로 어휘에 대한 척도개발이 요구 된다.
셋째, 본 연구는 타당성을 검증하기 위하여 특성론을 기반으로 하는 OPQ를 사용하였으나, OPQ가 모든 항목을 측정하는 것은 아니므로 타당성을 입증하는데 한계가 따른다. 따라서 후속연구를 통해 다양한 타당성 검증이 이루어져야 한다.

그럼에도 불구하고 본 연구는 다음과 같은 의의가 있다.
첫째, 본 연구를 진행한 결과 음양오행론은 인간의 속성을 설명할 수 있으며, 상당한 수준의 타당성이 있었다. 그러므로 음양오행론의 오행요소는 인간의 성격적 속성을 설명할 수 있는 새로운 이론적 기반을 제공할 수 있다는 측면에서 가치가 크다고 판단되었다. 그러므로 HRD 현장에서 사용되던 성인학습자 성격진단을 위한 각종 검사들의 성격구성요소와 구성요소간의 관계를 재정립할 수 있는 가능성을 제시할 수 있다고 판단된다.
둘째, 음양오행 유형은 개인의 선천적 자료(출생일)에 의해 추출할 수 있으므로, 여러 가지 이유로 자기보고식 지필검사를 하기 어려운 대상이나 상황에서 지필검사 의 한계를 보완할 수 있는 도구로서의 가능성이 크다고 볼 수 있다.
셋째, 상대론적인 음양오행적 관점에서의 성격구성 원리를 통해 성인학습자의 핵심역량을 규명하고 핵심역량개발을 위한 교육훈련 프로그램을 개발하고 운영 하는데 새로운 접근과 시도가 가능하다. 즉 음양오행론의 논리적 기반을 바탕으로 그 속성을 구체화하고 이를 기반으로 모듈형 단위프로그램을 개발할 수 있다. 또한 성인학습자의 개별요구에 맞춰 조합함으로써 조직 내 성인학습자를 대상으로 하는 HRD 업무의 효과성과 경제성을 제고할 수 있는 또 다른 시도가 가능하다.
넷째, 본 연구에서는 오행의 심리적 측면에 초점을 두었다. 그러나 음양오행의 물리적 속성과 상관관계에 대한 연구를 진행한다면 조직과 개인의 직무성과를 제고하고, 개인의 부족한 심리적 속성을 보완하고 개발하는데 오행의 물리적 관점(색, 맛, 방향, 음식, 시간, 공간, 체질 등)을 사용할 수 있을 것이다. 그러면 이를 통해 자기개발 및 조직 구성원들의 역량개발 및 성관관리를 위한 통합적인 관점에서 해법을 찾을 수 있을 것이다.

본 연구결과가 의미와 가치가 있음에도 불구하고 적용에는 많은 신중함이 필요하다. 본 연구에서는 오행의 속성을 극명하게 도출하기 위하여 일간과 일지 혹은 월지가 동일한 소수의 대상자를 중심으로 연구하였다. 따라서 일반적으로 출생일시 만으로 유사한 결과를 얻기는 어려우므로, 곧 바로 적용하는데 큰 한계가 있다.目次 I. 서론 1. 연구의 필요성과 목적 1 2. 연구문제 5

With the global trend of BIM applications shifting from technical development to practical adoption and domestic policy to adopt BIM, government facilities and companies in the architecture, engineering, and construction industries are eager to learn the proper way to use BIM from case studies. However, hands-on workers, such as architects and construction managers, find it difficult to collect case studies of BIM-applied construction projects (BIM projects). Despite the presence of various websites (e.g., Global BIM Dashboard, the BIM Hub, BIM+, and AECbytes) and literature (e.g., The BIM Handbook, The BIM magazine, and journal papers) that offer case studies of BIM projects, it is hard to find structured information, as case studies are scattered everywhere and described in natural language. Applying the text-classification method, a subcategory of text mining, can help solve this problem. But when we directly apply the text-classification method to the construction field, it creates other problems regarding BIM-related documents, because optimization of the method is required.
The purpose of this study is to develop a BIM document ) categorization method for BIM use. The BIM use proposed in the BIM Project Execution Planning Guide v2.1 was used as a classification category, and text classification methods were applied for categorization algorithms. Also, a web-based information system was developed to update and manage machine learning algorithms. More specifically, the study answered the following questions:
(1)How can we manage multi-word features, numbers, punctuation, and stop words when we generate a local dictionary for text preprocessing of BIM documents?
(2)What are the machine learning algorithms and algorithm variables that show the best performance in the text classification of BIM documents?
(3)How can we maintain optimized text-classification methods if the corpus is continuously changed by adding new raw data?
In this study, 240 BIM documents written in English were collected from BIM-related websites and literature. Each BIM document was labeled 1 (yes) or 0 (no) in regard to 22 items related to the presence of BIM-related description and 21 BIM uses to validate the performance of machine learning algorithms. The Porter stemmer was used for stemming. Features that showed the same order 2–5 times repeatedly, such as “Building Information Modeling,” were defined as multi-word features. Also, the rules to handle numbers and punctuation shown in features like 3D, 3-D, and quantity take-off were defined.
To reduce the amount of calculation for text classification of BIM documents, 21 BIM-use items were tested after the presence of BIM-related description was tested. Feature weighting, feature selection, and the number of features were used as variables for supervised learning. For unsupervised learning, the document-segmentation method, feature selection, a dimensionality-reduction parameter, iteration of estimation, the number of topics, the width of the word window, the minimum word count, and similarity thresholds were used as variables to estimate the performance of each algorithm. The performance of unsupervised learning algorithms was tested by comparing the cosine similarity between the definitions of each BIM use described in BIM Project Execution Planning v2.1 and testing the document with thresholds. Precision, recall, and F1 score were estimated to validate the performance of machine learning algorithms.
In the study, support vector machine (SVM) with ¬term-frequency (TF), chi-squared statistics, and 60% of the total features (3,171 features) showed the best performance for the classification of the presence of BIM-related description. The result was precision: 80.26%; recall: 82.99%; and F1 score: 81.61%. The unsupervised learning algorithm for the 10 BIM use items that have more than 10 samples showed the following F1 scores: existing conditions modeling: 84.52%; cost estimation: 85.71%; phase planning: 77.81%; design reviews: 85.02%; design authoring: 84.98%; engineering analysis: 84.57%; 3D coordination: 82.99%; digital fabrication: 79.10%; record model: 79.76%; maintenance scheduling: 78.53%. Each test extracted the best variable combination for the algorithms. The average F1 score of the unsupervised learning algorithms was 82.30%, and the unsupervised learning was better than supervised learning. The web-based information system to maintain optimized algorithms was constructed based on the study’s results.
In conclusion, text classification applied to construction will make it possible to collect tacit knowledge more efficiently thanks to reducing the time required to find the proper documents that contain the required information and allow workers to focus on their own interests. Also, the results of this study provide a chance to challenge the tracking the BIM-adoption trend and learn how to use BIM by categorizing a large number of BIM documents automatically.BIM이 이론적인 개발 단계에서 적용 단계로 넘어가려는 국제적인 동향과 국내 정책 방향에 따라 BIM 도입을 위한 노력들이 이루어 지고 있다. 이에 따라 BIM을 적용한 건설 프로젝트의 사례 조사가 지속적으로 수행되고 있다. 하지만 BIM 사용자들은 BIM 관련 건설 프로젝트 사례 조사 정보를 수집하는데 어려움을 겪고 있다. 사례 조사 정보를 제공하는 웹 사이트(Global BIM Dashboard, The BIM Hub, BIM+, AECbytes 등)나 문헌(The BIM Handbook, The BIM, 사례 조사 논문 등)들이 있지만, 대부분 자연어(natural language) 형태로 산재되어 있어 필요한 정보를 찾기가 힘들기 때문이다. 이러한 문제를 해결하기 위해 텍스트 마이닝(text mining) 기법 중 하나인 텍스트 분류(text classification)를 활용할 수 있겠지만 텍스트 분류 알고리즘이 건설 분야에 최적화되지 않아 건설 프로젝트 사례 조사 문서, 특히 BIM 관련 문서의 텍스트 분류에는 아직까지 어려움이 많다.
본 연구는 텍스트 분류 기법을 활용하여, BIM 문서 )를 BIM Project Execution Planning Guide v2.1에서 제시한 BIM 사용 유형에 따라 자동으로 분류할 수 있는 기법의 개발을 목표로 한다. 또한 기계 학습(machine learning) 알고리즘을 주기적으로 업데이트하는 웹 인터페이스를 구축하여 지속적인 관리가 가능하도록 하고자 한다. 구체적으로 본 연구를 통해 다음 연구 질문들에 대한 답을 찾고자 하였다.
(1)BIM 문서의 텍스트 전처리(text preprocess)를 위한 로컬 사전(local dictionary)을 제작할 때 다중단어 특징(multiword feature)과, 숫자, 문장 부호, 불용어(stopword)는 어떻게 처리할 것인가?
(2)BIM 문서의 텍스트 분류에서 가장 높은 성능을 나타내는 기계 학습 알고리즘과 변수 값은 무엇인가?
(3)원자료(raw data)가 추가됨에 따라 문서 집단(corpus)에 변화가 생길 때 텍스트 분류 알고리즘이 최적화된 상태를 유지하기 위해서는 어떻게 해야 하는가?
연구 수행 과정은 다음과 같다. 인터넷과 문헌에서 영문으로 작성된 240건의 건설 프로젝트 사례를 수집한 후, 기계 학습 알고리즘 검증을 위해 문서마다 BIM 관련 내용 유무와 BIM 사용 유형에 대한 21가지 항목, 총 22가지 항목에 대해 1(있음), 0(없음)으로 라벨링(labeling)하였다. 텍스트 전처리에서는 Porter stemmer의 어간 추출(stemming) 방법을 사용하였고 BIM 분야의 특화된 로컬 사전을 작성하기 위해서 Building Information Modeling과 같이 2-5번 동일하게 나열되는 특징의 패턴을 인식해 다중단어 특징으로 분류했다. 또한 3D, 3-D, quantity take-off 등과 같은 특징들의 전처리를 위해 숫자와 문장 부호에 대한 규칙을 정의하였다. BIM 사용 유형 21가지에 대한 분류의 연산 작업량을 줄이기 위해 BIM 관련 텍스트 유무에 대한 판별을 먼저 수행한 후, BIM 문서를 대상으로 BIM 사용 유형에 대한 분류를 수행하였다. 지도 학습(supervised learning)의 실험 변수로 특징 가중치 책정(feature weighting), 특징 선택(feature selection), 특징의 수를 활용하였다. 비지도 학습(unsupervised learning)에서는 문서 분절 방법, 특징 선택, 차원 축소(dimensionality reduction) 계수, 추정 반복 횟수(iteration), 주제의 수, 주변 단어의 수, 단어의 최소 빈도 수, 유사도 경계값(threshold)을 변수로 설정하여 각 조건에 따른 분류 성능을 측정하였다. 비지도 학습의 분류는 BIM Project Execution Planning v2.1에 기술된 BIM 사용 유형의 정의와 실험 문서 간의 유사도 측정 및 경계값 탐색을 통해 수행하였다. 텍스트 분류 알고리즘의 성능 검증을 위해 정밀도(precision), 재현율(recall), F1 점수(F1 score)를 측정하였다.
실험 결과, 지도 학습 알고리즘은 BIM 사용 유무 분류에서 순수 빈도 가중치(TF, Term Frequency)을 통한 특징 가중치 선택, 카이제곱 검정(chi-squared statistics)을 통한 특징 선택, 전체 특징의 60%(3,171단어)를 선정했을 때 SVM (Support Vector Machine)이 정밀도 80.26%, 재현율 82.99%, F1 점수 81.61%로 가장 높은 성능을 나타냈다. 하지만 세부적인 BIM 사용 유형의 분류에서는 좋은 성능을 보이지 못했다. 비지도 학습 알고리즘은 표본이 10건 이상인 BIM 사용 유형에 대하여 F1 점수가 각각 existing conditions modeling 84.52%, cost estimation 85.71%, phase planning 77.81%, design reviews 85.02%, design Authoring 84.98%, engineering analysis 84.57%, 3D coordination 82.99%, digital fabrication 79.10%, record model 79.76%, maintenance scheduling 78.53%로 나타났다. 또한 각 사용 유형에 따라 최적의 성능을 나타내는 변수 조합을 도출하였다. 비지도 학습은 전체적으로 F1 점수 82.30%의 성능을 보여 주었으며 지도 학습보다 우수한 성능을 보여 주었다. 마지막으로 실험 과정과 결과가 적용된 웹 인터페이스를 구축하여 분류 알고리즘의 지속적인 업데이트가 가능하도록 했다.
본 연구의 결과를 통해 BIM 사용 유형에 따라 관련 문서들을 분류할 수 있다면 BIM 사용자들이 건설 프로젝트 사례 조사 문서를 검색하는 시간을 줄이고 관심 분야에 집중할 수 있기 때문에 효율적인 경험 지식 습득이 가능할 것으로 생각된다. 또한 다량의 사례 조사 문서를 자동 분류하여 통해 BIM 도입의 경향이나 유형에 따른 활용 방법 등을 탐구할 수 있는 기회를 제공할 것이라 생각된다.

Literature influences personality development a lot because there are a spirit of forefathers and wisdom of the life. Especially, poets bring own spirits out with words to readers. The poetry words contain contemporary culture and circumstances of the period. The children's verse also shows the traditional spirits. This paper researched about values of traditional spirits involved in children's verse, It's focused on Jung Ji yong's, Yoon Dong joo's and Yoo Kyung hwan's.
The paper tries to find a writer's spirit to deal with circumstances of the time, changes of spirits with the trends of the time, and the power of the modern children's verse which is continued in spite of the loss of traditional spirit after the division of territory. The paper forecasts the way to go and the value of spirit of Korean modern children's verse studying writer's mental state which is placed under Oriental philosophy.
Jung Ji yong uses the language of nature to present spirits applying tribrach in his poems. He tries to make the first speaking to own words which are just like his fresh and blood. Every man has the original character to be united with the nature. An absence of mother is the main motive spreaded under his poems. An absence of mother which is injust with the trends of the time makes him weak. However, Jung Ji yong tried to find the process to recover an absence of mother to pursue the innocence of a child and Oriental philosophy. A self-control of Oriental philosophy is under the absolute loneliness.
Yoon's works tries to find an universal ego against darkness. He tries to develop the way to the universe to involve with the nature ceaselessly through meditation of the nature. The universal world is the world of an innocence of a child. Yoon tries to find an ego through a night(darkness) as a rest place. He trains to face a difficulty and adversity through a creative power with imagination. A 'darkness' is corresponded with 'snow' or 'mother'. The common meaning of 'snow' of 'mother' is providing a place to 'rest'. A place to 'rest' is the place where he can train himself to face difficulties. The special feature of Yoon Dong joo's children's verse is to overcome the reality to find a light in the darkness. The overcoming the reality begins from the finding an ego because the only thing that can be the light in the darkness is an universal go. It is possible through patience and self-control which is a basic of Oriental philosophy.
Yoo's works tries to recover difficulties from outside to inside in 1920s~1940s. The industrialized age after the Korean war in 1950 could solve the practical problems but bring a mental poverty. Disharmony of materials and spirits brings a trend to make light of life and mental devastation and a spiritual vacuum becomes the one of the factors of social disturbance. Yoo Kyung hwan takes all trends of the time and tries to find great illumination to accomplish spiritual transcendence. He prepares a place to rest at the calm state of 'winter' and 'tree' and makes 'mountain' and 'water' as a mental indicator. Three poets have common aspects of a holy place(聖) and the mundane world(俗) as a mental indicator. An 'absence', 'darkness', 'winter', 'water', 'mountain' and 'tree' are belongs to the mundane world(俗) and 'morning', 'light', 'recovery', 'the sun', 'a baby' and 'hope' belongs to a holy place(聖). A life born from the mundane world is a holy thing and it becomes 'death', that is, 'a mundane thing'. To divide 'the mundane world' and 'a holy world', 'view' is needed. The 'view' is the spiritual eye that can see the characteristic of materials. The world of the mind of a child is the world that the mundane things and a holy things are harmonized well.
In spite of a time gap, the common thing that three poets persist is a traditional spirit. Our traditional spirit is a transcendence of being free from distraction. Poems can preserve for long only if they incorporate a spiritual thinking. A mind of a child can move the spirits and it becomes a background of a children's verse. Jung Ji yong, Yoon Dong joo and Yoo Kyung hwan tried to take traditional spirits over and make aware of spirits of children's literature.이 논문은 한국현대 동시의 이면에 자리한 정신양상에 관한 연구다. 문학작품 속에는 시대를 살아 온 선인들의 정신이 언어의 옷을 입고 있으며 지혜가 담겨 있다. 또한 시어 하나하나에 시인의 정신과 당대의 문화가 내포되어 있다. 그러한 의미에서 동시에 내포되어 있는 정신양상에 대해 정지용, 윤동주, 유경환을 중심으로 연구 되었다.
세 시인의 동시는 단절되어가는 우리 전통정신의 맥을 잇고자 했다. 이에 동시의 가치와 시의식이 동시창작에 미치는 영향을 살피며 한국동시사의 형성과정과 정신적 특성을 연구 대상으로 삼았다.
정지용은 3음보격의 리듬을 활용하여 訥語로 현현되는 정신력을 자연어의 이미지에 실어내고자 했다. 정지용 작품의 주 모티브는 어머니다. 당 시대적 윤리관에 따르자면 정당하지 못했던 어머니의 부재는 시인을 나약하게 만들었다. 그러나 그는 마음의 상처를 동양정신의 극기로 자연의 미학적 정신력으로 승화시켰다.
윤동주는 어둠(암흑)과의 대응으로 우주적 자아를 발견하고자 한다. 시인은 쉼의 공간인 밤(암흑)속에서 자아를 발견했으며 고난과 역경에
대응할 수 있는 동시정신을 키웠다. ‘암흑’은 ‘겨울’ 또는 ‘어머니’로 대응되며 모든 생명에게 ‘휴식’ 공간을 제공한다. ‘휴식’ 공간은 고난과 역경에 대응할 수 있는 정신을 키우는 공간이다. 또한 윤동주 동시의 특징은 인내와 극기 정신에 있음을 찾을 수 있었다.
유경환은 현대화 과정에서 파생되는 정신적 시련을 극복하고자 노력하였다. 산업화 시대는 우리에게 물질적 풍요를 누리게 했지만 정신적 빈곤을 가져왔다. 물질과 정신의 부조화는 생명경시풍조와 정신적 황폐화를 가져왔다. 정신부재는 사회불안 요소로 떠오르게 되었다. 유경환은 정신적 압박을 ‘겨울’과 ‘나무’의 안정된 정서에 휴식공간을 마련하고 ‘산’과 ‘물’을 정신적지표로 삼고자 하였다.
세 시인이 정신적 지표로 삼는 전통정신은 조화가 가장 잘 이루어진 동심의 세계다. 1920~1940년대는 외부에서 안으로 주어지는 시련을 극복하고자 했다면 1950년 한국전쟁이 끝나고 이어진 시대적 변화는 정신적 압박을 하게 되었다. 이들은 전통정신의 원형상징이미지를 들어 주어진 상황과 조화를 이루고자 하였다.
시대적 큰 폭에도 불구하고 세 시인이 일관되게 주장한 것은 전통정신의 맥을 잇는 것이었다. 우리의 전통정신은 무경계, 무념의 초월에 있다. 동시가 정신적 사유를 담지 않는다면 그 생명력이 미약해질 것이다. 정지용, 윤동주, 유경환은 동심으로 정신적 초월을 이루고자 했으며 전통정신의 맥을 잇고자 했다.국문요약 = i Ⅰ. 서론 = 1 1. 문제의 제기 = 1 2. 연구사 검토 = 7 3. 연구의 방법과 범위 = 11

비교 어휘 패턴 및 기계 학습 기법을 이용한 한국어 비교 마이닝

Korean Comparison Mining Using Comparative
Lexical Patterns and Machine Learning Techniques

컴퓨터공학과 양 선
지도교수 고영중

자연어로 쓰인 대용량의 텍스트 문서들로부터 비교 정보를 자동으로 추출해내는 일은 텍스트 마이닝의 주요 과제 중 하나이다. 최근의 웹은 뉴스 기사나 개인 블로그뿐만 아니라, 온라인 토론 포럼, 전자 마켓 제품 리뷰, 비교 전문 사이트 등을 통해 질적 양적으로 매우 풍부한 비교 데이터를 축적하고 있으며 점점 더 많은 비교 데이터가 계속해서 생성되고 있다. 비교 마이닝(comparison mining)은 이러한 양질의 비교 정보를 자동으로 추출 및 분석하며, 마케팅, 벤치마킹 등 많은 응용 분야에서 유용하게 활용될 수 있다.
본 논문은 한국어 비교 마이닝에 대한 연구로 세 단계로 진행되는데, 먼저 텍스트 문서들로부터 비교 문장만을 식별하여 추출하고, 다음으로 추출된 비교 문장들을 일곱 가지 비교 유형으로 분류하며, 마지막으로 각 비교 유형의 특징을 기반으로 하여 비교 문장에서 비교 요소들을 추출한다. 첫 단계인 비교 문장 추출을 위하여, 먼저 언어학 연구를 참고하며, 또한 실제 사용되는 비교 문장들을 다수 관찰하여 비교 어휘 사전(comparison lexicon)을 구축하였으며, 이 사전에 있는 비교 키워드를 포함하는 문장을 비교 문장 후보로 간주한다. 그런 뒤 기계 학습(machine learning) 기법을 이용하여 비교 문장 후보들 중에 포함된 비(非)비교 문장들을 제외시킴으로써 최종적으로 비교 문장들만을 정제해낸다. 두 번째 단계인 비교 문장 유형 분류를 위하여, 먼저 비교 키워드 유형에 기반하여 비교 문장을 초기 분류하며, 그런 뒤 변환 기반 학습(transformation-based learning) 기법을 적용하여 초기 분류에서 발생된 오류를 정정한다. 세 번째 단계인 비교 요소 추출을 위해서, 먼저 몇 가지 규칙을 정하여 문장 내에서 비교 요소 후보를 정하고, 기계 학습 기법을 이용하여 후보들 중에서 정답 비교 요소를 찾아낸다.
본 논문에서 제안하는 기법은 실 생활에 응용하기에 충분할 만큼 우수한 실험 성능을 보여준다. 평가 메져는 각 단계 과제의 특성을 고려하여 F1-score혹은 정확도(accuracy)를 사용하며, 첫 단계인 비교 문장 추출에서 F1-score 90.30%, 두 번째 단계인 비교 문장 유형 분류에서 정확도 82.08%, 그리고 세 번째 단계인 비교 요소 추출에서는 정확도 87.10%의 우수한 성능을 산출하였다.

주요어 : 비교 마이닝, 비교 문장, 비교 어휘 사전, 비교 유형, 비교 요소, 기계 학습, 변환 기반 학습I. Introduction 1 1.1 How can these Five Problems be Solved? 5 1.2 Outline of this Dissertation 7 II. Related Works 8

탈냉전 이후 미국과 중국이 두 축이 되는 G2의 국제정치가 많은 주목을 받고 있다. 따라서 중국 정치에 대한 다양한 연구들이 진행되었다. 특히 중국정치를 연구하는 데 있어 텍스트의 중요성은 거듭 강조해도 지나치지 않다. 기존 텍스트 연구들이 주로 수동적(manually) 질적연구방법에 의존해왔다. 최근 컴퓨터를 활용한 텍스트 분석 기법이 활용되고 있으나, 이것은 출현빈도에 집중함으로써 정교한 정치문서를 분석하기에는 한계가 많았다. 본 논문에서는 탈냉전기 중국의 국방, 통일, 외교 정책을 연구하기 위해, 장쩌민 시기부터 중국 국방외교의 최고 권위 정치담화인 중국 공산당 전국대표대회의 정치보고서를 질적연구방법을 이용하여 분석했다.

이를 위해서 인공지능의 힘을 빌려, 자연어언어처리 기법을 활용한 과학적인 정치담화 분석 모델을 개발했다. 이 모델은 HJ's PDSA(Political Document and Speech Analysis) model로 실증적 구성주의 측면에서 제 14-18차 중국 당 대회 정치보고서의 국방, 통일, 외교 부문을 분석했다. 이를 통해 장쩌민부터 지금까지의 중국 국방외교 정책의 지속과 변화를 고찰하였다.

연구 결과는 다음과 같다. 첫째, 장쩌민부터 후진타오까지의 중국의 국방외교 정책은 국력신장에 따른 ‘증대된 공세성’에 비롯된다기 보다는 마오쩌둥·덩샤오핑 당시 구상해놓은 계획을 장기간에 걸쳐 체계적으로 실천하고 있는 연속성의 측면이 강했다. 하지만 둘째, 17기에서 18기로 넘어갈 당시 국방·외교정책에 있어 '질적인 변화', 즉, 패러다임의 전환이 포착되었다. 셋째, 정치 보고서 상의 인식은 실제 정책과 높은 연관성을 가지고 있었다. 즉, 특정 현상을 특정한 방향으로 인식하게 되면 그것이 수사적 표현(rhetoric)을 거쳐, 정책으로 연결되었다. 인식과 관련된 또 하나의 특징은 부정적 인식은 감소, 긍정적 인식은 증가 추세에 있었다는 것이다.

이것은 다음과 같은 정책적 함의를 제기한다. 우선 정치보고서의 내용은 과학적이며 체계적으로 구성되어있다는 것이다. 그동안 중국의 공식문서가 많은 연구자들에 의해서 '불투명'하게 느껴진 것은 과학적이고 체계적인 정치담화 분석 도구가 거의 존재하지 않았기 때문이다. 이것은 중국의 국방외교 연구에 있어서 집중적이고도 정밀한 정치담화 분석의 필요성을 강하게 제기한다. 즉, 과학적인 담화 연구가 강화될 필요가 있다. 여기에 HJ's PDSA　model이 기여하였다고 본다. 둘째, 대중 정책에 있어 각 국은 강대국으로 부상하는 중국을 인정하고 수용하는 관여 정책을 기저에 설정하는 것이 필요하다. 무엇보다도 중요한 것은 중국의 인식이 ‘공격적’으로 되지 않도록 관리하는 것이다. 특히 중국의 ‘분절된 권위주의’의 관료정치 구조를 고려했을 때, 그러한 노력은 여러 차원에서, 다각적이며 적극적으로 이루어져야 한다. 이것을 미중관계에 적용하면 ‘상호 전략적 신뢰’ 구축 필요성과 특히, 한국으로서는 ‘연미협중’전략과 그 맥을 같이 한다.I. 서론 1 1. 연구의 설계: 대상 및 시기의 선정 1 가. 연구의 설계 1 나. 연구대상 및 시기의 선정: 14-18기 중국 공산당 당 대회 국방통일외교부문 3 2. 문제의식 5



본 논문에서는 풍경구성기법의 그림 평가지표로부터 자연언어로 작성된 평가보고서를 자동 생성하는 시스템을 제시하였다. 본 시스템을 위해 풍경구성화의 평가지표 구성, 해석지표 지식베이스 구축, 자연언어 처리를 통한 감정분석 및 자연스러운 문장생성 등의 작업이 이루어진다.
본 연구에서는 시스템이 그림을 평가한다는 장점을 이용하여 평가지표를 더욱 세분화하고 확장시켜 구성하고 해석지표 지식베이스는 문헌과 연구 자료를 기반으로 구축하였다. 형태소 분석과 구문 분석을 통해 분석문의 핵심어 및 관련 용언을 추출하여 문장을 분석하였다. 내담자의 사고와 행동양식에 따른 범주로 분류하고 자연스러운 문장 생성을 위해 같은 원인을 소유한 문장들을 결합하였다.
본 시스템을 통해 도출한 결과는 다음과 같다. 첫째, 본 시스템을 통해 다량의 데이터가 축적되고 그 데이터를 기반하여 반복적으로 연구가 이루어진다면 그림검사의 신뢰도 문제는 비로소 해결될 것으로 보여 진다.
둘째, 문헌과 연구결과를 바탕으로 한 지식베이스가 구축으로 그림검사에 대한 객관성 문제를 해결하였다. 그림검사에 대한 객관성 문제 해결은 그림검사의 신뢰도를 보장하는 중요한 요소로 미술치료분야의 발전에 큰 영향을 미칠 것으로 사료된다.
셋째, 그림검사에 대한 평가보고서를 자동으로 생성하여 치료사의 업무량을 감소시킬 뿐만 아니라 오랜 시간 검사를 위해 기다려야 하는 내담자들의 고충을 해소할 것으로 보여 진다.
본 연구에서 제시한 평가보고서와 전문가가 직접 분석한 평가보고서를 비교하였다. 분석결과 전체 평가요소의 개수는 시스템이 더 많은 요소를 분석하는 것으로 나타났다. 더 많은 요소가 평가되고 분석된다는 것은 내담자의 그림에 대해 더욱 정밀하게 해석되는 것으로 시스템이 생성한 평가보고서의 유용성을 나타낸다.
본 시스템에서는 평가보고서를 자동으로 생성함으로서 미술치료사들의 업무량 해소에 직접적인 영향을 미칠 것으로 기대한다. 또한 개인의 부정적 감정을 조기에 발견하여 사회적 사건·사고를 사전에 예방하는데 일조 할 것으로 기대한다.This thesis proposes automatic generating system which creates assessment report scripted by natural language from drawing evaluation index of landscape montage technique. For this system we configured evaluation index for assessing the drawing of landscape montage technique, built up knowledge base for indicators, analyzed client’s emotion through natural language processing, and generated spontaneous sentences.
In this thesis, we configured evaluation index more subdivision and expanding using advantage that our proposed system assesses client’s drawing. Making knowledge base for interpretive index is based on the references and related research studies. The system in this thesis analyzes assessment sentences by extracting keyword and predicate using morphological and syntactic analysis. The system categorized client’s emotional state and action pattern which is analyzed from client’s drawing. To generate spontaneous sentences, the system group sentences with the same emotional state.
The contribution of this thesis is as the followings. Firstly, a great quantity of data is cumulated by using the system, and this can stimulate for studying reliability enhancement for landscape montage technique.
Secondly, objectivity about the drawing assessment is resolved by building knowledge base based on the recent research paper and book. Resolving objectivity problem about the drawing assessment is important factor for assuring the reliability of the drawing assessment. So we think that this contributes to advancement in the field of art therapy.
Thirdly, as the system automatically generates assessment report for the drawing assess, therapist’s workload can be dramatically reduced. And this system can resolve client’s distress waiting long time to get the assessment report.
In this thesis, assessment report automatically generated by system is compared to expert’s report. The results of comparison with two reports shows that assessment report generated by the system contains more number of evaluation index. This means that the system assesses client’s emotional state more detail. So the system can generate more useful assessment report.
The system proposed in this thesis automatically generate assessment report for client’s emotional state. So the system can make therapist’s workload reducing. And the system can help early finding the negative emotional state of client, and contribute to prevent accident and incident occurring by client’s negative emotion.Ⅰ. 서 론 1 1.1 연구 배경 1 1.2 연구 목표 및 내용 4 1.3 논문의 구성 6

선형중심어는 최종사용자를 위한 데이타베이스 질의어 중에서 가장 널리보급된 연어이다. 그러 나 초보자들은 선형중심어가 가지는 문법구조의경직성으로 인해 데이타베이스로 부터 필요한 정봅�내는데 어려움을겪고 있다. 이 문제의 대응책으로 초보자가 약간의 훈련을 받고도 사용할 수읒ㅐ岷助載�개발되었다. 본 연구의 목적은 초보자의 입장에서 자료검색작업을 위한 선형중심어와연어를 비교하는 것이다 보다 구체적으로질의종류(기본형, 내장함수형, AND/OT형, 혼합형)와 훈렉��쳔�때 두언어가 사용자 실적(정확성과 신속성)에 미치는 영향을 비교하였다. 본 연구의결같�정도의 훈련을 받은 초보자가 데이타베이스를 검색할 때한정자연어가 선형중심어를 사용할 때의 정확성과 작성시간 측면에서 더나은 성과를 보인 것으로 나타났다.

선형중심어는 최종사용자를 위한 데이타베이스 질의어 중에서 가장 널리보급된 연어이다. 그러 나 초보자들은 선형중심어가 가지는 문법구조의경직성으로 인해 데이타베이스로 부터 필요한 정봅�내는데 어려움을겪고 있다. 이 문제의 대응책으로 초보자가 약간의 훈련을 받고도 사용할 수읒ㅐ岷助載�개발되었다. 본 연구의 목적은 초보자의 입장에서 자료검색작업을 위한 선형중심어와연어를 비교하는 것이다 보다 구체적으로질의종류(기본형, 내장함수형, AND/OT형, 혼합형)와 훈렉��쳔�때 두언어가 사용자 실적(정확성과 신속성)에 미치는 영향을 비교하였다. 본 연구의결같�정도의 훈련을 받은 초보자가 데이타베이스를 검색할 때한정자연어가 선형중심어를 사용할 때의 정확성과 작성시간 측면에서 더나은 성과를 보인 것으로 나타났다.

본 연구는 중학교 교과서에 사용된 어휘 빈도수를 체언에 국한하여 찾아보고 이를 어종별로 분류하여 그 변화의 경향을 조사하는데 있었다.
연구의 대상은 대한민국 교육과정기의 제 1차 교육과정기부터 제 6차 교육과정기까지의 중학교 교과서 전 36권을 대상으로 하였다. 국어과 교과서의 단원 구성은 말하기·듣기·읽기·쓰기·언어·문학으로 되어 있으나 실제 교과서를 통해 어휘 자료를 가장 많이 수집할 수 있는 읽기 단원과 문학 영역의 소설 단원을 조사 대상으로 하였다. 각 교과서는 14쪽 이상의 지면을 취하여 총 1,386쪽의 본문을 분석대상으로 하였다.
조사 방법은 컴퓨터를 이용하여 자료를 입력하고, 입력된 자료의 통계 처리도 컴퓨터에 의존하였다. 분석에 사용된 프로그램은 1999년 1월 18일 작성된 고려대학교 컴퓨터학과 자연어처리연구실에서 발표된 한국어 용례 추출기 ‘글잡이 1.0’을 사용하였다. 먼저 교과서 본문을 텍스트로 입력한 다음 ‘글잡이 1.0’을 이용해 말뭉치로 분리해 낸 후 이를 어휘 자모순서로 빈도와 함께 배열한다. 여기서 체언만 골라내고, 어종명을 입력한 다음 어종별 빈도수를 출력하여 이를 토대로 각종 분석을 실시하였다.
연구 결과를 통하여 다음과 같은 결론을 얻었다.
1. 조사된 어휘는 다른말 총수 8,674개, 이은말 총수 65,889개로 평균 상대 빈도는 7.60으로 조사되었다.
2. 어종별 다른말 수 분포 조사에서 한자어의 비중이 고유어 보다 큰 비율로 조사되었으나, 이은말 수에서는 고유어가 더 많은 비율을 보였다. 고유어의 일부 고빈도 어휘에 사용빈도가 집중되어 한자어 보다 어휘부담량이 많았다.
3. 고빈도 어휘 중 고유어로 대체가 가능한 한자어(“언어, 점, 중, 인간” 8%)와 외래어(“플롯, 라일락, 엘리베이터, 노트, 포켓, 세트, 스토우브, 엔진” 16%)가 조사(한자어와 외래어의 고빈도 50위내 12%에 해당)되었다.
4. 각 교육과정기별 고유어와 한자어의 비율 조사에서 제 6차 교육과정기에 가까워지면서 고유어의 비율은 낮아지고 한자어의 구성 비율은 높아 졌으며, 외래어의 비율 변화는 거의 없었다.
연구 결과를 발전시켜 나가기 위하여 다음과 같이 제언한다.
1. 이 연구에서 얻어진 어종별 비율 변화 결과는 중학교 교과서를 편찬하는데 활용할 수 있을 것이다.
2. 고유어 어휘 확장과 활용 능력제고를 위해 대체가능 언어에 대한 연구가 이루어지고 그의 활용 방안이 실질적으로 뒤따라야 할 것이다.
3. 체계적인 연구를 위해 분석 대상선정과 방법에 있어 객관성, 타당성, 신뢰성을 확보할 수 있도록 보완되어야 할 것이다. 이를 위해 첫째, 체언 뿐 아니라 전체 문장 구성 성분을 대상으로 확대하고, 둘째, 자료 조사 대상의 선정에 신중을 기하고, 한글 전용 표기로 인한 연구(동음이의어) 방법이 분명해야 할 것이다. 셋째, 본 연구에 사용된 “글잡이1.0” 프로그램의 분리된 말뭉치 분석에 보강이 있어야한다.This study is to find the use frequency of vocabularies in Middle School textbook, sort them by word kind to know their change tendency.
This study investigated the entire 36 books of Middle School textbooks ranging from the first education period to the sixth one. The units of the language textbooks consist of speaking, hearing, reading, writing, language and literature, but this study is targeted at the reading units and the fiction units of literature where vocabularies can be most collected in the textbooks. Over 14 pages of each textbook were investigated and a total of pages of texts were analyzed therein.
Data were input by computer program and the data were treated statistically by computer. The program used in the analysis was 'Geuljabi 1.0', an example sampler published by the Nature Language Treatment Research Laboratory, Computer Department, Korea University, made on January 18, 1999. First, the texts of textbooks were input as text and then separated as word bundles by 'Geuljabi 1.0' and then the word bundles were aligned along with the use frequency according to the alphabetical ascending order. Then only substantives were sorted out, and the names of word kind were given. Then the use frequency of word by kind was produced. Various analyses were made on the basis of the production.
As a result, the conclusion was made as follows:
1. Among the investigated vocabularies, different words were a total of 8,674 and running words a total of 65,889. The average relative frequency was 7.60.
2. In the investigation of the distribution of different words by kind, the gravity of Chinese characters was greater than that of native words, native words were more than Chinese characters in running words. Since the use frequency was centered on some high frequency vocabularies, their vocabulary burden was greater.
3. Among the high frequency vocabularies, the Chinese characters("언어(language), 점(aspect), 중(middle), 인간(humankind)" were 8%) and foreign words("plot, lilac, elevator, note, pocket, set, stove, engine" were 16%) that can be replaced with native words were investigated(applicable to 12% within the 50th highest frequency of Chinese and foreign words and higher).
4. In the investigation of the native word and Chinese character rates by each education period, the native word rate was becoming lower as approaching the sixth education period but the composition rate of Chinese characters was becoming higher and there was almost no change in the foreign word rate.
So this study suggests the following in order to develop its outcome:
1. The rate change by word kind produced by this study can be used in writing Middle School textbooks.
2. Studies on the vocabularies which can be replaced to enhance the expansion and utilization ability of native words should be made and their utilization method should be used.
3. The selection and method of analysis objects should be secured in objectivity, adequacy and reliability for systematic study. First, the study should be extended to not only subjective but also the composition components of sentence to achieve this objective. Second, the selection of samples should be made seriously, and the study on problem arising from the exclusive use of Korean words including(homonyms) should be made. Third, the analysis of word bundles separated by "Geuljabi 1.0" which was used in this study should be improved.국문초록 = i I. 서론 = 1 1. 연구의 필요성 및 목적 = 1 2. 선행 연구 = 3 3. 연구 범위 및 방법 = 5

본 논문에서는 질의문 자동생성방식의 질의응답시스템의 설계 및 구축 기법에 대해 살펴본다. 질의응답시스템(Question Answering System)은 주어진 질의에 대해 응답을 구하는 시스템을 총칭하는 용어로 사용자의 질의 의도를 파악하여 질문과 매칭 되는 답변을 보여주는 시스템이다. 본 논문은 질의문을 자동으로 생성하는 방법과 자동 생성된 질의문에서 의미있는 질의문을 찾는 질의문 패턴을 제시한다.
첫 번째로, 질의문을 자동 생성하는 과정을 살펴본다. 질의문을 자동생성하기 위해서는 우선 평서문에서 질의가 가능한 고유명사를 찾아야 한다. 이러한 방법이 ‘고유명사인식기술(Named Entity Recognition)’이다. 고유명사인식은 인물의 이름을 나타내는 <PERSON>, 위치나 지위를 나타내는 <POSITION>, 지역을 나타내는 <LOCATION>, 시간을 나타내는 <TIME>, 기관을 나타내는 <ORGANIZATION>, 기타 고유명사를 나타내는 <MISCELLANY> 등을 식별하는 것이다.
그 다음으로는, 질의문 자동생성을 위해 두 가지 중요한 문제가 있다. 첫 번째 문제는 평서문에서 자동으로 질의문을 생성하는 것이고 두 번째 문제는 자동 생성된 질의문에서 ‘의미있는 질의문’을 선별하는 것이다.
본 논문에서 제시한 질의문 자동생성방식의 질의응답시스템은 기존의 질의응답시스템에 비해 사용자에게 질의응답시스템에 대한 사용의 편리함과 자연어질의문을 생성하는 번거로움을 최소화하는데 도움을 줄 수 있었다.Recently, question answering (QA) system has received much attention from the information retrieval, information extraction, machine learning, and natural language processing communities. QA system retrieves exact answers to questions whereas most information retrieval systems return full documents or best-matching passages for a given keyword.
In order to avoid a number of difficulties of developing QA systems, we propose a new style of question-answering system architecture that actively uses sentences within a document as a source of question/answer. Basically, our proposed QA system gives user a set of candidate query question for user information needs, and the candidate questions are automatically generated from significant sentences that are expected to contain meaningful facts or events. The QA system builds a complete database of (question, answer) pairs after analyzing a whole collection of documents. To construct the database, we need to perform the following steps: sentence split, named-entity recognition, question generation, question filtering, question/answer indexing. Here, the important things are question generation and question filtering.
To generate query question, we use the NER (named-entity recognition) technology. If an entity is identified to be ‘<PERSON>’, then we can generate a question that asks the entity <PERSON>.
All generated questions may not be meaningful one that can give an answer. The step of question filtering is to isolate significant sentences that have ‘meaningful question’ that users want. There are two rules for this. The first one is that if the subject phrase in a generated question has a pronoun as a part-of-speech, it is meaningless. The second one is that a question that does not contain enough definitive words can be meaningless. This rule can be used for passive voice sentences.제1장 서론 = 1 제1절 연구의 동기 = 1 제2절 연구의 내용 = 2 제3절 질의응답시스템의 중요성 = 2 제4절 현재 질의응답시스템의 한계 = 3

사이버 공격은 인터넷을 기반으로 발생하고 있기 때문에 적과 아군이 구별되지 않는 환경이며 특히, 우리나라의 경우 지정학적 이슈 때문에 북한의 사이버 공격과 사드 사태와 같은 정치적 이슈가 발생할 때 주변국의 해킹공격도 발생하고 있다. 사이버 공격을 실행하는 해킹집단은 특정한 기업, 국가를 대상으로 기밀데이터 탈취 및 서비스 중단을 목적으로 공격을 확대하고 있으며 최근의 해킹 공격기술은 APT 공격 및 랜섬웨어 기법을 활용하여 더욱 지능적으로 진화하고 있다.
하지만 기존 해킹공격 탐지모델은 네트워크에서 발생하는 단순한 이벤트를 분석하거나 악성코드의 공격행위가 실행되는 시점에 탐지하는 방법에 머무르고 있으며 공격이 발생한 뒤에도 공격자의 정보, 공격 방법 등이 해당 침해사고대응기관에 머무르고 있기 때문에 진화한 사이버 공격을 방어하는 데 한계가 있다.
해킹 공격을 방어하기 위해서 사이버 킬 체인에 근거하여 실시간의 시계열 스트림 데이터를 다양한 관점의 기술과 지식, 데이터베이스를 활용하여 탐지하고 차단하여야 한다. 더하여 미래에 발생할 수 있는 공격을 차단하기 위해서 사이버 침해대응기관 간의 지능적인 상호작용과 장기적이고 반복적인 공격 차단을 위해 많은 정보를 공유하여야 한다. 하지만 아직 침해사고대응기관 간의 사이버 위협정보 공유는 예산과 기관 내 역할의 불명확성 등 여러 문제 때문에 활성화되고 있지 않은 실정이다.
다중소스 기반의 지능형 사이버 위협 탐지 모델은 이러한 문제를 극복하기 위하여 CTA, OSINT 등 글로벌 위협 정보를 API, Crawler, XML Feed의 방법을 통해 데이터를 수집하고 화이트 해커·침해사고분석가를 통한 Humint 정보를 활용하여 사이버 위협 데이터를 수집한다. 웹 문서, 자연어검색 등을 통해 수집된 정보는 IP주소, URL, Hash, Behavior 로그, 위치 등의 정보를 중심으로 Indexing 되며 중복을 제거하고 수집된 데이터를 사이버 위협 분류체계를 기준으로 분류, 군집을 통해 지능형 사이버 위협 DB에 저장하여 위협정보와 분석결과, 공격탐지를 위한 패턴으로 실시간 프로파일링 한다.
사이버 위협 분류체계 따라 프로파일링 된 정보는 위협정보와 공격기법, 공개된 IoC에 따라 분류, 군집, 정제의 단계를 거쳐 자동적으로 재가공되며 사이버위협분석DB의 형태로 구성된다. 이 정보는 통합보안관제시스템을 통하여 자체 IoC정보, 추가 위협정보, 공개된 IoC를 연계하여 공격자정보에 기반을 둔 위협행위를 탐지 및 예측할 수 있다.
이는 과거의 사이버 공격 징후로부터 현재의 침해 현황, 미래의 사이버 공격의 확산에 대해 예측하고 사전에 대응할 수 있는 모델을 제시하며, 오픈 소스를 활용한 빅데이터 기반 통합보안관제시스템을 구성하고 실시간 시계열 분석을 가능하게 하는 CEP 기술을 활용하여 사이버 공격을 탐지할 수 있는 기술을 제안한다.Cyber attacks are based on the Internet, so it is an environment where enemies and friends are not distinguished. Especially, in the case of Korea, when there are political issues such as North Korea 's cyber attack and the THADD incident due to geopolitical issues, there is a hacking attack from neighboring countries.
The hacking group that conducts cyber attacks is expanding attacks aimed at the taking of confidential data and stopping service to specific companies and countries and the recent hacking attack technology is evolving more intelligently using APT attack and Ransomware technique.
However, the existing hacking attack detection model is a method to analyze simple events occurring in the network or to detect when attacking malicious code is executed and the attacker's information and attack methods stay in the corresponding incident response organization even after an attack occurs. So there is a limit to defending cyber attacks that have evolved.
In order to defend against hacking attacks, real-time series streaming data should be detected and blocked based on various points of view, knowledge, and database based on cyber kill chain.
In addition, in order to block attacks that may occur in the future, a lot of information should be shared for intelligent interactions among cyber-attacks responding organizations and long-term and repetitive attacks.
However, cyber threat information sharing between responding agencies is still not active due to various problems such as budget and ambiguity of roles in the organization.
In order to overcome these problems, intelligent cyber threat detection model based on multiple sources collects global threat information such as CTA and OSINT through API, Crawler, and XML feed method and cyber threat data using Humint information through white hacker and digital forensic analyzer.
The collected data stored in the intelligent cyber threat database is profiled in real time with threat information, analysis results, and patterns for attack detection through classification or clustering based on the cyber threat classification system.
The profiled information based on the cyber threat classification system is automatically reworked through classification, clustering, and refinement according to threat information, attack technique, and published IoC, and is configured in the form of a cyber threat analysis DB.
This information can be used to detect and anticipate threats based on attacker information by linking its IoC information, additional threat information, and the published IoC through the integrated security control system.
This proposes a model that predicts and responds to the present state of incident from the past cyber attack signs and the proliferation of future cyber attacks, and propose a technology that can detect cyber attacks by using Complex Event Processing technology which constructs a big data based integrated security control system using open source and real-time series analysis.국문초록 viii ABSTRACT x 제 1 장 서 론 1 1.1 연구 필요성 및 목적 1

